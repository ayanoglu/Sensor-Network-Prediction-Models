{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColdChainTransportation-Time Series Prediction For Sensor Networks - Prediction based on number of sensors ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQEACy8a4lXPRgzwJY50ek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayanoglu/Sensor-Network-Prediction-Models/blob/main-functionsgeneralized/ColdChainTransportation_Time_Series_Prediction_For_Sensor_Networks_Prediction_based_on_number_of_sensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a41nLlQT_f4E"
      },
      "source": [
        "**IMPORT LIBRARIES AND DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "TbxX5SxSsO05",
        "outputId": "33fe1c54-dee5-4c1d-cdbd-d3292fa045a9"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from numpy import array\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.set_printoptions(linewidth=160)\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from decimal import *\n",
        "from google.colab import files\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import itertools\n",
        "from itertools import combinations\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-68b8bc9b-44d6-416e-9c00-dbf6c8c10a50\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-68b8bc9b-44d6-416e-9c00-dbf6c8c10a50\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving S2.csv to S2 (1).csv\n",
            "Saving S3.csv to S3 (1).csv\n",
            "Saving S4.csv to S4 (1).csv\n",
            "Saving S5.csv to S5 (1).csv\n",
            "Saving S6.csv to S6 (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6i5vDVy_p0z"
      },
      "source": [
        "**READ IN THE CSV FILE, LOCATE COLUMNS AND SAVE AS FEATURES**\n",
        "\n",
        "**PLOT \"Front Top\", \"Mid Top\" AND \"Rear Top\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "id": "gN_c87WNPeLu",
        "outputId": "45ceeb55-d766-4b0f-a811-ddbf3d132d8c"
      },
      "source": [
        "import io\n",
        "S2 = pd.read_csv(io.BytesIO(uploaded['S2.csv']))\n",
        "S3 = pd.read_csv(io.BytesIO(uploaded['S3.csv']))\n",
        "S4 = pd.read_csv(io.BytesIO(uploaded['S4.csv']))\n",
        "S5 = pd.read_csv(io.BytesIO(uploaded['S5.csv']))\n",
        "S6 = pd.read_csv(io.BytesIO(uploaded['S6.csv']))\n",
        "\n",
        "def csvtocolumns(dataset):\n",
        "  FT = dataset.iloc[:,1]\n",
        "  FM = dataset.iloc[:,2]\n",
        "  FB = dataset.iloc[:,3]\n",
        "  MT = dataset.iloc[:,4]\n",
        "  MM = dataset.iloc[:,5]\n",
        "  MB = dataset.iloc[:,6]\n",
        "  RT = dataset.iloc[:,7]\n",
        "  RM = dataset.iloc[:,8]\n",
        "  RB = dataset.iloc[:,9]\n",
        "  return FT, FM, FB, MT, MM, MB, RT, RM, RB\n",
        "\n",
        "S2FT, S2FM, S2FB, S2MT, S2MM, S2MB, S2RT, S2RM, S2RB = csvtocolumns(S2)\n",
        "S3FT, S3FM, S3FB, S3MT, S3MM, S3MB, S3RT, S3RM, S3RB = csvtocolumns(S3)\n",
        "S4FT, S4FM, S4FB, S4MT, S4MM, S4MB, S4RT, S4RM, S4RB = csvtocolumns(S4)\n",
        "S5FT, S5FM, S5FB, S5MT, S5MM, S5MB, S5RT, S5RM, S5RB = csvtocolumns(S5)\n",
        "S6FT, S6FM, S6FB, S6MT, S6MM, S6MB, S6RT, S6RM, S6RB = csvtocolumns(S6)\n",
        "\n",
        "#print(f'Length of S2 {len(S2FT)} , {len(S2FM)} , {len(S2FB)} , {len(S2MT)} , {len(S2MM)} , {len(S2MB)} , {len(S2RT)} , {len(S2RM)} , {len(S2RB)}')\n",
        "#print(f'Length of S2 {len(S3FT)} , {len(S3FM)} , {len(S3FB)} , {len(S3MT)} , {len(S3MM)} , {len(S3MB)} , {len(S3RT)} , {len(S3RM)} , {len(S3RB)}')\n",
        "#print(f'Length of S2 {len(S4FT)} , {len(S4FM)} , {len(S4FB)} , {len(S4MT)} , {len(S4MM)} , {len(S4MB)} , {len(S4RT)} , {len(S4RM)} , {len(S4RB)}')\n",
        "#print(f'Length of S2 {len(S5FT)} , {len(S5FM)} , {len(S5FB)} , {len(S5MT)} , {len(S5MM)} , {len(S5MB)} , {len(S5RT)} , {len(S5RM)} , {len(S5RB)}')\n",
        "#print(f'Length of S2 {len(S6FT)} , {len(S6FM)} , {len(S6FB)} , {len(S6MT)} , {len(S6MM)} , {len(S6MB)} , {len(S6RT)} , {len(S6RM)} , {len(S6RB)}')\n",
        "\n",
        "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(20, 15))\n",
        "\n",
        "ax[0,0].plot(S2FT, \"b\", label='S2 Front Top')\n",
        "ax[0,0].plot(S2MT, \"r\", label='S2 Mid Top')\n",
        "ax[0,0].plot(S2RT, \"y\", label='S2 Rear Top')\n",
        "ax[0,0].legend()\n",
        "\n",
        "ax[0,1].plot(S3FT, \"b\", label='S3 Front Top')\n",
        "ax[0,1].plot(S3MT, \"r\", label='S3 Mid Top')\n",
        "ax[0,1].plot(S3RT, \"y\", label='S3 Rear Top')\n",
        "ax[0,1].legend()\n",
        "\n",
        "ax[1,0].plot(S4FT, \"b\", label='S4 Front Top')\n",
        "ax[1,0].plot(S4MT, \"r\", label='S4 Mid Top')\n",
        "ax[1,0].plot(S4RT, \"y\", label='S4 Rear Top')\n",
        "ax[1,0].legend()\n",
        "\n",
        "ax[1,1].plot(S5FT, \"b\", label='S5 Front Top')\n",
        "ax[1,1].plot(S5MT, \"r\", label='S5 Mid Top')\n",
        "ax[1,1].plot(S5RT, \"y\", label='S5 Rear Top')\n",
        "ax[1,1].legend()\n",
        "\n",
        "ax[2,0].plot(S6FT, \"b\", label='S6 Front Top')\n",
        "ax[2,0].plot(S6MT, \"r\", label='S6 Mid Top')\n",
        "ax[2,0].plot(S6RT, \"y\", label='S6 Rear Top')\n",
        "ax[2,0].legend()\n",
        "\n",
        "ax[2,1].plot(S3FT, \"b\", label='S3 Front Top')\n",
        "ax[2,1].plot(S4FT, \"r\", label='S4 Front Top')\n",
        "ax[2,1].plot(S5FT, \"y\", label='S5 Front Top')\n",
        "ax[2,1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1080 with 6 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El_x1gwpSrd0",
        "outputId": "8cc31e52-d06b-4f03-b223-6a84ce63d5dc"
      },
      "source": [
        "print(f'Any Nan values in S2? {(S2.isnull().values.any())}')\n",
        "print(f'Any Nan values in S3? {(S3.isnull().values.any())}')\n",
        "print(f'Any Nan values in S4? {(S4.isnull().values.any())}')\n",
        "print(f'Any Nan values in S5? {(S5.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6? {(S6.isnull().values.any())}')\n",
        "\n",
        "S6FT, S6FM, S6FB, S6MT, S6MM, S6MB, S6RT, S6RM, S6RB\n",
        "\n",
        "print(f'Any Nan values in S6FT? {(S6FT.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6FM? {(S6FM.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6FB? {(S6FB.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6MT? {(S6MT.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6MM? {(S6MM.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6MB? {(S6MB.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6RT? {(S6RT.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6RM? {(S6RM.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6RB? {(S6RB.isnull().values.any())}')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any Nan values in S2? False\n",
            "Any Nan values in S3? False\n",
            "Any Nan values in S4? False\n",
            "Any Nan values in S5? False\n",
            "Any Nan values in S6? True\n",
            "Any Nan values in S6FT? False\n",
            "Any Nan values in S6FM? False\n",
            "Any Nan values in S6FB? True\n",
            "Any Nan values in S6MT? False\n",
            "Any Nan values in S6MM? False\n",
            "Any Nan values in S6MB? False\n",
            "Any Nan values in S6RT? False\n",
            "Any Nan values in S6RM? False\n",
            "Any Nan values in S6RB? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrUyIUjCATVY"
      },
      "source": [
        "**DEFINE THE SEQUENCE CREATING FUNCTION, CONVERT DATASET DATA TO ARRAY AND CREATE SEQUENCE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nJ1eiQHpXnIC",
        "outputId": "51e6d320-d7f0-4b61-f183-f367b674078d"
      },
      "source": [
        "def create_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix-4] # so that we can get the 3rd data of 0-7 sequence \n",
        "\t\t# seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] \n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)\n",
        "\n",
        "S2FTA = pd.DataFrame.to_numpy(S2FT)\n",
        "S2MTA = pd.DataFrame.to_numpy(S2MT)\n",
        "S2RTA = pd.DataFrame.to_numpy(S2RT)\n",
        "S2FMA = pd.DataFrame.to_numpy(S2FM)\n",
        "S2MMA = pd.DataFrame.to_numpy(S2MM)\n",
        "S2RMA = pd.DataFrame.to_numpy(S2RM)\n",
        "S2FBA = pd.DataFrame.to_numpy(S2FB)\n",
        "S2MBA = pd.DataFrame.to_numpy(S2MB)\n",
        "S2RBA = pd.DataFrame.to_numpy(S2RB)\n",
        "\n",
        "S3FTA = pd.DataFrame.to_numpy(S3FT)\n",
        "S3MTA = pd.DataFrame.to_numpy(S3MT)\n",
        "S3RTA = pd.DataFrame.to_numpy(S3RT)\n",
        "S3FMA = pd.DataFrame.to_numpy(S3FM)\n",
        "S3MMA = pd.DataFrame.to_numpy(S3MM)\n",
        "S3RMA = pd.DataFrame.to_numpy(S3RM)\n",
        "S3FBA = pd.DataFrame.to_numpy(S3FB)\n",
        "S3MBA = pd.DataFrame.to_numpy(S3MB)\n",
        "S3RBA = pd.DataFrame.to_numpy(S3RB)\n",
        "\n",
        "S4FTA = pd.DataFrame.to_numpy(S4FT)\n",
        "S4MTA = pd.DataFrame.to_numpy(S4MT)\n",
        "S4RTA = pd.DataFrame.to_numpy(S4RT)\n",
        "S4FMA = pd.DataFrame.to_numpy(S4FM)\n",
        "S4MMA = pd.DataFrame.to_numpy(S4MM)\n",
        "S4RMA = pd.DataFrame.to_numpy(S4RM)\n",
        "S4FBA = pd.DataFrame.to_numpy(S4FB)\n",
        "S4MBA = pd.DataFrame.to_numpy(S4MB)\n",
        "S4RBA = pd.DataFrame.to_numpy(S4RB)\n",
        "\n",
        "S5FTA = pd.DataFrame.to_numpy(S5FT)\n",
        "S5MTA = pd.DataFrame.to_numpy(S5MT)\n",
        "S5RTA = pd.DataFrame.to_numpy(S5RT)\n",
        "S5FMA = pd.DataFrame.to_numpy(S5FM)\n",
        "S5MMA = pd.DataFrame.to_numpy(S5MM)\n",
        "S5RMA = pd.DataFrame.to_numpy(S5RM)\n",
        "S5FBA = pd.DataFrame.to_numpy(S5FB)\n",
        "S5MBA = pd.DataFrame.to_numpy(S5MB)\n",
        "S5RBA = pd.DataFrame.to_numpy(S5RB)\n",
        "\n",
        "S6FTA = pd.DataFrame.to_numpy(S6FT)\n",
        "S6MTA = pd.DataFrame.to_numpy(S6MT)\n",
        "S6RTA = pd.DataFrame.to_numpy(S6RT)\n",
        "S6FMA = pd.DataFrame.to_numpy(S6FM)\n",
        "S6MMA = pd.DataFrame.to_numpy(S6MM)\n",
        "S6RMA = pd.DataFrame.to_numpy(S6RM)\n",
        "S6FBA = pd.DataFrame.to_numpy(S6FB)\n",
        "S6MBA = pd.DataFrame.to_numpy(S6MB)\n",
        "S6RBA = pd.DataFrame.to_numpy(S6RB)\n",
        "\n",
        "n_steps = 7\n",
        "\n",
        "S2FTX, S2FTY = create_sequence(S2FTA, n_steps)\n",
        "S2MTX, S2MTY = create_sequence(S2MTA, n_steps)\n",
        "S2RTX, S2RTY = create_sequence(S2RTA, n_steps)\n",
        "S2FMX, S2FMY = create_sequence(S2FMA, n_steps)\n",
        "S2MMX, S2MMY = create_sequence(S2MMA, n_steps)\n",
        "S2RMX, S2RMY = create_sequence(S2RMA, n_steps)\n",
        "S2FBX, S2FBY = create_sequence(S2FBA, n_steps)\n",
        "S2MBX, S2MBY = create_sequence(S2MBA, n_steps)\n",
        "S2RBX, S2RBY = create_sequence(S2RBA, n_steps)\n",
        "\n",
        "S3FTX, S3FTY = create_sequence(S3FTA, n_steps)\n",
        "S3MTX, S3MTY = create_sequence(S3MTA, n_steps)\n",
        "S3RTX, S3RTY = create_sequence(S3RTA, n_steps)\n",
        "S3FMX, S3FMY = create_sequence(S3FMA, n_steps)\n",
        "S3MMX, S3MMY = create_sequence(S3MMA, n_steps)\n",
        "S3RMX, S3RMY = create_sequence(S3RMA, n_steps)\n",
        "S3FBX, S3FBY = create_sequence(S3FBA, n_steps)\n",
        "S3MBX, S3MBY = create_sequence(S3MBA, n_steps)\n",
        "S3RBX, S3RBY = create_sequence(S3RBA, n_steps)\n",
        "\n",
        "S4FTX, S4FTY = create_sequence(S4FTA, n_steps)\n",
        "S4MTX, S4MTY = create_sequence(S4MTA, n_steps)\n",
        "S4RTX, S4RTY = create_sequence(S4RTA, n_steps)\n",
        "S4FMX, S4FMY = create_sequence(S4FMA, n_steps)\n",
        "S4MMX, S4MMY = create_sequence(S4MMA, n_steps)\n",
        "S4RMX, S4RMY = create_sequence(S4RMA, n_steps)\n",
        "S4FBX, S4FBY = create_sequence(S4FBA, n_steps)\n",
        "S4MBX, S4MBY = create_sequence(S4MBA, n_steps)\n",
        "S4RBX, S4RBY = create_sequence(S4RBA, n_steps)\n",
        "\n",
        "S5FTX, S5FTY = create_sequence(S5FTA, n_steps)\n",
        "S5MTX, S5MTY = create_sequence(S5MTA, n_steps)\n",
        "S5RTX, S5RTY = create_sequence(S5RTA, n_steps)\n",
        "S5FMX, S5FMY = create_sequence(S5FMA, n_steps)\n",
        "S5MMX, S5MMY = create_sequence(S5MMA, n_steps)\n",
        "S5RMX, S5RMY = create_sequence(S5RMA, n_steps)\n",
        "S5FBX, S5FBY = create_sequence(S5FBA, n_steps)\n",
        "S5MBX, S5MBY = create_sequence(S5MBA, n_steps)\n",
        "S5RBX, S5RBY = create_sequence(S5RBA, n_steps)\n",
        "\n",
        "S6FTX, S6FTY = create_sequence(S6FTA, n_steps)\n",
        "S6MTX, S6MTY = create_sequence(S6MTA, n_steps)\n",
        "S6RTX, S6RTY = create_sequence(S6RTA, n_steps)\n",
        "S6FMX, S6FMY = create_sequence(S6FMA, n_steps)\n",
        "S6MMX, S6MMY = create_sequence(S6MMA, n_steps)\n",
        "S6RMX, S6RMY = create_sequence(S6RMA, n_steps)\n",
        "S6FBX, S6FBY = create_sequence(S6FBA, n_steps)\n",
        "S6MBX, S6MBY = create_sequence(S6MBA, n_steps)\n",
        "S6RBX, S6RBY = create_sequence(S6RBA, n_steps)\n",
        "\n",
        "len(S2FTX)\n",
        "len(S3FTX)\n",
        "\n",
        "X_FT = np.concatenate((S2FTX, S3FTX, S4FTX, S5FTX, S6FTX), axis=0)\n",
        "y_FT = np.concatenate((S2FTY, S3FTY, S4FTY, S5FTY, S6FTY), axis=0)\n",
        "X_FM = np.concatenate((S2FMX, S3FMX, S4FMX, S5FMX, S6FMX), axis=0)\n",
        "y_FM = np.concatenate((S2FMY, S3FMY, S4FMY, S5FMY, S6FMY), axis=0)\n",
        "X_FB = np.concatenate((S2FBX, S3FBX, S4FBX, S5FBX, S6FBX), axis=0)\n",
        "y_FB = np.concatenate((S2FBY, S3FBY, S4FBY, S5FBY, S6FBY), axis=0)\n",
        "\n",
        "X_MT = np.concatenate((S2MTX, S3MTX, S4MTX, S5MTX, S6MTX), axis=0)\n",
        "y_MT = np.concatenate((S2MTY, S3MTY, S4MTY, S5MTY, S6MTY), axis=0)\n",
        "X_MM = np.concatenate((S2MMX, S3MMX, S4MMX, S5MMX, S6MMX), axis=0)\n",
        "y_MM = np.concatenate((S2MMY, S3MMY, S4MMY, S5MMY, S6MMY), axis=0)\n",
        "X_MB = np.concatenate((S2MBX, S3MBX, S4MBX, S5MBX, S6MBX), axis=0)\n",
        "y_MB = np.concatenate((S2MBY, S3MBY, S4MBY, S5MBY, S6MBY), axis=0)\n",
        "\n",
        "X_RT = np.concatenate((S2RTX, S3RTX, S4RTX, S5RTX, S6RTX), axis=0)\n",
        "y_RT = np.concatenate((S2RTY, S3RTY, S4RTY, S5RTY, S6RTY), axis=0)\n",
        "X_RM = np.concatenate((S2RMX, S3RMX, S4RMX, S5RMX, S6RMX), axis=0)\n",
        "y_RM = np.concatenate((S2RMY, S3RMY, S4RMY, S5RMY, S6RMY), axis=0)\n",
        "X_RB = np.concatenate((S2RBX, S3RBX, S4RBX, S5RBX, S6RBX), axis=0)\n",
        "y_RB = np.concatenate((S2RBY, S3RBY, S4RBY, S5RBY, S6RBY), axis=0)\n",
        "\n",
        "len(X_FT)\n",
        "\n",
        "#plt.figure(figsize=(12, 10))\n",
        "#plt.plot(y_FT, \"b\", label=\"y_FT\")\n",
        "#plt.plot(y_MT, \"r\", label=\"y_MT\")\n",
        "#plt.plot(y_RT, \"y\", label=\"y_RT\")\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=y_FT, name=\"y_FT\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_MT, name=\"y_MT\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_RT, name=\"y_RT\", line_shape='linear'))\n",
        "fig.show()\n",
        "\n",
        "fig1 = go.Figure()\n",
        "fig1.add_trace(go.Scatter(y=y_FM, name=\"y_FM\", line_shape='linear'))\n",
        "fig1.add_trace(go.Scatter(y=y_MM, name=\"y_MM\", line_shape='linear'))\n",
        "fig1.add_trace(go.Scatter(y=y_RM, name=\"y_RM\", line_shape='linear'))\n",
        "fig1.show()\n",
        "\n",
        "fig2 = go.Figure()\n",
        "fig2.add_trace(go.Scatter(y=y_FB, name=\"y_FB\", line_shape='linear'))\n",
        "fig2.add_trace(go.Scatter(y=y_MB, name=\"y_MB\", line_shape='linear'))\n",
        "fig2.add_trace(go.Scatter(y=y_RB, name=\"y_RB\", line_shape='linear'))\n",
        "fig2.show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"8d655141-5ff3-4406-b194-619c6c36cdbc\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"8d655141-5ff3-4406-b194-619c6c36cdbc\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '8d655141-5ff3-4406-b194-619c6c36cdbc',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_FT\", \"type\": \"scatter\", \"y\": [34.2, 34.5, 34.5, 34.5, 34.5, 34.9, 34.9, 34.5, 34.2, 34.0, 33.8, 33.8, 33.8, 33.8, 34.0, 34.5, 34.3, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.3, 34.2, 34.2, 34.2, 34.0, 33.8, 33.3, 33.1, 34.0, 33.8, 33.4, 33.4, 34.0, 34.2, 35.4, 35.2, 35.2, 35.1, 35.2, 35.2, 35.4, 35.4, 35.6, 35.4, 35.6, 35.4, 35.4, 35.6, 35.8, 36.0, 36.0, 36.3, 36.1, 36.5, 34.5, 34.7, 34.9, 35.1, 35.2, 35.2, 35.8, 36.0, 36.0, 36.1, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.7, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 38.7, 37.9, 37.6, 37.6, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 38.1, 38.3, 38.3, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.6, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.9, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.4, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.9, 39.7, 39.7, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.3, 40.1, 40.3, 40.3, 40.3, 40.1, 40.5, 40.3, 40.1, 40.1, 40.1, 40.1, 39.9, 39.9, 39.9, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.4, 39.4, 39.4, 39.6, 39.4, 39.4, 39.2, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.7, 38.7, 38.7, 38.7, 38.5, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.9, 36.9, 37.8, 37.9, 37.9, 37.8, 37.6, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.4, 37.6, 37.8, 37.8, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.2, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.8, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.9, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 39.9, 39.9, 39.9, 39.9, 39.7, 39.6, 39.6, 39.4, 39.4, 39.2, 39.0, 38.8, 38.7, 38.5, 38.3, 38.3, 38.1, 38.1, 37.9, 37.9, 37.8, 37.6, 37.6, 37.6, 37.4, 37.2, 37.0, 37.0, 37.0, 39.4, 37.9, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.1, 33.6, 33.8, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.8, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 33.4, 33.4, 33.6, 33.6, 33.6, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 35.1, 35.2, 35.6, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 36.0, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_MT\", \"type\": \"scatter\", \"y\": [34.5, 34.7, 34.5, 34.5, 34.9, 34.9, 34.3, 34.0, 33.8, 33.6, 33.8, 33.6, 33.6, 33.8, 34.2, 34.3, 34.3, 34.5, 34.3, 34.5, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.0, 33.8, 33.6, 34.5, 34.2, 34.0, 34.2, 34.3, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 32.4, 32.5, 32.7, 32.9, 33.3, 33.6, 34.0, 34.3, 34.3, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.5, 36.7, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 39.0, 37.4, 36.9, 36.9, 36.9, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.5, 36.3, 36.1, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.6, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 32.7, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.6, 33.8, 34.0, 34.3, 34.7, 34.7, 34.5, 34.5, 34.3, 34.3, 34.2, 34.3, 34.2, 34.2, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.1, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.1, 33.4, 33.3, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.6, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 34.2, 33.8, 33.6, 33.4, 33.8, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 34.0, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.3, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.0, 34.2, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.3, 34.3, 34.7, 34.3, 34.2, 34.2, 34.5, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 32.9, 33.1, 33.4, 33.6, 34.0, 34.2, 34.2, 34.3, 34.5, 34.9, 34.9, 34.5, 34.5, 34.7, 34.9, 34.9, 34.9, 34.9, 35.1, 35.2, 35.4, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.6, 35.4, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.2, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.3, 34.3, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 33.3, 33.4, 33.4, 33.6, 33.8, 34.2, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.9, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.3, 40.3, 40.3, 40.3, 40.3, 40.3, 40.3, 40.3, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.8, 40.8, 40.8, 40.8, 40.8, 40.8, 40.8, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.2, 41.2, 41.2, 41.2, 41.2, 41.2, 41.2, 41.2, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 42.1, 41.9, 41.9, 41.9, 41.9, 41.9, 41.5, 41.2, 41.4, 41.4, 41.4, 41.4, 40.8]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RT\", \"type\": \"scatter\", \"y\": [34.3, 34.7, 34.7, 34.5, 34.5, 34.7, 34.9, 35.1, 34.5, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 34.0, 34.2, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.7, 34.5, 34.3, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 34.5, 34.7, 34.7, 34.9, 35.1, 35.1, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 35.8, 35.8, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 39.7, 39.2, 39.2, 39.0, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.8, 38.7, 38.8, 39.2, 40.5, 39.6, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 39.0, 38.8, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.6, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.5, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 37.9, 37.6, 37.4, 37.6, 37.6, 37.4, 37.4, 37.6, 37.6, 37.4, 36.7, 33.1, 33.3, 33.4, 33.4, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.4, 33.8, 34.2, 34.3, 34.5, 34.7, 34.7, 34.5, 34.5, 34.7, 34.5, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.9, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.7, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.7, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.1, 36.1, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 36.3, 36.7, 36.9, 37.2, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.0, 36.9, 36.9, 37.0, 37.0, 37.0, 37.2, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.6, 39.7, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.7, 39.6, 39.7, 39.6, 39.6, 39.6, 39.7, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 34.5, 34.7, 35.1, 35.2, 35.4, 35.4, 35.6, 35.6, 35.8, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 37.2, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.3, 36.7, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8d655141-5ff3-4406-b194-619c6c36cdbc');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"168b9dd0-2cf2-4d2d-a4c3-a46567ba9d78\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"168b9dd0-2cf2-4d2d-a4c3-a46567ba9d78\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '168b9dd0-2cf2-4d2d-a4c3-a46567ba9d78',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_FM\", \"type\": \"scatter\", \"y\": [34.3, 34.5, 34.7, 34.7, 34.7, 35.2, 35.2, 35.1, 34.7, 34.3, 34.3, 34.2, 34.2, 34.0, 34.3, 34.3, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 34.7, 34.3, 34.2, 34.0, 34.0, 34.0, 34.0, 33.4, 32.9, 32.9, 34.3, 33.6, 33.4, 33.4, 33.6, 33.8, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.9, 33.3, 33.4, 33.4, 33.3, 33.3, 33.1, 33.3, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.4, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.6, 35.8, 35.6, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.0, 37.2, 37.2, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.5, 38.5, 38.3, 38.3, 38.3, 38.5, 38.5, 38.3, 38.5, 38.7, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 37.9, 37.6, 37.2, 37.2, 37.2, 37.2, 37.4, 37.8, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 37.8, 37.2, 37.2, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.6, 37.2, 37.4, 37.8, 37.8, 37.2, 37.2, 37.0, 37.2, 37.0, 37.2, 37.4, 37.6, 37.8, 37.8, 37.8, 37.2, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 37.2, 37.6, 37.8, 37.8, 37.8, 37.8, 37.9, 37.8, 37.2, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.4, 36.9, 36.5, 36.3, 36.3, 36.1, 36.1, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 36.0, 36.1, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 36.1, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 36.0, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.3, 36.1, 36.0, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 32.0, 32.0, 32.0, 32.4, 32.0, 32.0, 32.0, 32.2, 32.2, 32.2, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.0, 36.0, 35.8, 35.8, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.7, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.2, 34.2, 34.3, 34.2, 34.2, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 32.2, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.9, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.6, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.4, 33.4, 33.3, 33.3, 33.4, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.8, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.8, 33.6, 33.6, 33.4, 33.6, 33.6, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 34.2, 34.0, 34.0, 33.8, 34.0, 33.8, 33.8, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.1, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 34.7, 34.9, 34.9, 35.1, 35.1, 35.2, 35.4, 35.4, 35.6, 35.6, 35.6, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.3, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.9, 36.7, 37.2, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.6, 38.1, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.4, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_MM\", \"type\": \"scatter\", \"y\": [34.5, 34.9, 34.9, 34.7, 34.7, 35.2, 35.2, 34.9, 34.5, 34.2, 34.2, 34.2, 34.2, 34.0, 34.2, 34.3, 34.5, 34.5, 34.5, 34.5, 34.7, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.3, 33.8, 33.6, 34.5, 34.3, 34.0, 34.0, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 30.9, 31.3, 31.3, 31.6, 31.6, 31.8, 32.0, 32.0, 32.2, 32.0, 32.0, 32.0, 32.0, 31.8, 31.8, 31.8, 31.8, 32.0, 32.0, 32.2, 32.0, 32.0, 32.2, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 37.9, 37.0, 36.9, 36.7, 36.7, 36.5, 36.7, 36.7, 36.9, 36.7, 36.5, 36.5, 36.5, 36.5, 37.2, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 36.9, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.0, 37.0, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.2, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.4, 35.2, 35.1, 35.1, 35.2, 35.2, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 31.6, 31.6, 31.8, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 33.1, 33.1, 32.9, 33.1, 32.9, 32.9, 33.1, 32.9, 33.1, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.3, 33.6, 33.4, 33.3, 33.3, 33.3, 33.1, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.6, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.6, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.8, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 34.2, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 32.7, 32.9, 32.7, 32.9, 33.1, 33.8, 34.0, 34.2, 34.3, 34.5, 34.3, 33.8, 34.3, 34.7, 34.7, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.4, 35.2, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 35.1, 35.1, 35.2, 35.1, 35.1, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.5, 34.7, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.9, 34.9, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.9, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.9, 34.7, 34.9, 34.7, 34.9, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 33.4, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.7, 38.3, 38.7, 38.5, 38.7, 38.7, 38.5]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RM\", \"type\": \"scatter\", \"y\": [35.1, 35.4, 35.6, 35.2, 35.6, 36.0, 36.0, 35.6, 35.2, 35.1, 35.2, 35.2, 35.1, 35.1, 35.4, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.6, 35.6, 35.6, 35.4, 35.1, 34.7, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 33.6, 33.6, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.4, 39.4, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.8, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.7, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.3, 38.3, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.8, 37.9, 38.3, 38.1, 38.1, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.7, 36.7, 36.7, 36.9, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.5, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.1, 36.1, 35.8, 35.6, 35.6, 35.6, 35.4, 35.2, 35.2, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 32.9, 32.7, 32.7, 32.7, 32.7, 32.7, 33.1, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 35.4, 34.7, 34.3, 34.3, 34.7, 34.5, 34.5, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.2, 34.2, 34.3, 34.3, 34.2, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.8, 35.8, 35.6, 35.4, 35.4, 35.4, 35.2, 35.2, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.2, 35.4, 35.2, 35.4, 35.2, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 35.1, 34.9, 34.7, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.2, 34.2, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 35.8, 36.0, 36.1, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.0, 35.8, 35.8, 36.1, 36.3, 36.5, 36.5, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.8, 37.4, 37.4, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 33.8, 33.8, 34.0, 34.0, 34.2, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 39.9, 38.3, 37.9, 37.9, 37.8, 37.6, 37.4, 37.4, 37.2, 36.9, 36.9, 36.9, 36.9]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('168b9dd0-2cf2-4d2d-a4c3-a46567ba9d78');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"5633a16b-91db-4167-bd9c-9c0e7ec127c7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"5633a16b-91db-4167-bd9c-9c0e7ec127c7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '5633a16b-91db-4167-bd9c-9c0e7ec127c7',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_FB\", \"type\": \"scatter\", \"y\": [34.3, 34.5, 34.5, 34.3, 34.5, 34.7, 34.9, 34.5, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.5, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.7, 34.7, 34.5, 34.5, 34.5, 34.3, 34.2, 33.8, 34.0, 34.5, 34.2, 34.0, 34.0, 34.0, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 33.6, 33.8, 33.8, 34.0, 34.0, 34.2, 34.3, 34.2, 34.3, 34.3, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 36.3, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.9, 37.8, 37.8, 37.6, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.3, 38.3, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.3, 38.1, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.3, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.7, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.5, 38.7, 38.5, 38.5, 38.7, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.5, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 31.6, 31.8, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 35.8, 35.6, 35.4, 35.2, 35.2, 35.2, 35.2, 35.1, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.3, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 35.1, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.9, 34.9, 35.2, 35.1, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 36.0, 36.0, 35.6, 35.6, 35.6, 35.8, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.6, 35.6, 35.6, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_MB\", \"type\": \"scatter\", \"y\": [34.3, 34.5, 34.7, 34.5, 34.5, 34.9, 35.1, 34.7, 34.3, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.5, 34.3, 34.0, 34.0, 34.0, 34.0, 33.6, 33.1, 32.9, 34.2, 33.8, 33.4, 33.4, 33.8, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 31.1, 31.1, 31.3, 31.5, 32.0, 32.2, 32.2, 32.2, 32.0, 31.8, 31.8, 32.0, 32.0, 32.0, 31.8, 31.8, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.5, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.6, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.1, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.2, 35.2, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.3, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.2, 34.2, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.8, 33.8, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.2, 34.3, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 32.4, 32.5, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.2, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 33.8, 33.8, 34.0, 33.8, 33.8, 34.0, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.7, 34.5, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 33.8, 34.0, 34.0, 34.0, 34.0, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.2, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.2, 35.2, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 32.4, 32.4, 32.5, 32.5, 32.5, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RB\", \"type\": \"scatter\", \"y\": [35.6, 36.0, 36.1, 35.8, 36.0, 36.3, 36.1, 35.6, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.4, 35.6, 35.6, 35.6, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 36.0, 36.0, 35.8, 35.6, 35.4, 35.4, 35.2, 35.2, 35.1, 34.7, 34.5, 34.9, 34.9, 34.7, 34.9, 34.9, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 33.4, 33.6, 33.8, 33.8, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 37.0, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.8, 37.4, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.8, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.8, 37.8, 37.8, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.8, 37.9, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.8, 37.8, 37.6, 37.6, 37.4, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.2, 37.4, 37.2, 37.4, 37.2, 37.2, 37.2, 37.0, 37.0, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 31.8, 31.8, 31.8, 31.8, 31.8, 32.0, 31.8, 32.0, 32.4, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 33.1, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.8, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 36.0, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.8, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 36.3, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.6, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 35.8, 35.6, 35.6, 35.8, 36.0, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.7, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 34.3, 34.5, 34.5, 34.7, 34.5, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.2, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.6, 35.6, 35.4, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.5, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5633a16b-91db-4167-bd9c-9c0e7ec127c7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrWvDeTvBuN1",
        "outputId": "ffc00ef5-fb5a-4df6-91fe-0c3cd98283c0"
      },
      "source": [
        "X_FT[10:20]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[34.5, 34.2, 34. , 33.8, 33.8, 33.8, 33.8],\n",
              "       [34.2, 34. , 33.8, 33.8, 33.8, 33.8, 34. ],\n",
              "       [34. , 33.8, 33.8, 33.8, 33.8, 34. , 34.5],\n",
              "       [33.8, 33.8, 33.8, 33.8, 34. , 34.5, 34.3],\n",
              "       [33.8, 33.8, 33.8, 34. , 34.5, 34.3, 34.5],\n",
              "       [33.8, 33.8, 34. , 34.5, 34.3, 34.5, 34.5],\n",
              "       [33.8, 34. , 34.5, 34.3, 34.5, 34.5, 34.5],\n",
              "       [34. , 34.5, 34.3, 34.5, 34.5, 34.5, 34.7],\n",
              "       [34.5, 34.3, 34.5, 34.5, 34.5, 34.7, 34.5],\n",
              "       [34.3, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUar_pv0B69U",
        "outputId": "5f6aa6a2-411c-46eb-dc5c-19f82a456325"
      },
      "source": [
        "y_FT[10:20]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([33.8, 33.8, 33.8, 33.8, 34. , 34.5, 34.3, 34.5, 34.5, 34.5])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XhBtO6ACRj"
      },
      "source": [
        "**Predict Rear Bottom Sensor Values by the values given for the front top sensor**\n",
        "\n",
        "**INPUT IS ASSIGNED TO MODEL INPUT VARIABLE**\n",
        "\n",
        "**ADD THE THIRD DIMENSION FOR NUMBER OF FEATURES TO THE TRAIN INPUT.**\n",
        "\n",
        "**THIS IS NECESSARY FOR CONV1D MODEL**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq51Cs1jQl-h"
      },
      "source": [
        "# define baseline model 1\n",
        "# create model\n",
        "def modelG(inp_shp):\n",
        "  global model\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(inp_shp, 1)))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dense(1))\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "  model.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "  #model1.summary()\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ev6TwMNRjN"
      },
      "source": [
        "def datagenerator(n_sensors, Y_in, X_in1=np.array([]), X_in2=np.array([]), X_in3=np.array([]), X_in4=np.array([]), X_in5=np.array([]), X_in6=np.array([]), X_in7=np.array([])):\n",
        "#def datagenerator(Y_in, X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, X_in7, n_sensors):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  InputsX = [X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, X_in7]\n",
        "  InputsX = InputsX[0:n_sensors]\n",
        "  X_in = np.concatenate([x for x in InputsX if x.size > 0], axis=1)\n",
        "\n",
        "  #X_in = np.concatenate((X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, X_in7), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "  \n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], 1))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], 1))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], 1))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n",
        "#X_train_Input.shape"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(X_in1 = X_FT, X_in2 = X_FM, X_in3, X_in4, X_in5, X_in6, X_in7, Y_in = y_MB)\n",
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(2, y_MB, X_FT, X_FM, np.array([]), np.array([]), np.array([]), np.array([]), np.array([]))"
      ],
      "metadata": {
        "id": "QS3uoY5Mr9d_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_Input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeNY9TMS0-7o",
        "outputId": "a4fe73c8-ffb9-46fd-869f-7d9efb802df0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1128, 14, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nItmhieXwgdb"
      },
      "source": [
        "def evaldata(n_sensors, \n",
        "             traindata1 = None, traindata2 = None, traindata3 = None, traindata4 = None, \n",
        "             traindata5 = None, traindata6 = None, traindata7 = None, testdata = None, \n",
        "             X_in1=np.array([]), X_in2=np.array([]), X_in3=np.array([]), X_in4=np.array([]), \n",
        "             X_in5=np.array([]), X_in6=np.array([]), X_in7=np.array([]), Y_in=np.array([])):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(n_sensors, Y_in, X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, X_in7)\n",
        "  \n",
        "  history = model.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  #print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  #print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  #print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q41wARCq_Iac",
        "outputId": "05394e10-8898-4b83-efe4-421fd072aad9"
      },
      "source": [
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "inp_shp = 7\n",
        "n_sensors = 1\n",
        "\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "modelG(inp_shp)\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "my_dictMF1 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), n_sensors):\n",
        "  kX1 = combo[0][0]\n",
        "  vX1 = combo[0][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF1 = evaldata(n_sensors, X_in1=vX1, Y_in=vy, traindata1 = kX1, testdata = ky)\n",
        "      TestLossTotal = resultsMF1[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF1[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF1[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF1[\"DATA_X\"].append(kX1)\n",
        "    my_dictMF1[\"DATA_y\"].append(ky)\n",
        "    my_dictMF1[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model.load_weights('model.h5')\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4654292225837708, and Avg Validation Loss (mae) is 1.5014062166213988\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5254292488098145\n",
            "kx1 = X_FT, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8638123989105224, and Avg Validation Loss (mae) is 1.8799743056297302\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8553394198417663\n",
            "kx1 = X_FT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7391000866889954, and Avg Validation Loss (mae) is 1.7770164608955383\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7693098545074464\n",
            "kx1 = X_FT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.455135464668274, and Avg Validation Loss (mae) is 1.4614208877086639\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4593962609767914\n",
            "kx1 = X_FT, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8759569644927978, and Avg Validation Loss (mae) is 1.907732617855072\n",
            "After 10 runs; Avg Test Loss (mae) is 1.919050943851471\n",
            "kx1 = X_FT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8507601618766785, and Avg Validation Loss (mae) is 1.878681230545044\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8485089421272278\n",
            "kx1 = X_FT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3429813861846924, and Avg Validation Loss (mae) is 1.464010441303253\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4811838865280151\n",
            "kx1 = X_FM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5652260541915894, and Avg Validation Loss (mae) is 1.6202280879020692\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6071138262748719\n",
            "kx1 = X_FM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4316457152366637, and Avg Validation Loss (mae) is 1.5183734059333802\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5398174047470092\n",
            "kx1 = X_FM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9733953058719635, and Avg Validation Loss (mae) is 0.9430477142333984\n",
            "After 10 runs; Avg Test Loss (mae) is 0.938442200422287\n",
            "kx1 = X_FM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0815377831459045, and Avg Validation Loss (mae) is 1.0283206880092621\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0288945853710174\n",
            "kx1 = X_FM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4625951409339906, and Avg Validation Loss (mae) is 1.6058009624481202\n",
            "After 10 runs; Avg Test Loss (mae) is 1.586221408843994\n",
            "kx1 = X_FM, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2601155757904052, and Avg Validation Loss (mae) is 1.3884878873825073\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4220867037773133\n",
            "kx1 = X_FM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9519861340522766, and Avg Validation Loss (mae) is 0.9814356446266175\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9768223285675048\n",
            "kx1 = X_MT, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 2.0112205386161803, and Avg Validation Loss (mae) is 1.9286878943443297\n",
            "After 10 runs; Avg Test Loss (mae) is 1.98680819272995\n",
            "kx1 = X_MT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.321577286720276, and Avg Validation Loss (mae) is 1.5053871750831604\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5216258764266968\n",
            "kx1 = X_MT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9569439709186554, and Avg Validation Loss (mae) is 1.0572345852851868\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0546365201473236\n",
            "kx1 = X_MT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.951535028219223, and Avg Validation Loss (mae) is 1.0663439452648162\n",
            "After 10 runs; Avg Test Loss (mae) is 1.080024129152298\n",
            "kx1 = X_MT, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 2.0768317103385927, and Avg Validation Loss (mae) is 2.0463125824928285\n",
            "After 10 runs; Avg Test Loss (mae) is 2.053180956840515\n",
            "kx1 = X_MT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8180221438407898, and Avg Validation Loss (mae) is 1.7206975817680359\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7372250437736512\n",
            "kx1 = X_MT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3204658508300782, and Avg Validation Loss (mae) is 1.6190134644508363\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6503914952278138\n",
            "kx1 = X_MM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9380355715751647, and Avg Validation Loss (mae) is 1.8983852505683898\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9077658176422119\n",
            "kx1 = X_MM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9688740074634552, and Avg Validation Loss (mae) is 1.0406185805797576\n",
            "After 10 runs; Avg Test Loss (mae) is 1.053126186132431\n",
            "kx1 = X_MM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.126268208026886, and Avg Validation Loss (mae) is 1.1396605789661407\n",
            "After 10 runs; Avg Test Loss (mae) is 1.166318303346634\n",
            "kx1 = X_MM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6669355809688569, and Avg Validation Loss (mae) is 0.7731123358011246\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7632001996040344\n",
            "kx1 = X_MM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5126209497451781, and Avg Validation Loss (mae) is 1.6195011615753174\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6186415672302246\n",
            "kx1 = X_MM, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.072414720058441, and Avg Validation Loss (mae) is 1.1591515243053436\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1504254400730134\n",
            "kx1 = X_MM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9846153080463409, and Avg Validation Loss (mae) is 1.0959429025650025\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0978423714637757\n",
            "kx1 = X_MB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9731003046035767, and Avg Validation Loss (mae) is 1.9320765495300294\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9507116556167603\n",
            "kx1 = X_MB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2419153094291686, and Avg Validation Loss (mae) is 1.3346289992332458\n",
            "After 10 runs; Avg Test Loss (mae) is 1.337148332595825\n",
            "kx1 = X_MB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3742963433265687, and Avg Validation Loss (mae) is 1.4936292052268982\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4743151903152465\n",
            "kx1 = X_MB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7811119318008423, and Avg Validation Loss (mae) is 0.9541562020778656\n",
            "After 10 runs; Avg Test Loss (mae) is 0.957008320093155\n",
            "kx1 = X_MB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7174990773200989, and Avg Validation Loss (mae) is 1.7212275385856628\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6862138986587525\n",
            "kx1 = X_MB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3951908588409423, and Avg Validation Loss (mae) is 1.4929266929626466\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4823029279708861\n",
            "kx1 = X_MB, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1769146025180817, and Avg Validation Loss (mae) is 1.2163118124008179\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2280197501182557\n",
            "kx1 = X_RT, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8875276446342468, and Avg Validation Loss (mae) is 2.0500732421875\n",
            "After 10 runs; Avg Test Loss (mae) is 2.0675613284111023\n",
            "kx1 = X_RT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4877492547035218, and Avg Validation Loss (mae) is 1.5216647028923034\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5079763174057006\n",
            "kx1 = X_RT, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.916389524936676, and Avg Validation Loss (mae) is 1.9719677686691284\n",
            "After 10 runs; Avg Test Loss (mae) is 2.013069248199463\n",
            "kx1 = X_RT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4014101505279541, and Avg Validation Loss (mae) is 1.3895368456840516\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4111181855201722\n",
            "kx1 = X_RT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1658308506011963, and Avg Validation Loss (mae) is 1.1871950805187226\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1569710552692414\n",
            "kx1 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.811713707447052, and Avg Validation Loss (mae) is 0.9124020278453827\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9100717782974244\n",
            "kx1 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0192579329013824, and Avg Validation Loss (mae) is 0.8941989302635193\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8743851602077484\n",
            "kx1 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9243536353111268, and Avg Validation Loss (mae) is 2.0054075360298156\n",
            "After 10 runs; Avg Test Loss (mae) is 1.979977059364319\n",
            "kx1 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1970964550971985, and Avg Validation Loss (mae) is 1.2153450846672058\n",
            "After 10 runs; Avg Test Loss (mae) is 1.213983815908432\n",
            "kx1 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7729454636573792, and Avg Validation Loss (mae) is 1.758814311027527\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7828534126281739\n",
            "kx1 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9678548514842987, and Avg Validation Loss (mae) is 1.2022012591362\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2028705418109893\n",
            "kx1 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.057261836528778, and Avg Validation Loss (mae) is 1.153127920627594\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1285695433616638\n",
            "kx1 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8167401790618897, and Avg Validation Loss (mae) is 0.850511908531189\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8347982943058014\n",
            "kx1 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7652792811393738, and Avg Validation Loss (mae) is 0.7225051522254944\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7158323347568512\n",
            "kx1 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5896760106086731, and Avg Validation Loss (mae) is 1.55318945646286\n",
            "After 10 runs; Avg Test Loss (mae) is 1.58856143951416\n",
            "kx1 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.023063164949417, and Avg Validation Loss (mae) is 1.0041726589202882\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0081906199455262\n",
            "kx1 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4238324165344238, and Avg Validation Loss (mae) is 1.4076554417610168\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3928495049476624\n",
            "kx1 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.004372137784958, and Avg Validation Loss (mae) is 0.9778925955295563\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9914542436599731\n",
            "kx1 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0421795487403869, and Avg Validation Loss (mae) is 1.2451575160026551\n",
            "After 10 runs; Avg Test Loss (mae) is 1.251087087392807\n",
            "kx1 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1290021240711212, and Avg Validation Loss (mae) is 0.9926838457584382\n",
            "After 10 runs; Avg Test Loss (mae) is 0.977823156118393\n",
            "kx1 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8785109519958496, and Avg Validation Loss (mae) is 1.0321659088134765\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0242796123027802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "kgncoI3oyMKg",
        "outputId": "5f166e05-707a-48f8-ed6a-359eb48c4835"
      },
      "source": [
        "CombResults1 = pd.DataFrame.from_dict(my_dictMF1)\n",
        "CombResultsSorted1 = CombResults1.sort_values(by=['Test Loss'])\n",
        "CombResultsSorted1.to_csv('CombResultsSorted1.csv')\n",
        "CombResultsSorted1\n",
        "files.download(\"CombResultsSorted1.csv\")\n",
        "fig = px.box(CombResultsSorted1, x=\"DATA_X\", y=\"Test Loss\", hover_data=[\"DATA_y\"])\n",
        "fig.show()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_95aab5a5-a80b-4c03-a0cc-8b2fe26b8158\", \"CombResultsSorted1.csv\", 1789)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"2bf93f71-a9a2-41f7-a88e-6ddf9bac3379\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"2bf93f71-a9a2-41f7-a88e-6ddf9bac3379\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '2bf93f71-a9a2-41f7-a88e-6ddf9bac3379',\n",
              "                        [{\"alignmentgroup\": \"True\", \"customdata\": [[\"y_RB\"], [\"y_MB\"], [\"y_RT\"], [\"y_RB\"], [\"y_RM\"], [\"y_MM\"], [\"y_MM\"], [\"y_RB\"], [\"y_RT\"], [\"y_MM\"], [\"y_FM\"], [\"y_RM\"], [\"y_MB\"], [\"y_FM\"], [\"y_MM\"], [\"y_MB\"], [\"y_RB\"], [\"y_MB\"], [\"y_RM\"], [\"y_MB\"], [\"y_MT\"], [\"y_MM\"], [\"y_FM\"], [\"y_RB\"], [\"y_MB\"], [\"y_FM\"], [\"y_MT\"], [\"y_MM\"], [\"y_RM\"], [\"y_MB\"], [\"y_MT\"], [\"y_RB\"], [\"y_RM\"], [\"y_FM\"], [\"y_FM\"], [\"y_FM\"], [\"y_MT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FT\"], [\"y_RT\"], [\"y_RB\"], [\"y_RT\"], [\"y_RM\"], [\"y_MM\"], [\"y_MT\"], [\"y_RM\"], [\"y_MT\"], [\"y_FT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_MT\"], [\"y_RT\"], [\"y_FT\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}<br>DATA_y=%{customdata[0]}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_RM\", \"X_MM\", \"X_RM\", \"X_RT\", \"X_RT\", \"X_FM\", \"X_MB\", \"X_FM\", \"X_RB\", \"X_RB\", \"X_RB\", \"X_RB\", \"X_FM\", \"X_MM\", \"X_MT\", \"X_MT\", \"X_MM\", \"X_RM\", \"X_MM\", \"X_RT\", \"X_MM\", \"X_RM\", \"X_RM\", \"X_MB\", \"X_RB\", \"X_MB\", \"X_RB\", \"X_RT\", \"X_FM\", \"X_FT\", \"X_MB\", \"X_FT\", \"X_MB\", \"X_RT\", \"X_MT\", \"X_FT\", \"X_FM\", \"X_FM\", \"X_RB\", \"X_FM\", \"X_MM\", \"X_MT\", \"X_MB\", \"X_MT\", \"X_FT\", \"X_RM\", \"X_FT\", \"X_FT\", \"X_MM\", \"X_FT\", \"X_MB\", \"X_RM\", \"X_MT\", \"X_RT\", \"X_MT\", \"X_RT\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.7158323347568512, 0.7632001996040344, 0.8347982943058014, 0.8743851602077484, 0.9100717782974244, 0.938442200422287, 0.957008320093155, 0.9768223285675048, 0.977823156118393, 0.9914542436599731, 1.0081906199455262, 1.0242796123027802, 1.0288945853710174, 1.053126186132431, 1.0546365201473236, 1.080024129152298, 1.0978423714637757, 1.1285695433616638, 1.1504254400730134, 1.1569710552692414, 1.166318303346634, 1.2028705418109893, 1.213983815908432, 1.2280197501182557, 1.251087087392807, 1.337148332595825, 1.3928495049476624, 1.4111181855201722, 1.4220867037773133, 1.4593962609767914, 1.4743151903152465, 1.4811838865280151, 1.4823029279708861, 1.5079763174057006, 1.5216258764266968, 1.5254292488098145, 1.5398174047470092, 1.586221408843994, 1.58856143951416, 1.6071138262748719, 1.6186415672302246, 1.6503914952278138, 1.6862138986587525, 1.7372250437736512, 1.7693098545074464, 1.7828534126281739, 1.8485089421272278, 1.8553394198417663, 1.9077658176422119, 1.919050943851471, 1.9507116556167603, 1.979977059364319, 1.98680819272995, 2.013069248199463, 2.053180956840515, 2.0675613284111023], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2bf93f71-a9a2-41f7-a88e-6ddf9bac3379');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Rny1g3a9_bTz",
        "outputId": "4f3c3d2f-66dd-4375-8396-8bf284c7fd69"
      },
      "source": [
        "CombResultsSortedgrouped1 = CombResultsSorted1.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedgroupedsortedMF1 = CombResultsSortedgrouped1.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedgroupedsortedMF1.to_csv('CombResultsSortedgroupedsortedMF1.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedgroupedsortedMF1.csv\")\n",
        "CombResultsSortedgroupedsortedMF1"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f7132bbe-01b0-4a01-be82-8955f973c97b\", \"CombResultsSortedgroupedsortedMF1.csv\", 208)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_RB</th>\n",
              "      <td>1.176321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MM</th>\n",
              "      <td>1.251046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RM</th>\n",
              "      <td>1.265555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FM</th>\n",
              "      <td>1.299914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RT</th>\n",
              "      <td>1.420165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MB</th>\n",
              "      <td>1.445103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MT</th>\n",
              "      <td>1.583413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FT</th>\n",
              "      <td>1.694031</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Test Loss\n",
              "DATA_X           \n",
              "X_RB     1.176321\n",
              "X_MM     1.251046\n",
              "X_RM     1.265555\n",
              "X_FM     1.299914\n",
              "X_RT     1.420165\n",
              "X_MB     1.445103\n",
              "X_MT     1.583413\n",
              "X_FT     1.694031"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JyL559ClGHM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ2Eaz3aGH4Y",
        "outputId": "6fde49ca-a1cb-44c9-a234-2c9e67018672"
      },
      "source": [
        "#Combinations of 2 Sensors, therefore 14 features, 14 inputs\n",
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "inp_shp = 14\n",
        "n_sensors = 2\n",
        "\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "modelG(inp_shp)\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "my_dictMF2 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), n_sensors):\n",
        "  kX1, kX2 = combo[0][0], combo[1][0]\n",
        "  vX1, vX2 = combo[0][1], combo[1][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF2 = evaldata(n_sensors, X_in1=vX1, X_in2=vX2, Y_in=vy, traindata1 = kX1, traindata2 = kX2, testdata = ky)\n",
        "      TestLossTotal = resultsMF2[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF2[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF2[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}, {kX2} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF2[\"DATA_X\"].append(kX1 + kX2)\n",
        "    my_dictMF2[\"DATA_y\"].append(ky)\n",
        "    my_dictMF2[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model.load_weights('model.h5')\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3490669012069703, and Avg Validation Loss (mae) is 1.460232388973236\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4638543128967285\n",
            "kx1 = X_FT, kx2 = X_FM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9367406129837036, and Avg Validation Loss (mae) is 0.9588613748550415\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9495458245277405\n",
            "kx1 = X_FT, kx2 = X_FM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1274786412715911, and Avg Validation Loss (mae) is 1.0287945806980132\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0415800750255584\n",
            "kx1 = X_FT, kx2 = X_FM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5103663206100464, and Avg Validation Loss (mae) is 1.5253069639205932\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5036638140678407\n",
            "kx1 = X_FT, kx2 = X_FM, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2321333408355712, and Avg Validation Loss (mae) is 1.2594169437885285\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2497532308101653\n",
            "kx1 = X_FT, kx2 = X_FM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8899413406848907, and Avg Validation Loss (mae) is 1.0159980177879333\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0309414625167848\n",
            "kx1 = X_FT, kx2 = X_MT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0288538336753845, and Avg Validation Loss (mae) is 1.2074356019496917\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1841034471988678\n",
            "kx1 = X_FT, kx2 = X_MT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9416761994361877, and Avg Validation Loss (mae) is 0.9167488217353821\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9261094093322754\n",
            "kx1 = X_FT, kx2 = X_MT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8828364312648773, and Avg Validation Loss (mae) is 0.8969966888427734\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8857374489307404\n",
            "kx1 = X_FT, kx2 = X_MT, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7082732319831848, and Avg Validation Loss (mae) is 1.6860078930854798\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6979487538337708\n",
            "kx1 = X_FT, kx2 = X_MT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5075027346611023, and Avg Validation Loss (mae) is 1.5956928133964539\n",
            "After 10 runs; Avg Test Loss (mae) is 1.596909499168396\n",
            "kx1 = X_FT, kx2 = X_MT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.008862417936325, and Avg Validation Loss (mae) is 1.0367943108081819\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0680660128593444\n",
            "kx1 = X_FT, kx2 = X_MM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7924424529075622, and Avg Validation Loss (mae) is 0.7408114671707153\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7505771338939666\n",
            "kx1 = X_FT, kx2 = X_MM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0819683969020844, and Avg Validation Loss (mae) is 1.065086555480957\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0617017030715943\n",
            "kx1 = X_FT, kx2 = X_MM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7458255589008331, and Avg Validation Loss (mae) is 0.693934577703476\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6916130065917969\n",
            "kx1 = X_FT, kx2 = X_MM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4109338641166687, and Avg Validation Loss (mae) is 1.3706212401390077\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3949745893478394\n",
            "kx1 = X_FT, kx2 = X_MM, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.077519989013672, and Avg Validation Loss (mae) is 1.2076409578323364\n",
            "After 10 runs; Avg Test Loss (mae) is 1.208127099275589\n",
            "kx1 = X_FT, kx2 = X_MM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8466943323612213, and Avg Validation Loss (mae) is 0.7560414373874664\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7606109380722046\n",
            "kx1 = X_FT, kx2 = X_MB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.079216331243515, and Avg Validation Loss (mae) is 1.1484680354595185\n",
            "After 10 runs; Avg Test Loss (mae) is 1.155889880657196\n",
            "kx1 = X_FT, kx2 = X_MB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.326447081565857, and Avg Validation Loss (mae) is 1.2699223637580872\n",
            "After 10 runs; Avg Test Loss (mae) is 1.262123966217041\n",
            "kx1 = X_FT, kx2 = X_MB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7792054116725922, and Avg Validation Loss (mae) is 0.7599773287773133\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7514180958271026\n",
            "kx1 = X_FT, kx2 = X_MB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5983986854553223, and Avg Validation Loss (mae) is 1.6266955614089966\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5869382977485658\n",
            "kx1 = X_FT, kx2 = X_MB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2895371437072753, and Avg Validation Loss (mae) is 1.307959508895874\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2709764003753663\n",
            "kx1 = X_FT, kx2 = X_MB, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9628861963748931, and Avg Validation Loss (mae) is 1.0701904833316802\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0888793289661407\n",
            "kx1 = X_FT, kx2 = X_RT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2123350739479064, and Avg Validation Loss (mae) is 1.4124226093292236\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4306795120239257\n",
            "kx1 = X_FT, kx2 = X_RT, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6367913842201234, and Avg Validation Loss (mae) is 1.737607765197754\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7415463328361511\n",
            "kx1 = X_FT, kx2 = X_RT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2772440314292908, and Avg Validation Loss (mae) is 1.3014854848384858\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3269877791404725\n",
            "kx1 = X_FT, kx2 = X_RT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1602217078208923, and Avg Validation Loss (mae) is 1.096572607755661\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1432534217834474\n",
            "kx1 = X_FT, kx2 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8388136565685272, and Avg Validation Loss (mae) is 0.7300625741481781\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7413856863975525\n",
            "kx1 = X_FT, kx2 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8200084328651428, and Avg Validation Loss (mae) is 0.8226956367492676\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7854244172573089\n",
            "kx1 = X_FT, kx2 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9876452386379242, and Avg Validation Loss (mae) is 1.0928780674934386\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0760237753391266\n",
            "kx1 = X_FT, kx2 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5903987526893615, and Avg Validation Loss (mae) is 1.6041593670845031\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5703253388404845\n",
            "kx1 = X_FT, kx2 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9596052169799805, and Avg Validation Loss (mae) is 0.9589009821414948\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9695333242416382\n",
            "kx1 = X_FT, kx2 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9934816181659698, and Avg Validation Loss (mae) is 0.9384778320789338\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9316480875015258\n",
            "kx1 = X_FT, kx2 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8023316144943238, and Avg Validation Loss (mae) is 0.9059330880641937\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8975850284099579\n",
            "kx1 = X_FT, kx2 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6135632157325744, and Avg Validation Loss (mae) is 0.5968645334243774\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6081672221422195\n",
            "kx1 = X_FT, kx2 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9291576027870179, and Avg Validation Loss (mae) is 0.9906815528869629\n",
            "After 10 runs; Avg Test Loss (mae) is 0.992942875623703\n",
            "kx1 = X_FT, kx2 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4120509266853332, and Avg Validation Loss (mae) is 1.4376025319099426\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4898621678352355\n",
            "kx1 = X_FT, kx2 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9871420443058014, and Avg Validation Loss (mae) is 0.8494659185409545\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8502495884895325\n",
            "kx1 = X_FT, kx2 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9946097671985626, and Avg Validation Loss (mae) is 1.0253696680068969\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0342887878417968\n",
            "kx1 = X_FT, kx2 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0248778522014619, and Avg Validation Loss (mae) is 1.091808009147644\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0907401978969573\n",
            "kx1 = X_FT, kx2 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8444128453731536, and Avg Validation Loss (mae) is 0.8319880247116089\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8416688203811645\n",
            "kx1 = X_FM, kx2 = X_MT, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5159115195274353, and Avg Validation Loss (mae) is 1.5850523710250854\n",
            "After 10 runs; Avg Test Loss (mae) is 1.562978494167328\n",
            "kx1 = X_FM, kx2 = X_MT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7929105222225189, and Avg Validation Loss (mae) is 0.759044885635376\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7551390409469605\n",
            "kx1 = X_FM, kx2 = X_MT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9267377972602844, and Avg Validation Loss (mae) is 0.9158167004585266\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9375527143478394\n",
            "kx1 = X_FM, kx2 = X_MT, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5133597135543824, and Avg Validation Loss (mae) is 1.5047980785369872\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5396087646484375\n",
            "kx1 = X_FM, kx2 = X_MT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2892752885818481, and Avg Validation Loss (mae) is 1.3410584330558777\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3459650039672852\n",
            "kx1 = X_FM, kx2 = X_MT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8819055736064911, and Avg Validation Loss (mae) is 0.8982507646083832\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9150851190090179\n",
            "kx1 = X_FM, kx2 = X_MM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5739495992660522, and Avg Validation Loss (mae) is 1.5704512238502502\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5269010186195373\n",
            "kx1 = X_FM, kx2 = X_MM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1405929446220398, and Avg Validation Loss (mae) is 1.2323962867259979\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2483367443084716\n",
            "kx1 = X_FM, kx2 = X_MM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6560103714466095, and Avg Validation Loss (mae) is 0.6207788795232773\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6283347010612488\n",
            "kx1 = X_FM, kx2 = X_MM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4405597567558288, and Avg Validation Loss (mae) is 1.508657741546631\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5309898257255554\n",
            "kx1 = X_FM, kx2 = X_MM, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1049144625663758, and Avg Validation Loss (mae) is 1.1904590487480164\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2035548210144043\n",
            "kx1 = X_FM, kx2 = X_MM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7859283685684204, and Avg Validation Loss (mae) is 0.9020899415016175\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9003340423107147\n",
            "kx1 = X_FM, kx2 = X_MB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5782968997955322, and Avg Validation Loss (mae) is 1.6054789900779725\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6391132593154907\n",
            "kx1 = X_FM, kx2 = X_MB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1998469710350037, and Avg Validation Loss (mae) is 1.1776407957077026\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1835631012916565\n",
            "kx1 = X_FM, kx2 = X_MB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6107042074203491, and Avg Validation Loss (mae) is 0.5918240398168564\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6061331063508988\n",
            "kx1 = X_FM, kx2 = X_MB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4733065366744995, and Avg Validation Loss (mae) is 1.5190223097801208\n",
            "After 10 runs; Avg Test Loss (mae) is 1.495566439628601\n",
            "kx1 = X_FM, kx2 = X_MB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.236537802219391, and Avg Validation Loss (mae) is 1.1128443837165833\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1333646059036255\n",
            "kx1 = X_FM, kx2 = X_MB, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.917580109834671, and Avg Validation Loss (mae) is 1.0944308042526245\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1174793720245362\n",
            "kx1 = X_FM, kx2 = X_RT, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5215424656867982, and Avg Validation Loss (mae) is 1.5937564492225647\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6077875971794129\n",
            "kx1 = X_FM, kx2 = X_RT, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4150887846946716, and Avg Validation Loss (mae) is 1.5796977758407593\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5375003695487977\n",
            "kx1 = X_FM, kx2 = X_RT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9216253578662872, and Avg Validation Loss (mae) is 0.8873701155185699\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9033596694469452\n",
            "kx1 = X_FM, kx2 = X_RT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9986838459968567, and Avg Validation Loss (mae) is 1.0092964172363281\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9726866543292999\n",
            "kx1 = X_FM, kx2 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6783172726631165, and Avg Validation Loss (mae) is 0.7345819890499115\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7205377459526062\n",
            "kx1 = X_FM, kx2 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6277765333652496, and Avg Validation Loss (mae) is 0.5507003545761109\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5511409521102906\n",
            "kx1 = X_FM, kx2 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5679811000823975, and Avg Validation Loss (mae) is 1.524199378490448\n",
            "After 10 runs; Avg Test Loss (mae) is 1.479405403137207\n",
            "kx1 = X_FM, kx2 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4280794143676758, and Avg Validation Loss (mae) is 1.5586977124214172\n",
            "After 10 runs; Avg Test Loss (mae) is 1.58326313495636\n",
            "kx1 = X_FM, kx2 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7926108121871949, and Avg Validation Loss (mae) is 0.9102334201335907\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9297232508659363\n",
            "kx1 = X_FM, kx2 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9711442768573761, and Avg Validation Loss (mae) is 0.9821913301944732\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9731529235839844\n",
            "kx1 = X_FM, kx2 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8023069322109222, and Avg Validation Loss (mae) is 0.7798195898532867\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7996279776096344\n",
            "kx1 = X_FM, kx2 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5962396383285522, and Avg Validation Loss (mae) is 0.5664722830057144\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5707116067409516\n",
            "kx1 = X_FM, kx2 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5105653047561645, and Avg Validation Loss (mae) is 1.5457730531692504\n",
            "After 10 runs; Avg Test Loss (mae) is 1.573851466178894\n",
            "kx1 = X_FM, kx2 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.330542230606079, and Avg Validation Loss (mae) is 1.6319721341133118\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6306536555290223\n",
            "kx1 = X_FM, kx2 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8996875941753387, and Avg Validation Loss (mae) is 0.9548300921916961\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9418928563594818\n",
            "kx1 = X_FM, kx2 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0128970921039582, and Avg Validation Loss (mae) is 1.194005936384201\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1930528998374939\n",
            "kx1 = X_FM, kx2 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0274509370326996, and Avg Validation Loss (mae) is 0.9892646849155426\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9688057780265809\n",
            "kx1 = X_FM, kx2 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8728164255619049, and Avg Validation Loss (mae) is 0.9915526151657105\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9868360340595246\n",
            "kx1 = X_MT, kx2 = X_MM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9731637597084046, and Avg Validation Loss (mae) is 2.0588372945785522\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9861797094345093\n",
            "kx1 = X_MT, kx2 = X_MM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0239250302314757, and Avg Validation Loss (mae) is 1.0764555633068085\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0915172338485717\n",
            "kx1 = X_MT, kx2 = X_MM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6907891035079956, and Avg Validation Loss (mae) is 0.6368242889642716\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6240069061517716\n",
            "kx1 = X_MT, kx2 = X_MM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4450836062431336, and Avg Validation Loss (mae) is 1.3029165506362914\n",
            "After 10 runs; Avg Test Loss (mae) is 1.301672601699829\n",
            "kx1 = X_MT, kx2 = X_MM, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.943805593252182, and Avg Validation Loss (mae) is 0.9445801556110383\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9493351459503174\n",
            "kx1 = X_MT, kx2 = X_MM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0480881929397583, and Avg Validation Loss (mae) is 0.9066043853759765\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9256593823432923\n",
            "kx1 = X_MT, kx2 = X_MB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9653416633605958, and Avg Validation Loss (mae) is 1.943514621257782\n",
            "After 10 runs; Avg Test Loss (mae) is 2.0007660508155825\n",
            "kx1 = X_MT, kx2 = X_MB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1693435192108155, and Avg Validation Loss (mae) is 1.0754824578762054\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0824551105499267\n",
            "kx1 = X_MT, kx2 = X_MB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7116366982460022, and Avg Validation Loss (mae) is 0.6337523460388184\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6163342773914338\n",
            "kx1 = X_MT, kx2 = X_MB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6353889107704163, and Avg Validation Loss (mae) is 1.6280326008796693\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6333985805511475\n",
            "kx1 = X_MT, kx2 = X_MB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2984829068183898, and Avg Validation Loss (mae) is 1.3587101936340331\n",
            "After 10 runs; Avg Test Loss (mae) is 1.364431869983673\n",
            "kx1 = X_MT, kx2 = X_MB, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0768350481987, and Avg Validation Loss (mae) is 1.114534080028534\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0853442251682281\n",
            "kx1 = X_MT, kx2 = X_RT, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7122280478477478, and Avg Validation Loss (mae) is 1.707334613800049\n",
            "After 10 runs; Avg Test Loss (mae) is 1.736498522758484\n",
            "kx1 = X_MT, kx2 = X_RT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9546285986900329, and Avg Validation Loss (mae) is 1.0470112562179565\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0274184405803681\n",
            "kx1 = X_MT, kx2 = X_RT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6860496938228607, and Avg Validation Loss (mae) is 0.8185272991657258\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8327656209468841\n",
            "kx1 = X_MT, kx2 = X_RT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7280523538589477, and Avg Validation Loss (mae) is 0.864648163318634\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8794699311256409\n",
            "kx1 = X_MT, kx2 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6974027514457702, and Avg Validation Loss (mae) is 0.5902166277170181\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5757323384284974\n",
            "kx1 = X_MT, kx2 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7479515790939331, and Avg Validation Loss (mae) is 0.7803464293479919\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7617635250091552\n",
            "kx1 = X_MT, kx2 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6258846998214722, and Avg Validation Loss (mae) is 1.717824900150299\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7875150442123413\n",
            "kx1 = X_MT, kx2 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.992800360918045, and Avg Validation Loss (mae) is 0.97575141787529\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9786731958389282\n",
            "kx1 = X_MT, kx2 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.48887054026126864, and Avg Validation Loss (mae) is 0.5154058933258057\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5112164080142975\n",
            "kx1 = X_MT, kx2 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8180865585803986, and Avg Validation Loss (mae) is 0.8117828011512757\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8187583327293396\n",
            "kx1 = X_MT, kx2 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7367011308670044, and Avg Validation Loss (mae) is 0.8052937030792237\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8064806818962097\n",
            "kx1 = X_MT, kx2 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6989709496498108, and Avg Validation Loss (mae) is 0.7336681902408599\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7319449484348297\n",
            "kx1 = X_MT, kx2 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6130217790603638, and Avg Validation Loss (mae) is 1.5265124797821046\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5210548043251038\n",
            "kx1 = X_MT, kx2 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8114843189716339, and Avg Validation Loss (mae) is 0.7837848663330078\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7901447057723999\n",
            "kx1 = X_MT, kx2 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6605216860771179, and Avg Validation Loss (mae) is 0.6330643624067307\n",
            "After 10 runs; Avg Test Loss (mae) is 0.649009957909584\n",
            "kx1 = X_MT, kx2 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7626985728740692, and Avg Validation Loss (mae) is 0.7430962145328521\n",
            "After 10 runs; Avg Test Loss (mae) is 0.75978524684906\n",
            "kx1 = X_MT, kx2 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9589810490608215, and Avg Validation Loss (mae) is 0.9903519570827484\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0123087525367738\n",
            "kx1 = X_MT, kx2 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7720061063766479, and Avg Validation Loss (mae) is 0.8591320872306824\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8672696053981781\n",
            "kx1 = X_MM, kx2 = X_MB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8822832107543945, and Avg Validation Loss (mae) is 2.025017333030701\n",
            "After 10 runs; Avg Test Loss (mae) is 2.088290536403656\n",
            "kx1 = X_MM, kx2 = X_MB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0529005765914916, and Avg Validation Loss (mae) is 1.118342399597168\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0888317227363586\n",
            "kx1 = X_MM, kx2 = X_MB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0790434241294862, and Avg Validation Loss (mae) is 1.1695412397384644\n",
            "After 10 runs; Avg Test Loss (mae) is 1.148037987947464\n",
            "kx1 = X_MM, kx2 = X_MB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5236645102500916, and Avg Validation Loss (mae) is 1.5559956073760985\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5547988891601563\n",
            "kx1 = X_MM, kx2 = X_MB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1606526911258697, and Avg Validation Loss (mae) is 1.1129414200782777\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0957476317882537\n",
            "kx1 = X_MM, kx2 = X_MB, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.055958616733551, and Avg Validation Loss (mae) is 0.9250291228294373\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9062203764915466\n",
            "kx1 = X_MM, kx2 = X_RT, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8753284454345702, and Avg Validation Loss (mae) is 1.7552700757980346\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7794792532920838\n",
            "kx1 = X_MM, kx2 = X_RT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9813361585140228, and Avg Validation Loss (mae) is 1.0248785018920898\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0334927022457123\n",
            "kx1 = X_MM, kx2 = X_RT, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9636956691741944, and Avg Validation Loss (mae) is 0.9875064849853515\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9713224828243255\n",
            "kx1 = X_MM, kx2 = X_RT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7157333135604859, and Avg Validation Loss (mae) is 0.7635486394166946\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7625967621803283\n",
            "kx1 = X_MM, kx2 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5744180768728256, and Avg Validation Loss (mae) is 0.5388806670904159\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5345162063837051\n",
            "kx1 = X_MM, kx2 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7674938023090363, and Avg Validation Loss (mae) is 0.9139936327934265\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9093605756759644\n",
            "kx1 = X_MM, kx2 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 2.0137235045433046, and Avg Validation Loss (mae) is 2.0185981035232543\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9593390226364136\n",
            "kx1 = X_MM, kx2 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0078352451324464, and Avg Validation Loss (mae) is 1.0367872178554536\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0412234306335448\n",
            "kx1 = X_MM, kx2 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9540730774402618, and Avg Validation Loss (mae) is 0.9576868891716004\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9579090297222137\n",
            "kx1 = X_MM, kx2 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7298217833042144, and Avg Validation Loss (mae) is 0.723392915725708\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7068947076797485\n",
            "kx1 = X_MM, kx2 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7311219394207, and Avg Validation Loss (mae) is 0.7666535139083862\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7555523574352264\n",
            "kx1 = X_MM, kx2 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7441305100917817, and Avg Validation Loss (mae) is 0.781654667854309\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7698174774646759\n",
            "kx1 = X_MM, kx2 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6031141996383667, and Avg Validation Loss (mae) is 1.8974175572395324\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8788320541381835\n",
            "kx1 = X_MM, kx2 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8082677066326142, and Avg Validation Loss (mae) is 0.859887170791626\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8420031726360321\n",
            "kx1 = X_MM, kx2 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0644585847854615, and Avg Validation Loss (mae) is 1.0639301776885985\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0710422575473786\n",
            "kx1 = X_MM, kx2 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7443910419940949, and Avg Validation Loss (mae) is 0.7616329759359359\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7774355739355088\n",
            "kx1 = X_MM, kx2 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0065265595912933, and Avg Validation Loss (mae) is 1.0178503751754762\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0162787318229676\n",
            "kx1 = X_MM, kx2 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8365775585174561, and Avg Validation Loss (mae) is 0.8109193623065949\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8229906260967255\n",
            "kx1 = X_MB, kx2 = X_RT, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8815907478332519, and Avg Validation Loss (mae) is 1.7997676849365234\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8086572289466858\n",
            "kx1 = X_MB, kx2 = X_RT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2380380272865295, and Avg Validation Loss (mae) is 1.3149550080299377\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3322336792945861\n",
            "kx1 = X_MB, kx2 = X_RT, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.325200617313385, and Avg Validation Loss (mae) is 1.3681442260742187\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3277695536613465\n",
            "kx1 = X_MB, kx2 = X_RT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8536536395549774, and Avg Validation Loss (mae) is 0.8064585983753204\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7973048150539398\n",
            "kx1 = X_MB, kx2 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6955580472946167, and Avg Validation Loss (mae) is 0.5715961992740631\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5614795863628388\n",
            "kx1 = X_MB, kx2 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8015808403491974, and Avg Validation Loss (mae) is 0.8087409317493439\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8012562096118927\n",
            "kx1 = X_MB, kx2 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9214244127273559, and Avg Validation Loss (mae) is 1.9281046986579895\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9006698846817016\n",
            "kx1 = X_MB, kx2 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1429564654827118, and Avg Validation Loss (mae) is 1.2226501047611236\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2199995338916778\n",
            "kx1 = X_MB, kx2 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.355694305896759, and Avg Validation Loss (mae) is 1.3552871584892272\n",
            "After 10 runs; Avg Test Loss (mae) is 1.316838002204895\n",
            "kx1 = X_MB, kx2 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7449390411376953, and Avg Validation Loss (mae) is 0.8949317097663879\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9031728744506836\n",
            "kx1 = X_MB, kx2 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.919580602645874, and Avg Validation Loss (mae) is 0.9435605049133301\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9658793807029724\n",
            "kx1 = X_MB, kx2 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7444269120693207, and Avg Validation Loss (mae) is 0.8157419264316559\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8178320825099945\n",
            "kx1 = X_MB, kx2 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6387328863143922, and Avg Validation Loss (mae) is 1.5775394320487977\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5836740493774415\n",
            "kx1 = X_MB, kx2 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.898410975933075, and Avg Validation Loss (mae) is 1.0876768767833709\n",
            "After 10 runs; Avg Test Loss (mae) is 1.070069044828415\n",
            "kx1 = X_MB, kx2 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3105591058731079, and Avg Validation Loss (mae) is 1.3220181941986084\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3415046095848084\n",
            "kx1 = X_MB, kx2 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.716856837272644, and Avg Validation Loss (mae) is 0.6939278900623321\n",
            "After 10 runs; Avg Test Loss (mae) is 0.67539342045784\n",
            "kx1 = X_MB, kx2 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0782532513141632, and Avg Validation Loss (mae) is 1.1813686847686768\n",
            "After 10 runs; Avg Test Loss (mae) is 1.155192095041275\n",
            "kx1 = X_MB, kx2 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8275273621082306, and Avg Validation Loss (mae) is 0.8447548747062683\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8693414330482483\n",
            "kx1 = X_RT, kx2 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.865386426448822, and Avg Validation Loss (mae) is 1.8317307353019714\n",
            "After 10 runs; Avg Test Loss (mae) is 1.832228147983551\n",
            "kx1 = X_RT, kx2 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.181426453590393, and Avg Validation Loss (mae) is 1.1919990122318267\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2067394614219666\n",
            "kx1 = X_RT, kx2 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6677188396453857, and Avg Validation Loss (mae) is 1.9057588815689086\n",
            "After 10 runs; Avg Test Loss (mae) is 1.841887640953064\n",
            "kx1 = X_RT, kx2 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8902525842189789, and Avg Validation Loss (mae) is 0.8702637135982514\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8798883199691773\n",
            "kx1 = X_RT, kx2 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0831683933734895, and Avg Validation Loss (mae) is 1.0943997204303741\n",
            "After 10 runs; Avg Test Loss (mae) is 1.047763466835022\n",
            "kx1 = X_RT, kx2 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7932680428028107, and Avg Validation Loss (mae) is 0.7930206775665283\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7824414670467377\n",
            "kx1 = X_RT, kx2 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5782954096794128, and Avg Validation Loss (mae) is 1.579753291606903\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6074462890625\n",
            "kx1 = X_RT, kx2 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9952596247196197, and Avg Validation Loss (mae) is 1.1552355229854583\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1635685801506042\n",
            "kx1 = X_RT, kx2 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4135130047798157, and Avg Validation Loss (mae) is 1.2834273815155028\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2890677332878113\n",
            "kx1 = X_RT, kx2 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9638485848903656, and Avg Validation Loss (mae) is 0.9278322577476501\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9110416352748871\n",
            "kx1 = X_RT, kx2 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0065568923950194, and Avg Validation Loss (mae) is 1.0013040721416473\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9888121366500855\n",
            "kx1 = X_RT, kx2 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6700667440891266, and Avg Validation Loss (mae) is 0.8470950722694397\n",
            "After 10 runs; Avg Test Loss (mae) is 0.850044459104538\n",
            "kx1 = X_RM, kx2 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.664411759376526, and Avg Validation Loss (mae) is 1.626173186302185\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6113769292831421\n",
            "kx1 = X_RM, kx2 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9759684026241302, and Avg Validation Loss (mae) is 1.137845516204834\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1317843437194823\n",
            "kx1 = X_RM, kx2 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.475451922416687, and Avg Validation Loss (mae) is 1.5051872253417968\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5489330530166625\n",
            "kx1 = X_RM, kx2 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8889899253845215, and Avg Validation Loss (mae) is 0.86607745885849\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8703832924365997\n",
            "kx1 = X_RM, kx2 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0474717974662782, and Avg Validation Loss (mae) is 1.1494992077350616\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1803465008735656\n",
            "kx1 = X_RM, kx2 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8612414777278901, and Avg Validation Loss (mae) is 0.9413839042186737\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8863918125629425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "964610e5-974c-44e2-d22e-df40f57ec232",
        "id": "3HqONl60GH4Y"
      },
      "source": [
        "CombResults2 = pd.DataFrame.from_dict(my_dictMF2)\n",
        "CombResultsSorted2 = CombResults2.sort_values(by=['Test Loss'])\n",
        "CombResultsSorted2.to_csv('CombResultsSorted2.csv')\n",
        "CombResultsSorted2\n",
        "files.download(\"CombResultsSorted2.csv\")\n",
        "fig = px.box(CombResultsSorted2, x=\"DATA_X\", y=\"Test Loss\", hover_data=[\"DATA_y\"])\n",
        "fig.show()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_79ac1ac1-4f86-4ac8-94ae-2abcc1a5bff0\", \"CombResultsSorted2.csv\", 6100)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9f5b4a8d-b3fd-476b-9d86-a891200138c7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9f5b4a8d-b3fd-476b-9d86-a891200138c7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9f5b4a8d-b3fd-476b-9d86-a891200138c7',\n",
              "                        [{\"alignmentgroup\": \"True\", \"customdata\": [[\"y_MM\"], [\"y_RM\"], [\"y_RB\"], [\"y_RM\"], [\"y_RB\"], [\"y_RM\"], [\"y_MM\"], [\"y_RB\"], [\"y_MM\"], [\"y_MB\"], [\"y_MB\"], [\"y_MM\"], [\"y_MM\"], [\"y_MB\"], [\"y_MB\"], [\"y_RM\"], [\"y_RB\"], [\"y_RM\"], [\"y_FM\"], [\"y_MM\"], [\"y_MM\"], [\"y_RT\"], [\"y_MB\"], [\"y_RB\"], [\"y_RB\"], [\"y_MB\"], [\"y_RB\"], [\"y_MB\"], [\"y_RB\"], [\"y_RB\"], [\"y_FM\"], [\"y_MM\"], [\"y_RT\"], [\"y_RB\"], [\"y_RT\"], [\"y_RB\"], [\"y_MB\"], [\"y_RM\"], [\"y_MM\"], [\"y_RM\"], [\"y_FM\"], [\"y_RM\"], [\"y_MM\"], [\"y_RM\"], [\"y_RM\"], [\"y_MM\"], [\"y_MB\"], [\"y_MM\"], [\"y_MB\"], [\"y_RT\"], [\"y_RT\"], [\"y_RB\"], [\"y_MM\"], [\"y_MM\"], [\"y_RB\"], [\"y_RB\"], [\"y_MM\"], [\"y_RB\"], [\"y_RB\"], [\"y_MM\"], [\"y_MM\"], [\"y_MB\"], [\"y_MB\"], [\"y_MM\"], [\"y_RM\"], [\"y_MM\"], [\"y_MT\"], [\"y_RT\"], [\"y_RT\"], [\"y_MM\"], [\"y_MT\"], [\"y_MB\"], [\"y_MB\"], [\"y_FM\"], [\"y_RM\"], [\"y_MB\"], [\"y_FM\"], [\"y_RT\"], [\"y_RT\"], [\"y_FM\"], [\"y_RB\"], [\"y_FM\"], [\"y_MB\"], [\"y_FM\"], [\"y_MB\"], [\"y_MB\"], [\"y_MT\"], [\"y_RB\"], [\"y_FM\"], [\"y_MT\"], [\"y_FM\"], [\"y_FM\"], [\"y_RB\"], [\"y_FM\"], [\"y_RB\"], [\"y_RT\"], [\"y_FM\"], [\"y_RM\"], [\"y_RB\"], [\"y_FM\"], [\"y_RM\"], [\"y_MB\"], [\"y_MT\"], [\"y_RT\"], [\"y_FM\"], [\"y_FM\"], [\"y_MB\"], [\"y_MT\"], [\"y_FM\"], [\"y_MB\"], [\"y_RM\"], [\"y_FM\"], [\"y_RM\"], [\"y_FM\"], [\"y_MT\"], [\"y_RM\"], [\"y_MT\"], [\"y_RM\"], [\"y_MT\"], [\"y_RT\"], [\"y_MT\"], [\"y_MM\"], [\"y_MT\"], [\"y_FM\"], [\"y_MT\"], [\"y_RM\"], [\"y_RM\"], [\"y_RT\"], [\"y_FM\"], [\"y_MT\"], [\"y_FT\"], [\"y_MT\"], [\"y_RT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FT\"], [\"y_RT\"], [\"y_MT\"], [\"y_RT\"], [\"y_MT\"], [\"y_RT\"], [\"y_FT\"], [\"y_MT\"], [\"y_FT\"], [\"y_MT\"], [\"y_FT\"], [\"y_RT\"], [\"y_RM\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_MT\"], [\"y_RT\"], [\"y_FT\"], [\"y_RT\"], [\"y_FT\"], [\"y_MT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_MT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}<br>DATA_y=%{customdata[0]}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_MTX_RM\", \"X_MMX_RT\", \"X_FMX_RT\", \"X_MBX_RT\", \"X_FMX_RM\", \"X_MTX_RT\", \"X_FMX_MB\", \"X_FTX_RM\", \"X_MTX_MB\", \"X_MTX_MM\", \"X_FMX_MM\", \"X_MTX_RB\", \"X_MBX_RB\", \"X_FTX_MM\", \"X_MMX_RM\", \"X_FMX_RT\", \"X_MTX_RM\", \"X_FTX_RT\", \"X_FTX_MM\", \"X_FTX_MB\", \"X_FMX_MT\", \"X_MMX_RM\", \"X_MTX_RB\", \"X_FTX_MM\", \"X_MTX_RT\", \"X_MMX_RT\", \"X_MMX_RM\", \"X_MMX_RB\", \"X_RTX_RM\", \"X_FTX_RT\", \"X_MTX_RB\", \"X_MBX_RT\", \"X_FMX_RM\", \"X_MBX_RT\", \"X_MTX_RM\", \"X_MBX_RM\", \"X_MTX_RM\", \"X_MMX_RB\", \"X_MTX_RT\", \"X_FTX_RB\", \"X_MMX_RB\", \"X_RTX_RB\", \"X_FTX_RB\", \"X_MTX_RB\", \"X_MBX_RB\", \"X_RMX_RB\", \"X_MTX_RT\", \"X_RTX_RM\", \"X_FTX_MT\", \"X_RMX_RB\", \"X_FTX_RM\", \"X_FMX_MM\", \"X_MBX_RM\", \"X_FMX_RT\", \"X_MMX_MB\", \"X_MMX_RT\", \"X_RTX_RB\", \"X_FMX_MT\", \"X_MTX_MM\", \"X_FTX_MT\", \"X_FMX_RM\", \"X_FTX_RM\", \"X_FMX_MT\", \"X_FMX_RB\", \"X_MTX_MM\", \"X_FTX_FM\", \"X_MMX_RM\", \"X_MBX_RM\", \"X_FMX_RB\", \"X_FTX_RM\", \"X_MMX_RT\", \"X_FMX_RT\", \"X_FMX_RM\", \"X_MTX_RM\", \"X_FMX_RB\", \"X_RTX_RB\", \"X_FTX_RB\", \"X_MTX_RB\", \"X_MMX_RB\", \"X_MTX_RT\", \"X_FTX_FM\", \"X_MMX_RT\", \"X_FTX_RB\", \"X_MMX_RM\", \"X_FTX_FM\", \"X_RTX_RM\", \"X_FTX_MM\", \"X_FTX_MT\", \"X_MBX_RB\", \"X_MMX_RB\", \"X_FTX_RM\", \"X_MTX_MB\", \"X_MTX_MB\", \"X_MMX_MB\", \"X_FTX_MB\", \"X_FTX_RB\", \"X_MTX_MM\", \"X_MMX_MB\", \"X_FMX_MB\", \"X_RMX_RB\", \"X_FMX_MB\", \"X_FTX_RT\", \"X_MMX_MB\", \"X_MBX_RB\", \"X_FTX_MB\", \"X_RTX_RB\", \"X_RMX_RB\", \"X_FMX_MB\", \"X_FTX_MT\", \"X_FMX_RB\", \"X_FMX_MM\", \"X_RTX_RM\", \"X_FTX_MM\", \"X_MBX_RM\", \"X_FMX_MM\", \"X_FTX_FM\", \"X_FTX_MB\", \"X_FTX_MB\", \"X_RTX_RB\", \"X_MTX_MM\", \"X_MBX_RM\", \"X_FTX_RT\", \"X_MBX_RT\", \"X_MBX_RT\", \"X_MBX_RB\", \"X_FMX_MT\", \"X_MTX_MB\", \"X_FTX_MM\", \"X_FTX_RT\", \"X_FTX_FM\", \"X_FMX_RM\", \"X_FTX_RB\", \"X_FMX_MB\", \"X_FTX_FM\", \"X_MTX_RB\", \"X_FMX_MM\", \"X_FMX_MM\", \"X_FMX_RT\", \"X_FMX_MT\", \"X_RMX_RB\", \"X_MMX_MB\", \"X_FMX_MT\", \"X_FTX_RM\", \"X_FMX_RB\", \"X_FMX_RM\", \"X_MBX_RB\", \"X_FTX_MB\", \"X_FTX_MT\", \"X_RTX_RB\", \"X_FMX_RT\", \"X_RMX_RB\", \"X_FMX_RB\", \"X_MTX_MB\", \"X_FMX_MB\", \"X_FTX_MT\", \"X_MTX_RT\", \"X_FTX_RT\", \"X_MMX_RT\", \"X_MTX_RM\", \"X_MBX_RT\", \"X_RTX_RM\", \"X_RTX_RM\", \"X_MMX_RB\", \"X_MBX_RM\", \"X_MMX_RM\", \"X_MTX_MM\", \"X_MTX_MB\", \"X_MMX_MB\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.5112164080142975, 0.5345162063837051, 0.5511409521102906, 0.5614795863628388, 0.5707116067409516, 0.5757323384284974, 0.6061331063508988, 0.6081672221422195, 0.6163342773914338, 0.6240069061517716, 0.6283347010612488, 0.649009957909584, 0.67539342045784, 0.6916130065917969, 0.7068947076797485, 0.7205377459526062, 0.7319449484348297, 0.7413856863975525, 0.7505771338939666, 0.7514180958271026, 0.7551390409469605, 0.7555523574352264, 0.75978524684906, 0.7606109380722046, 0.7617635250091552, 0.7625967621803283, 0.7698174774646759, 0.7774355739355088, 0.7824414670467377, 0.7854244172573089, 0.7901447057723999, 0.7973048150539398, 0.7996279776096344, 0.8012562096118927, 0.8064806818962097, 0.8178320825099945, 0.8187583327293396, 0.8229906260967255, 0.8327656209468841, 0.8416688203811645, 0.8420031726360321, 0.850044459104538, 0.8502495884895325, 0.8672696053981781, 0.8693414330482483, 0.8703832924365997, 0.8794699311256409, 0.8798883199691773, 0.8857374489307404, 0.8863918125629425, 0.8975850284099579, 0.9003340423107147, 0.9031728744506836, 0.9033596694469452, 0.9062203764915466, 0.9093605756759644, 0.9110416352748871, 0.9150851190090179, 0.9256593823432923, 0.9261094093322754, 0.9297232508659363, 0.9316480875015258, 0.9375527143478394, 0.9418928563594818, 0.9493351459503174, 0.9495458245277405, 0.9579090297222137, 0.9658793807029724, 0.9688057780265809, 0.9695333242416382, 0.9713224828243255, 0.9726866543292999, 0.9731529235839844, 0.9786731958389282, 0.9868360340595246, 0.9888121366500855, 0.992942875623703, 1.0123087525367738, 1.0162787318229676, 1.0274184405803681, 1.0309414625167848, 1.0334927022457123, 1.0342887878417968, 1.0412234306335448, 1.0415800750255584, 1.047763466835022, 1.0617017030715943, 1.0680660128593444, 1.070069044828415, 1.0710422575473786, 1.0760237753391266, 1.0824551105499267, 1.0853442251682281, 1.0888317227363586, 1.0888793289661407, 1.0907401978969573, 1.0915172338485717, 1.0957476317882537, 1.1174793720245362, 1.1317843437194823, 1.1333646059036255, 1.1432534217834474, 1.148037987947464, 1.155192095041275, 1.155889880657196, 1.1635685801506042, 1.1803465008735656, 1.1835631012916565, 1.1841034471988678, 1.1930528998374939, 1.2035548210144043, 1.2067394614219666, 1.208127099275589, 1.2199995338916778, 1.2483367443084716, 1.2497532308101653, 1.262123966217041, 1.2709764003753663, 1.2890677332878113, 1.301672601699829, 1.316838002204895, 1.3269877791404725, 1.3277695536613465, 1.3322336792945861, 1.3415046095848084, 1.3459650039672852, 1.364431869983673, 1.3949745893478394, 1.4306795120239257, 1.4638543128967285, 1.479405403137207, 1.4898621678352355, 1.495566439628601, 1.5036638140678407, 1.5210548043251038, 1.5269010186195373, 1.5309898257255554, 1.5375003695487977, 1.5396087646484375, 1.5489330530166625, 1.5547988891601563, 1.562978494167328, 1.5703253388404845, 1.573851466178894, 1.58326313495636, 1.5836740493774415, 1.5869382977485658, 1.596909499168396, 1.6074462890625, 1.6077875971794129, 1.6113769292831421, 1.6306536555290223, 1.6333985805511475, 1.6391132593154907, 1.6979487538337708, 1.736498522758484, 1.7415463328361511, 1.7794792532920838, 1.7875150442123413, 1.8086572289466858, 1.832228147983551, 1.841887640953064, 1.8788320541381835, 1.9006698846817016, 1.9593390226364136, 1.9861797094345093, 2.0007660508155825, 2.088290536403656], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9f5b4a8d-b3fd-476b-9d86-a891200138c7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "outputId": "14b9bfa7-ff5b-4f32-eb1c-cbeb69781d13",
        "id": "1OAGYvO2GH4a"
      },
      "source": [
        "CombResultsSortedgrouped2 = CombResultsSorted2.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedgroupedsortedMF2 = CombResultsSortedgrouped2.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedgroupedsortedMF2.to_csv('CombResultsSortedgroupedsortedMF2.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedgroupedsortedMF2.csv\")\n",
        "CombResultsSortedgroupedsortedMF2"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_33b0127f-d467-479c-b269-7f643976a793\", \"CombResultsSortedgroupedsortedMF2.csv\", 797)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_MTX_RB</th>\n",
              "      <td>0.933262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RM</th>\n",
              "      <td>0.939098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RT</th>\n",
              "      <td>0.968941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MM</th>\n",
              "      <td>0.977934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RT</th>\n",
              "      <td>0.998461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RM</th>\n",
              "      <td>1.008880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RM</th>\n",
              "      <td>1.031789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RT</th>\n",
              "      <td>1.048835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RB</th>\n",
              "      <td>1.049959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RM</th>\n",
              "      <td>1.055981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RB</th>\n",
              "      <td>1.068097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RT</th>\n",
              "      <td>1.104784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RB</th>\n",
              "      <td>1.115862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RTX_RB</th>\n",
              "      <td>1.134997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MM</th>\n",
              "      <td>1.146395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MM</th>\n",
              "      <td>1.173075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MT</th>\n",
              "      <td>1.176055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MB</th>\n",
              "      <td>1.186038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RM</th>\n",
              "      <td>1.187399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RT</th>\n",
              "      <td>1.194880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MB</th>\n",
              "      <td>1.195870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RMX_RB</th>\n",
              "      <td>1.204869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FM</th>\n",
              "      <td>1.206556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RB</th>\n",
              "      <td>1.215849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MT</th>\n",
              "      <td>1.226479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RTX_RM</th>\n",
              "      <td>1.265158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MB</th>\n",
              "      <td>1.297122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MB</th>\n",
              "      <td>1.313655</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Test Loss\n",
              "DATA_X             \n",
              "X_MTX_RB   0.933262\n",
              "X_MTX_RM   0.939098\n",
              "X_MTX_RT   0.968941\n",
              "X_FTX_MM   0.977934\n",
              "X_MMX_RT   0.998461\n",
              "X_FTX_RM   1.008880\n",
              "X_MMX_RM   1.031789\n",
              "X_FMX_RT   1.048835\n",
              "X_FTX_RB   1.049959\n",
              "X_FMX_RM   1.055981\n",
              "X_MMX_RB   1.068097\n",
              "X_MBX_RT   1.104784\n",
              "X_MBX_RB   1.115862\n",
              "X_RTX_RB   1.134997\n",
              "X_MTX_MM   1.146395\n",
              "X_FMX_MM   1.173075\n",
              "X_FMX_MT   1.176055\n",
              "X_FTX_MB   1.186038\n",
              "X_MBX_RM   1.187399\n",
              "X_FTX_RT   1.194880\n",
              "X_FMX_MB   1.195870\n",
              "X_RMX_RB   1.204869\n",
              "X_FTX_FM   1.206556\n",
              "X_FMX_RB   1.215849\n",
              "X_FTX_MT   1.226479\n",
              "X_RTX_RM   1.265158\n",
              "X_MTX_MB   1.297122\n",
              "X_MMX_MB   1.313655"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nL5z_lxZXX4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "2c8e4c9f-7a32-414c-d052-fd0e94dc9a7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmI8tyPKXeOF"
      },
      "source": [
        "#Combinations of 3 Sensors, therefore 21 features, 21 inputs\n",
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "inp_shp = 21\n",
        "n_sensors = 3\n",
        "\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "modelG(inp_shp)\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "my_dictMF3 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), n_sensors):\n",
        "  kX1, kX2, kX3 = combo[0][0], combo[1][0], combo[2][0]\n",
        "  vX1, vX2, vX3 = combo[0][1], combo[1][1], combo[2][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, ky = {ky}')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF3 = evaldata(n_sensors, X_in1=vX1, X_in2=vX2, X_in3=vX3, Y_in=vy, traindata1 = kX1, traindata2 = kX2, traindata3 = kX3, testdata = ky)\n",
        "      TestLossTotal = resultsMF3[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF3[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF3[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}, {kX2}, {kX3} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF3[\"DATA_X\"].append(kX1 + kX2 + kX3)\n",
        "    my_dictMF3[\"DATA_y\"].append(ky)\n",
        "    my_dictMF3[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model.load_weights('model.h5')\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9385205566883087, and Avg Validation Loss (mae) is 0.8020997226238251\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8069062173366547\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.922955983877182, and Avg Validation Loss (mae) is 1.0008366465568543\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0209195554256438\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.542907702922821, and Avg Validation Loss (mae) is 1.518668258190155\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5017580389976501\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2950472593307496, and Avg Validation Loss (mae) is 1.2948506355285645\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3021220803260802\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9333669304847717, and Avg Validation Loss (mae) is 0.9619087636470794\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9445780932903289\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1313653588294983, and Avg Validation Loss (mae) is 1.0996078908443452\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1210790276527405\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9151470899581909, and Avg Validation Loss (mae) is 0.8764535665512085\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8653838068246842\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4914930701255797, and Avg Validation Loss (mae) is 1.5340181589126587\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5697898030281068\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0872495830059052, and Avg Validation Loss (mae) is 1.0551561295986176\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0518385350704194\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8130670309066772, and Avg Validation Loss (mae) is 0.8371080815792084\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8526480853557586\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2014437437057495, and Avg Validation Loss (mae) is 1.1522653162479402\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1636512458324433\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.606793013215065, and Avg Validation Loss (mae) is 0.5763131290674209\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5676087260246276\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.492902362346649, and Avg Validation Loss (mae) is 1.5640925645828248\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5562897086143495\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1520420432090759, and Avg Validation Loss (mae) is 1.1201738953590392\n",
            "After 10 runs; Avg Test Loss (mae) is 1.124195009469986\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8773206949234009, and Avg Validation Loss (mae) is 0.7907018005847931\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7940728724002838\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3475993514060973, and Avg Validation Loss (mae) is 1.5130788564682007\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4638778686523437\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9265344858169555, and Avg Validation Loss (mae) is 0.9254348278045654\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9478971004486084\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0324861884117127, and Avg Validation Loss (mae) is 1.0710822999477387\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0628319442272187\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6559297859668731, and Avg Validation Loss (mae) is 0.7225464463233948\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7348278105258942\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6406471908092499, and Avg Validation Loss (mae) is 0.6240058183670044\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6352699339389801\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.485324001312256, and Avg Validation Loss (mae) is 1.5095760941505432\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5107017040252686\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8137577652931214, and Avg Validation Loss (mae) is 0.9125493466854095\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9007088720798493\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9818357765674591, and Avg Validation Loss (mae) is 1.1460636615753175\n",
            "After 10 runs; Avg Test Loss (mae) is 1.155196213722229\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8059669554233551, and Avg Validation Loss (mae) is 0.8094157159328461\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8095706343650818\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6616836309432983, and Avg Validation Loss (mae) is 0.6666854202747345\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6693149477243423\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3308151602745055, and Avg Validation Loss (mae) is 1.3779688000679016\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3487169861793518\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8724200785160064, and Avg Validation Loss (mae) is 0.8545376300811768\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8729348361492157\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0234711110591888, and Avg Validation Loss (mae) is 0.9576059699058532\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9770181894302368\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1027765274047852, and Avg Validation Loss (mae) is 1.031523722410202\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0373864948749543\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8282592236995697, and Avg Validation Loss (mae) is 0.9104966759681702\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9362728476524353\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7753778636455536, and Avg Validation Loss (mae) is 0.8635543704032898\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8708916783332825\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7442916393280029, and Avg Validation Loss (mae) is 0.7040106058120728\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6994934439659118\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2785184621810912, and Avg Validation Loss (mae) is 1.4556795179843902\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4579180121421813\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9290778160095214, and Avg Validation Loss (mae) is 0.97044717669487\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9692094206809998\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8786856770515442, and Avg Validation Loss (mae) is 1.0202813148498535\n",
            "After 10 runs; Avg Test Loss (mae) is 1.006624311208725\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9822025179862977, and Avg Validation Loss (mae) is 1.1130717754364015\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1226681292057037\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7241498708724976, and Avg Validation Loss (mae) is 0.7023837089538574\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7370169579982757\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4568875789642335, and Avg Validation Loss (mae) is 1.4800546884536743\n",
            "After 10 runs; Avg Test Loss (mae) is 1.460636830329895\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3105829238891602, and Avg Validation Loss (mae) is 1.4906540989875794\n",
            "After 10 runs; Avg Test Loss (mae) is 1.504037892818451\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9307152152061462, and Avg Validation Loss (mae) is 1.1002332866191864\n",
            "After 10 runs; Avg Test Loss (mae) is 1.099531537294388\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9332650125026702, and Avg Validation Loss (mae) is 0.9460662305355072\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9515971660614013\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8185442984104156, and Avg Validation Loss (mae) is 0.9942704617977143\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9922923386096955\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7761096358299255, and Avg Validation Loss (mae) is 0.7524049401283264\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7461553752422333\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5855128705501557, and Avg Validation Loss (mae) is 0.5847574383020401\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5743899673223496\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6862197816371918, and Avg Validation Loss (mae) is 0.7401577889919281\n",
            "After 10 runs; Avg Test Loss (mae) is 0.730345857143402\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8135815441608429, and Avg Validation Loss (mae) is 0.8844681441783905\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8889604151248932\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5611410230398178, and Avg Validation Loss (mae) is 0.4843839108943939\n",
            "After 10 runs; Avg Test Loss (mae) is 0.48141456842422486\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7991532683372498, and Avg Validation Loss (mae) is 0.8077077686786651\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8191404163837432\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6924253344535828, and Avg Validation Loss (mae) is 0.7675976753234863\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7685153126716614\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.61634840965271, and Avg Validation Loss (mae) is 0.7680331647396088\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7715485036373139\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.734699422121048, and Avg Validation Loss (mae) is 0.8167072236537933\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8172483801841736\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6332013309001923, and Avg Validation Loss (mae) is 0.6875169783830642\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7067836940288543\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.768881332874298, and Avg Validation Loss (mae) is 0.8932685375213623\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8930773675441742\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.037991362810135, and Avg Validation Loss (mae) is 1.1275441110134126\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1395806431770326\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8149059653282166, and Avg Validation Loss (mae) is 0.9311401307582855\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9226683437824249\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7297779858112335, and Avg Validation Loss (mae) is 0.8017407715320587\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8214966833591462\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.14135080575943, and Avg Validation Loss (mae) is 1.086334615945816\n",
            "After 10 runs; Avg Test Loss (mae) is 1.073938286304474\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4926104426383973, and Avg Validation Loss (mae) is 1.5289907932281495\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5470864534378053\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1319287180900575, and Avg Validation Loss (mae) is 1.1738076865673066\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1595117926597596\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0854261457920074, and Avg Validation Loss (mae) is 0.9974342703819274\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9916864454746246\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8512867212295532, and Avg Validation Loss (mae) is 0.9220267474651337\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9054917991161346\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0462770462036133, and Avg Validation Loss (mae) is 1.0129161477088928\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0003697276115417\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.788412743806839, and Avg Validation Loss (mae) is 0.7194276601076126\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7218107044696808\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5273678451776505, and Avg Validation Loss (mae) is 0.6741064846515655\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6708148628473282\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6467669248580933, and Avg Validation Loss (mae) is 0.7232673227787018\n",
            "After 10 runs; Avg Test Loss (mae) is 0.711008271574974\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8387389481067657, and Avg Validation Loss (mae) is 0.8275339603424072\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8277995109558105\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9857327461242675, and Avg Validation Loss (mae) is 1.0022935271263123\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0032496094703673\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7396468222141266, and Avg Validation Loss (mae) is 0.623992794752121\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6346156120300293\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6944752871990204, and Avg Validation Loss (mae) is 0.7746441960334778\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7706968903541564\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.639114499092102, and Avg Validation Loss (mae) is 0.7354084074497222\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6996729165315628\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.765850180387497, and Avg Validation Loss (mae) is 0.652922374010086\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6498382568359375\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1218824088573456, and Avg Validation Loss (mae) is 1.0725316286087037\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0600169360637666\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7926452815532684, and Avg Validation Loss (mae) is 0.8712113559246063\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8637095540761948\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.062675154209137, and Avg Validation Loss (mae) is 1.0488262832164765\n",
            "After 10 runs; Avg Test Loss (mae) is 1.029696959257126\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7745139062404632, and Avg Validation Loss (mae) is 0.8580480337142944\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8320986926555634\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.133463716506958, and Avg Validation Loss (mae) is 1.1368925034999848\n",
            "After 10 runs; Avg Test Loss (mae) is 1.13150395154953\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3724560260772705, and Avg Validation Loss (mae) is 1.4409412264823913\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4252679824829102\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8959518194198608, and Avg Validation Loss (mae) is 1.0256710171699523\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0313876688480377\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7217413365840912, and Avg Validation Loss (mae) is 0.8112356930971145\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8004673004150391\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7056438624858856, and Avg Validation Loss (mae) is 0.7760065257549286\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7653055787086487\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0089512825012208, and Avg Validation Loss (mae) is 1.2132243514060974\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1954802393913269\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3340404152870178, and Avg Validation Loss (mae) is 1.3475834369659423\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3020593523979187\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7401485443115234, and Avg Validation Loss (mae) is 0.8864741861820221\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8739145457744598\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8246904909610748, and Avg Validation Loss (mae) is 0.8718786895275116\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8556422770023346\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6307714700698852, and Avg Validation Loss (mae) is 0.7669178962707519\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7687268614768982\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9644818544387818, and Avg Validation Loss (mae) is 0.9891078352928162\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9775696635246277\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3073975324630738, and Avg Validation Loss (mae) is 1.346607232093811\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3820177912712097\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7741344749927521, and Avg Validation Loss (mae) is 0.9663685321807861\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9660838782787323\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0430782616138459, and Avg Validation Loss (mae) is 1.1484584927558898\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1292606830596923\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8329871416091919, and Avg Validation Loss (mae) is 0.9724637508392334\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9733841419219971\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0190493285655975, and Avg Validation Loss (mae) is 0.9572680950164795\n",
            "After 10 runs; Avg Test Loss (mae) is 0.946785968542099\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5473645210266114, and Avg Validation Loss (mae) is 1.485679531097412\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4914325952529908\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.88905548453331, and Avg Validation Loss (mae) is 0.8218204915523529\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7875349462032318\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0723243832588196, and Avg Validation Loss (mae) is 1.1022085547447205\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1180821061134338\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6738868236541748, and Avg Validation Loss (mae) is 0.7179469287395477\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7241043239831925\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9372204780578614, and Avg Validation Loss (mae) is 0.9986231803894043\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9577507257461548\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3905123710632323, and Avg Validation Loss (mae) is 1.4133822202682496\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3827694296836852\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1057596564292909, and Avg Validation Loss (mae) is 1.2578931212425233\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2590919375419616\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0765735745429992, and Avg Validation Loss (mae) is 1.2183143079280854\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2217285454273223\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7525332927703857, and Avg Validation Loss (mae) is 0.7912213653326035\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7857086598873139\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9102173864841461, and Avg Validation Loss (mae) is 0.9493065536022186\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9440505683422089\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.494992733001709, and Avg Validation Loss (mae) is 1.5077855944633485\n",
            "After 10 runs; Avg Test Loss (mae) is 1.516019070148468\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9509560585021972, and Avg Validation Loss (mae) is 0.9002130508422852\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8936754882335662\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0018028497695923, and Avg Validation Loss (mae) is 1.1168297350406646\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1271689295768739\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8546461522579193, and Avg Validation Loss (mae) is 0.8667119026184082\n",
            "After 10 runs; Avg Test Loss (mae) is 0.887708044052124\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5592547297477721, and Avg Validation Loss (mae) is 1.575186550617218\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5781718373298645\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.71319540143013, and Avg Validation Loss (mae) is 0.6992843955755234\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7102572947740555\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4678119540214538, and Avg Validation Loss (mae) is 1.3228171586990356\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3599364161491394\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9799762487411499, and Avg Validation Loss (mae) is 0.9684387922286988\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9804146111011505\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8820081889629364, and Avg Validation Loss (mae) is 0.9950440883636474\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9996809184551239\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.622557020187378, and Avg Validation Loss (mae) is 1.4862508177757263\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4885076522827148\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.620147442817688, and Avg Validation Loss (mae) is 0.5811814188957214\n",
            "After 10 runs; Avg Test Loss (mae) is 0.590923348069191\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.343682599067688, and Avg Validation Loss (mae) is 1.6560042262077332\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6862996339797973\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1384422659873963, and Avg Validation Loss (mae) is 1.3145047068595885\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2934688746929168\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9072511076927186, and Avg Validation Loss (mae) is 0.9059333622455596\n",
            "After 10 runs; Avg Test Loss (mae) is 0.899726790189743\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5017142176628113, and Avg Validation Loss (mae) is 1.5228777170181274\n",
            "After 10 runs; Avg Test Loss (mae) is 1.53171204328537\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6796842277050018, and Avg Validation Loss (mae) is 0.5798518538475037\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5784668952226639\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7904914259910584, and Avg Validation Loss (mae) is 0.9335044503211976\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9332553029060364\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6194487541913987, and Avg Validation Loss (mae) is 0.8230434834957123\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8237928509712219\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6073505699634552, and Avg Validation Loss (mae) is 0.6326122760772706\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6491017788648605\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.455044686794281, and Avg Validation Loss (mae) is 1.468583607673645\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4148869037628173\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5867878049612045, and Avg Validation Loss (mae) is 0.49210824370384215\n",
            "After 10 runs; Avg Test Loss (mae) is 0.4919242650270462\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8822624504566192, and Avg Validation Loss (mae) is 0.7248546302318573\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7340444922447205\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7522794723510742, and Avg Validation Loss (mae) is 0.8315433919429779\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8203304171562195\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6796268880367279, and Avg Validation Loss (mae) is 0.7260357439517975\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7339640885591507\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4787529706954956, and Avg Validation Loss (mae) is 1.5397847175598145\n",
            "After 10 runs; Avg Test Loss (mae) is 1.562300431728363\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7173850536346436, and Avg Validation Loss (mae) is 0.9767860382795334\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9794941306114197\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8576236546039582, and Avg Validation Loss (mae) is 0.8374952733516693\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8258308827877044\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0745105266571044, and Avg Validation Loss (mae) is 1.039258313179016\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0662187457084655\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8914973556995391, and Avg Validation Loss (mae) is 1.0062959671020508\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0089041233062743\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.51866455078125, and Avg Validation Loss (mae) is 1.5843969464302063\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6082055807113647\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0961928486824035, and Avg Validation Loss (mae) is 1.164311945438385\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1346913695335388\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4419564127922058, and Avg Validation Loss (mae) is 1.4801666021347046\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5091780066490172\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1018511295318603, and Avg Validation Loss (mae) is 1.0716940104961394\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0765615224838256\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8311476588249207, and Avg Validation Loss (mae) is 0.9570991098880768\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9901897490024567\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5262400388717652, and Avg Validation Loss (mae) is 1.5041386127471923\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5102976560592651\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9949985206127167, and Avg Validation Loss (mae) is 1.0425911605358125\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0500174462795258\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7293993473052979, and Avg Validation Loss (mae) is 0.755068165063858\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7518178045749664\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.559468799829483, and Avg Validation Loss (mae) is 0.6593349814414978\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6824382156133652\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6103645026683807, and Avg Validation Loss (mae) is 0.71896713078022\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7135150313377381\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5504806756973266, and Avg Validation Loss (mae) is 1.5651397466659547\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5610921502113342\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0253518104553223, and Avg Validation Loss (mae) is 1.0567519307136535\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0692068040370941\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8088834345340729, and Avg Validation Loss (mae) is 0.7605873078107834\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7803786873817444\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7264329135417938, and Avg Validation Loss (mae) is 0.8094634979963302\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8194677710533143\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6782077312469482, and Avg Validation Loss (mae) is 0.7023045569658279\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7031470030546189\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.477658712863922, and Avg Validation Loss (mae) is 1.549538791179657\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5586414337158203\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0689517438411713, and Avg Validation Loss (mae) is 1.0070716977119445\n",
            "After 10 runs; Avg Test Loss (mae) is 1.008823847770691\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7388396561145782, and Avg Validation Loss (mae) is 1.0254185318946838\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0407220780849458\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1508720457553863, and Avg Validation Loss (mae) is 0.9970731317996979\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0394194543361663\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7945701062679291, and Avg Validation Loss (mae) is 0.8868757665157319\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8812305092811584\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5813832998275756, and Avg Validation Loss (mae) is 1.5417132139205934\n",
            "After 10 runs; Avg Test Loss (mae) is 1.525921595096588\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2120445609092712, and Avg Validation Loss (mae) is 1.270996880531311\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2543603837490083\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6836326241493225, and Avg Validation Loss (mae) is 0.7975167006254196\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8106918662786484\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6902690887451172, and Avg Validation Loss (mae) is 0.629855963587761\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6313661098480224\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.667274409532547, and Avg Validation Loss (mae) is 0.6745120793581009\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6741507291793823\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.610355544090271, and Avg Validation Loss (mae) is 1.6779066681861878\n",
            "After 10 runs; Avg Test Loss (mae) is 1.691315197944641\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1952569603919982, and Avg Validation Loss (mae) is 1.115668350458145\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1386876106262207\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5831275045871734, and Avg Validation Loss (mae) is 0.5729676753282547\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5739515453577042\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8452949166297913, and Avg Validation Loss (mae) is 0.9139334738254548\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9128207862377167\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6368810057640075, and Avg Validation Loss (mae) is 0.6586273610591888\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6594168573617936\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5316003084182739, and Avg Validation Loss (mae) is 1.4968722939491272\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5382004261016846\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.182730770111084, and Avg Validation Loss (mae) is 1.3457939743995666\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3766312420368194\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7480826318264008, and Avg Validation Loss (mae) is 0.8056880444288254\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8003992736339569\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.067949801683426, and Avg Validation Loss (mae) is 1.1538022994995116\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1724614799022675\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8607250452041626, and Avg Validation Loss (mae) is 1.0683451056480409\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0739587604999543\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5285308361053467, and Avg Validation Loss (mae) is 1.523519790172577\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4987666487693787\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3432856917381286, and Avg Validation Loss (mae) is 1.4293275117874145\n",
            "After 10 runs; Avg Test Loss (mae) is 1.412057363986969\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7886384129524231, and Avg Validation Loss (mae) is 0.8339957773685456\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8515551388263702\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0523781597614288, and Avg Validation Loss (mae) is 1.0602132022380828\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1005134642124177\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5820776939392089, and Avg Validation Loss (mae) is 0.6753673881292344\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6759009242057801\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5243729948997498, and Avg Validation Loss (mae) is 1.5754289150238037\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6247448325157166\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2632915496826171, and Avg Validation Loss (mae) is 1.4224489450454711\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4085176944732667\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.939265501499176, and Avg Validation Loss (mae) is 0.8506127178668976\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8663781940937042\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0053047716617585, and Avg Validation Loss (mae) is 0.9997523963451386\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0204002261161804\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.725559514760971, and Avg Validation Loss (mae) is 0.8915721774101257\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9077925115823746\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5821452617645264, and Avg Validation Loss (mae) is 1.555312693119049\n",
            "After 10 runs; Avg Test Loss (mae) is 1.560234582424164\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3417720794677734, and Avg Validation Loss (mae) is 1.320644450187683\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2766409397125245\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8322865068912506, and Avg Validation Loss (mae) is 0.8019944608211518\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8085688412189483\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.977211332321167, and Avg Validation Loss (mae) is 1.1842290997505187\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1788223505020141\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9000877201557159, and Avg Validation Loss (mae) is 0.9573823571205139\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9564993917942047\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8999671578407287, and Avg Validation Loss (mae) is 2.0558466792106627\n",
            "After 10 runs; Avg Test Loss (mae) is 2.07482488155365\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0017881095409393, and Avg Validation Loss (mae) is 1.0206888794898987\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0176614284515382\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4396859049797057, and Avg Validation Loss (mae) is 1.4621267318725586\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4669180989265442\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0144865334033966, and Avg Validation Loss (mae) is 1.1702549636363984\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1543962478637695\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0185318112373352, and Avg Validation Loss (mae) is 1.0237163126468658\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0188823461532592\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8381902813911437, and Avg Validation Loss (mae) is 2.0719683289527895\n",
            "After 10 runs; Avg Test Loss (mae) is 2.0748077750205995\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9136033475399017, and Avg Validation Loss (mae) is 0.9467095017433167\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9570398449897766\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8085147857666015, and Avg Validation Loss (mae) is 0.7973463952541351\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8192681312561035\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5829651713371277, and Avg Validation Loss (mae) is 0.5369105219841004\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5282787322998047\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7745871484279633, and Avg Validation Loss (mae) is 0.8910711705684662\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8871417164802551\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9192367315292358, and Avg Validation Loss (mae) is 1.9527454137802125\n",
            "After 10 runs; Avg Test Loss (mae) is 1.958996546268463\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.997500067949295, and Avg Validation Loss (mae) is 1.0407064259052277\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0245101392269134\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6971592724323272, and Avg Validation Loss (mae) is 0.7296429216861725\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7235266119241714\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8459114491939544, and Avg Validation Loss (mae) is 0.8010921657085419\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8039781391620636\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7722008347511291, and Avg Validation Loss (mae) is 0.9545223951339722\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9597787976264953\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.654386854171753, and Avg Validation Loss (mae) is 1.619611668586731\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5820372700691223\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8103878676891327, and Avg Validation Loss (mae) is 0.8881340563297272\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8757041752338409\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7932632744312287, and Avg Validation Loss (mae) is 0.763271552324295\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7572792887687683\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0286311686038971, and Avg Validation Loss (mae) is 0.9148530185222625\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9229367315769196\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7848286926746368, and Avg Validation Loss (mae) is 0.8933180510997772\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8845209330320358\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8339483737945557, and Avg Validation Loss (mae) is 1.8255884647369385\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8299251079559327\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.985624247789383, and Avg Validation Loss (mae) is 0.953959983587265\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9595872581005096\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6086155623197556, and Avg Validation Loss (mae) is 0.6173507362604141\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6155367761850357\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6590437293052673, and Avg Validation Loss (mae) is 0.5767410218715667\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5861369162797928\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7593287706375123, and Avg Validation Loss (mae) is 0.7510083794593811\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7443216800689697\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 2.014688014984131, and Avg Validation Loss (mae) is 1.8229101777076722\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8623785138130189\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9731554567813874, and Avg Validation Loss (mae) is 0.9023809254169464\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8995490074157715\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5325875014066697, and Avg Validation Loss (mae) is 0.5100870817899704\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5154469519853592\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7358625054359436, and Avg Validation Loss (mae) is 0.9189763844013215\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9208679974079133\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7407151103019715, and Avg Validation Loss (mae) is 0.8369671523571014\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8442654073238373\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5928106904029846, and Avg Validation Loss (mae) is 1.8618029832839966\n",
            "After 10 runs; Avg Test Loss (mae) is 1.871997630596161\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8963622927665711, and Avg Validation Loss (mae) is 0.8288321137428284\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8255265653133392\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.718987089395523, and Avg Validation Loss (mae) is 0.8299617648124695\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8222362607717514\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8877390265464783, and Avg Validation Loss (mae) is 0.9945978105068207\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9992977440357208\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8484625875949859, and Avg Validation Loss (mae) is 0.7368336856365204\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7648503959178925\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8484202861785888, and Avg Validation Loss (mae) is 2.0086740255355835\n",
            "After 10 runs; Avg Test Loss (mae) is 2.01796612739563\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9895091116428375, and Avg Validation Loss (mae) is 1.1461786866188048\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1813231885433197\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6824388712644577, and Avg Validation Loss (mae) is 0.567392098903656\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5729000896215439\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9174733877182006, and Avg Validation Loss (mae) is 1.1995211243629456\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2040932774543762\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7304757416248322, and Avg Validation Loss (mae) is 0.6750105559825897\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6636917531490326\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6680267810821534, and Avg Validation Loss (mae) is 1.6719622969627381\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6182412981987\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8088971257209778, and Avg Validation Loss (mae) is 0.8976951479911804\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8966548264026641\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6508908689022064, and Avg Validation Loss (mae) is 0.5937094032764435\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6021129071712494\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8330774784088135, and Avg Validation Loss (mae) is 0.9049478948116303\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9068316221237183\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5839261621236801, and Avg Validation Loss (mae) is 0.6227620422840119\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6266163468360901\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5956615328788757, and Avg Validation Loss (mae) is 1.7059480905532838\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7306612253189086\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.825499814748764, and Avg Validation Loss (mae) is 0.8904066145420074\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9091208040714264\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6012884318828583, and Avg Validation Loss (mae) is 0.4900800555944443\n",
            "After 10 runs; Avg Test Loss (mae) is 0.49985400438308714\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7620641052722931, and Avg Validation Loss (mae) is 0.7250564992427826\n",
            "After 10 runs; Avg Test Loss (mae) is 0.711983585357666\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7515074431896209, and Avg Validation Loss (mae) is 1.0326428979635238\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0323630630970002\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8500533819198608, and Avg Validation Loss (mae) is 1.8935447454452514\n",
            "After 10 runs; Avg Test Loss (mae) is 1.836233103275299\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9296609580516815, and Avg Validation Loss (mae) is 1.0766171216964722\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0873755931854248\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.97039715051651, and Avg Validation Loss (mae) is 1.0923347353935242\n",
            "After 10 runs; Avg Test Loss (mae) is 1.079262524843216\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5491780877113343, and Avg Validation Loss (mae) is 0.5351541966199875\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5319921433925628\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7694897055625916, and Avg Validation Loss (mae) is 0.7814348638057709\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7818948090076446\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8636883974075318, and Avg Validation Loss (mae) is 2.077593433856964\n",
            "After 10 runs; Avg Test Loss (mae) is 2.039247715473175\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0179487586021423, and Avg Validation Loss (mae) is 1.2259170174598695\n",
            "After 10 runs; Avg Test Loss (mae) is 1.263730102777481\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0126587331295014, and Avg Validation Loss (mae) is 0.9691980898380279\n",
            "After 10 runs; Avg Test Loss (mae) is 0.965563702583313\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7722405910491943, and Avg Validation Loss (mae) is 0.8812792599201202\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8803076833486557\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7747955679893493, and Avg Validation Loss (mae) is 0.8657173752784729\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8619413018226624\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.635712993144989, and Avg Validation Loss (mae) is 1.6361744284629822\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6614161252975463\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8552888751029968, and Avg Validation Loss (mae) is 0.9153039634227753\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8926316201686859\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.118200159072876, and Avg Validation Loss (mae) is 1.0750849902629853\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0843814373016358\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1043665051460265, and Avg Validation Loss (mae) is 1.0316807866096496\n",
            "After 10 runs; Avg Test Loss (mae) is 1.028370201587677\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8221893787384034, and Avg Validation Loss (mae) is 0.8110125124454498\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8342327296733856\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8854537844657897, and Avg Validation Loss (mae) is 1.8180082798004151\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8115325927734376\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0187727212905884, and Avg Validation Loss (mae) is 1.110802286863327\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0792296171188354\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9158951818943024, and Avg Validation Loss (mae) is 1.1005515515804292\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0966349482536315\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7465191781520844, and Avg Validation Loss (mae) is 0.7879869908094406\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7741755425930024\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7285270929336548, and Avg Validation Loss (mae) is 0.7231428563594818\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7086247384548188\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6897345304489135, and Avg Validation Loss (mae) is 1.6790869593620301\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7008501648902894\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.796579509973526, and Avg Validation Loss (mae) is 0.8803414702415466\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8769107818603515\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.002288383245468, and Avg Validation Loss (mae) is 1.2014054715633393\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1854787111282348\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7308152616024017, and Avg Validation Loss (mae) is 0.6301956832408905\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6264600634574891\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6161351919174194, and Avg Validation Loss (mae) is 0.6983913213014603\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6989714592695236\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6765955686569214, and Avg Validation Loss (mae) is 1.6529092907905578\n",
            "After 10 runs; Avg Test Loss (mae) is 1.614736223220825\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8269202649593353, and Avg Validation Loss (mae) is 0.9180587470531464\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9207554697990418\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9888032138347626, and Avg Validation Loss (mae) is 1.0900555908679963\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1012196302413941\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7786022067070008, and Avg Validation Loss (mae) is 0.7939262151718139\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7812124192714691\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.72769575715065, and Avg Validation Loss (mae) is 0.6825806647539139\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6786793053150177\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.893270444869995, and Avg Validation Loss (mae) is 1.8418693780899047\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8673439264297484\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1651411771774292, and Avg Validation Loss (mae) is 1.3442253947257996\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3669739365577698\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2272225975990296, and Avg Validation Loss (mae) is 1.409887421131134\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4061773538589477\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6847171247005462, and Avg Validation Loss (mae) is 0.5919535547494889\n",
            "After 10 runs; Avg Test Loss (mae) is 0.585852575302124\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7202731013298035, and Avg Validation Loss (mae) is 0.8087212145328522\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8049734830856323\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6967223763465882, and Avg Validation Loss (mae) is 1.7091923117637635\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6759677052497863\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9602270305156708, and Avg Validation Loss (mae) is 1.0833993136882782\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0938368320465088\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1561262488365174, and Avg Validation Loss (mae) is 1.1902547299861908\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2108080625534057\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7776645302772522, and Avg Validation Loss (mae) is 0.7716295957565308\n",
            "After 10 runs; Avg Test Loss (mae) is 0.767118501663208\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5778089940547944, and Avg Validation Loss (mae) is 0.6314031034708023\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6296187043190002\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.609105634689331, and Avg Validation Loss (mae) is 1.719105851650238\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7368866562843324\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9817922353744507, and Avg Validation Loss (mae) is 0.9565591871738434\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9427432298660279\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4649289011955262, and Avg Validation Loss (mae) is 1.3277284264564515\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3306286096572877\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8204302072525025, and Avg Validation Loss (mae) is 0.7958646953105927\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7911834597587586\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8220468044281006, and Avg Validation Loss (mae) is 0.8968179583549499\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9110295653343201\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5911206126213073, and Avg Validation Loss (mae) is 1.8272809267044068\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7887741804122925\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9093785345554352, and Avg Validation Loss (mae) is 0.938694417476654\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9286008238792419\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.412337350845337, and Avg Validation Loss (mae) is 1.4422107458114624\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3959777712821961\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8851704180240632, and Avg Validation Loss (mae) is 0.937999552488327\n",
            "After 10 runs; Avg Test Loss (mae) is 0.953426718711853\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0360819101333618, and Avg Validation Loss (mae) is 1.214252918958664\n",
            "After 10 runs; Avg Test Loss (mae) is 1.231146550178528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "e796b4d0-900a-4e28-bcf2-6b4d9f404c57",
        "id": "PrjNinafXeOH"
      },
      "source": [
        "CombResults3 = pd.DataFrame.from_dict(my_dictMF3)\n",
        "CombResultsSorted3 = CombResults3.sort_values(by=['Test Loss'])\n",
        "CombResultsSorted3.to_csv('CombResultsSorted3.csv')\n",
        "CombResultsSorted3\n",
        "files.download(\"CombResultsSorted3.csv\")\n",
        "fig = px.box(CombResultsSorted3, x=\"DATA_X\", y=\"Test Loss\", hover_data=[\"DATA_y\"])\n",
        "fig.show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ad0c8583-ba04-4189-9954-79edce5ba640\", \"CombResultsSorted3.csv\", 11351)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"8cb599f3-6f44-4af8-b589-6126176e0fdd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"8cb599f3-6f44-4af8-b589-6126176e0fdd\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '8cb599f3-6f44-4af8-b589-6126176e0fdd',\n",
              "                        [{\"alignmentgroup\": \"True\", \"customdata\": [[\"y_MM\"], [\"y_MM\"], [\"y_MM\"], [\"y_MM\"], [\"y_RM\"], [\"y_RM\"], [\"y_MM\"], [\"y_MM\"], [\"y_MM\"], [\"y_RM\"], [\"y_MM\"], [\"y_MM\"], [\"y_RM\"], [\"y_MM\"], [\"y_MM\"], [\"y_MM\"], [\"y_MB\"], [\"y_RM\"], [\"y_RM\"], [\"y_RM\"], [\"y_MB\"], [\"y_RB\"], [\"y_RB\"], [\"y_FM\"], [\"y_RB\"], [\"y_RB\"], [\"y_RB\"], [\"y_RM\"], [\"y_RB\"], [\"y_RB\"], [\"y_RT\"], [\"y_RM\"], [\"y_RM\"], [\"y_MB\"], [\"y_RB\"], [\"y_RB\"], [\"y_MM\"], [\"y_RB\"], [\"y_MB\"], [\"y_RB\"], [\"y_MB\"], [\"y_RB\"], [\"y_MB\"], [\"y_MB\"], [\"y_RB\"], [\"y_RB\"], [\"y_RB\"], [\"y_MB\"], [\"y_RM\"], [\"y_MM\"], [\"y_RB\"], [\"y_MB\"], [\"y_MB\"], [\"y_MB\"], [\"y_RM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RT\"], [\"y_RB\"], [\"y_RT\"], [\"y_RB\"], [\"y_MB\"], [\"y_MB\"], [\"y_MB\"], [\"y_RB\"], [\"y_RM\"], [\"y_MM\"], [\"y_MM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RM\"], [\"y_RT\"], [\"y_RB\"], [\"y_MM\"], [\"y_MM\"], [\"y_RT\"], [\"y_MM\"], [\"y_FM\"], [\"y_MB\"], [\"y_MB\"], [\"y_RT\"], [\"y_RT\"], [\"y_FM\"], [\"y_MM\"], [\"y_RM\"], [\"y_FM\"], [\"y_MB\"], [\"y_FM\"], [\"y_RM\"], [\"y_RM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RB\"], [\"y_RT\"], [\"y_RB\"], [\"y_MB\"], [\"y_MB\"], [\"y_MM\"], [\"y_FM\"], [\"y_MM\"], [\"y_MM\"], [\"y_FM\"], [\"y_FM\"], [\"y_RT\"], [\"y_RM\"], [\"y_RM\"], [\"y_RB\"], [\"y_RT\"], [\"y_FM\"], [\"y_FM\"], [\"y_MB\"], [\"y_MM\"], [\"y_FM\"], [\"y_FM\"], [\"y_RB\"], [\"y_MM\"], [\"y_FM\"], [\"y_MB\"], [\"y_RM\"], [\"y_FM\"], [\"y_RT\"], [\"y_RT\"], [\"y_FM\"], [\"y_RT\"], [\"y_RM\"], [\"y_RT\"], [\"y_FM\"], [\"y_MB\"], [\"y_RM\"], [\"y_FM\"], [\"y_FM\"], [\"y_RB\"], [\"y_FM\"], [\"y_MM\"], [\"y_FM\"], [\"y_MM\"], [\"y_RT\"], [\"y_FM\"], [\"y_FM\"], [\"y_FM\"], [\"y_RB\"], [\"y_MT\"], [\"y_MM\"], [\"y_RM\"], [\"y_RM\"], [\"y_MB\"], [\"y_FM\"], [\"y_MM\"], [\"y_RM\"], [\"y_RB\"], [\"y_RB\"], [\"y_MM\"], [\"y_RT\"], [\"y_RB\"], [\"y_MT\"], [\"y_MT\"], [\"y_RB\"], [\"y_MT\"], [\"y_RM\"], [\"y_FM\"], [\"y_RB\"], [\"y_MB\"], [\"y_MB\"], [\"y_FM\"], [\"y_RT\"], [\"y_RT\"], [\"y_MM\"], [\"y_RT\"], [\"y_RT\"], [\"y_RT\"], [\"y_MB\"], [\"y_MT\"], [\"y_RM\"], [\"y_MT\"], [\"y_MB\"], [\"y_RT\"], [\"y_MT\"], [\"y_MT\"], [\"y_RM\"], [\"y_RM\"], [\"y_FM\"], [\"y_MT\"], [\"y_MT\"], [\"y_FM\"], [\"y_FM\"], [\"y_MT\"], [\"y_RB\"], [\"y_MB\"], [\"y_MT\"], [\"y_MB\"], [\"y_MT\"], [\"y_FM\"], [\"y_RM\"], [\"y_MB\"], [\"y_RT\"], [\"y_FM\"], [\"y_MT\"], [\"y_MT\"], [\"y_RT\"], [\"y_RM\"], [\"y_MB\"], [\"y_RM\"], [\"y_MT\"], [\"y_RT\"], [\"y_MB\"], [\"y_FM\"], [\"y_MT\"], [\"y_FM\"], [\"y_MB\"], [\"y_MT\"], [\"y_MB\"], [\"y_MB\"], [\"y_MT\"], [\"y_MM\"], [\"y_FM\"], [\"y_MT\"], [\"y_RM\"], [\"y_MT\"], [\"y_RM\"], [\"y_MT\"], [\"y_MT\"], [\"y_RT\"], [\"y_FM\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_FT\"], [\"y_MT\"], [\"y_RT\"], [\"y_RT\"], [\"y_MT\"], [\"y_RT\"], [\"y_FT\"], [\"y_MT\"], [\"y_FT\"], [\"y_RT\"], [\"y_RM\"], [\"y_RT\"], [\"y_FT\"], [\"y_MT\"], [\"y_MT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_RT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}<br>DATA_y=%{customdata[0]}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_FTX_MTX_RM\", \"X_FMX_MTX_RM\", \"X_MTX_RMX_RB\", \"X_MTX_MBX_RM\", \"X_MTX_MMX_RT\", \"X_MMX_MBX_RT\", \"X_FTX_FMX_MB\", \"X_MTX_RTX_RM\", \"X_FMX_MBX_RM\", \"X_FTX_MTX_RT\", \"X_FMX_MTX_RT\", \"X_MBX_RTX_RM\", \"X_MTX_MBX_RT\", \"X_FMX_MTX_MB\", \"X_MTX_RTX_RB\", \"X_MTX_MBX_RT\", \"X_MMX_RTX_RB\", \"X_MTX_RTX_RB\", \"X_MBX_RTX_RB\", \"X_FMX_MBX_RT\", \"X_FTX_MMX_RM\", \"X_FTX_FMX_RT\", \"X_FMX_MTX_RT\", \"X_FTX_MMX_RB\", \"X_FMX_MBX_RM\", \"X_MTX_RTX_RM\", \"X_FTX_FMX_RM\", \"X_FTX_MMX_RT\", \"X_FMX_MBX_RT\", \"X_FMX_RTX_RM\", \"X_MMX_RMX_RB\", \"X_FMX_MMX_RT\", \"X_MMX_RTX_RB\", \"X_FTX_MTX_MM\", \"X_FTX_MMX_RM\", \"X_FMX_MMX_RM\", \"X_FTX_MTX_RB\", \"X_MMX_RTX_RM\", \"X_FMX_MTX_MM\", \"X_FTX_MMX_RT\", \"X_MTX_RMX_RB\", \"X_FMX_MMX_RT\", \"X_FTX_MMX_RT\", \"X_MTX_MMX_RM\", \"X_FTX_RTX_RM\", \"X_FTX_MTX_RT\", \"X_FMX_MTX_RM\", \"X_FMX_MTX_RM\", \"X_FTX_FMX_RT\", \"X_FTX_MTX_MB\", \"X_MTX_MBX_RT\", \"X_FTX_MTX_RT\", \"X_FMX_MMX_RT\", \"X_MTX_MMX_RB\", \"X_MTX_MBX_RB\", \"X_FTX_MBX_RT\", \"X_MBX_RTX_RB\", \"X_FTX_MTX_RM\", \"X_FTX_MBX_RM\", \"X_FTX_MMX_RM\", \"X_FTX_MTX_RM\", \"X_MMX_RTX_RM\", \"X_FMX_MMX_RM\", \"X_MMX_RMX_RB\", \"X_MMX_MBX_RT\", \"X_FTX_RTX_RB\", \"X_FTX_RTX_RM\", \"X_MBX_RMX_RB\", \"X_FTX_FMX_MB\", \"X_FMX_MBX_RB\", \"X_FTX_MBX_RT\", \"X_MTX_MMX_RM\", \"X_MBX_RTX_RM\", \"X_FTX_FMX_MT\", \"X_FMX_RMX_RB\", \"X_FTX_FMX_RM\", \"X_FMX_MBX_RT\", \"X_FTX_MTX_RB\", \"X_FTX_MTX_RM\", \"X_MTX_MMX_RT\", \"X_FMX_MMX_RM\", \"X_FMX_MTX_RM\", \"X_FTX_MMX_MB\", \"X_MTX_MBX_RB\", \"X_FMX_MTX_RT\", \"X_MTX_MBX_RB\", \"X_FMX_MTX_RB\", \"X_FTX_MMX_RM\", \"X_FTX_MMX_RB\", \"X_MMX_MBX_RB\", \"X_MTX_MBX_RM\", \"X_FMX_RTX_RM\", \"X_FTX_FMX_MM\", \"X_FTX_MBX_RM\", \"X_MMX_MBX_RM\", \"X_FTX_MMX_RB\", \"X_FTX_FMX_MM\", \"X_FMX_RTX_RB\", \"X_FTX_MTX_MM\", \"X_FTX_FMX_RB\", \"X_FTX_MBX_RM\", \"X_MTX_MMX_RB\", \"X_MMX_RTX_RB\", \"X_MMX_MBX_RM\", \"X_FMX_MMX_RB\", \"X_MTX_MMX_RB\", \"X_MTX_MMX_RT\", \"X_FTX_RMX_RB\", \"X_FTX_MTX_RM\", \"X_MMX_MBX_RB\", \"X_FTX_MTX_RB\", \"X_FTX_RMX_RB\", \"X_MTX_RTX_RB\", \"X_MTX_MBX_RM\", \"X_FMX_MTX_MB\", \"X_FTX_FMX_RM\", \"X_FTX_MMX_RT\", \"X_MTX_RTX_RB\", \"X_FMX_RTX_RB\", \"X_MTX_RMX_RB\", \"X_MBX_RMX_RB\", \"X_FMX_MBX_RM\", \"X_MMX_RMX_RB\", \"X_MTX_MBX_RM\", \"X_FTX_MTX_RB\", \"X_MTX_MMX_RB\", \"X_RTX_RMX_RB\", \"X_FMX_MTX_RT\", \"X_FTX_FMX_RB\", \"X_MBX_RMX_RB\", \"X_FTX_RMX_RB\", \"X_FTX_FMX_MT\", \"X_FTX_RTX_RM\", \"X_FTX_FMX_RT\", \"X_FTX_MTX_RT\", \"X_RTX_RMX_RB\", \"X_FMX_RMX_RB\", \"X_MTX_MMX_RT\", \"X_FTX_RTX_RB\", \"X_MTX_MBX_RT\", \"X_MTX_MMX_RM\", \"X_MMX_MBX_RM\", \"X_FTX_MBX_RB\", \"X_FTX_MTX_MM\", \"X_FTX_MBX_RB\", \"X_FTX_FMX_RB\", \"X_FTX_MBX_RB\", \"X_FMX_MTX_RB\", \"X_FMX_MTX_MM\", \"X_FMX_MMX_MB\", \"X_FTX_MMX_MB\", \"X_FTX_MTX_RT\", \"X_MTX_MBX_RB\", \"X_FMX_MTX_MM\", \"X_FTX_MMX_RT\", \"X_FTX_MMX_RM\", \"X_FTX_MTX_MM\", \"X_FMX_MMX_RB\", \"X_FMX_MTX_RB\", \"X_MTX_MMX_MB\", \"X_MTX_MMX_MB\", \"X_FMX_RTX_RB\", \"X_FTX_FMX_MT\", \"X_MTX_MMX_RM\", \"X_MMX_MBX_RB\", \"X_FTX_MMX_RB\", \"X_FTX_MBX_RT\", \"X_MTX_RMX_RB\", \"X_FTX_FMX_RB\", \"X_FMX_MMX_RB\", \"X_FMX_MMX_RB\", \"X_FMX_MMX_RT\", \"X_FTX_FMX_MM\", \"X_FTX_MMX_RB\", \"X_FTX_FMX_RT\", \"X_FMX_MTX_RB\", \"X_FMX_MMX_RM\", \"X_FTX_MMX_MB\", \"X_FMX_MBX_RB\", \"X_FMX_MMX_MB\", \"X_MMX_RTX_RM\", \"X_MMX_MBX_RT\", \"X_MMX_MBX_RB\", \"X_MMX_MBX_RT\", \"X_MBX_RTX_RB\", \"X_MMX_RTX_RM\", \"X_FTX_MTX_MB\", \"X_FMX_RTX_RM\", \"X_MMX_RMX_RB\", \"X_FTX_RTX_RM\", \"X_FTX_FMX_MM\", \"X_FTX_MTX_MB\", \"X_FTX_FMX_MB\", \"X_FTX_RMX_RB\", \"X_FTX_MBX_RB\", \"X_FTX_MBX_RT\", \"X_FMX_MMX_MB\", \"X_FMX_MBX_RM\", \"X_FTX_MTX_RB\", \"X_MTX_MMX_MB\", \"X_FTX_FMX_RM\", \"X_FTX_MMX_MB\", \"X_FTX_FMX_MB\", \"X_FMX_MBX_RB\", \"X_FMX_RMX_RB\", \"X_MTX_RTX_RM\", \"X_MMX_RTX_RB\", \"X_FTX_MBX_RM\", \"X_MTX_RTX_RM\", \"X_MBX_RTX_RB\", \"X_FTX_RTX_RB\", \"X_RTX_RMX_RB\", \"X_FMX_MBX_RT\", \"X_FTX_RTX_RB\", \"X_MMX_MBX_RM\", \"X_FMX_RMX_RB\", \"X_FMX_MTX_MB\", \"X_FTX_MBX_RM\", \"X_FTX_FMX_MT\", \"X_MBX_RMX_RB\", \"X_FTX_FMX_RB\", \"X_FMX_MTX_MM\", \"X_MBX_RTX_RM\", \"X_FMX_MBX_RB\", \"X_FTX_MBX_RB\", \"X_FTX_RTX_RB\", \"X_RTX_RMX_RB\", \"X_MBX_RTX_RM\", \"X_FMX_RTX_RB\", \"X_FMX_RTX_RM\", \"X_FMX_MTX_RM\", \"X_FTX_MBX_RT\", \"X_FTX_MTX_MM\", \"X_FTX_MTX_MB\", \"X_FTX_FMX_RT\", \"X_MTX_MMX_MB\", \"X_FMX_MTX_MB\", \"X_FTX_RTX_RM\", \"X_FMX_RTX_RM\", \"X_FTX_FMX_MT\", \"X_FTX_MTX_MB\", \"X_FMX_MMX_MB\", \"X_FMX_MMX_RT\", \"X_FTX_FMX_RM\", \"X_FTX_RMX_RB\", \"X_FMX_MBX_RT\", \"X_FMX_MTX_RT\", \"X_FMX_MBX_RB\", \"X_FTX_MMX_MB\", \"X_FTX_FMX_MB\", \"X_FMX_MMX_RB\", \"X_FMX_RMX_RB\", \"X_FMX_MMX_RM\", \"X_FMX_MTX_RB\", \"X_FTX_FMX_MM\", \"X_FMX_MTX_MM\", \"X_MTX_MMX_RB\", \"X_FMX_MMX_MB\", \"X_MMX_RMX_RB\", \"X_MTX_RTX_RB\", \"X_FMX_RTX_RB\", \"X_MMX_MBX_RB\", \"X_MBX_RTX_RB\", \"X_FMX_MTX_MB\", \"X_FMX_MBX_RM\", \"X_MMX_RTX_RB\", \"X_MTX_RMX_RB\", \"X_MBX_RMX_RB\", \"X_RTX_RMX_RB\", \"X_MMX_RTX_RM\", \"X_MTX_MBX_RT\", \"X_MMX_MBX_RT\", \"X_MTX_MBX_RM\", \"X_MBX_RTX_RM\", \"X_MTX_MBX_RB\", \"X_MTX_MMX_RM\", \"X_MTX_RTX_RM\", \"X_MMX_MBX_RM\", \"X_MTX_MMX_RT\", \"X_MTX_MMX_MB\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.48141456842422486, 0.4919242650270462, 0.49985400438308714, 0.5154469519853592, 0.5282787322998047, 0.5319921433925628, 0.5676087260246276, 0.5729000896215439, 0.5739515453577042, 0.5743899673223496, 0.5784668952226639, 0.585852575302124, 0.5861369162797928, 0.590923348069191, 0.6021129071712494, 0.6155367761850357, 0.6264600634574891, 0.6266163468360901, 0.6296187043190002, 0.6313661098480224, 0.6346156120300293, 0.6352699339389801, 0.6491017788648605, 0.6498382568359375, 0.6594168573617936, 0.6636917531490326, 0.6693149477243423, 0.6708148628473282, 0.6741507291793823, 0.6759009242057801, 0.6786793053150177, 0.6824382156133652, 0.6989714592695236, 0.6994934439659118, 0.6996729165315628, 0.7031470030546189, 0.7067836940288543, 0.7086247384548188, 0.7102572947740555, 0.711008271574974, 0.711983585357666, 0.7135150313377381, 0.7218107044696808, 0.7235266119241714, 0.7241043239831925, 0.730345857143402, 0.7339640885591507, 0.7340444922447205, 0.7348278105258942, 0.7370169579982757, 0.7443216800689697, 0.7461553752422333, 0.7518178045749664, 0.7572792887687683, 0.7648503959178925, 0.7653055787086487, 0.767118501663208, 0.7685153126716614, 0.7687268614768982, 0.7706968903541564, 0.7715485036373139, 0.7741755425930024, 0.7803786873817444, 0.7812124192714691, 0.7818948090076446, 0.7857086598873139, 0.7875349462032318, 0.7911834597587586, 0.7940728724002838, 0.8003992736339569, 0.8004673004150391, 0.8039781391620636, 0.8049734830856323, 0.8069062173366547, 0.8085688412189483, 0.8095706343650818, 0.8106918662786484, 0.8172483801841736, 0.8191404163837432, 0.8192681312561035, 0.8194677710533143, 0.8203304171562195, 0.8214966833591462, 0.8222362607717514, 0.8237928509712219, 0.8255265653133392, 0.8258308827877044, 0.8277995109558105, 0.8320986926555634, 0.8342327296733856, 0.8442654073238373, 0.8515551388263702, 0.8526480853557586, 0.8556422770023346, 0.8619413018226624, 0.8637095540761948, 0.8653838068246842, 0.8663781940937042, 0.8708916783332825, 0.8729348361492157, 0.8739145457744598, 0.8757041752338409, 0.8769107818603515, 0.8803076833486557, 0.8812305092811584, 0.8845209330320358, 0.8871417164802551, 0.887708044052124, 0.8889604151248932, 0.8926316201686859, 0.8930773675441742, 0.8936754882335662, 0.8966548264026641, 0.8995490074157715, 0.899726790189743, 0.9007088720798493, 0.9054917991161346, 0.9068316221237183, 0.9077925115823746, 0.9091208040714264, 0.9110295653343201, 0.9128207862377167, 0.9207554697990418, 0.9208679974079133, 0.9226683437824249, 0.9229367315769196, 0.9286008238792419, 0.9332553029060364, 0.9362728476524353, 0.9427432298660279, 0.9440505683422089, 0.9445780932903289, 0.946785968542099, 0.9478971004486084, 0.9515971660614013, 0.953426718711853, 0.9564993917942047, 0.9570398449897766, 0.9577507257461548, 0.9595872581005096, 0.9597787976264953, 0.965563702583313, 0.9660838782787323, 0.9692094206809998, 0.9733841419219971, 0.9770181894302368, 0.9775696635246277, 0.9794941306114197, 0.9804146111011505, 0.9901897490024567, 0.9916864454746246, 0.9922923386096955, 0.9992977440357208, 0.9996809184551239, 1.0003697276115417, 1.0032496094703673, 1.006624311208725, 1.008823847770691, 1.0089041233062743, 1.0176614284515382, 1.0188823461532592, 1.0204002261161804, 1.0209195554256438, 1.0245101392269134, 1.028370201587677, 1.029696959257126, 1.0313876688480377, 1.0323630630970002, 1.0373864948749543, 1.0394194543361663, 1.0407220780849458, 1.0500174462795258, 1.0518385350704194, 1.0600169360637666, 1.0628319442272187, 1.0662187457084655, 1.0692068040370941, 1.073938286304474, 1.0739587604999543, 1.0765615224838256, 1.0792296171188354, 1.079262524843216, 1.0843814373016358, 1.0873755931854248, 1.0938368320465088, 1.0966349482536315, 1.099531537294388, 1.1005134642124177, 1.1012196302413941, 1.1180821061134338, 1.1210790276527405, 1.1226681292057037, 1.124195009469986, 1.1271689295768739, 1.1292606830596923, 1.13150395154953, 1.1346913695335388, 1.1386876106262207, 1.1395806431770326, 1.1543962478637695, 1.155196213722229, 1.1595117926597596, 1.1636512458324433, 1.1724614799022675, 1.1788223505020141, 1.1813231885433197, 1.1854787111282348, 1.1954802393913269, 1.2040932774543762, 1.2108080625534057, 1.2217285454273223, 1.231146550178528, 1.2543603837490083, 1.2590919375419616, 1.263730102777481, 1.2766409397125245, 1.2934688746929168, 1.3020593523979187, 1.3021220803260802, 1.3306286096572877, 1.3487169861793518, 1.3599364161491394, 1.3669739365577698, 1.3766312420368194, 1.3820177912712097, 1.3827694296836852, 1.3959777712821961, 1.4061773538589477, 1.4085176944732667, 1.412057363986969, 1.4148869037628173, 1.4252679824829102, 1.4579180121421813, 1.460636830329895, 1.4638778686523437, 1.4669180989265442, 1.4885076522827148, 1.4914325952529908, 1.4987666487693787, 1.5017580389976501, 1.504037892818451, 1.5091780066490172, 1.5102976560592651, 1.5107017040252686, 1.516019070148468, 1.525921595096588, 1.53171204328537, 1.5382004261016846, 1.5470864534378053, 1.5562897086143495, 1.5586414337158203, 1.560234582424164, 1.5610921502113342, 1.562300431728363, 1.5697898030281068, 1.5781718373298645, 1.5820372700691223, 1.6082055807113647, 1.614736223220825, 1.6182412981987, 1.6247448325157166, 1.6614161252975463, 1.6759677052497863, 1.6862996339797973, 1.691315197944641, 1.7008501648902894, 1.7306612253189086, 1.7368866562843324, 1.7887741804122925, 1.8115325927734376, 1.8299251079559327, 1.836233103275299, 1.8623785138130189, 1.8673439264297484, 1.871997630596161, 1.958996546268463, 2.01796612739563, 2.039247715473175, 2.0748077750205995, 2.07482488155365], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8cb599f3-6f44-4af8-b589-6126176e0fdd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0dcf14c-83f1-4721-a306-da9e85fb2b5e",
        "id": "guftmFwAXeOH"
      },
      "source": [
        "CombResultsSortedgrouped3 = CombResultsSorted3.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedgroupedsortedMF3 = CombResultsSortedgrouped3.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedgroupedsortedMF3.to_csv('CombResultsSortedgroupedsortedMF3.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedgroupedsortedMF3.csv\")\n",
        "CombResultsSortedgroupedsortedMF3"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cc15bbb2-843c-41a3-8028-ac12436b1197\", \"CombResultsSortedgroupedsortedMF3.csv\", 1796)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RM</th>\n",
              "      <td>0.745916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RM</th>\n",
              "      <td>0.787207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RT</th>\n",
              "      <td>0.798956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RT</th>\n",
              "      <td>0.801899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RM</th>\n",
              "      <td>0.839030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RB</th>\n",
              "      <td>0.887072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RB</th>\n",
              "      <td>0.895872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RT</th>\n",
              "      <td>0.903266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RTX_RB</th>\n",
              "      <td>0.930091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RT</th>\n",
              "      <td>0.941617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RT</th>\n",
              "      <td>0.947102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RT</th>\n",
              "      <td>0.968941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RMX_RB</th>\n",
              "      <td>0.976797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RT</th>\n",
              "      <td>0.979298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RM</th>\n",
              "      <td>0.986658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RM</th>\n",
              "      <td>0.995238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RM</th>\n",
              "      <td>0.999165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MM</th>\n",
              "      <td>1.000827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RB</th>\n",
              "      <td>1.004496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RM</th>\n",
              "      <td>1.008502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RM</th>\n",
              "      <td>1.009098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RTX_RM</th>\n",
              "      <td>1.013588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RTX_RB</th>\n",
              "      <td>1.017734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RMX_RB</th>\n",
              "      <td>1.019321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RT</th>\n",
              "      <td>1.030786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RB</th>\n",
              "      <td>1.034466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MB</th>\n",
              "      <td>1.041164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RT</th>\n",
              "      <td>1.053307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RB</th>\n",
              "      <td>1.056782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RT</th>\n",
              "      <td>1.063352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RMX_RB</th>\n",
              "      <td>1.073724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RTX_RB</th>\n",
              "      <td>1.075470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RB</th>\n",
              "      <td>1.085663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RB</th>\n",
              "      <td>1.088550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MM</th>\n",
              "      <td>1.092148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RTX_RM</th>\n",
              "      <td>1.094039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RM</th>\n",
              "      <td>1.094158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RB</th>\n",
              "      <td>1.100206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RB</th>\n",
              "      <td>1.105767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RTX_RM</th>\n",
              "      <td>1.107759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MT</th>\n",
              "      <td>1.115257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MB</th>\n",
              "      <td>1.118744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RTX_RB</th>\n",
              "      <td>1.121410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MM</th>\n",
              "      <td>1.125692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RTX_RM</th>\n",
              "      <td>1.127995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RMX_RB</th>\n",
              "      <td>1.142494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RMX_RB</th>\n",
              "      <td>1.156153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RTX_RB</th>\n",
              "      <td>1.165567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MB</th>\n",
              "      <td>1.184778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MB</th>\n",
              "      <td>1.191785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RB</th>\n",
              "      <td>1.192330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RM</th>\n",
              "      <td>1.202158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RTX_RM</th>\n",
              "      <td>1.206264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RTX_RMX_RB</th>\n",
              "      <td>1.259585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MB</th>\n",
              "      <td>1.263765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MB</th>\n",
              "      <td>1.346537</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Test Loss\n",
              "DATA_X                 \n",
              "X_FTX_MTX_RM   0.745916\n",
              "X_FTX_MMX_RM   0.787207\n",
              "X_FTX_MTX_RT   0.798956\n",
              "X_FTX_MMX_RT   0.801899\n",
              "X_FMX_MTX_RM   0.839030\n",
              "X_FTX_MMX_RB   0.887072\n",
              "X_FTX_MTX_RB   0.895872\n",
              "X_FMX_MTX_RT   0.903266\n",
              "X_MTX_RTX_RB   0.930091\n",
              "X_FMX_MMX_RT   0.941617\n",
              "X_MTX_MBX_RT   0.947102\n",
              "X_FTX_FMX_RT   0.968941\n",
              "X_MTX_RMX_RB   0.976797\n",
              "X_FMX_MBX_RT   0.979298\n",
              "X_FMX_MMX_RM   0.986658\n",
              "X_FMX_MBX_RM   0.995238\n",
              "X_FTX_MBX_RM   0.999165\n",
              "X_FTX_MTX_MM   1.000827\n",
              "X_MTX_MMX_RB   1.004496\n",
              "X_MTX_MBX_RM   1.008502\n",
              "X_FTX_FMX_RM   1.009098\n",
              "X_FTX_RTX_RM   1.013588\n",
              "X_MMX_RTX_RB   1.017734\n",
              "X_MMX_RMX_RB   1.019321\n",
              "X_FTX_MBX_RT   1.030786\n",
              "X_FTX_FMX_RB   1.034466\n",
              "X_FTX_FMX_MB   1.041164\n",
              "X_MTX_MMX_RT   1.053307\n",
              "X_MTX_MBX_RB   1.056782\n",
              "X_MMX_MBX_RT   1.063352\n",
              "X_FTX_RMX_RB   1.073724\n",
              "X_MBX_RTX_RB   1.075470\n",
              "X_FTX_MBX_RB   1.085663\n",
              "X_FMX_MTX_RB   1.088550\n",
              "X_FTX_FMX_MM   1.092148\n",
              "X_MMX_RTX_RM   1.094039\n",
              "X_MTX_MMX_RM   1.094158\n",
              "X_MMX_MBX_RB   1.100206\n",
              "X_FMX_MMX_RB   1.105767\n",
              "X_FMX_RTX_RM   1.107759\n",
              "X_FTX_FMX_MT   1.115257\n",
              "X_FTX_MMX_MB   1.118744\n",
              "X_FTX_RTX_RB   1.121410\n",
              "X_FMX_MTX_MM   1.125692\n",
              "X_MTX_RTX_RM   1.127995\n",
              "X_MBX_RMX_RB   1.142494\n",
              "X_FMX_RMX_RB   1.156153\n",
              "X_FMX_RTX_RB   1.165567\n",
              "X_FTX_MTX_MB   1.184778\n",
              "X_FMX_MTX_MB   1.191785\n",
              "X_FMX_MBX_RB   1.192330\n",
              "X_MMX_MBX_RM   1.202158\n",
              "X_MBX_RTX_RM   1.206264\n",
              "X_RTX_RMX_RB   1.259585\n",
              "X_FMX_MMX_MB   1.263765\n",
              "X_MTX_MMX_MB   1.346537"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hIeajestYw6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0390695-3dcc-4888-c2d9-f33a35a17638",
        "id": "iMedS33YY6DB"
      },
      "source": [
        "#Combinations of 4 Sensors, therefore 28 features, 28 inputs\n",
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "inp_shp = 28\n",
        "n_sensors = 4\n",
        "\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "modelG(inp_shp)\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "my_dictMF4 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), n_sensors):\n",
        "  kX1, kX2, kX3, kX4 = combo[0][0], combo[1][0], combo[2][0], combo[3][0]\n",
        "  vX1, vX2, vX3, vX4 = combo[0][1], combo[1][1], combo[2][1], combo[3][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:] or ky[-2:] == kX4[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, kx4 = {kX4}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF4 = evaldata(n_sensors, X_in1=vX1, X_in2=vX2, X_in3=vX3, X_in4=vX4, Y_in=vy, traindata1 = kX1, traindata2 = kX2, traindata3 = kX3, traindata4 = kX4, testdata = ky)\n",
        "      TestLossTotal = resultsMF4[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF4[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF4[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}, {kX2}, {kX3}, {kX4} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF4[\"DATA_X\"].append(kX1 + kX2 + kX3 + kX4)\n",
        "    my_dictMF4[\"DATA_y\"].append(ky)\n",
        "    my_dictMF4[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model.load_weights('model.h5')\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8304283499717713, and Avg Validation Loss (mae) is 1.207629179954529\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1940179347991944\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3682222843170166, and Avg Validation Loss (mae) is 1.4621190190315247\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4655804991722108\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.022467851638794, and Avg Validation Loss (mae) is 1.0495033323764802\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0413529038429261\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.829106080532074, and Avg Validation Loss (mae) is 0.7865472495555877\n",
            "After 10 runs; Avg Test Loss (mae) is 0.805118978023529\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.604348087310791, and Avg Validation Loss (mae) is 0.6622622042894364\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6816399157047272\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4351226925849914, and Avg Validation Loss (mae) is 1.5356088995933532\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5606137752532958\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1320109367370605, and Avg Validation Loss (mae) is 1.2503022074699401\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2504640817642212\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8690062582492828, and Avg Validation Loss (mae) is 1.0022067725658417\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9820497035980225\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6925632536411286, and Avg Validation Loss (mae) is 0.676560389995575\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6734501600265503\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7660161137580872, and Avg Validation Loss (mae) is 0.7479329466819763\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7404148936271667\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6449335068464279, and Avg Validation Loss (mae) is 0.7147689819335937\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7326854228973388\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6425757348537445, and Avg Validation Loss (mae) is 0.6510512828826904\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6468014746904374\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5110624969005585, and Avg Validation Loss (mae) is 0.6552070766687393\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6564424067735672\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7451592147350311, and Avg Validation Loss (mae) is 0.7310940325260162\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7335404813289642\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8051732182502747, and Avg Validation Loss (mae) is 0.9049609541893006\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8964014530181885\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5853638678789139, and Avg Validation Loss (mae) is 0.5955377072095871\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5980755776166916\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6371205508708954, and Avg Validation Loss (mae) is 0.7763161957263947\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7907719910144806\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7737223923206329, and Avg Validation Loss (mae) is 0.8204129934310913\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8228475630283356\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.020141565799713, and Avg Validation Loss (mae) is 1.0202028155326843\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0182160317897797\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8566205203533173, and Avg Validation Loss (mae) is 0.9059147536754608\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9121588945388794\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0898687541484833, and Avg Validation Loss (mae) is 1.0821419537067414\n",
            "After 10 runs; Avg Test Loss (mae) is 1.105036860704422\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4444251775741577, and Avg Validation Loss (mae) is 1.412480354309082\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4240432739257813\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1231465101242066, and Avg Validation Loss (mae) is 1.2261090874671936\n",
            "After 10 runs; Avg Test Loss (mae) is 1.218846595287323\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8285599708557129, and Avg Validation Loss (mae) is 0.7367089807987213\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7610987544059753\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9792426347732544, and Avg Validation Loss (mae) is 0.9555894196033478\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9632059514522553\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.707067608833313, and Avg Validation Loss (mae) is 0.7394845485687256\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7401647627353668\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5837011694908142, and Avg Validation Loss (mae) is 0.6187264919281006\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6242697775363922\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5688534021377564, and Avg Validation Loss (mae) is 0.5977403491735458\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5761305183172226\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0021144568920135, and Avg Validation Loss (mae) is 1.0949070155620575\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1006887018680573\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6949349164962768, and Avg Validation Loss (mae) is 0.807074910402298\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7917083501815796\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7827588796615601, and Avg Validation Loss (mae) is 0.8441840827465057\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8397876739501953\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6524819135665894, and Avg Validation Loss (mae) is 0.7122808754444122\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7037221640348434\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1340739011764527, and Avg Validation Loss (mae) is 1.1794815540313721\n",
            "After 10 runs; Avg Test Loss (mae) is 1.16432363986969\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7812962263822556, and Avg Validation Loss (mae) is 0.7948406100273132\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7949518293142319\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0344547271728515, and Avg Validation Loss (mae) is 1.1566390156745912\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1321434259414673\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7918111205101013, and Avg Validation Loss (mae) is 0.7488185942173005\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7673852562904357\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0240873098373413, and Avg Validation Loss (mae) is 1.0505999982357026\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0433352828025817\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5803621888160706, and Avg Validation Loss (mae) is 0.6639274150133133\n",
            "After 10 runs; Avg Test Loss (mae) is 0.652310261130333\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6206886768341064, and Avg Validation Loss (mae) is 0.6853376924991608\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6892980128526688\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6916553676128387, and Avg Validation Loss (mae) is 0.7051423490047455\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7047994315624238\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.162286788225174, and Avg Validation Loss (mae) is 1.3312001466751098\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3045071244239808\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5343918353319168, and Avg Validation Loss (mae) is 0.5924697577953338\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6080870211124421\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8270975947380066, and Avg Validation Loss (mae) is 0.8866060733795166\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8694509923458099\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5969226330518722, and Avg Validation Loss (mae) is 0.7841335624456406\n",
            "After 10 runs; Avg Test Loss (mae) is 0.788896444439888\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1596617579460144, and Avg Validation Loss (mae) is 1.2077657461166382\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2280493021011352\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6068897753953933, and Avg Validation Loss (mae) is 0.615017780661583\n",
            "After 10 runs; Avg Test Loss (mae) is 0.61023308634758\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.063734197616577, and Avg Validation Loss (mae) is 1.010617870092392\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0104422509670257\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8176960289478302, and Avg Validation Loss (mae) is 0.8167912721633911\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8289529621601105\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.391073429584503, and Avg Validation Loss (mae) is 1.6327965259552002\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6437848091125489\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8055776715278625, and Avg Validation Loss (mae) is 0.8756616473197937\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8846574485301971\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0254360020160675, and Avg Validation Loss (mae) is 1.030179101228714\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0355250239372253\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6549704164266587, and Avg Validation Loss (mae) is 0.6201993107795716\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6168471068143845\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3586624145507813, and Avg Validation Loss (mae) is 1.230716836452484\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2260679364204408\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8565294027328492, and Avg Validation Loss (mae) is 0.8450726687908172\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8469954013824463\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0162596344947814, and Avg Validation Loss (mae) is 1.0487108051776886\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0445774972438813\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.618078762292862, and Avg Validation Loss (mae) is 0.737334030866623\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7486312210559845\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4054600834846496, and Avg Validation Loss (mae) is 1.4746504545211792\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5200745463371277\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8075217187404633, and Avg Validation Loss (mae) is 0.7622000098228454\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7368160665035248\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9512186169624328, and Avg Validation Loss (mae) is 1.095149999856949\n",
            "After 10 runs; Avg Test Loss (mae) is 1.064910876750946\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7771975994110107, and Avg Validation Loss (mae) is 0.8651340365409851\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8510358512401581\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8214832603931427, and Avg Validation Loss (mae) is 0.8570319950580597\n",
            "After 10 runs; Avg Test Loss (mae) is 0.861862200498581\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.299814021587372, and Avg Validation Loss (mae) is 1.3997923016548157\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3926246523857118\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9699488461017609, and Avg Validation Loss (mae) is 1.040601259469986\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0475331127643586\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8834410667419433, and Avg Validation Loss (mae) is 1.0536016702651978\n",
            "After 10 runs; Avg Test Loss (mae) is 1.068821656703949\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8048898279666901, and Avg Validation Loss (mae) is 0.7528941333293915\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7433302879333497\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7134041965007782, and Avg Validation Loss (mae) is 0.743633234500885\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7498057007789611\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5311498939990997, and Avg Validation Loss (mae) is 0.5508108973503113\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5520252704620361\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6298314750194549, and Avg Validation Loss (mae) is 0.7536055833101273\n",
            "After 10 runs; Avg Test Loss (mae) is 0.747004035115242\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8040768027305603, and Avg Validation Loss (mae) is 0.6947928011417389\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7024003207683563\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.725893634557724, and Avg Validation Loss (mae) is 0.7315028369426727\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7111395120620727\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.716714721918106, and Avg Validation Loss (mae) is 0.7893296301364898\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7715468347072602\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.644047600030899, and Avg Validation Loss (mae) is 0.7929569900035858\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7884241431951523\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7290755808353424, and Avg Validation Loss (mae) is 0.6708162188529968\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6658670604228973\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7005081355571747, and Avg Validation Loss (mae) is 0.6489723682403564\n",
            "After 10 runs; Avg Test Loss (mae) is 0.641539016366005\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.959450078010559, and Avg Validation Loss (mae) is 1.0231655299663545\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0109638929367066\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7527212619781494, and Avg Validation Loss (mae) is 0.8825447767972946\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8779925256967545\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8176748931407929, and Avg Validation Loss (mae) is 0.7972841024398803\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7718661963939667\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6032985061407089, and Avg Validation Loss (mae) is 0.6115283817052841\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5955235093832016\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6228977739810944, and Avg Validation Loss (mae) is 0.5663293659687042\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5719264894723892\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6983165562152862, and Avg Validation Loss (mae) is 0.7141342222690582\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6972301125526428\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7922763526439667, and Avg Validation Loss (mae) is 0.8536978840827942\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8628148257732391\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5057316035032272, and Avg Validation Loss (mae) is 0.45702971518039703\n",
            "After 10 runs; Avg Test Loss (mae) is 0.45317806899547575\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7843723356723785, and Avg Validation Loss (mae) is 0.7311026930809021\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7311005175113678\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6220565050840378, and Avg Validation Loss (mae) is 0.6223423540592193\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5991460800170898\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7779539287090301, and Avg Validation Loss (mae) is 0.8426856756210327\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8491374731063843\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6174452513456344, and Avg Validation Loss (mae) is 0.5644679725170135\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5906493127346039\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.916447538137436, and Avg Validation Loss (mae) is 0.8715748071670533\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8355596840381623\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8244692564010621, and Avg Validation Loss (mae) is 0.9188404381275177\n",
            "After 10 runs; Avg Test Loss (mae) is 0.923194533586502\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7911486983299255, and Avg Validation Loss (mae) is 0.8952688992023468\n",
            "After 10 runs; Avg Test Loss (mae) is 0.882059109210968\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5929745078086853, and Avg Validation Loss (mae) is 0.7895717442035675\n",
            "After 10 runs; Avg Test Loss (mae) is 0.787420180439949\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7184913218021393, and Avg Validation Loss (mae) is 0.8111680746078491\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8120190262794494\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.594480562210083, and Avg Validation Loss (mae) is 0.6150809109210968\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6118973672389985\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.796527773141861, and Avg Validation Loss (mae) is 0.8252322196960449\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8312719881534576\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5824815988540649, and Avg Validation Loss (mae) is 0.7274936974048615\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7182064592838288\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7343287885189056, and Avg Validation Loss (mae) is 0.749482923746109\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7420194149017334\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5204809159040451, and Avg Validation Loss (mae) is 0.6340031296014785\n",
            "After 10 runs; Avg Test Loss (mae) is 0.633814811706543\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8041714012622834, and Avg Validation Loss (mae) is 0.8866099953651428\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8806401789188385\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5913750499486923, and Avg Validation Loss (mae) is 0.6019321918487549\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6205780059099197\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7347933948040009, and Avg Validation Loss (mae) is 0.695932400226593\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6971578657627105\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7469700098037719, and Avg Validation Loss (mae) is 0.9645228832960129\n",
            "After 10 runs; Avg Test Loss (mae) is 0.970748245716095\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7799523591995239, and Avg Validation Loss (mae) is 0.7882547974586487\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7904458522796631\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9745834052562714, and Avg Validation Loss (mae) is 1.0935433447360992\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1068134248256682\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6144481897354126, and Avg Validation Loss (mae) is 0.7174781620502472\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7107647448778153\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6734601378440856, and Avg Validation Loss (mae) is 0.6701062202453614\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6822333693504333\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7420428097248077, and Avg Validation Loss (mae) is 0.7624806880950927\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7741055905818939\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9762772142887115, and Avg Validation Loss (mae) is 0.9819021463394165\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9769019305706024\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8236541330814362, and Avg Validation Loss (mae) is 0.7340376943349838\n",
            "After 10 runs; Avg Test Loss (mae) is 0.737662410736084\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6945775508880615, and Avg Validation Loss (mae) is 0.6186304211616516\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6292011260986328\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7316678404808045, and Avg Validation Loss (mae) is 0.8155810922384262\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8322987616062164\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0664869785308837, and Avg Validation Loss (mae) is 1.1548215568065643\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1447224378585816\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1077198028564452, and Avg Validation Loss (mae) is 1.2333614110946656\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2278103232383728\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7961369693279267, and Avg Validation Loss (mae) is 0.7190109431743622\n",
            "After 10 runs; Avg Test Loss (mae) is 0.698075932264328\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8044803440570831, and Avg Validation Loss (mae) is 0.8017244875431061\n",
            "After 10 runs; Avg Test Loss (mae) is 0.802299964427948\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9715078294277191, and Avg Validation Loss (mae) is 1.0368639409542084\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0365294456481933\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7342061817646026, and Avg Validation Loss (mae) is 0.7317997723817825\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7284363925457\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6225792050361634, and Avg Validation Loss (mae) is 0.7367971628904343\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7570182502269744\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7873358130455017, and Avg Validation Loss (mae) is 0.9401484668254853\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9229599416255951\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9858489155769348, and Avg Validation Loss (mae) is 0.9949599444866181\n",
            "After 10 runs; Avg Test Loss (mae) is 0.994964873790741\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8210792481899262, and Avg Validation Loss (mae) is 0.6541961669921875\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6576942831277848\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4963544994592667, and Avg Validation Loss (mae) is 0.5923307776451111\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6115810215473175\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7681962370872497, and Avg Validation Loss (mae) is 0.8321953654289246\n",
            "After 10 runs; Avg Test Loss (mae) is 0.82529336810112\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.080970686674118, and Avg Validation Loss (mae) is 1.2067964494228363\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1873818576335906\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7035871624946595, and Avg Validation Loss (mae) is 0.7230874389410019\n",
            "After 10 runs; Avg Test Loss (mae) is 0.712770962715149\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8019885540008544, and Avg Validation Loss (mae) is 0.9415986567735672\n",
            "After 10 runs; Avg Test Loss (mae) is 0.92898890376091\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9878294765949249, and Avg Validation Loss (mae) is 1.0642412841320037\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0832867622375488\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2877713561058044, and Avg Validation Loss (mae) is 1.3010088205337524\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3010459065437316\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6908744096755981, and Avg Validation Loss (mae) is 0.7520245105028153\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7464412301778793\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6604263961315155, and Avg Validation Loss (mae) is 0.7034198105335235\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6991054743528367\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9470495402812957, and Avg Validation Loss (mae) is 0.9952038347721099\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9987908720970153\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2322681903839112, and Avg Validation Loss (mae) is 1.24271502494812\n",
            "After 10 runs; Avg Test Loss (mae) is 1.282236063480377\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8210299432277679, and Avg Validation Loss (mae) is 0.9140616655349731\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9415434777736664\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5351412773132325, and Avg Validation Loss (mae) is 0.6016302525997161\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6063609629869461\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9283522069454193, and Avg Validation Loss (mae) is 1.1180158615112306\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1015166640281677\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3033943891525268, and Avg Validation Loss (mae) is 1.3722745180130005\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4042662858963013\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7517594754695892, and Avg Validation Loss (mae) is 0.8262775659561157\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8027739346027374\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8271026074886322, and Avg Validation Loss (mae) is 0.8112384915351868\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8187171757221222\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8855895519256591, and Avg Validation Loss (mae) is 1.0514177441596986\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0438588976860046\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4224409699440002, and Avg Validation Loss (mae) is 1.5014634728431702\n",
            "After 10 runs; Avg Test Loss (mae) is 1.501419222354889\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8337194740772247, and Avg Validation Loss (mae) is 0.9891989350318908\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9863842964172364\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9671231210231781, and Avg Validation Loss (mae) is 0.9927820563316345\n",
            "After 10 runs; Avg Test Loss (mae) is 0.992010360956192\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5293933033943177, and Avg Validation Loss (mae) is 1.5181175470352173\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5436729550361634\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.390614140033722, and Avg Validation Loss (mae) is 1.43036128282547\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4338923454284669\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9682615876197815, and Avg Validation Loss (mae) is 1.090835440158844\n",
            "After 10 runs; Avg Test Loss (mae) is 1.083473652601242\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.797166359424591, and Avg Validation Loss (mae) is 0.8675306379795075\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8727470874786377\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5662735223770141, and Avg Validation Loss (mae) is 1.585395610332489\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6102115869522096\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7571870625019074, and Avg Validation Loss (mae) is 0.7469947755336761\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7688095331192016\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5149831444025039, and Avg Validation Loss (mae) is 0.6287987560033799\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6358159810304642\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6155269026756287, and Avg Validation Loss (mae) is 0.6608517169952393\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6797571420669556\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6024965643882751, and Avg Validation Loss (mae) is 1.495928657054901\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5027491688728332\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6988204300403595, and Avg Validation Loss (mae) is 0.7360081672668457\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7230614304542542\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7696141839027405, and Avg Validation Loss (mae) is 0.8803543150424957\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8753629565238953\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6562433004379272, and Avg Validation Loss (mae) is 0.6999873161315918\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7016273677349091\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.458399248123169, and Avg Validation Loss (mae) is 1.447733759880066\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5232584953308106\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7336988508701324, and Avg Validation Loss (mae) is 0.7495921194553375\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7255649328231811\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9659104764461517, and Avg Validation Loss (mae) is 1.2547994077205658\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2308342635631562\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7713947772979737, and Avg Validation Loss (mae) is 0.8807146370410919\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8850309312343597\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.574694037437439, and Avg Validation Loss (mae) is 1.7164736151695252\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7391458392143249\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5826776385307312, and Avg Validation Loss (mae) is 0.5087108612060547\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5270719140768051\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6475545763969421, and Avg Validation Loss (mae) is 0.8799293160438537\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8866410851478577\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6444610357284546, and Avg Validation Loss (mae) is 0.6132636457681656\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6154504746198655\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6474451899528504, and Avg Validation Loss (mae) is 1.6654528021812438\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6664417266845704\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5173562347888947, and Avg Validation Loss (mae) is 0.5560149997472763\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5551836550235748\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.73055020570755, and Avg Validation Loss (mae) is 0.8145526051521301\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7935962557792664\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6121171295642853, and Avg Validation Loss (mae) is 0.7847857177257538\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7813098222017288\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.453584408760071, and Avg Validation Loss (mae) is 1.5429266452789308\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5549245476722717\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5594464510679245, and Avg Validation Loss (mae) is 0.618956133723259\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6143997043371201\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0118458449840546, and Avg Validation Loss (mae) is 0.9417642831802369\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9447485029697418\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8712124049663543, and Avg Validation Loss (mae) is 0.8266387641429901\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8313892960548401\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5782000541687011, and Avg Validation Loss (mae) is 1.5963125586509705\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6031663775444032\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5121693134307861, and Avg Validation Loss (mae) is 0.6434097826480866\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6293061792850494\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8158547580242157, and Avg Validation Loss (mae) is 0.9326043248176574\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9339944660663605\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6304679691791535, and Avg Validation Loss (mae) is 0.6225198239088059\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6209382474422455\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5183952450752258, and Avg Validation Loss (mae) is 1.6159639716148377\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6103044271469116\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6653939783573151, and Avg Validation Loss (mae) is 0.7083637773990631\n",
            "After 10 runs; Avg Test Loss (mae) is 0.705637514591217\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7451606869697571, and Avg Validation Loss (mae) is 0.8758387088775634\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8869146108627319\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5806754618883133, and Avg Validation Loss (mae) is 0.5219825327396392\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5436902523040772\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5693607330322266, and Avg Validation Loss (mae) is 1.6291867971420289\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6752639532089233\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.552048510313034, and Avg Validation Loss (mae) is 0.6120935708284378\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6224978893995285\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.796889454126358, and Avg Validation Loss (mae) is 0.8304192423820496\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8360061705112457\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7076273262500763, and Avg Validation Loss (mae) is 0.7896801441907882\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7994877159595489\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5811428785324098, and Avg Validation Loss (mae) is 1.6827441811561585\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6681461930274963\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9107355773448944, and Avg Validation Loss (mae) is 0.9518940687179566\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9419668257236481\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5421611040830612, and Avg Validation Loss (mae) is 0.6140843540430069\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6166439712047577\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5927371740341186, and Avg Validation Loss (mae) is 0.7228002071380615\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7228504627943039\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6648096442222595, and Avg Validation Loss (mae) is 1.55329167842865\n",
            "After 10 runs; Avg Test Loss (mae) is 1.527135396003723\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0384511709213258, and Avg Validation Loss (mae) is 0.905680525302887\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9301419198513031\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8473195433616638, and Avg Validation Loss (mae) is 0.9068724632263183\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9088176786899567\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6374473452568055, and Avg Validation Loss (mae) is 0.5528104096651077\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5578108191490173\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4673983812332154, and Avg Validation Loss (mae) is 1.484118640422821\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4557566881179809\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0976278066635132, and Avg Validation Loss (mae) is 1.1257514595985412\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1142493784427643\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1456930041313171, and Avg Validation Loss (mae) is 1.0445546865463258\n",
            "After 10 runs; Avg Test Loss (mae) is 1.016937267780304\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8285668611526489, and Avg Validation Loss (mae) is 0.7247707605361938\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7282253503799438\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5289192914962768, and Avg Validation Loss (mae) is 1.4489542245864868\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4969630241394043\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0028519332408905, and Avg Validation Loss (mae) is 1.0005726516246796\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9754154980182648\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7511743545532227, and Avg Validation Loss (mae) is 0.7181101083755493\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7372654914855957\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6233112156391144, and Avg Validation Loss (mae) is 0.7355093657970428\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7368616193532944\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5019205331802368, and Avg Validation Loss (mae) is 1.6121081709861755\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5859376549720765\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0006302058696748, and Avg Validation Loss (mae) is 1.0782793819904328\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0685115337371827\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7518558621406555, and Avg Validation Loss (mae) is 0.8644680500030517\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8619099408388138\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5105263084173203, and Avg Validation Loss (mae) is 0.5722311854362487\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5731276899576188\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5058745265007019, and Avg Validation Loss (mae) is 1.536236321926117\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5553661227226256\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9927457630634308, and Avg Validation Loss (mae) is 1.0021314799785614\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9932751834392548\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7185931861400604, and Avg Validation Loss (mae) is 0.7597367525100708\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7658221244812011\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7750319480895996, and Avg Validation Loss (mae) is 1.0677144885063172\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0683336198329925\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.604941189289093, and Avg Validation Loss (mae) is 1.5641425132751465\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5692709803581237\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1003874123096467, and Avg Validation Loss (mae) is 1.2983674645423888\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2902620792388917\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6224589109420776, and Avg Validation Loss (mae) is 0.575803741812706\n",
            "After 10 runs; Avg Test Loss (mae) is 0.583706122636795\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6028369009494782, and Avg Validation Loss (mae) is 0.5921313881874084\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5923692047595978\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5707032084465027, and Avg Validation Loss (mae) is 1.4660041093826295\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4699799776077271\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.073239266872406, and Avg Validation Loss (mae) is 1.107777601480484\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1242275536060333\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6828690767288208, and Avg Validation Loss (mae) is 0.6594571024179459\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6768473029136658\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5926828265190125, and Avg Validation Loss (mae) is 0.6011878192424774\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6214897990226745\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5201520442962646, and Avg Validation Loss (mae) is 1.5927870988845825\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5619912147521973\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1884340047836304, and Avg Validation Loss (mae) is 1.214063048362732\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2371330738067627\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6676167666912078, and Avg Validation Loss (mae) is 0.627094230055809\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6181780993938446\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8362255871295929, and Avg Validation Loss (mae) is 0.8146125495433807\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8315522789955139\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5099095821380615, and Avg Validation Loss (mae) is 1.5007431864738465\n",
            "After 10 runs; Avg Test Loss (mae) is 1.482507860660553\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3322080492973327, and Avg Validation Loss (mae) is 1.4454348921775817\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4057047247886658\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.733811104297638, and Avg Validation Loss (mae) is 0.8551614642143249\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8748642802238464\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9571982860565186, and Avg Validation Loss (mae) is 1.1515260875225066\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1328244507312775\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8828200817108154, and Avg Validation Loss (mae) is 2.0791736006736756\n",
            "After 10 runs; Avg Test Loss (mae) is 2.0968245387077333\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9719529509544372, and Avg Validation Loss (mae) is 1.021163135766983\n",
            "After 10 runs; Avg Test Loss (mae) is 1.028321671485901\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.564960566163063, and Avg Validation Loss (mae) is 0.5354038804769516\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5323021829128265\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7496918916702271, and Avg Validation Loss (mae) is 0.7946310043334961\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7980462849140167\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9053366661071778, and Avg Validation Loss (mae) is 1.921502137184143\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9178756833076478\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9302112042903901, and Avg Validation Loss (mae) is 0.9135875701904297\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8928882002830505\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7674103796482086, and Avg Validation Loss (mae) is 0.7518745839595795\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7366920441389084\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7031732320785522, and Avg Validation Loss (mae) is 0.6991847693920136\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7105726063251495\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6840405344963074, and Avg Validation Loss (mae) is 1.9186948657035827\n",
            "After 10 runs; Avg Test Loss (mae) is 1.942382037639618\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.779638147354126, and Avg Validation Loss (mae) is 0.7246500670909881\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7388625800609588\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9134873867034912, and Avg Validation Loss (mae) is 1.0323727011680603\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0171932697296142\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7934508442878723, and Avg Validation Loss (mae) is 0.9311935484409333\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9125117063522339\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9216037273406983, and Avg Validation Loss (mae) is 1.888643455505371\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8969095826148987\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9751793146133423, and Avg Validation Loss (mae) is 1.0295881807804108\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0069549202919006\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7672748029232025, and Avg Validation Loss (mae) is 0.6738873660564423\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6680443286895752\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7049414992332459, and Avg Validation Loss (mae) is 0.6737959682941437\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6518204689025879\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6724559664726257, and Avg Validation Loss (mae) is 1.709107553958893\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7150521874427795\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8159289717674255, and Avg Validation Loss (mae) is 0.7411565840244293\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7458920419216156\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6920859396457673, and Avg Validation Loss (mae) is 0.7438213348388671\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7435952365398407\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5409820407629014, and Avg Validation Loss (mae) is 0.6165048152208328\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6087179601192474\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6641388654708862, and Avg Validation Loss (mae) is 1.5967724800109864\n",
            "After 10 runs; Avg Test Loss (mae) is 1.629260003566742\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8105316936969758, and Avg Validation Loss (mae) is 0.8129269123077393\n",
            "After 10 runs; Avg Test Loss (mae) is 0.82466561794281\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.718245679140091, and Avg Validation Loss (mae) is 0.660829609632492\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6489276766777039\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.724774831533432, and Avg Validation Loss (mae) is 0.7793725878000259\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7699328839778901\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8524876713752747, and Avg Validation Loss (mae) is 1.7942423939704895\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7983123660087585\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9538460075855255, and Avg Validation Loss (mae) is 0.9401497423648835\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9163126707077026\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5583051562309265, and Avg Validation Loss (mae) is 0.6034661263227463\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6026698589324951\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6993277907371521, and Avg Validation Loss (mae) is 0.6708566665649414\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6588764607906341\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7797701597213744, and Avg Validation Loss (mae) is 1.7422229170799255\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7293279647827149\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.848828274011612, and Avg Validation Loss (mae) is 0.883503520488739\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8887400388717651\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6857177138328552, and Avg Validation Loss (mae) is 0.6094731688499451\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6023712128400802\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5820085674524307, and Avg Validation Loss (mae) is 0.5892623871564865\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5878718346357346\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7436964988708497, and Avg Validation Loss (mae) is 1.661643135547638\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6726415514945985\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.847020173072815, and Avg Validation Loss (mae) is 1.0228034377098083\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0079474866390228\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4866219758987427, and Avg Validation Loss (mae) is 0.5290290445089341\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5182165324687957\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6896356582641602, and Avg Validation Loss (mae) is 0.6034957528114319\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5931662440299987\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.667250144481659, and Avg Validation Loss (mae) is 1.697833502292633\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6744612455368042\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8358389794826507, and Avg Validation Loss (mae) is 0.928569895029068\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9283644914627075\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.554437056183815, and Avg Validation Loss (mae) is 0.7209061324596405\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7106801092624664\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7887133300304413, and Avg Validation Loss (mae) is 0.846372252702713\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8557667911052704\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.880263125896454, and Avg Validation Loss (mae) is 1.788417398929596\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7869232773780823\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9899582743644715, and Avg Validation Loss (mae) is 0.9401977360248566\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9307971835136414\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9643506407737732, and Avg Validation Loss (mae) is 1.0607473254203796\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0776620507240295\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7098373770713806, and Avg Validation Loss (mae) is 0.6787984251976014\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6703985750675201\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6813159823417663, and Avg Validation Loss (mae) is 1.6728005170822144\n",
            "After 10 runs; Avg Test Loss (mae) is 1.631746518611908\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8351961433887481, and Avg Validation Loss (mae) is 0.8357874572277069\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8319871962070465\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0430653870105744, and Avg Validation Loss (mae) is 1.0736083805561065\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0740977585315705\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4807596802711487, and Avg Validation Loss (mae) is 0.5796457678079605\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5642567485570907\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6388089656829834, and Avg Validation Loss (mae) is 1.6619717717170714\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6937344551086426\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9115999817848206, and Avg Validation Loss (mae) is 0.8401411056518555\n",
            "After 10 runs; Avg Test Loss (mae) is 0.861855411529541\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1020185112953187, and Avg Validation Loss (mae) is 1.0071127235889434\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0124400854110718\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7015205413103104, and Avg Validation Loss (mae) is 0.7755996614694596\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7749163031578064\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6554116249084472, and Avg Validation Loss (mae) is 1.7030654549598694\n",
            "After 10 runs; Avg Test Loss (mae) is 1.64532527923584\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8656309604644775, and Avg Validation Loss (mae) is 0.8784515738487244\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8631819069385529\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0308891355991363, and Avg Validation Loss (mae) is 1.0459023714065552\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0571422636508943\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7081095457077027, and Avg Validation Loss (mae) is 0.6343266636133194\n",
            "After 10 runs; Avg Test Loss (mae) is 0.630168828368187\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7039881229400635, and Avg Validation Loss (mae) is 1.5944369077682494\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6355813145637512\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.917156708240509, and Avg Validation Loss (mae) is 1.0064961731433868\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9968905568122863\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2139957070350647, and Avg Validation Loss (mae) is 1.367044949531555\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3887563467025756\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.712313848733902, and Avg Validation Loss (mae) is 0.7905346333980561\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7908713817596436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "5794c706-2afc-43d8-9f07-b8dab7191db1",
        "id": "RpzbQxzGY6DC"
      },
      "source": [
        "CombResults4 = pd.DataFrame.from_dict(my_dictMF4)\n",
        "CombResultsSorted4 = CombResults4.sort_values(by=['Test Loss'])\n",
        "CombResultsSorted4.to_csv('CombResultsSorted4.csv')\n",
        "CombResultsSorted4\n",
        "files.download(\"CombResultsSorted4.csv\")\n",
        "fig = px.box(CombResultsSorted4, x=\"DATA_X\", y=\"Test Loss\", hover_data=[\"DATA_y\"])\n",
        "fig.show()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cf615b2d-e11c-4ed7-9c1f-37bc0b6875c6\", \"CombResultsSorted4.csv\", 12466)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"1d449cb4-df3f-4f76-9c78-c2175c725061\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"1d449cb4-df3f-4f76-9c78-c2175c725061\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '1d449cb4-df3f-4f76-9c78-c2175c725061',\n",
              "                        [{\"alignmentgroup\": \"True\", \"customdata\": [[\"y_MM\"], [\"y_MM\"], [\"y_MM\"], [\"y_RM\"], [\"y_RM\"], [\"y_RM\"], [\"y_MM\"], [\"y_RB\"], [\"y_RM\"], [\"y_RM\"], [\"y_RM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RM\"], [\"y_MM\"], [\"y_RB\"], [\"y_RT\"], [\"y_MM\"], [\"y_RB\"], [\"y_RB\"], [\"y_MM\"], [\"y_MM\"], [\"y_RM\"], [\"y_MM\"], [\"y_RM\"], [\"y_MM\"], [\"y_RM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RB\"], [\"y_RM\"], [\"y_RB\"], [\"y_MM\"], [\"y_MM\"], [\"y_RB\"], [\"y_RM\"], [\"y_MM\"], [\"y_RM\"], [\"y_RB\"], [\"y_MM\"], [\"y_MB\"], [\"y_RM\"], [\"y_RM\"], [\"y_MB\"], [\"y_RB\"], [\"y_MB\"], [\"y_RB\"], [\"y_MM\"], [\"y_MM\"], [\"y_MB\"], [\"y_RB\"], [\"y_FM\"], [\"y_MB\"], [\"y_RB\"], [\"y_MM\"], [\"y_MM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RB\"], [\"y_RM\"], [\"y_MB\"], [\"y_RB\"], [\"y_RM\"], [\"y_RB\"], [\"y_RB\"], [\"y_FM\"], [\"y_RB\"], [\"y_RB\"], [\"y_MM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RM\"], [\"y_MB\"], [\"y_MB\"], [\"y_MM\"], [\"y_RB\"], [\"y_MB\"], [\"y_MB\"], [\"y_RM\"], [\"y_MB\"], [\"y_RT\"], [\"y_RM\"], [\"y_MB\"], [\"y_RT\"], [\"y_MM\"], [\"y_RB\"], [\"y_MB\"], [\"y_RT\"], [\"y_FM\"], [\"y_MB\"], [\"y_MB\"], [\"y_MB\"], [\"y_FM\"], [\"y_MB\"], [\"y_FM\"], [\"y_MM\"], [\"y_RB\"], [\"y_RM\"], [\"y_MB\"], [\"y_RB\"], [\"y_RB\"], [\"y_MB\"], [\"y_RM\"], [\"y_MB\"], [\"y_RT\"], [\"y_RT\"], [\"y_FM\"], [\"y_FM\"], [\"y_RT\"], [\"y_RB\"], [\"y_MM\"], [\"y_RB\"], [\"y_RB\"], [\"y_FM\"], [\"y_MM\"], [\"y_MM\"], [\"y_MB\"], [\"y_RT\"], [\"y_MB\"], [\"y_RB\"], [\"y_RT\"], [\"y_FM\"], [\"y_MM\"], [\"y_RB\"], [\"y_MB\"], [\"y_RT\"], [\"y_MB\"], [\"y_FM\"], [\"y_FM\"], [\"y_RM\"], [\"y_FM\"], [\"y_RM\"], [\"y_RT\"], [\"y_FM\"], [\"y_FM\"], [\"y_RT\"], [\"y_MB\"], [\"y_RT\"], [\"y_MM\"], [\"y_FM\"], [\"y_RT\"], [\"y_MB\"], [\"y_FM\"], [\"y_FM\"], [\"y_MB\"], [\"y_FM\"], [\"y_FM\"], [\"y_RT\"], [\"y_RB\"], [\"y_MM\"], [\"y_RT\"], [\"y_RM\"], [\"y_FM\"], [\"y_FM\"], [\"y_MM\"], [\"y_RM\"], [\"y_RM\"], [\"y_MB\"], [\"y_FM\"], [\"y_FM\"], [\"y_RT\"], [\"y_RT\"], [\"y_RM\"], [\"y_RM\"], [\"y_FM\"], [\"y_FM\"], [\"y_RM\"], [\"y_FM\"], [\"y_RT\"], [\"y_MT\"], [\"y_FM\"], [\"y_MB\"], [\"y_MM\"], [\"y_MT\"], [\"y_RT\"], [\"y_MT\"], [\"y_RT\"], [\"y_MT\"], [\"y_MT\"], [\"y_RB\"], [\"y_MM\"], [\"y_MB\"], [\"y_MT\"], [\"y_MT\"], [\"y_FM\"], [\"y_FM\"], [\"y_FM\"], [\"y_FM\"], [\"y_RT\"], [\"y_RT\"], [\"y_MT\"], [\"y_RT\"], [\"y_RT\"], [\"y_RT\"], [\"y_FM\"], [\"y_MB\"], [\"y_MT\"], [\"y_RM\"], [\"y_MT\"], [\"y_FM\"], [\"y_MB\"], [\"y_RM\"], [\"y_MT\"], [\"y_MB\"], [\"y_RT\"], [\"y_MT\"], [\"y_RB\"], [\"y_MT\"], [\"y_MT\"], [\"y_FM\"], [\"y_RM\"], [\"y_MT\"], [\"y_FM\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_RT\"], [\"y_MB\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MB\"], [\"y_RM\"], [\"y_MT\"], [\"y_RT\"], [\"y_MT\"], [\"y_RT\"], [\"y_MT\"], [\"y_RM\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_RT\"], [\"y_MT\"], [\"y_MT\"], [\"y_RT\"], [\"y_RT\"], [\"y_FT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_MT\"], [\"y_FT\"], [\"y_MT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_MT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}<br>DATA_y=%{customdata[0]}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_FTX_MTX_MBX_RM\", \"X_MTX_MBX_RMX_RB\", \"X_FMX_MTX_MBX_RT\", \"X_MTX_MMX_MBX_RT\", \"X_FMX_MTX_RTX_RB\", \"X_FTX_MTX_MMX_RT\", \"X_FMX_MTX_MBX_RM\", \"X_FMX_MMX_MBX_RM\", \"X_MMX_MBX_RTX_RB\", \"X_FTX_MTX_MBX_RT\", \"X_FMX_MMX_RTX_RB\", \"X_FTX_FMX_MMX_RT\", \"X_FMX_MBX_RTX_RM\", \"X_MTX_MBX_RTX_RB\", \"X_FTX_MTX_MBX_RB\", \"X_FMX_MBX_RTX_RM\", \"X_MTX_MBX_RMX_RB\", \"X_FTX_MTX_MBX_RT\", \"X_FTX_FMX_MTX_RM\", \"X_FTX_MTX_MBX_RM\", \"X_MTX_MBX_RTX_RB\", \"X_MTX_MBX_RTX_RM\", \"X_FTX_MBX_RTX_RB\", \"X_FTX_FMX_MBX_RM\", \"X_MTX_MMX_RTX_RB\", \"X_FTX_FMX_MBX_RB\", \"X_FTX_MMX_RTX_RB\", \"X_FTX_MTX_RTX_RM\", \"X_FMX_MTX_MBX_RB\", \"X_FMX_MTX_MBX_RT\", \"X_FMX_MMX_MBX_RT\", \"X_FTX_FMX_RTX_RM\", \"X_FMX_MBX_RMX_RB\", \"X_FTX_MTX_RMX_RB\", \"X_FMX_MTX_RTX_RM\", \"X_FMX_MBX_RTX_RB\", \"X_FMX_MTX_RMX_RB\", \"X_FTX_FMX_MMX_RT\", \"X_FTX_MMX_MBX_RM\", \"X_FMX_MTX_RTX_RM\", \"X_MMX_RTX_RMX_RB\", \"X_FTX_MTX_RTX_RB\", \"X_FMX_MTX_MMX_RT\", \"X_FTX_MTX_MMX_RB\", \"X_FTX_FMX_MTX_RT\", \"X_MTX_MMX_RMX_RB\", \"X_MTX_MMX_RTX_RM\", \"X_FTX_FMX_MBX_RT\", \"X_FTX_FMX_MTX_RM\", \"X_FTX_MMX_RTX_RB\", \"X_MTX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_RB\", \"X_MTX_MMX_RTX_RM\", \"X_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_RT\", \"X_FMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RT\", \"X_FTX_FMX_MTX_MB\", \"X_FTX_MMX_MBX_RT\", \"X_FTX_FMX_MBX_RT\", \"X_FTX_MTX_RMX_RB\", \"X_FTX_MTX_MBX_RT\", \"X_FTX_MMX_MBX_RB\", \"X_FTX_MBX_RTX_RM\", \"X_FMX_MTX_MMX_RM\", \"X_FTX_MTX_MMX_RM\", \"X_FTX_FMX_MMX_RM\", \"X_FTX_FMX_MBX_RT\", \"X_FMX_MTX_RTX_RB\", \"X_MTX_MMX_MBX_RM\", \"X_MTX_RTX_RMX_RB\", \"X_FTX_MMX_MBX_RT\", \"X_FTX_MTX_MMX_RM\", \"X_FTX_MMX_RMX_RB\", \"X_FTX_MTX_RTX_RB\", \"X_FMX_MMX_MBX_RT\", \"X_FMX_MTX_MMX_RM\", \"X_FMX_MTX_MMX_RB\", \"X_FMX_MMX_MBX_RB\", \"X_FTX_MMX_RTX_RM\", \"X_FTX_MTX_MBX_RM\", \"X_FTX_FMX_MTX_RT\", \"X_FTX_FMX_MTX_RM\", \"X_MTX_MMX_MBX_RM\", \"X_FTX_FMX_RMX_RB\", \"X_FMX_MMX_RTX_RM\", \"X_FMX_MMX_RTX_RM\", \"X_FTX_MMX_MBX_RM\", \"X_MTX_MMX_MBX_RB\", \"X_FTX_FMX_MMX_RT\", \"X_FTX_FMX_MTX_RT\", \"X_FTX_MTX_RTX_RB\", \"X_FTX_MTX_MMX_RT\", \"X_MTX_MMX_RTX_RB\", \"X_MTX_MMX_RTX_RB\", \"X_FTX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_RT\", \"X_FTX_FMX_RTX_RB\", \"X_FTX_MTX_MMX_RT\", \"X_FTX_MMX_RTX_RM\", \"X_FTX_FMX_MMX_MB\", \"X_FMX_MMX_RMX_RB\", \"X_FTX_FMX_MMX_RB\", \"X_FMX_MTX_MMX_RT\", \"X_MTX_MMX_RMX_RB\", \"X_FTX_MTX_MMX_RM\", \"X_FTX_MTX_MBX_RT\", \"X_FTX_MMX_MBX_RM\", \"X_MMX_MBX_RMX_RB\", \"X_FMX_MTX_MBX_RM\", \"X_FTX_MTX_RTX_RM\", \"X_FTX_MTX_MMX_RM\", \"X_FTX_FMX_MBX_RM\", \"X_FTX_MMX_MBX_RT\", \"X_FTX_FMX_MTX_RB\", \"X_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_RM\", \"X_FMX_MTX_MBX_RM\", \"X_FTX_FMX_MMX_RB\", \"X_MTX_MMX_MBX_RT\", \"X_FMX_MTX_RMX_RB\", \"X_FTX_MMX_RTX_RM\", \"X_FTX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MM\", \"X_FTX_MTX_RTX_RM\", \"X_FTX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_RB\", \"X_MTX_MMX_RMX_RB\", \"X_FTX_MMX_RMX_RB\", \"X_FTX_FMX_MBX_RB\", \"X_FTX_MTX_RTX_RB\", \"X_FMX_MTX_MBX_RB\", \"X_FMX_MBX_RMX_RB\", \"X_MMX_MBX_RTX_RB\", \"X_FTX_MMX_MBX_RB\", \"X_FTX_MTX_MBX_RB\", \"X_FMX_MTX_RMX_RB\", \"X_FTX_FMX_MMX_RM\", \"X_FTX_FMX_RTX_RB\", \"X_FTX_MTX_MBX_RB\", \"X_FTX_FMX_RMX_RB\", \"X_MTX_RTX_RMX_RB\", \"X_MMX_MBX_RMX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_FMX_MMX_RTX_RB\", \"X_FTX_MTX_MBX_RM\", \"X_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MBX_RM\", \"X_FMX_MTX_MMX_MB\", \"X_FMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_RM\", \"X_FTX_MTX_MMX_RB\", \"X_FTX_MTX_RMX_RB\", \"X_FTX_MTX_RTX_RM\", \"X_FTX_FMX_RTX_RM\", \"X_FMX_MTX_MMX_RB\", \"X_FMX_MTX_MBX_RT\", \"X_FMX_MTX_RTX_RB\", \"X_MTX_MBX_RTX_RB\", \"X_MTX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_RM\", \"X_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_RB\", \"X_MTX_MMX_MBX_RB\", \"X_MTX_MBX_RTX_RM\", \"X_FTX_MMX_RTX_RB\", \"X_FTX_MTX_MBX_RB\", \"X_MTX_RTX_RMX_RB\", \"X_FTX_MMX_RMX_RB\", \"X_FMX_MMX_MBX_RM\", \"X_MMX_MBX_RTX_RM\", \"X_FMX_MTX_RTX_RM\", \"X_FTX_MBX_RTX_RB\", \"X_FMX_MMX_MBX_RT\", \"X_FMX_MTX_MBX_RB\", \"X_FTX_FMX_MMX_RT\", \"X_FTX_MTX_RMX_RB\", \"X_FMX_MMX_RTX_RM\", \"X_FTX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_MB\", \"X_FTX_RTX_RMX_RB\", \"X_FTX_RTX_RMX_RB\", \"X_FMX_MMX_RMX_RB\", \"X_FTX_MMX_RTX_RB\", \"X_MBX_RTX_RMX_RB\", \"X_FTX_MBX_RTX_RB\", \"X_MTX_MMX_RTX_RM\", \"X_MTX_MBX_RMX_RB\", \"X_FTX_FMX_MBX_RB\", \"X_FTX_MTX_MMX_RB\", \"X_MMX_MBX_RMX_RB\", \"X_FMX_MMX_MBX_RB\", \"X_MTX_MMX_MBX_RB\", \"X_FTX_FMX_MTX_RB\", \"X_MTX_MMX_MBX_RT\", \"X_FTX_FMX_RTX_RM\", \"X_FTX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_MM\", \"X_FTX_FMX_MBX_RT\", \"X_FTX_RTX_RMX_RB\", \"X_FTX_FMX_RTX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_MMX_RTX_RMX_RB\", \"X_FTX_FMX_RMX_RB\", \"X_FMX_MMX_RMX_RB\", \"X_FMX_MMX_RTX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_MMX_MBX_RTX_RB\", \"X_MMX_MBX_RTX_RM\", \"X_FTX_MBX_RTX_RM\", \"X_FMX_MTX_MMX_MB\", \"X_FTX_FMX_MMX_RM\", \"X_FTX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_MB\", \"X_FTX_MMX_MBX_RT\", \"X_FMX_MMX_MBX_RB\", \"X_FMX_MBX_RTX_RB\", \"X_FTX_FMX_MMX_RB\", \"X_FMX_RTX_RMX_RB\", \"X_FTX_MMX_MBX_RB\", \"X_FTX_FMX_MMX_RB\", \"X_FTX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_MM\", \"X_FTX_FMX_MMX_MB\", \"X_FTX_FMX_RTX_RB\", \"X_FTX_MMX_MBX_RB\", \"X_FTX_FMX_MBX_RB\", \"X_FMX_MTX_MMX_RB\", \"X_FMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MB\", \"X_FTX_MBX_RTX_RB\", \"X_FMX_MBX_RTX_RM\", \"X_FTX_MBX_RTX_RM\", \"X_FTX_FMX_MBX_RM\", \"X_MBX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_FTX_MBX_RMX_RB\", \"X_FMX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MB\", \"X_FMX_MTX_MMX_MB\", \"X_FMX_MMX_MBX_RB\", \"X_FTX_FMX_MTX_MM\", \"X_FMX_MBX_RTX_RB\", \"X_FMX_RTX_RMX_RB\", \"X_FMX_MMX_RTX_RM\", \"X_FTX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_RM\", \"X_FTX_FMX_RMX_RB\", \"X_FMX_MTX_MMX_RB\", \"X_FMX_MMX_MBX_RM\", \"X_FMX_MTX_MMX_MB\", \"X_FMX_MTX_MBX_RB\", \"X_FMX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_MB\", \"X_FMX_MBX_RMX_RB\", \"X_FMX_MBX_RTX_RM\", \"X_FMX_MMX_RTX_RB\", \"X_FMX_MTX_RTX_RM\", \"X_FMX_MTX_MMX_RT\", \"X_FMX_MTX_RTX_RB\", \"X_MTX_MMX_RMX_RB\", \"X_MMX_MBX_RTX_RB\", \"X_MBX_RTX_RMX_RB\", \"X_FTX_FMX_RTX_RM\", \"X_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MBX_RM\", \"X_FMX_MMX_MBX_RT\", \"X_MTX_MBX_RMX_RB\", \"X_MTX_RTX_RMX_RB\", \"X_FMX_MTX_RMX_RB\", \"X_MMX_MBX_RMX_RB\", \"X_MTX_MMX_RTX_RB\", \"X_MTX_MBX_RTX_RB\", \"X_FMX_MTX_MBX_RT\", \"X_MMX_MBX_RTX_RM\", \"X_MTX_MBX_RTX_RM\", \"X_MTX_MMX_RTX_RM\", \"X_MTX_MMX_MBX_RM\", \"X_MTX_MMX_MBX_RB\", \"X_MTX_MMX_MBX_RT\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.45317806899547575, 0.5182165324687957, 0.5270719140768051, 0.5323021829128265, 0.5436902523040772, 0.5520252704620361, 0.5551836550235748, 0.5578108191490173, 0.5642567485570907, 0.5719264894723892, 0.5731276899576188, 0.5761305183172226, 0.583706122636795, 0.5878718346357346, 0.5906493127346039, 0.5923692047595978, 0.5931662440299987, 0.5955235093832016, 0.5980755776166916, 0.5991460800170898, 0.6023712128400802, 0.6026698589324951, 0.6063609629869461, 0.6080870211124421, 0.6087179601192474, 0.61023308634758, 0.6115810215473175, 0.6118973672389985, 0.6143997043371201, 0.6154504746198655, 0.6166439712047577, 0.6168471068143845, 0.6181780993938446, 0.6205780059099197, 0.6209382474422455, 0.6214897990226745, 0.6224978893995285, 0.6242697775363922, 0.6292011260986328, 0.6293061792850494, 0.630168828368187, 0.633814811706543, 0.6358159810304642, 0.641539016366005, 0.6468014746904374, 0.6489276766777039, 0.6518204689025879, 0.652310261130333, 0.6564424067735672, 0.6576942831277848, 0.6588764607906341, 0.6658670604228973, 0.6680443286895752, 0.6703985750675201, 0.6734501600265503, 0.6768473029136658, 0.6797571420669556, 0.6816399157047272, 0.6822333693504333, 0.6892980128526688, 0.6971578657627105, 0.6972301125526428, 0.698075932264328, 0.6991054743528367, 0.7016273677349091, 0.7024003207683563, 0.7037221640348434, 0.7047994315624238, 0.705637514591217, 0.7105726063251495, 0.7106801092624664, 0.7107647448778153, 0.7111395120620727, 0.712770962715149, 0.7182064592838288, 0.7228504627943039, 0.7230614304542542, 0.7255649328231811, 0.7282253503799438, 0.7284363925457, 0.7311005175113678, 0.7326854228973388, 0.7335404813289642, 0.7366920441389084, 0.7368160665035248, 0.7368616193532944, 0.7372654914855957, 0.737662410736084, 0.7388625800609588, 0.7401647627353668, 0.7404148936271667, 0.7420194149017334, 0.7433302879333497, 0.7435952365398407, 0.7458920419216156, 0.7464412301778793, 0.747004035115242, 0.7486312210559845, 0.7498057007789611, 0.7570182502269744, 0.7610987544059753, 0.7658221244812011, 0.7673852562904357, 0.7688095331192016, 0.7699328839778901, 0.7715468347072602, 0.7718661963939667, 0.7741055905818939, 0.7749163031578064, 0.7813098222017288, 0.787420180439949, 0.7884241431951523, 0.788896444439888, 0.7904458522796631, 0.7907719910144806, 0.7908713817596436, 0.7917083501815796, 0.7935962557792664, 0.7949518293142319, 0.7980462849140167, 0.7994877159595489, 0.802299964427948, 0.8027739346027374, 0.805118978023529, 0.8120190262794494, 0.8187171757221222, 0.8228475630283356, 0.82466561794281, 0.82529336810112, 0.8289529621601105, 0.8312719881534576, 0.8313892960548401, 0.8315522789955139, 0.8319871962070465, 0.8322987616062164, 0.8355596840381623, 0.8360061705112457, 0.8397876739501953, 0.8469954013824463, 0.8491374731063843, 0.8510358512401581, 0.8557667911052704, 0.861855411529541, 0.861862200498581, 0.8619099408388138, 0.8628148257732391, 0.8631819069385529, 0.8694509923458099, 0.8727470874786377, 0.8748642802238464, 0.8753629565238953, 0.8779925256967545, 0.8806401789188385, 0.882059109210968, 0.8846574485301971, 0.8850309312343597, 0.8866410851478577, 0.8869146108627319, 0.8887400388717651, 0.8928882002830505, 0.8964014530181885, 0.9088176786899567, 0.9121588945388794, 0.9125117063522339, 0.9163126707077026, 0.9229599416255951, 0.923194533586502, 0.9283644914627075, 0.92898890376091, 0.9301419198513031, 0.9307971835136414, 0.9339944660663605, 0.9415434777736664, 0.9419668257236481, 0.9447485029697418, 0.9632059514522553, 0.970748245716095, 0.9754154980182648, 0.9769019305706024, 0.9820497035980225, 0.9863842964172364, 0.992010360956192, 0.9932751834392548, 0.994964873790741, 0.9968905568122863, 0.9987908720970153, 1.0069549202919006, 1.0079474866390228, 1.0104422509670257, 1.0109638929367066, 1.0124400854110718, 1.016937267780304, 1.0171932697296142, 1.0182160317897797, 1.028321671485901, 1.0355250239372253, 1.0365294456481933, 1.0413529038429261, 1.0433352828025817, 1.0438588976860046, 1.0445774972438813, 1.0475331127643586, 1.0571422636508943, 1.064910876750946, 1.0683336198329925, 1.0685115337371827, 1.068821656703949, 1.0740977585315705, 1.0776620507240295, 1.0832867622375488, 1.083473652601242, 1.1006887018680573, 1.1015166640281677, 1.105036860704422, 1.1068134248256682, 1.1142493784427643, 1.1242275536060333, 1.1321434259414673, 1.1328244507312775, 1.1447224378585816, 1.16432363986969, 1.1873818576335906, 1.1940179347991944, 1.218846595287323, 1.2260679364204408, 1.2278103232383728, 1.2280493021011352, 1.2308342635631562, 1.2371330738067627, 1.2504640817642212, 1.282236063480377, 1.2902620792388917, 1.3010459065437316, 1.3045071244239808, 1.3887563467025756, 1.3926246523857118, 1.4042662858963013, 1.4057047247886658, 1.4240432739257813, 1.4338923454284669, 1.4557566881179809, 1.4655804991722108, 1.4699799776077271, 1.482507860660553, 1.4969630241394043, 1.501419222354889, 1.5027491688728332, 1.5200745463371277, 1.5232584953308106, 1.527135396003723, 1.5436729550361634, 1.5549245476722717, 1.5553661227226256, 1.5606137752532958, 1.5619912147521973, 1.5692709803581237, 1.5859376549720765, 1.6031663775444032, 1.6102115869522096, 1.6103044271469116, 1.629260003566742, 1.631746518611908, 1.6355813145637512, 1.6437848091125489, 1.64532527923584, 1.6664417266845704, 1.6681461930274963, 1.6726415514945985, 1.6744612455368042, 1.6752639532089233, 1.6937344551086426, 1.7150521874427795, 1.7293279647827149, 1.7391458392143249, 1.7869232773780823, 1.7983123660087585, 1.8969095826148987, 1.9178756833076478, 1.942382037639618, 2.0968245387077333], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1d449cb4-df3f-4f76-9c78-c2175c725061');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "d7f23d43-7687-4b07-940e-9e52a38ca0ad",
        "id": "VnCbmYAHY6DD"
      },
      "source": [
        "CombResultsSortedgrouped4 = CombResultsSorted4.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedgroupedsortedMF4 = CombResultsSortedgrouped4.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedgroupedsortedMF4.to_csv('CombResultsSortedgroupedsortedMF4.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedgroupedsortedMF4.csv\")\n",
        "CombResultsSortedgroupedsortedMF4"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2e208c84-a7e1-4561-8dcb-436a9d3699c5\", \"CombResultsSortedgroupedsortedMF4.csv\", 2521)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RT</th>\n",
              "      <td>0.659137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RM</th>\n",
              "      <td>0.661560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RT</th>\n",
              "      <td>0.698041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RT</th>\n",
              "      <td>0.698338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RM</th>\n",
              "      <td>0.721115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RTX_RMX_RB</th>\n",
              "      <td>1.130918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RB</th>\n",
              "      <td>1.152737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.203025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RTX_RMX_RB</th>\n",
              "      <td>1.223975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MB</th>\n",
              "      <td>1.233447</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Test Loss\n",
              "DATA_X                     \n",
              "X_FTX_MTX_MBX_RT   0.659137\n",
              "X_FTX_MTX_MBX_RM   0.661560\n",
              "X_FTX_MTX_MMX_RT   0.698041\n",
              "X_FTX_FMX_MTX_RT   0.698338\n",
              "X_FTX_FMX_MTX_RM   0.721115\n",
              "...                     ...\n",
              "X_FTX_RTX_RMX_RB   1.130918\n",
              "X_MTX_MMX_MBX_RB   1.152737\n",
              "X_MBX_RTX_RMX_RB   1.203025\n",
              "X_FMX_RTX_RMX_RB   1.223975\n",
              "X_FMX_MTX_MMX_MB   1.233447\n",
              "\n",
              "[70 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8c3C3RbitS4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4356345-0369-46cb-9519-971e521f2682",
        "id": "j693AlQBtTkX"
      },
      "source": [
        "#Combinations of 5 Sensors, therefore 35 features, 35 inputs\n",
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "inp_shp = 35\n",
        "n_sensors = 5\n",
        "\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "modelG(inp_shp)\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "my_dictMF5 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), n_sensors):\n",
        "  kX1, kX2, kX3, kX4, kX5 = combo[0][0], combo[1][0], combo[2][0], combo[3][0], combo[4][0]\n",
        "  vX1, vX2, vX3, vX4, vX5 = combo[0][1], combo[1][1], combo[2][1], combo[3][1], combo[4][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:] or ky[-2:] == kX4[-2:] or ky[-2:] == kX5[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, kx4 = {kX4}, kx5 = {kX5}, ky = {ky}')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF5 = evaldata(n_sensors, X_in1=vX1, X_in2=vX2, X_in3=vX3, X_in4=vX4, X_in5=vX5, Y_in=vy, traindata1 = kX1, traindata2 = kX2, traindata3 = kX3, traindata4 = kX4, traindata5 = kX5, testdata = ky)\n",
        "      TestLossTotal = resultsMF5[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF5[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF5[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}, {kX2}, {kX3}, {kX4}, {kX5} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF5[\"DATA_X\"].append(kX1 + kX2 + kX3 + kX4 + kX5)\n",
        "    my_dictMF5[\"DATA_y\"].append(ky)\n",
        "    my_dictMF5[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model.load_weights('model.h5')\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3799450993537903, and Avg Validation Loss (mae) is 1.607702624797821\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5894007563591004\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.000454878807068, and Avg Validation Loss (mae) is 1.019024622440338\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0090260624885559\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8252382397651672, and Avg Validation Loss (mae) is 0.7734337568283081\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7679009854793548\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7883044064044953, and Avg Validation Loss (mae) is 0.6507981181144714\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6443597733974457\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5732058703899383, and Avg Validation Loss (mae) is 0.6679277181625366\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6643171578645706\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6465705335140228, and Avg Validation Loss (mae) is 0.6753599017858505\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6710421353578567\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7327706038951873, and Avg Validation Loss (mae) is 0.8441757440567017\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8649742990732193\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7984034359455109, and Avg Validation Loss (mae) is 0.8022582054138183\n",
            "After 10 runs; Avg Test Loss (mae) is 0.823357778787613\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7146417796611786, and Avg Validation Loss (mae) is 0.7570130825042725\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7581356763839722\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7614086508750916, and Avg Validation Loss (mae) is 0.777380320429802\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7605716824531555\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0905296564102174, and Avg Validation Loss (mae) is 0.9925122737884522\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9826938688755036\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8124099373817444, and Avg Validation Loss (mae) is 0.8938419789075851\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9175852298736572\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6088276624679565, and Avg Validation Loss (mae) is 0.636408481001854\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6410115152597428\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6127896428108215, and Avg Validation Loss (mae) is 0.7168983310461045\n",
            "After 10 runs; Avg Test Loss (mae) is 0.712155020236969\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6443552553653717, and Avg Validation Loss (mae) is 0.675405991077423\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6790668368339539\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5506103575229645, and Avg Validation Loss (mae) is 0.4836964577436447\n",
            "After 10 runs; Avg Test Loss (mae) is 0.4896452844142914\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7952426075935364, and Avg Validation Loss (mae) is 0.7495142459869385\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7655639290809632\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6043036639690399, and Avg Validation Loss (mae) is 0.6828309386968613\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6879593431949615\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6366974711418152, and Avg Validation Loss (mae) is 0.656728184223175\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6524884819984436\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.020810616016388, and Avg Validation Loss (mae) is 1.0483059108257293\n",
            "After 10 runs; Avg Test Loss (mae) is 1.064403909444809\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8351529896259308, and Avg Validation Loss (mae) is 0.9928564965724945\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0082775413990022\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.55759257376194, and Avg Validation Loss (mae) is 0.713823887705803\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7085574865341187\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7451517224311829, and Avg Validation Loss (mae) is 0.7565592408180237\n",
            "After 10 runs; Avg Test Loss (mae) is 0.747688502073288\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.669333690404892, and Avg Validation Loss (mae) is 0.6504646629095078\n",
            "After 10 runs; Avg Test Loss (mae) is 0.647355580329895\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6155767560005188, and Avg Validation Loss (mae) is 0.8545370608568191\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8602491229772568\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7498211979866027, and Avg Validation Loss (mae) is 0.9602737784385681\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9532909393310547\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.592345067858696, and Avg Validation Loss (mae) is 0.6763118714094162\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6792305290699006\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6476485371589661, and Avg Validation Loss (mae) is 0.7694465756416321\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7675232350826263\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8011037349700928, and Avg Validation Loss (mae) is 0.8563233613967896\n",
            "After 10 runs; Avg Test Loss (mae) is 0.870468258857727\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7121175289154053, and Avg Validation Loss (mae) is 0.7985907703638077\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8376083910465241\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9955336928367615, and Avg Validation Loss (mae) is 1.247657948732376\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2358265817165375\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5532225370407104, and Avg Validation Loss (mae) is 0.7202442646026611\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7297075688838959\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6425416767597198, and Avg Validation Loss (mae) is 0.5833077877759933\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5884479463100434\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.019480812549591, and Avg Validation Loss (mae) is 0.9390274941921234\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9432756662368774\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8123728513717652, and Avg Validation Loss (mae) is 0.825140404701233\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8336196213960647\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6568461626768112, and Avg Validation Loss (mae) is 0.6429585963487625\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6477502554655075\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.092663788795471, and Avg Validation Loss (mae) is 1.086787623167038\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0924286842346191\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0875112652778625, and Avg Validation Loss (mae) is 1.0193938851356505\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0277075231075288\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8469037711620331, and Avg Validation Loss (mae) is 0.9028195083141327\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9113071501255036\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9889176189899445, and Avg Validation Loss (mae) is 1.1616914987564086\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1628105878829955\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7541157126426696, and Avg Validation Loss (mae) is 0.7145251572132111\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7271194636821747\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.70122309923172, and Avg Validation Loss (mae) is 0.7552582889795303\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7399915993213654\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0224524021148682, and Avg Validation Loss (mae) is 0.9997602939605713\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0009610414505006\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7509238600730896, and Avg Validation Loss (mae) is 0.6900192528963089\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7090144544839859\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6644595772027969, and Avg Validation Loss (mae) is 0.6956212639808654\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6945286303758621\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0603066861629487, and Avg Validation Loss (mae) is 1.3090314865112305\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3081883907318115\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7104196369647979, and Avg Validation Loss (mae) is 0.705045685172081\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7078349769115448\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7546510517597198, and Avg Validation Loss (mae) is 0.8625551372766495\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8554501950740814\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1123081564903259, and Avg Validation Loss (mae) is 0.980361384153366\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9765572667121887\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6374220609664917, and Avg Validation Loss (mae) is 0.6332246512174606\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6489262163639069\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6492355227470398, and Avg Validation Loss (mae) is 0.606817090511322\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6039435297250748\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0918757617473602, and Avg Validation Loss (mae) is 1.1561282277107239\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1810413599014282\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6015435457229614, and Avg Validation Loss (mae) is 0.7414525508880615\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7274696975946426\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6621474742889404, and Avg Validation Loss (mae) is 0.6937161773443222\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6928680747747421\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.140611457824707, and Avg Validation Loss (mae) is 1.0353190004825592\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0449582040309906\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6466504365205765, and Avg Validation Loss (mae) is 0.7446087211370468\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7569675773382187\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8208118915557862, and Avg Validation Loss (mae) is 0.9197995483875274\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9311692833900451\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.429463255405426, and Avg Validation Loss (mae) is 1.4320110917091369\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3887821435928345\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8316905856132507, and Avg Validation Loss (mae) is 1.0388687908649445\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0426179468631744\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9867166996002197, and Avg Validation Loss (mae) is 1.0263046205043793\n",
            "After 10 runs; Avg Test Loss (mae) is 1.041464626789093\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8251630425453186, and Avg Validation Loss (mae) is 0.9111499011516571\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9137429237365723\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6156050741672516, and Avg Validation Loss (mae) is 0.5461429446935654\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5319697737693787\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6574851393699646, and Avg Validation Loss (mae) is 0.6671420335769653\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6577983677387238\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7910893082618713, and Avg Validation Loss (mae) is 0.9350579917430878\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9294250309467316\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7858530521392822, and Avg Validation Loss (mae) is 0.7728268086910248\n",
            "After 10 runs; Avg Test Loss (mae) is 0.76760293841362\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6930152773857117, and Avg Validation Loss (mae) is 0.8939096033573151\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8929218709468841\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7693293869495392, and Avg Validation Loss (mae) is 0.7306119978427887\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7287082135677337\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9609282732009887, and Avg Validation Loss (mae) is 0.8870733141899109\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8999246776103973\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7451390862464905, and Avg Validation Loss (mae) is 0.6929324179887771\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6933363080024719\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7661820888519287, and Avg Validation Loss (mae) is 0.842944759130478\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8675545692443848\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7405616641044617, and Avg Validation Loss (mae) is 0.6754222929477691\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6800466418266297\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6327757209539413, and Avg Validation Loss (mae) is 0.6099692076444626\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6080698937177658\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7308174788951873, and Avg Validation Loss (mae) is 0.9393593549728394\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9431354761123657\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7380762040615082, and Avg Validation Loss (mae) is 0.7742087006568908\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7714860260486602\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6164670407772064, and Avg Validation Loss (mae) is 0.6031876713037491\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6015739291906357\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7937088370323181, and Avg Validation Loss (mae) is 0.9067243754863739\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9276616394519805\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7918896198272705, and Avg Validation Loss (mae) is 0.7593530535697937\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7473659455776215\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6670735120773316, and Avg Validation Loss (mae) is 0.7020847052335739\n",
            "After 10 runs; Avg Test Loss (mae) is 0.716923275589943\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.84077467918396, and Avg Validation Loss (mae) is 0.8231312334537506\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8298101127147675\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5321603089570999, and Avg Validation Loss (mae) is 0.6156540483236312\n",
            "After 10 runs; Avg Test Loss (mae) is 0.615435716509819\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6390550434589386, and Avg Validation Loss (mae) is 0.6603666484355927\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6643442332744598\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7853316068649292, and Avg Validation Loss (mae) is 0.775672847032547\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7744389593601226\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.592675843834877, and Avg Validation Loss (mae) is 0.7332265377044678\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7377741485834122\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5893825024366379, and Avg Validation Loss (mae) is 0.5210910230875015\n",
            "After 10 runs; Avg Test Loss (mae) is 0.516474387049675\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7717100858688355, and Avg Validation Loss (mae) is 0.7487829864025116\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7515243530273438\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5725902199745179, and Avg Validation Loss (mae) is 0.5756762266159058\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5615256488323211\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6731447637081146, and Avg Validation Loss (mae) is 0.8978803783655167\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9168340146541596\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7565171480178833, and Avg Validation Loss (mae) is 0.7993439197540283\n",
            "After 10 runs; Avg Test Loss (mae) is 0.796795254945755\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6188708662986755, and Avg Validation Loss (mae) is 0.5511264503002167\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5525244742631912\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7615249633789063, and Avg Validation Loss (mae) is 0.7497978389263154\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7561680853366852\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8036669731140137, and Avg Validation Loss (mae) is 0.7293972671031952\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7621321439743042\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9781431436538697, and Avg Validation Loss (mae) is 0.9091252982616425\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9072666645050049\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6329657375812531, and Avg Validation Loss (mae) is 0.7412146776914597\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7402277141809464\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7630254864692688, and Avg Validation Loss (mae) is 0.7967742264270783\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7870293021202087\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9370578229427338, and Avg Validation Loss (mae) is 1.0856119871139527\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0438237607479095\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.628188681602478, and Avg Validation Loss (mae) is 0.7128816813230514\n",
            "After 10 runs; Avg Test Loss (mae) is 0.714850303530693\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8364738643169403, and Avg Validation Loss (mae) is 0.7988127708435059\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8140719473361969\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.014715814590454, and Avg Validation Loss (mae) is 1.0341692388057708\n",
            "After 10 runs; Avg Test Loss (mae) is 1.033756297826767\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7953079104423523, and Avg Validation Loss (mae) is 0.9491111129522324\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9578251212835311\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7798062205314636, and Avg Validation Loss (mae) is 0.7854535400867462\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7715451180934906\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.043241137266159, and Avg Validation Loss (mae) is 1.1021799087524413\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0828614413738251\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8224125146865845, and Avg Validation Loss (mae) is 0.7862360417842865\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7862783133983612\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8477576017379761, and Avg Validation Loss (mae) is 1.0482648491859436\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0532406449317933\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3095273494720459, and Avg Validation Loss (mae) is 1.401541244983673\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4168769478797913\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MB, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8138531267642974, and Avg Validation Loss (mae) is 1.0275644719600678\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0175928801298142\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6235293745994568, and Avg Validation Loss (mae) is 1.5674940586090087\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6258222222328187\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6054993361234665, and Avg Validation Loss (mae) is 0.5969093620777131\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6129620015621186\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7256494343280793, and Avg Validation Loss (mae) is 0.7067597031593322\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7069332450628281\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.550596296787262, and Avg Validation Loss (mae) is 1.6898425579071046\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6804510712623597\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7824620306491852, and Avg Validation Loss (mae) is 0.8691429316997528\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8641578316688537\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.67107794880867, and Avg Validation Loss (mae) is 0.8913470447063446\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8808568596839905\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5584627509117126, and Avg Validation Loss (mae) is 1.4501879334449768\n",
            "After 10 runs; Avg Test Loss (mae) is 1.48566073179245\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.062749695777893, and Avg Validation Loss (mae) is 1.0544983088970183\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0251981139183044\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7695884466171264, and Avg Validation Loss (mae) is 0.9272817313671112\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9234088361263275\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5855072259902954, and Avg Validation Loss (mae) is 1.6500351905822754\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5932409644126893\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7338373482227325, and Avg Validation Loss (mae) is 0.6793057918548584\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6676001101732254\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.667507815361023, and Avg Validation Loss (mae) is 0.5846334844827652\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5998126029968261\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.485297191143036, and Avg Validation Loss (mae) is 1.6635253787040711\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6656042218208313\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.694420337677002, and Avg Validation Loss (mae) is 0.7166014581918716\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7216343432664871\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5702250212430954, and Avg Validation Loss (mae) is 0.5743972778320312\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5715694934129715\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5089719533920287, and Avg Validation Loss (mae) is 1.523377239704132\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5346869230270386\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7475046694278717, and Avg Validation Loss (mae) is 0.9807339429855346\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9835504710674285\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6484528362751008, and Avg Validation Loss (mae) is 0.8048140168190002\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7907678365707398\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6137020468711853, and Avg Validation Loss (mae) is 1.5494539141654968\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5296367168426515\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4857982099056244, and Avg Validation Loss (mae) is 0.6682443171739578\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6714874297380448\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6170733809471131, and Avg Validation Loss (mae) is 0.6997605860233307\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6924594908952713\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5297745704650878, and Avg Validation Loss (mae) is 1.5809701323509215\n",
            "After 10 runs; Avg Test Loss (mae) is 1.573837399482727\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.580247163772583, and Avg Validation Loss (mae) is 0.7137167066335678\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7097736299037933\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6235371112823487, and Avg Validation Loss (mae) is 0.7314089387655258\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7480687409639358\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5802607893943788, and Avg Validation Loss (mae) is 1.446219527721405\n",
            "After 10 runs; Avg Test Loss (mae) is 1.417473590373993\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4979335069656372, and Avg Validation Loss (mae) is 0.7709988266229629\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7641268670558929\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6877918601036072, and Avg Validation Loss (mae) is 0.806594067811966\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8238872140645981\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4631547808647156, and Avg Validation Loss (mae) is 1.6197100639343263\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5897136211395264\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6954548567533493, and Avg Validation Loss (mae) is 0.72583809196949\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7158109873533249\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7498303055763245, and Avg Validation Loss (mae) is 0.9302595436573029\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9369261622428894\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6116837859153748, and Avg Validation Loss (mae) is 1.4677999496459961\n",
            "After 10 runs; Avg Test Loss (mae) is 1.439200472831726\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9851012349128723, and Avg Validation Loss (mae) is 1.0902925729751587\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0788897454738617\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6266710221767425, and Avg Validation Loss (mae) is 0.7158199787139893\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7115960419178009\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4515399098396302, and Avg Validation Loss (mae) is 1.555998194217682\n",
            "After 10 runs; Avg Test Loss (mae) is 1.591223454475403\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9478247821331024, and Avg Validation Loss (mae) is 1.009689736366272\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0180826544761659\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5743542373180389, and Avg Validation Loss (mae) is 0.47962703108787536\n",
            "After 10 runs; Avg Test Loss (mae) is 0.4872525632381439\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5771547913551331, and Avg Validation Loss (mae) is 1.4552422761917114\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4326835036277772\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.983655571937561, and Avg Validation Loss (mae) is 1.0187896132469176\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0233844935894012\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8195352852344513, and Avg Validation Loss (mae) is 0.7026807367801666\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7049738824367523\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5692626953125, and Avg Validation Loss (mae) is 1.4659987568855286\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4927450299263\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0646618187427521, and Avg Validation Loss (mae) is 1.285822069644928\n",
            "After 10 runs; Avg Test Loss (mae) is 1.27238529920578\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7286618947982788, and Avg Validation Loss (mae) is 0.7397915244102478\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7556658029556275\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5413352251052856, and Avg Validation Loss (mae) is 1.630363404750824\n",
            "After 10 runs; Avg Test Loss (mae) is 1.592409884929657\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.105328196287155, and Avg Validation Loss (mae) is 1.190853875875473\n",
            "After 10 runs; Avg Test Loss (mae) is 1.224165952205658\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MB, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.642916026711464, and Avg Validation Loss (mae) is 0.7258683741092682\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7306910753250122\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.864469337463379, and Avg Validation Loss (mae) is 1.7408601999282838\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7310853242874145\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.888942402601242, and Avg Validation Loss (mae) is 0.9281662404537201\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9085443556308747\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6769058346748352, and Avg Validation Loss (mae) is 0.8097035884857178\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8149394273757935\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6516746044158936, and Avg Validation Loss (mae) is 1.5744330644607545\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5905714273452758\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8168166518211365, and Avg Validation Loss (mae) is 0.8247465312480926\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8114244401454925\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5796401500701904, and Avg Validation Loss (mae) is 0.788065841794014\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7805281519889832\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7053738951683044, and Avg Validation Loss (mae) is 1.7033336997032165\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7341665267944335\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7886901199817657, and Avg Validation Loss (mae) is 0.8423949122428894\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8511977195739746\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7298074424266815, and Avg Validation Loss (mae) is 0.7487644493579865\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7364401787519455\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6828091740608215, and Avg Validation Loss (mae) is 1.718525493144989\n",
            "After 10 runs; Avg Test Loss (mae) is 1.754943323135376\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.853289520740509, and Avg Validation Loss (mae) is 0.8251274526119232\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7999058425426483\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7489040851593017, and Avg Validation Loss (mae) is 0.7927794575691223\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8110578298568726\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8147134900093078, and Avg Validation Loss (mae) is 2.01613427400589\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9401810050010682\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8090457558631897, and Avg Validation Loss (mae) is 0.7835490822792053\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8156831979751586\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MB, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5714350134134293, and Avg Validation Loss (mae) is 0.639521935582161\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6430430889129639\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.669450855255127, and Avg Validation Loss (mae) is 1.6146434903144837\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5981160402297974\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9035032451152801, and Avg Validation Loss (mae) is 1.0561041712760926\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0205535590648651\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0493003249168396, and Avg Validation Loss (mae) is 0.9476441264152526\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9383977174758911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "QcS7qsnutTkY",
        "outputId": "1214b822-f635-4a08-ef0e-a2aad9712a5b"
      },
      "source": [
        "CombResults5 = pd.DataFrame.from_dict(my_dictMF5)\n",
        "CombResultsSorted5 = CombResults5.sort_values(by=['Test Loss'])\n",
        "CombResultsSorted5.to_csv('CombResultsSorted5.csv')\n",
        "CombResultsSorted5\n",
        "files.download(\"CombResultsSorted5.csv\")\n",
        "fig = px.box(CombResultsSorted5, x=\"DATA_X\", y=\"Test Loss\", hover_data=[\"DATA_y\"])\n",
        "fig.show()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_fdad082d-d648-4c81-b523-6fa081afcd72\", \"CombResultsSorted5.csv\", 8118)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"fcd942ee-d396-4be8-8f42-5799c19aa29c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"fcd942ee-d396-4be8-8f42-5799c19aa29c\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'fcd942ee-d396-4be8-8f42-5799c19aa29c',\n",
              "                        [{\"alignmentgroup\": \"True\", \"customdata\": [[\"y_RM\"], [\"y_MM\"], [\"y_RM\"], [\"y_RM\"], [\"y_MM\"], [\"y_MM\"], [\"y_RM\"], [\"y_RB\"], [\"y_RB\"], [\"y_RM\"], [\"y_RB\"], [\"y_RB\"], [\"y_RM\"], [\"y_MM\"], [\"y_MM\"], [\"y_MM\"], [\"y_MB\"], [\"y_RB\"], [\"y_RB\"], [\"y_MM\"], [\"y_MM\"], [\"y_RB\"], [\"y_RM\"], [\"y_RB\"], [\"y_MB\"], [\"y_RB\"], [\"y_MM\"], [\"y_RB\"], [\"y_RM\"], [\"y_MB\"], [\"y_RB\"], [\"y_RB\"], [\"y_RM\"], [\"y_RM\"], [\"y_RM\"], [\"y_RT\"], [\"y_RB\"], [\"y_MB\"], [\"y_MM\"], [\"y_MB\"], [\"y_MM\"], [\"y_RB\"], [\"y_RM\"], [\"y_RM\"], [\"y_MM\"], [\"y_RT\"], [\"y_MB\"], [\"y_MB\"], [\"y_MM\"], [\"y_FM\"], [\"y_RM\"], [\"y_MM\"], [\"y_RT\"], [\"y_MM\"], [\"y_RB\"], [\"y_RB\"], [\"y_MB\"], [\"y_MB\"], [\"y_RM\"], [\"y_FM\"], [\"y_MB\"], [\"y_MB\"], [\"y_MM\"], [\"y_RB\"], [\"y_MB\"], [\"y_FM\"], [\"y_MM\"], [\"y_RT\"], [\"y_MM\"], [\"y_RT\"], [\"y_RB\"], [\"y_MB\"], [\"y_FM\"], [\"y_FM\"], [\"y_RM\"], [\"y_MB\"], [\"y_FM\"], [\"y_RT\"], [\"y_FM\"], [\"y_FM\"], [\"y_MB\"], [\"y_FM\"], [\"y_FM\"], [\"y_RB\"], [\"y_FM\"], [\"y_RT\"], [\"y_RT\"], [\"y_FM\"], [\"y_RT\"], [\"y_RT\"], [\"y_FM\"], [\"y_RT\"], [\"y_MM\"], [\"y_RT\"], [\"y_MB\"], [\"y_FM\"], [\"y_MB\"], [\"y_RB\"], [\"y_RB\"], [\"y_RT\"], [\"y_MT\"], [\"y_FM\"], [\"y_RM\"], [\"y_FM\"], [\"y_RT\"], [\"y_RM\"], [\"y_RM\"], [\"y_FM\"], [\"y_FM\"], [\"y_RT\"], [\"y_MB\"], [\"y_MT\"], [\"y_FM\"], [\"y_MT\"], [\"y_MB\"], [\"y_RT\"], [\"y_MT\"], [\"y_RT\"], [\"y_MB\"], [\"y_MT\"], [\"y_RM\"], [\"y_RM\"], [\"y_MM\"], [\"y_MT\"], [\"y_FM\"], [\"y_MT\"], [\"y_RT\"], [\"y_RT\"], [\"y_MT\"], [\"y_MB\"], [\"y_MM\"], [\"y_MT\"], [\"y_MT\"], [\"y_FM\"], [\"y_RT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}<br>DATA_y=%{customdata[0]}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_FMX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MBX_RM\", \"X_FTX_MTX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_MBX_RT\", \"X_FTX_MTX_RTX_RMX_RB\", \"X_FTX_MTX_MBX_RMX_RB\", \"X_FMX_MTX_MMX_RTX_RB\", \"X_FTX_FMX_MMX_MBX_RT\", \"X_FMX_MTX_MMX_RTX_RM\", \"X_FTX_MTX_MMX_RTX_RB\", \"X_FTX_FMX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_RTX_RM\", \"X_FMX_MTX_MMX_MBX_RT\", \"X_FTX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RT\", \"X_MTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_RT\", \"X_FTX_FMX_MTX_RTX_RM\", \"X_FTX_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RB\", \"X_FTX_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MTX_MMX_RT\", \"X_FTX_MTX_MBX_RTX_RM\", \"X_FMX_MTX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RT\", \"X_FMX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RT\", \"X_FTX_FMX_MTX_RTX_RB\", \"X_FTX_MTX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RM\", \"X_FMX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_MBX_RB\", \"X_FTX_FMX_MMX_RTX_RB\", \"X_FMX_MMX_MBX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_RTX_RM\", \"X_FTX_FMX_MMX_RTX_RB\", \"X_FMX_MTX_MBX_RTX_RB\", \"X_FMX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RT\", \"X_FTX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RMX_RB\", \"X_FMX_MTX_MMX_RTX_RB\", \"X_FTX_FMX_MMX_RTX_RM\", \"X_FTX_FMX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_MBX_RB\", \"X_FTX_FMX_MMX_MBX_RT\", \"X_FMX_MBX_RTX_RMX_RB\", \"X_MTX_MMX_MBX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RB\", \"X_FTX_FMX_MMX_RTX_RM\", \"X_FTX_MMX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_RTX_RM\", \"X_FMX_MTX_MBX_RTX_RB\", \"X_FTX_MTX_MBX_RMX_RB\", \"X_FMX_MMX_RTX_RMX_RB\", \"X_FTX_MTX_RTX_RMX_RB\", \"X_FTX_FMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_RM\", \"X_FTX_FMX_MTX_MMX_RB\", \"X_FTX_MMX_MBX_RTX_RM\", \"X_FMX_MTX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RM\", \"X_FTX_FMX_MTX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_MMX_MB\", \"X_FTX_MTX_MMX_RTX_RB\", \"X_FTX_MMX_RTX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RB\", \"X_MTX_MMX_MBX_RTX_RB\", \"X_FTX_MMX_RTX_RMX_RB\", \"X_FTX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RMX_RB\", \"X_FTX_MTX_RTX_RMX_RB\", \"X_MTX_MMX_RTX_RMX_RB\", \"X_MTX_MMX_RTX_RMX_RB\", \"X_MTX_MMX_MBX_RTX_RB\", \"X_FTX_MMX_MBX_RMX_RB\", \"X_MTX_MMX_MBX_RTX_RM\", \"X_MTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_RM\", \"X_FMX_MTX_MBX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_RMX_RB\", \"X_MTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_RTX_RB\", \"X_FMX_MTX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_MMX_RM\", \"X_FTX_MTX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RM\", \"X_FTX_MTX_MMX_MBX_RM\", \"X_FTX_MTX_MMX_MBX_RB\", \"X_FTX_MMX_MBX_RTX_RM\", \"X_MTX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MMX_MBX_RB\", \"X_FTX_MTX_MMX_MBX_RT\", \"X_FTX_MTX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_RB\", \"X_FMX_MTX_MMX_MBX_RB\", \"X_FTX_MTX_MMX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RM\", \"X_FTX_FMX_MBX_RMX_RB\", \"X_FMX_MTX_RTX_RMX_RB\", \"X_MMX_MBX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RB\", \"X_FTX_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_RTX_RB\", \"X_FTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RB\", \"X_FMX_MTX_MMX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RB\", \"X_FTX_FMX_MTX_MBX_RB\", \"X_FTX_FMX_MTX_MMX_MB\", \"X_FTX_MBX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RTX_RB\", \"X_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RB\", \"X_FTX_FMX_MMX_MBX_RB\", \"X_FTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_RTX_RMX_RB\", \"X_FTX_FMX_RTX_RMX_RB\", \"X_FTX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MBX_RMX_RB\", \"X_FTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RB\", \"X_FMX_MMX_MBX_RTX_RM\", \"X_FTX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RB\", \"X_FTX_FMX_MMX_RTX_RM\", \"X_FTX_FMX_MBX_RTX_RB\", \"X_FMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RT\", \"X_FMX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_RMX_RB\", \"X_FTX_FMX_RTX_RMX_RB\", \"X_FTX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MBX_RMX_RB\", \"X_FMX_MMX_MBX_RMX_RB\", \"X_FMX_MMX_MBX_RTX_RM\", \"X_FMX_MTX_MMX_MBX_RB\", \"X_FMX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MBX_RTX_RM\", \"X_FMX_MTX_MMX_RMX_RB\", \"X_FMX_MTX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_MB\", \"X_FMX_MTX_RTX_RMX_RB\", \"X_MTX_MMX_MBX_RTX_RB\", \"X_FMX_MMX_MBX_RTX_RB\", \"X_FMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_RTX_RM\", \"X_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RT\", \"X_FMX_MTX_MMX_RTX_RB\", \"X_FMX_MTX_MMX_MBX_RM\", \"X_MTX_MMX_MBX_RTX_RM\", \"X_MTX_MMX_MBX_RMX_RB\", \"X_MTX_MMX_RTX_RMX_RB\", \"X_MTX_MBX_RTX_RMX_RB\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.4872525632381439, 0.4896452844142914, 0.516474387049675, 0.5319697737693787, 0.5525244742631912, 0.5615256488323211, 0.5715694934129715, 0.5884479463100434, 0.5998126029968261, 0.6015739291906357, 0.6039435297250748, 0.6080698937177658, 0.6129620015621186, 0.615435716509819, 0.6410115152597428, 0.6430430889129639, 0.6443597733974457, 0.647355580329895, 0.6477502554655075, 0.6489262163639069, 0.6524884819984436, 0.6577983677387238, 0.6643171578645706, 0.6643442332744598, 0.6676001101732254, 0.6710421353578567, 0.6714874297380448, 0.6790668368339539, 0.6792305290699006, 0.6800466418266297, 0.6879593431949615, 0.6924594908952713, 0.6928680747747421, 0.6933363080024719, 0.6945286303758621, 0.7049738824367523, 0.7069332450628281, 0.7078349769115448, 0.7085574865341187, 0.7090144544839859, 0.7097736299037933, 0.7115960419178009, 0.712155020236969, 0.714850303530693, 0.7158109873533249, 0.716923275589943, 0.7216343432664871, 0.7271194636821747, 0.7274696975946426, 0.7287082135677337, 0.7297075688838959, 0.7306910753250122, 0.7364401787519455, 0.7377741485834122, 0.7399915993213654, 0.7402277141809464, 0.7473659455776215, 0.747688502073288, 0.7480687409639358, 0.7515243530273438, 0.7556658029556275, 0.7561680853366852, 0.7569675773382187, 0.7581356763839722, 0.7605716824531555, 0.7621321439743042, 0.7641268670558929, 0.7655639290809632, 0.7675232350826263, 0.76760293841362, 0.7679009854793548, 0.7714860260486602, 0.7715451180934906, 0.7744389593601226, 0.7805281519889832, 0.7862783133983612, 0.7870293021202087, 0.7907678365707398, 0.796795254945755, 0.7999058425426483, 0.8110578298568726, 0.8114244401454925, 0.8140719473361969, 0.8149394273757935, 0.8156831979751586, 0.823357778787613, 0.8238872140645981, 0.8298101127147675, 0.8336196213960647, 0.8376083910465241, 0.8511977195739746, 0.8554501950740814, 0.8602491229772568, 0.8641578316688537, 0.8649742990732193, 0.8675545692443848, 0.870468258857727, 0.8808568596839905, 0.8929218709468841, 0.8999246776103973, 0.9072666645050049, 0.9085443556308747, 0.9113071501255036, 0.9137429237365723, 0.9168340146541596, 0.9175852298736572, 0.9234088361263275, 0.9276616394519805, 0.9294250309467316, 0.9311692833900451, 0.9369261622428894, 0.9383977174758911, 0.9431354761123657, 0.9432756662368774, 0.9532909393310547, 0.9578251212835311, 0.9765572667121887, 0.9826938688755036, 0.9835504710674285, 1.0009610414505006, 1.0082775413990022, 1.0090260624885559, 1.0175928801298142, 1.0180826544761659, 1.0205535590648651, 1.0233844935894012, 1.0251981139183044, 1.0277075231075288, 1.033756297826767, 1.041464626789093, 1.0426179468631744, 1.0438237607479095, 1.0449582040309906, 1.0532406449317933, 1.064403909444809, 1.0788897454738617, 1.0828614413738251, 1.0924286842346191, 1.1628105878829955, 1.1810413599014282, 1.224165952205658, 1.2358265817165375, 1.27238529920578, 1.3081883907318115, 1.3887821435928345, 1.4168769478797913, 1.417473590373993, 1.4326835036277772, 1.439200472831726, 1.48566073179245, 1.4927450299263, 1.5296367168426515, 1.5346869230270386, 1.573837399482727, 1.5894007563591004, 1.5897136211395264, 1.5905714273452758, 1.591223454475403, 1.592409884929657, 1.5932409644126893, 1.5981160402297974, 1.6258222222328187, 1.6656042218208313, 1.6804510712623597, 1.7310853242874145, 1.7341665267944335, 1.754943323135376, 1.9401810050010682], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('fcd942ee-d396-4be8-8f42-5799c19aa29c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47181a43-9579-4029-8902-8439ea3a7a1d",
        "id": "gDsiZ5e5tTkY"
      },
      "source": [
        "CombResultsSortedgrouped5 = CombResultsSorted5.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedgroupedsortedMF5 = CombResultsSortedgrouped5.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedgroupedsortedMF5.to_csv('CombResultsSortedgroupedsortedMF5.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedgroupedsortedMF5.csv\")\n",
        "CombResultsSortedgroupedsortedMF5"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1e4f2d75-4e5d-4f8f-a8f8-2f66000acf1d\", \"CombResultsSortedgroupedsortedMF5.csv\", 2250)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RM</th>\n",
              "      <td>0.647723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RT</th>\n",
              "      <td>0.659906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RTX_RB</th>\n",
              "      <td>0.676229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RT</th>\n",
              "      <td>0.677411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RT</th>\n",
              "      <td>0.701170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RTX_RM</th>\n",
              "      <td>0.701201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RTX_RMX_RB</th>\n",
              "      <td>0.701829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RTX_RM</th>\n",
              "      <td>0.703197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RTX_RM</th>\n",
              "      <td>0.718557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RTX_RM</th>\n",
              "      <td>0.743142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RMX_RB</th>\n",
              "      <td>0.743295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RTX_RB</th>\n",
              "      <td>0.772065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RB</th>\n",
              "      <td>0.773990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RMX_RB</th>\n",
              "      <td>0.797317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RTX_RB</th>\n",
              "      <td>0.801501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>0.803209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RM</th>\n",
              "      <td>0.808215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RM</th>\n",
              "      <td>0.815489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RMX_RB</th>\n",
              "      <td>0.825200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RTX_RB</th>\n",
              "      <td>0.830924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.848568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RT</th>\n",
              "      <td>0.851327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RM</th>\n",
              "      <td>0.863317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RTX_RB</th>\n",
              "      <td>0.867126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RTX_RM</th>\n",
              "      <td>0.876641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>0.880228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RB</th>\n",
              "      <td>0.886950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RB</th>\n",
              "      <td>0.908390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RMX_RB</th>\n",
              "      <td>0.911032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>0.935218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RTX_RM</th>\n",
              "      <td>0.953551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RMX_RB</th>\n",
              "      <td>0.957158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RTX_RM</th>\n",
              "      <td>0.964528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RT</th>\n",
              "      <td>0.981906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RTX_RB</th>\n",
              "      <td>0.986269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RMX_RB</th>\n",
              "      <td>1.001829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RB</th>\n",
              "      <td>1.010481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RTX_RB</th>\n",
              "      <td>1.010560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RTX_RB</th>\n",
              "      <td>1.032186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RMX_RB</th>\n",
              "      <td>1.053681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>1.060841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RTX_RM</th>\n",
              "      <td>1.076562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RTX_RMX_RB</th>\n",
              "      <td>1.080817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RMX_RB</th>\n",
              "      <td>1.103002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>1.107268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>1.121969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MB</th>\n",
              "      <td>1.122109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.132969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RM</th>\n",
              "      <td>1.141822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RB</th>\n",
              "      <td>1.144756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>1.151523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RTX_RMX_RB</th>\n",
              "      <td>1.157622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.162570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RTX_RMX_RB</th>\n",
              "      <td>1.173599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.182422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.185689</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Test Loss\n",
              "DATA_X                         \n",
              "X_FTX_FMX_MTX_MBX_RM   0.647723\n",
              "X_FTX_FMX_MTX_MMX_RT   0.659906\n",
              "X_FTX_MTX_MBX_RTX_RB   0.676229\n",
              "X_FTX_FMX_MTX_MBX_RT   0.677411\n",
              "X_FTX_MTX_MMX_MBX_RT   0.701170\n",
              "X_FTX_FMX_MTX_RTX_RM   0.701201\n",
              "X_FTX_MTX_RTX_RMX_RB   0.701829\n",
              "X_FTX_MTX_MBX_RTX_RM   0.703197\n",
              "X_FTX_MTX_MMX_RTX_RM   0.718557\n",
              "X_FTX_FMX_MBX_RTX_RM   0.743142\n",
              "X_FTX_MTX_MBX_RMX_RB   0.743295\n",
              "X_FTX_MTX_MMX_RTX_RB   0.772065\n",
              "X_FTX_MTX_MMX_MBX_RB   0.773990\n",
              "X_FTX_MTX_MMX_RMX_RB   0.797317\n",
              "X_FTX_FMX_MMX_RTX_RB   0.801501\n",
              "X_FTX_MMX_MBX_RTX_RM   0.803209\n",
              "X_FTX_FMX_MMX_MBX_RM   0.808215\n",
              "X_FTX_FMX_MTX_MMX_RM   0.815489\n",
              "X_FTX_FMX_MTX_RMX_RB   0.825200\n",
              "X_FTX_FMX_MTX_RTX_RB   0.830924\n",
              "X_FTX_MMX_MBX_RTX_RB   0.848568\n",
              "X_FTX_FMX_MMX_MBX_RT   0.851327\n",
              "X_FTX_MTX_MMX_MBX_RM   0.863317\n",
              "X_FTX_FMX_MBX_RTX_RB   0.867126\n",
              "X_FTX_FMX_MMX_RTX_RM   0.876641\n",
              "X_FTX_MMX_RTX_RMX_RB   0.880228\n",
              "X_FTX_FMX_MTX_MMX_RB   0.886950\n",
              "X_FTX_FMX_MTX_MBX_RB   0.908390\n",
              "X_FTX_FMX_MBX_RMX_RB   0.911032\n",
              "X_FTX_MMX_MBX_RMX_RB   0.935218\n",
              "X_FMX_MTX_MMX_RTX_RM   0.953551\n",
              "X_FTX_FMX_MMX_RMX_RB   0.957158\n",
              "X_FMX_MTX_MBX_RTX_RM   0.964528\n",
              "X_FMX_MTX_MMX_MBX_RT   0.981906\n",
              "X_FMX_MTX_MMX_RTX_RB   0.986269\n",
              "X_FMX_MTX_MBX_RMX_RB   1.001829\n",
              "X_FTX_FMX_MMX_MBX_RB   1.010481\n",
              "X_FMX_MTX_MBX_RTX_RB   1.010560\n",
              "X_FMX_MMX_MBX_RTX_RB   1.032186\n",
              "X_FMX_MMX_MBX_RMX_RB   1.053681\n",
              "X_MTX_MMX_MBX_RTX_RB   1.060841\n",
              "X_FMX_MMX_MBX_RTX_RM   1.076562\n",
              "X_FMX_MTX_RTX_RMX_RB   1.080817\n",
              "X_FMX_MTX_MMX_RMX_RB   1.103002\n",
              "X_MTX_MMX_MBX_RMX_RB   1.107268\n",
              "X_MTX_MMX_RTX_RMX_RB   1.121969\n",
              "X_FTX_FMX_MTX_MMX_MB   1.122109\n",
              "X_MTX_MBX_RTX_RMX_RB   1.132969\n",
              "X_FMX_MTX_MMX_MBX_RM   1.141822\n",
              "X_FMX_MTX_MMX_MBX_RB   1.144756\n",
              "X_MTX_MMX_MBX_RTX_RM   1.151523\n",
              "X_FTX_FMX_RTX_RMX_RB   1.157622\n",
              "X_FTX_MBX_RTX_RMX_RB   1.162570\n",
              "X_FMX_MMX_RTX_RMX_RB   1.173599\n",
              "X_FMX_MBX_RTX_RMX_RB   1.182422\n",
              "X_MMX_MBX_RTX_RMX_RB   1.185689"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CdAZ2jBauOPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmyDm9tauVmV",
        "outputId": "076c033b-69ef-4607-b19d-293f6eb88992"
      },
      "source": [
        "#Combinations of 6 Sensors, therefore 42 features, 42 inputs\n",
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "inp_shp = 42\n",
        "n_sensors = 6\n",
        "\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "modelG(inp_shp)\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "my_dictMF6 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), n_sensors):\n",
        "  kX1, kX2, kX3, kX4, kX5, kX6 = combo[0][0], combo[1][0], combo[2][0], combo[3][0], combo[4][0], combo[5][0]\n",
        "  vX1, vX2, vX3, vX4, vX5, vX6 = combo[0][1], combo[1][1], combo[2][1], combo[3][1], combo[4][1], combo[5][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:] or ky[-2:] == kX4[-2:] or ky[-2:] == kX5[-2:] or ky[-2:] == kX6[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, kx4 = {kX4}, kx5 = {kX5}, kx6 = {kX6}, ky = {ky}')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF6 = evaldata(n_sensors, X_in1=vX1, X_in2=vX2, X_in3=vX3, X_in4=vX4, X_in5=vX5, X_in6=vX6, Y_in=vy, traindata1 = kX1, traindata2 = kX2, traindata3 = kX3, traindata4 = kX4, traindata5 = kX5, traindata6 = kX6, testdata = ky)\n",
        "      TestLossTotal = resultsMF6[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF6[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF6[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}, {kX2}, {kX3}, {kX4}, {kX5}, {kX6} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF6[\"DATA_X\"].append(kX1 + kX2 + kX3 + kX4 + kX5 + kX6)\n",
        "    my_dictMF6[\"DATA_y\"].append(ky)\n",
        "    my_dictMF6[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model.load_weights('model.h5')\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RT, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB, X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8088268071413041, and Avg Validation Loss (mae) is 1.0269912481307983\n",
            "After 10 runs; Avg Test Loss (mae) is 1.007518357038498\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RT, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB, X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7061010360717773, and Avg Validation Loss (mae) is 0.6179820418357849\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6398127973079681\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RM, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB, X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7821192681789398, and Avg Validation Loss (mae) is 0.9013587236404419\n",
            "After 10 runs; Avg Test Loss (mae) is 0.884413480758667\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.639833390712738, and Avg Validation Loss (mae) is 0.6315158486366272\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6264708459377288\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0588228404521942, and Avg Validation Loss (mae) is 0.9594793260097504\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0030250668525695\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7827824711799621, and Avg Validation Loss (mae) is 0.7260690629482269\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7090746164321899\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RM, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RT, X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.687562245130539, and Avg Validation Loss (mae) is 0.8335503429174423\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8396224468946457\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6326509177684784, and Avg Validation Loss (mae) is 0.6052999854087829\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6146658152341843\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RT, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7267923772335052, and Avg Validation Loss (mae) is 0.7479307234287262\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7086800396442413\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5928672403097153, and Avg Validation Loss (mae) is 0.7361147433519364\n",
            "After 10 runs; Avg Test Loss (mae) is 0.724073302745819\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, kx6 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7060586988925934, and Avg Validation Loss (mae) is 0.8426815390586853\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8471093297004699\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, kx6 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6622038155794143, and Avg Validation Loss (mae) is 0.6170540094375611\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6158799260854722\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RT, X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4809752106666565, and Avg Validation Loss (mae) is 0.6239951372146606\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6106168538331985\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.670521330833435, and Avg Validation Loss (mae) is 0.7260904967784881\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7159108430147171\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RT, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5525170862674713, and Avg Validation Loss (mae) is 0.5525202125310897\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5700571864843369\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6243024587631225, and Avg Validation Loss (mae) is 0.6621912509202957\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6498753994703292\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5264781236648559, and Avg Validation Loss (mae) is 0.5219846397638321\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5213296860456467\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7421343624591827, and Avg Validation Loss (mae) is 0.7422341227531433\n",
            "After 10 runs; Avg Test Loss (mae) is 0.770192539691925\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.540086105465889, and Avg Validation Loss (mae) is 0.4655245393514633\n",
            "After 10 runs; Avg Test Loss (mae) is 0.45769267082214354\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.76416797041893, and Avg Validation Loss (mae) is 0.9272502541542054\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9274573862552643\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RT, X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9953828692436218, and Avg Validation Loss (mae) is 0.9104256749153137\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9350729823112488\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6018936395645141, and Avg Validation Loss (mae) is 0.5471384286880493\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5437969714403152\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RT, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9688066720962525, and Avg Validation Loss (mae) is 0.965738719701767\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9628767549991608\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5857241988182068, and Avg Validation Loss (mae) is 0.632179656624794\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6332468032836914\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0273846507072448, and Avg Validation Loss (mae) is 1.0268507480621338\n",
            "After 10 runs; Avg Test Loss (mae) is 1.040682029724121\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8212924182415009, and Avg Validation Loss (mae) is 0.8133918851613998\n",
            "After 10 runs; Avg Test Loss (mae) is 0.813314613699913\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0580902874469758, and Avg Validation Loss (mae) is 1.078860205411911\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0550369560718535\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7395057678222656, and Avg Validation Loss (mae) is 0.8128880828619003\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8175849139690399\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1309518575668336, and Avg Validation Loss (mae) is 1.0116130828857421\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0209176957607269\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MB, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.591744726896286, and Avg Validation Loss (mae) is 0.6496013432741166\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6451461255550385\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RT, X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.793315988779068, and Avg Validation Loss (mae) is 0.885457843542099\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8609164774417877\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5886168956756592, and Avg Validation Loss (mae) is 0.5634550958871841\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5818756878376007\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RT, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7597604632377625, and Avg Validation Loss (mae) is 0.7888125300407409\n",
            "After 10 runs; Avg Test Loss (mae) is 0.785048371553421\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6319707274436951, and Avg Validation Loss (mae) is 0.5768691837787628\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5770073086023331\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7367683827877045, and Avg Validation Loss (mae) is 0.7668598890304565\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7813278257846832\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7138489246368408, and Avg Validation Loss (mae) is 0.7368113219738006\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7210528761148453\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7573555707931519, and Avg Validation Loss (mae) is 0.7170205891132355\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7207440793514251\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7743462741374969, and Avg Validation Loss (mae) is 0.8126030564308167\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8220605552196503\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8702288925647735, and Avg Validation Loss (mae) is 0.8683437764644623\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8944653987884521\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MB, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5464091092348099, and Avg Validation Loss (mae) is 0.5286401659250259\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5293348282575607\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8307015895843506, and Avg Validation Loss (mae) is 0.9615658104419709\n",
            "After 10 runs; Avg Test Loss (mae) is 0.957571280002594\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1035198748111725, and Avg Validation Loss (mae) is 1.0922628819942475\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1241622269153595\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RT, X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6141568183898927, and Avg Validation Loss (mae) is 1.497008240222931\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4536319971084595\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6538508892059326, and Avg Validation Loss (mae) is 0.5413838565349579\n",
            "After 10 runs; Avg Test Loss (mae) is 0.552565187215805\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RT, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5103352189064025, and Avg Validation Loss (mae) is 1.5789631485939026\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5793723583221435\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5491587430238724, and Avg Validation Loss (mae) is 0.4723944187164307\n",
            "After 10 runs; Avg Test Loss (mae) is 0.46514759957790375\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5793906688690185, and Avg Validation Loss (mae) is 1.7520164251327515\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7585121631622314\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7168134599924088, and Avg Validation Loss (mae) is 0.9235422551631928\n",
            "After 10 runs; Avg Test Loss (mae) is 0.921927210688591\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5507320761680603, and Avg Validation Loss (mae) is 1.6576910495758057\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6861864805221558\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8091693878173828, and Avg Validation Loss (mae) is 0.8261865496635437\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8428083717823028\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6169453620910645, and Avg Validation Loss (mae) is 1.5857728719711304\n",
            "After 10 runs; Avg Test Loss (mae) is 1.554308831691742\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MB, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.503791457414627, and Avg Validation Loss (mae) is 0.5502679258584976\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5516441106796265\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4796493411064149, and Avg Validation Loss (mae) is 1.623602557182312\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6126602292060852\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9837118268013001, and Avg Validation Loss (mae) is 0.9094995617866516\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9091721594333648\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7498029708862304, and Avg Validation Loss (mae) is 1.8674779891967774\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8529404878616333\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT, X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8423061788082122, and Avg Validation Loss (mae) is 0.8862269520759583\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8825763881206512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "_2chbE-kuVmX",
        "outputId": "05f44b2d-e9aa-4896-f71d-0645f68762d0"
      },
      "source": [
        "CombResults6 = pd.DataFrame.from_dict(my_dictMF6)\n",
        "CombResultsSorted6 = CombResults6.sort_values(by=['Test Loss'])\n",
        "CombResultsSorted6.to_csv('CombResultsSorted6.csv')\n",
        "CombResultsSorted6\n",
        "files.download(\"CombResultsSorted6.csv\")\n",
        "fig = px.box(CombResultsSorted6, x=\"DATA_X\", y=\"Test Loss\", hover_data=[\"DATA_y\"])\n",
        "fig.show()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b99b4836-19f1-4011-a05b-42852ea0282b\", \"CombResultsSorted6.csv\", 2918)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4708366e-0b86-4963-bf85-cb7d3495c7ce\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4708366e-0b86-4963-bf85-cb7d3495c7ce\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4708366e-0b86-4963-bf85-cb7d3495c7ce',\n",
              "                        [{\"alignmentgroup\": \"True\", \"customdata\": [[\"y_MM\"], [\"y_RM\"], [\"y_MM\"], [\"y_MM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RB\"], [\"y_RT\"], [\"y_RB\"], [\"y_RM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RM\"], [\"y_MB\"], [\"y_RM\"], [\"y_RB\"], [\"y_FM\"], [\"y_RT\"], [\"y_RM\"], [\"y_RT\"], [\"y_FM\"], [\"y_FM\"], [\"y_RT\"], [\"y_MB\"], [\"y_MB\"], [\"y_MB\"], [\"y_MB\"], [\"y_MB\"], [\"y_FM\"], [\"y_FM\"], [\"y_RT\"], [\"y_FM\"], [\"y_MT\"], [\"y_RT\"], [\"y_MB\"], [\"y_MT\"], [\"y_FM\"], [\"y_MT\"], [\"y_RT\"], [\"y_RM\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_MT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}<br>DATA_y=%{customdata[0]}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_FTX_FMX_MTX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MBX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RTX_RM\", \"X_FMX_MTX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RM\", \"X_FTX_FMX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RB\", \"X_FTX_FMX_MTX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_RTX_RB\", \"X_FTX_FMX_MTX_MBX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_RTX_RM\", \"X_FMX_MTX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RTX_RM\", \"X_MTX_MMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RM\", \"X_FTX_MTX_MBX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RTX_RM\", \"X_FTX_MMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RMX_RB\", \"X_FTX_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RM\", \"X_FMX_MTX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RB\", \"X_FMX_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RMX_RB\", \"X_MTX_MMX_MBX_RTX_RMX_RB\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.45769267082214354, 0.46514759957790375, 0.5213296860456467, 0.5293348282575607, 0.5437969714403152, 0.5516441106796265, 0.552565187215805, 0.5700571864843369, 0.5770073086023331, 0.5818756878376007, 0.6106168538331985, 0.6146658152341843, 0.6158799260854722, 0.6264708459377288, 0.6332468032836914, 0.6398127973079681, 0.6451461255550385, 0.6498753994703292, 0.7086800396442413, 0.7090746164321899, 0.7159108430147171, 0.7207440793514251, 0.7210528761148453, 0.724073302745819, 0.770192539691925, 0.7813278257846832, 0.785048371553421, 0.813314613699913, 0.8175849139690399, 0.8220605552196503, 0.8396224468946457, 0.8428083717823028, 0.8471093297004699, 0.8609164774417877, 0.8825763881206512, 0.884413480758667, 0.8944653987884521, 0.9091721594333648, 0.921927210688591, 0.9274573862552643, 0.9350729823112488, 0.957571280002594, 0.9628767549991608, 1.0030250668525695, 1.007518357038498, 1.0209176957607269, 1.040682029724121, 1.0550369560718535, 1.1241622269153595, 1.4536319971084595, 1.554308831691742, 1.5793723583221435, 1.6126602292060852, 1.6861864805221558, 1.7585121631622314, 1.8529404878616333], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4708366e-0b86-4963-bf85-cb7d3495c7ce');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "id": "XqsPt6XUuVmY",
        "outputId": "b2d6a00f-3288-4430-a066-3cdd3d3f5017"
      },
      "source": [
        "CombResultsSortedgrouped6 = CombResultsSorted6.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedgroupedsortedMF6 = CombResultsSortedgrouped6.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedgroupedsortedMF6.to_csv('CombResultsSortedgroupedsortedMF6.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedgroupedsortedMF6.csv\")\n",
        "CombResultsSortedgroupedsortedMF6"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_70214857-7d5a-41ba-ab99-123c5c025a62\", \"CombResultsSortedgroupedsortedMF6.csv\", 1244)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RTX_RB</th>\n",
              "      <td>0.609966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RMX_RB</th>\n",
              "      <td>0.645761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RTX_RM</th>\n",
              "      <td>0.663264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.681028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RTX_RMX_RB</th>\n",
              "      <td>0.692575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>0.711900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RTX_RB</th>\n",
              "      <td>0.716377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>0.721396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RTX_RM</th>\n",
              "      <td>0.727144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RMX_RB</th>\n",
              "      <td>0.731495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RTX_RM</th>\n",
              "      <td>0.739435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>0.751190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RM</th>\n",
              "      <td>0.755442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>0.771402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.798062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RT</th>\n",
              "      <td>0.823666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>0.833032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RB</th>\n",
              "      <td>0.856050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RMX_RB</th>\n",
              "      <td>0.926998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RTX_RMX_RB</th>\n",
              "      <td>0.936311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>1.003099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>1.022260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.040867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.052976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.260916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>1.264497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>1.340220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.367758</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          Test Loss\n",
              "DATA_X                             \n",
              "X_FTX_FMX_MTX_MBX_RTX_RB   0.609966\n",
              "X_FTX_FMX_MTX_MBX_RMX_RB   0.645761\n",
              "X_FTX_FMX_MTX_MBX_RTX_RM   0.663264\n",
              "X_FTX_MTX_MMX_MBX_RTX_RB   0.681028\n",
              "X_FTX_FMX_MTX_RTX_RMX_RB   0.692575\n",
              "X_FTX_MTX_MBX_RTX_RMX_RB   0.711900\n",
              "X_FTX_FMX_MTX_MMX_RTX_RB   0.716377\n",
              "X_FTX_MTX_MMX_MBX_RTX_RM   0.721396\n",
              "X_FTX_FMX_MTX_MMX_RTX_RM   0.727144\n",
              "X_FTX_FMX_MTX_MMX_RMX_RB   0.731495\n",
              "X_FTX_FMX_MMX_MBX_RTX_RM   0.739435\n",
              "X_FTX_MTX_MMX_MBX_RMX_RB   0.751190\n",
              "X_FTX_FMX_MTX_MMX_MBX_RM   0.755442\n",
              "X_FTX_MTX_MMX_RTX_RMX_RB   0.771402\n",
              "X_FTX_FMX_MMX_MBX_RTX_RB   0.798062\n",
              "X_FTX_FMX_MTX_MMX_MBX_RT   0.823666\n",
              "X_FTX_FMX_MBX_RTX_RMX_RB   0.833032\n",
              "X_FTX_FMX_MTX_MMX_MBX_RB   0.856050\n",
              "X_FTX_FMX_MMX_MBX_RMX_RB   0.926998\n",
              "X_FTX_FMX_MMX_RTX_RMX_RB   0.936311\n",
              "X_FMX_MTX_MMX_MBX_RTX_RM   1.003099\n",
              "X_FMX_MTX_MMX_MBX_RTX_RB   1.022260\n",
              "X_FTX_MMX_MBX_RTX_RMX_RB   1.040867\n",
              "X_FMX_MTX_MBX_RTX_RMX_RB   1.052976\n",
              "X_FMX_MMX_MBX_RTX_RMX_RB   1.260916\n",
              "X_FMX_MTX_MMX_RTX_RMX_RB   1.264497\n",
              "X_FMX_MTX_MMX_MBX_RMX_RB   1.340220\n",
              "X_MTX_MMX_MBX_RTX_RMX_RB   1.367758"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "66FMSh8DvbGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq5xI6YyvbpA",
        "outputId": "0d4071a7-923b-4758-e0a2-f87599d1457e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Combinations of 7 Sensors, therefore 49 features, 49 inputs\n",
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "inp_shp = 49\n",
        "n_sensors = 7\n",
        "\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "modelG(inp_shp)\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "my_dictMF7 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), n_sensors):\n",
        "  kX1, kX2, kX3, kX4, kX5, kX6, kX7 = combo[0][0], combo[1][0], combo[2][0], combo[3][0], combo[4][0], combo[5][0], combo[6][0]\n",
        "  vX1, vX2, vX3, vX4, vX5, vX6, vX7 = combo[0][1], combo[1][1], combo[2][1], combo[3][1], combo[4][1], combo[5][1], combo[6][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:] or ky[-2:] == kX4[-2:] or ky[-2:] == kX5[-2:] or ky[-2:] == kX6[-2:] or ky[-2:] == kX7[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, kx4 = {kX4}, kx5 = {kX5}, kx6 = {kX6}, kx7 = {kX7}, ky = {ky}')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF7 = evaldata(n_sensors, X_in1=vX1, X_in2=vX2, X_in3=vX3, X_in4=vX4, X_in5=vX5, X_in6=vX6, X_in7=vX7, Y_in=vy, traindata1 = kX1, traindata2 = kX2, traindata3 = kX3, traindata4 = kX4, traindata5 = kX5, traindata6 = kX6, traindata7 = kX7, testdata = ky)\n",
        "      TestLossTotal = resultsMF7[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF7[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF7[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}, {kX2}, {kX3}, {kX4}, {kX5}, {kX6}, {kX7} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF7[\"DATA_X\"].append(kX1 + kX2 + kX3 + kX4 + kX5 + kX6 + kX7)\n",
        "    my_dictMF7[\"DATA_y\"].append(ky)\n",
        "    my_dictMF7[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model.load_weights('model.h5')\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RT, kx7 = X_RM, ky = y_RB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB, X_RT, X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6094319194555282, and Avg Validation Loss (mae) is 0.6623960733413696\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6527139872312546\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RT, kx7 = X_RB, ky = y_RM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB, X_RT, X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6573131561279297, and Avg Validation Loss (mae) is 0.6272068113088608\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6281053274869919\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RM, kx7 = X_RB, ky = y_RT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_MB, X_RM, X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7125739216804504, and Avg Validation Loss (mae) is 0.7180254071950912\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7191651493310929\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RM, kx7 = X_RB, ky = y_MB\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MM, X_RT, X_RM, X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7165274918079376, and Avg Validation Loss (mae) is 0.7590212464332581\n",
            "After 10 runs; Avg Test Loss (mae) is 0.766008734703064\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, kx7 = X_RB, ky = y_MM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MT, X_MB, X_RT, X_RM, X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5179811924695968, and Avg Validation Loss (mae) is 0.5475715845823288\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5501567006111145\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, kx7 = X_RB, ky = y_MT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_FM, X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9802190899848938, and Avg Validation Loss (mae) is 1.3185517251491548\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2822935104370117\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, kx7 = X_RB, ky = y_FM\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT, X_MT, X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8022199690341949, and Avg Validation Loss (mae) is 0.8358681619167327\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8375009536743164\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, kx7 = X_RB, ky = y_FT\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM, X_MT, X_MM, X_MB, X_RT, X_RM, X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5179942846298218, and Avg Validation Loss (mae) is 1.6086281299591065\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5928322672843933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rT4sSylvbpC",
        "outputId": "76af2658-8b47-4456-e7a2-106a47a44be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "CombResults7 = pd.DataFrame.from_dict(my_dictMF7)\n",
        "CombResultsSorted7 = CombResults7.sort_values(by=['Test Loss'])\n",
        "CombResultsSorted7.to_csv('CombResultsSorted7.csv')\n",
        "CombResultsSorted7\n",
        "files.download(\"CombResultsSorted7.csv\")\n",
        "fig = px.box(CombResultsSorted7, x=\"DATA_X\", y=\"Test Loss\", hover_data=[\"DATA_y\"])\n",
        "fig.show()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a794bf7d-ccf3-4662-abb9-4590cd3d7dad\", \"CombResultsSorted7.csv\", 464)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"bee37d93-a84d-4996-91aa-1733cbb7eb36\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"bee37d93-a84d-4996-91aa-1733cbb7eb36\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'bee37d93-a84d-4996-91aa-1733cbb7eb36',\n",
              "                        [{\"alignmentgroup\": \"True\", \"customdata\": [[\"y_MM\"], [\"y_RM\"], [\"y_RB\"], [\"y_RT\"], [\"y_MB\"], [\"y_FM\"], [\"y_MT\"], [\"y_FT\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}<br>DATA_y=%{customdata[0]}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_FTX_FMX_MTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RMX_RB\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.5501567006111145, 0.6281053274869919, 0.6527139872312546, 0.7191651493310929, 0.766008734703064, 0.8375009536743164, 1.2822935104370117, 1.5928322672843933], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bee37d93-a84d-4996-91aa-1733cbb7eb36');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh-blHC-vbpC",
        "outputId": "b0cb06af-411d-4dce-fb1b-e72edda7f543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "CombResultsSortedgrouped7 = CombResultsSorted7.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedgroupedsortedMF7 = CombResultsSortedgrouped7.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedgroupedsortedMF7.to_csv('CombResultsSortedgroupedsortedMF7.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedgroupedsortedMF7.csv\")\n",
        "CombResultsSortedgroupedsortedMF7"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_31261bcb-9e76-46e4-bdf0-38a8c04186a5\", \"CombResultsSortedgroupedsortedMF7.csv\", 400)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>0.550157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.628105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>0.652714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>0.719165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>0.766009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>0.837501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.282294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.592832</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Test Loss\n",
              "DATA_X                                 \n",
              "X_FTX_FMX_MTX_MBX_RTX_RMX_RB   0.550157\n",
              "X_FTX_FMX_MTX_MMX_MBX_RTX_RB   0.628105\n",
              "X_FTX_FMX_MTX_MMX_MBX_RTX_RM   0.652714\n",
              "X_FTX_FMX_MTX_MMX_MBX_RMX_RB   0.719165\n",
              "X_FTX_FMX_MTX_MMX_RTX_RMX_RB   0.766009\n",
              "X_FTX_MTX_MMX_MBX_RTX_RMX_RB   0.837501\n",
              "X_FTX_FMX_MMX_MBX_RTX_RMX_RB   1.282294\n",
              "X_FMX_MTX_MMX_MBX_RTX_RMX_RB   1.592832"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    }
  ]
}