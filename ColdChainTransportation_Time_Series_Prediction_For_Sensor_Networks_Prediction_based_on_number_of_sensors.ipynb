{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColdChainTransportation-Time Series Prediction For Sensor Networks - Prediction based on number of sensors ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIMQ+2j4iEVjC/ovAfIoy2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayanoglu/Sensor-Network-Prediction-Models/blob/main/ColdChainTransportation_Time_Series_Prediction_For_Sensor_Networks_Prediction_based_on_number_of_sensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a41nLlQT_f4E"
      },
      "source": [
        "**IMPORT LIBRARIES AND DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "TbxX5SxSsO05",
        "outputId": "78bc62a6-8f30-448f-d5f6-5be384800fec"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from numpy import array\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.set_printoptions(linewidth=160)\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from decimal import *\n",
        "from google.colab import files\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import itertools\n",
        "from itertools import combinations\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-74e2176f-4485-40b9-a3a0-4b6e5d2d1a86\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-74e2176f-4485-40b9-a3a0-4b6e5d2d1a86\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving S2.csv to S2.csv\n",
            "Saving S3.csv to S3.csv\n",
            "Saving S4.csv to S4.csv\n",
            "Saving S5.csv to S5.csv\n",
            "Saving S6.csv to S6.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6i5vDVy_p0z"
      },
      "source": [
        "**READ IN THE CSV FILE, LOCATE COLUMNS AND SAVE AS FEATURES**\n",
        "\n",
        "**PLOT \"Front Top\", \"Mid Top\" AND \"Rear Top\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "id": "gN_c87WNPeLu",
        "outputId": "cb5d373c-0f18-47b1-f65a-10733677aaed"
      },
      "source": [
        "import io\n",
        "S2 = pd.read_csv(io.BytesIO(uploaded['S2.csv']))\n",
        "S3 = pd.read_csv(io.BytesIO(uploaded['S3.csv']))\n",
        "S4 = pd.read_csv(io.BytesIO(uploaded['S4.csv']))\n",
        "S5 = pd.read_csv(io.BytesIO(uploaded['S5.csv']))\n",
        "S6 = pd.read_csv(io.BytesIO(uploaded['S6.csv']))\n",
        "\n",
        "def csvtocolumns(dataset):\n",
        "  FT = dataset.iloc[:,1]\n",
        "  FM = dataset.iloc[:,2]\n",
        "  FB = dataset.iloc[:,3]\n",
        "  MT = dataset.iloc[:,4]\n",
        "  MM = dataset.iloc[:,5]\n",
        "  MB = dataset.iloc[:,6]\n",
        "  RT = dataset.iloc[:,7]\n",
        "  RM = dataset.iloc[:,8]\n",
        "  RB = dataset.iloc[:,9]\n",
        "  return FT, FM, FB, MT, MM, MB, RT, RM, RB\n",
        "\n",
        "S2FT, S2FM, S2FB, S2MT, S2MM, S2MB, S2RT, S2RM, S2RB = csvtocolumns(S2)\n",
        "S3FT, S3FM, S3FB, S3MT, S3MM, S3MB, S3RT, S3RM, S3RB = csvtocolumns(S3)\n",
        "S4FT, S4FM, S4FB, S4MT, S4MM, S4MB, S4RT, S4RM, S4RB = csvtocolumns(S4)\n",
        "S5FT, S5FM, S5FB, S5MT, S5MM, S5MB, S5RT, S5RM, S5RB = csvtocolumns(S5)\n",
        "S6FT, S6FM, S6FB, S6MT, S6MM, S6MB, S6RT, S6RM, S6RB = csvtocolumns(S6)\n",
        "\n",
        "#print(f'Length of S2 {len(S2FT)} , {len(S2FM)} , {len(S2FB)} , {len(S2MT)} , {len(S2MM)} , {len(S2MB)} , {len(S2RT)} , {len(S2RM)} , {len(S2RB)}')\n",
        "#print(f'Length of S2 {len(S3FT)} , {len(S3FM)} , {len(S3FB)} , {len(S3MT)} , {len(S3MM)} , {len(S3MB)} , {len(S3RT)} , {len(S3RM)} , {len(S3RB)}')\n",
        "#print(f'Length of S2 {len(S4FT)} , {len(S4FM)} , {len(S4FB)} , {len(S4MT)} , {len(S4MM)} , {len(S4MB)} , {len(S4RT)} , {len(S4RM)} , {len(S4RB)}')\n",
        "#print(f'Length of S2 {len(S5FT)} , {len(S5FM)} , {len(S5FB)} , {len(S5MT)} , {len(S5MM)} , {len(S5MB)} , {len(S5RT)} , {len(S5RM)} , {len(S5RB)}')\n",
        "#print(f'Length of S2 {len(S6FT)} , {len(S6FM)} , {len(S6FB)} , {len(S6MT)} , {len(S6MM)} , {len(S6MB)} , {len(S6RT)} , {len(S6RM)} , {len(S6RB)}')\n",
        "\n",
        "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(20, 15))\n",
        "\n",
        "ax[0,0].plot(S2FT, \"b\", label='S2 Front Top')\n",
        "ax[0,0].plot(S2MT, \"r\", label='S2 Mid Top')\n",
        "ax[0,0].plot(S2RT, \"y\", label='S2 Rear Top')\n",
        "ax[0,0].legend()\n",
        "\n",
        "ax[0,1].plot(S3FT, \"b\", label='S3 Front Top')\n",
        "ax[0,1].plot(S3MT, \"r\", label='S3 Mid Top')\n",
        "ax[0,1].plot(S3RT, \"y\", label='S3 Rear Top')\n",
        "ax[0,1].legend()\n",
        "\n",
        "ax[1,0].plot(S4FT, \"b\", label='S4 Front Top')\n",
        "ax[1,0].plot(S4MT, \"r\", label='S4 Mid Top')\n",
        "ax[1,0].plot(S4RT, \"y\", label='S4 Rear Top')\n",
        "ax[1,0].legend()\n",
        "\n",
        "ax[1,1].plot(S5FT, \"b\", label='S5 Front Top')\n",
        "ax[1,1].plot(S5MT, \"r\", label='S5 Mid Top')\n",
        "ax[1,1].plot(S5RT, \"y\", label='S5 Rear Top')\n",
        "ax[1,1].legend()\n",
        "\n",
        "ax[2,0].plot(S6FT, \"b\", label='S6 Front Top')\n",
        "ax[2,0].plot(S6MT, \"r\", label='S6 Mid Top')\n",
        "ax[2,0].plot(S6RT, \"y\", label='S6 Rear Top')\n",
        "ax[2,0].legend()\n",
        "\n",
        "ax[2,1].plot(S3FT, \"b\", label='S3 Front Top')\n",
        "ax[2,1].plot(S4FT, \"r\", label='S4 Front Top')\n",
        "ax[2,1].plot(S5FT, \"y\", label='S5 Front Top')\n",
        "ax[2,1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1080 with 6 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El_x1gwpSrd0",
        "outputId": "e91ba5f5-4a86-4b0c-8086-534945e3cb26"
      },
      "source": [
        "print(f'Any Nan values in S2? {(S2.isnull().values.any())}')\n",
        "print(f'Any Nan values in S3? {(S3.isnull().values.any())}')\n",
        "print(f'Any Nan values in S4? {(S4.isnull().values.any())}')\n",
        "print(f'Any Nan values in S5? {(S5.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6? {(S6.isnull().values.any())}')\n",
        "\n",
        "S6FT, S6FM, S6FB, S6MT, S6MM, S6MB, S6RT, S6RM, S6RB\n",
        "\n",
        "print(f'Any Nan values in S6FT? {(S6FT.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6FM? {(S6FM.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6FB? {(S6FB.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6MT? {(S6MT.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6MM? {(S6MM.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6MB? {(S6MB.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6RT? {(S6RT.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6RM? {(S6RM.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6RB? {(S6RB.isnull().values.any())}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any Nan values in S2? False\n",
            "Any Nan values in S3? False\n",
            "Any Nan values in S4? False\n",
            "Any Nan values in S5? False\n",
            "Any Nan values in S6? True\n",
            "Any Nan values in S6FT? False\n",
            "Any Nan values in S6FM? False\n",
            "Any Nan values in S6FB? True\n",
            "Any Nan values in S6MT? False\n",
            "Any Nan values in S6MM? False\n",
            "Any Nan values in S6MB? False\n",
            "Any Nan values in S6RT? False\n",
            "Any Nan values in S6RM? False\n",
            "Any Nan values in S6RB? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrUyIUjCATVY"
      },
      "source": [
        "**DEFINE THE SEQUENCE CREATING FUNCTION, CONVERT DATASET DATA TO ARRAY AND CREATE SEQUENCE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nJ1eiQHpXnIC",
        "outputId": "bf6b033d-9384-4330-8b84-0311f03affda"
      },
      "source": [
        "def create_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix-4] # so that we can get the 3rd data of 0-7 sequence \n",
        "\t\t# seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] \n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)\n",
        "\n",
        "S2FTA = pd.DataFrame.to_numpy(S2FT)\n",
        "S2MTA = pd.DataFrame.to_numpy(S2MT)\n",
        "S2RTA = pd.DataFrame.to_numpy(S2RT)\n",
        "S2FMA = pd.DataFrame.to_numpy(S2FM)\n",
        "S2MMA = pd.DataFrame.to_numpy(S2MM)\n",
        "S2RMA = pd.DataFrame.to_numpy(S2RM)\n",
        "S2FBA = pd.DataFrame.to_numpy(S2FB)\n",
        "S2MBA = pd.DataFrame.to_numpy(S2MB)\n",
        "S2RBA = pd.DataFrame.to_numpy(S2RB)\n",
        "\n",
        "S3FTA = pd.DataFrame.to_numpy(S3FT)\n",
        "S3MTA = pd.DataFrame.to_numpy(S3MT)\n",
        "S3RTA = pd.DataFrame.to_numpy(S3RT)\n",
        "S3FMA = pd.DataFrame.to_numpy(S3FM)\n",
        "S3MMA = pd.DataFrame.to_numpy(S3MM)\n",
        "S3RMA = pd.DataFrame.to_numpy(S3RM)\n",
        "S3FBA = pd.DataFrame.to_numpy(S3FB)\n",
        "S3MBA = pd.DataFrame.to_numpy(S3MB)\n",
        "S3RBA = pd.DataFrame.to_numpy(S3RB)\n",
        "\n",
        "S4FTA = pd.DataFrame.to_numpy(S4FT)\n",
        "S4MTA = pd.DataFrame.to_numpy(S4MT)\n",
        "S4RTA = pd.DataFrame.to_numpy(S4RT)\n",
        "S4FMA = pd.DataFrame.to_numpy(S4FM)\n",
        "S4MMA = pd.DataFrame.to_numpy(S4MM)\n",
        "S4RMA = pd.DataFrame.to_numpy(S4RM)\n",
        "S4FBA = pd.DataFrame.to_numpy(S4FB)\n",
        "S4MBA = pd.DataFrame.to_numpy(S4MB)\n",
        "S4RBA = pd.DataFrame.to_numpy(S4RB)\n",
        "\n",
        "S5FTA = pd.DataFrame.to_numpy(S5FT)\n",
        "S5MTA = pd.DataFrame.to_numpy(S5MT)\n",
        "S5RTA = pd.DataFrame.to_numpy(S5RT)\n",
        "S5FMA = pd.DataFrame.to_numpy(S5FM)\n",
        "S5MMA = pd.DataFrame.to_numpy(S5MM)\n",
        "S5RMA = pd.DataFrame.to_numpy(S5RM)\n",
        "S5FBA = pd.DataFrame.to_numpy(S5FB)\n",
        "S5MBA = pd.DataFrame.to_numpy(S5MB)\n",
        "S5RBA = pd.DataFrame.to_numpy(S5RB)\n",
        "\n",
        "S6FTA = pd.DataFrame.to_numpy(S6FT)\n",
        "S6MTA = pd.DataFrame.to_numpy(S6MT)\n",
        "S6RTA = pd.DataFrame.to_numpy(S6RT)\n",
        "S6FMA = pd.DataFrame.to_numpy(S6FM)\n",
        "S6MMA = pd.DataFrame.to_numpy(S6MM)\n",
        "S6RMA = pd.DataFrame.to_numpy(S6RM)\n",
        "S6FBA = pd.DataFrame.to_numpy(S6FB)\n",
        "S6MBA = pd.DataFrame.to_numpy(S6MB)\n",
        "S6RBA = pd.DataFrame.to_numpy(S6RB)\n",
        "\n",
        "n_steps = 7\n",
        "\n",
        "S2FTX, S2FTY = create_sequence(S2FTA, n_steps)\n",
        "S2MTX, S2MTY = create_sequence(S2MTA, n_steps)\n",
        "S2RTX, S2RTY = create_sequence(S2RTA, n_steps)\n",
        "S2FMX, S2FMY = create_sequence(S2FMA, n_steps)\n",
        "S2MMX, S2MMY = create_sequence(S2MMA, n_steps)\n",
        "S2RMX, S2RMY = create_sequence(S2RMA, n_steps)\n",
        "S2FBX, S2FBY = create_sequence(S2FBA, n_steps)\n",
        "S2MBX, S2MBY = create_sequence(S2MBA, n_steps)\n",
        "S2RBX, S2RBY = create_sequence(S2RBA, n_steps)\n",
        "\n",
        "S3FTX, S3FTY = create_sequence(S3FTA, n_steps)\n",
        "S3MTX, S3MTY = create_sequence(S3MTA, n_steps)\n",
        "S3RTX, S3RTY = create_sequence(S3RTA, n_steps)\n",
        "S3FMX, S3FMY = create_sequence(S3FMA, n_steps)\n",
        "S3MMX, S3MMY = create_sequence(S3MMA, n_steps)\n",
        "S3RMX, S3RMY = create_sequence(S3RMA, n_steps)\n",
        "S3FBX, S3FBY = create_sequence(S3FBA, n_steps)\n",
        "S3MBX, S3MBY = create_sequence(S3MBA, n_steps)\n",
        "S3RBX, S3RBY = create_sequence(S3RBA, n_steps)\n",
        "\n",
        "S4FTX, S4FTY = create_sequence(S4FTA, n_steps)\n",
        "S4MTX, S4MTY = create_sequence(S4MTA, n_steps)\n",
        "S4RTX, S4RTY = create_sequence(S4RTA, n_steps)\n",
        "S4FMX, S4FMY = create_sequence(S4FMA, n_steps)\n",
        "S4MMX, S4MMY = create_sequence(S4MMA, n_steps)\n",
        "S4RMX, S4RMY = create_sequence(S4RMA, n_steps)\n",
        "S4FBX, S4FBY = create_sequence(S4FBA, n_steps)\n",
        "S4MBX, S4MBY = create_sequence(S4MBA, n_steps)\n",
        "S4RBX, S4RBY = create_sequence(S4RBA, n_steps)\n",
        "\n",
        "S5FTX, S5FTY = create_sequence(S5FTA, n_steps)\n",
        "S5MTX, S5MTY = create_sequence(S5MTA, n_steps)\n",
        "S5RTX, S5RTY = create_sequence(S5RTA, n_steps)\n",
        "S5FMX, S5FMY = create_sequence(S5FMA, n_steps)\n",
        "S5MMX, S5MMY = create_sequence(S5MMA, n_steps)\n",
        "S5RMX, S5RMY = create_sequence(S5RMA, n_steps)\n",
        "S5FBX, S5FBY = create_sequence(S5FBA, n_steps)\n",
        "S5MBX, S5MBY = create_sequence(S5MBA, n_steps)\n",
        "S5RBX, S5RBY = create_sequence(S5RBA, n_steps)\n",
        "\n",
        "S6FTX, S6FTY = create_sequence(S6FTA, n_steps)\n",
        "S6MTX, S6MTY = create_sequence(S6MTA, n_steps)\n",
        "S6RTX, S6RTY = create_sequence(S6RTA, n_steps)\n",
        "S6FMX, S6FMY = create_sequence(S6FMA, n_steps)\n",
        "S6MMX, S6MMY = create_sequence(S6MMA, n_steps)\n",
        "S6RMX, S6RMY = create_sequence(S6RMA, n_steps)\n",
        "S6FBX, S6FBY = create_sequence(S6FBA, n_steps)\n",
        "S6MBX, S6MBY = create_sequence(S6MBA, n_steps)\n",
        "S6RBX, S6RBY = create_sequence(S6RBA, n_steps)\n",
        "\n",
        "len(S2FTX)\n",
        "len(S3FTX)\n",
        "\n",
        "X_FT = np.concatenate((S2FTX, S3FTX, S4FTX, S5FTX, S6FTX), axis=0)\n",
        "y_FT = np.concatenate((S2FTY, S3FTY, S4FTY, S5FTY, S6FTY), axis=0)\n",
        "X_FM = np.concatenate((S2FMX, S3FMX, S4FMX, S5FMX, S6FMX), axis=0)\n",
        "y_FM = np.concatenate((S2FMY, S3FMY, S4FMY, S5FMY, S6FMY), axis=0)\n",
        "X_FB = np.concatenate((S2FBX, S3FBX, S4FBX, S5FBX, S6FBX), axis=0)\n",
        "y_FB = np.concatenate((S2FBY, S3FBY, S4FBY, S5FBY, S6FBY), axis=0)\n",
        "\n",
        "X_MT = np.concatenate((S2MTX, S3MTX, S4MTX, S5MTX, S6MTX), axis=0)\n",
        "y_MT = np.concatenate((S2MTY, S3MTY, S4MTY, S5MTY, S6MTY), axis=0)\n",
        "X_MM = np.concatenate((S2MMX, S3MMX, S4MMX, S5MMX, S6MMX), axis=0)\n",
        "y_MM = np.concatenate((S2MMY, S3MMY, S4MMY, S5MMY, S6MMY), axis=0)\n",
        "X_MB = np.concatenate((S2MBX, S3MBX, S4MBX, S5MBX, S6MBX), axis=0)\n",
        "y_MB = np.concatenate((S2MBY, S3MBY, S4MBY, S5MBY, S6MBY), axis=0)\n",
        "\n",
        "X_RT = np.concatenate((S2RTX, S3RTX, S4RTX, S5RTX, S6RTX), axis=0)\n",
        "y_RT = np.concatenate((S2RTY, S3RTY, S4RTY, S5RTY, S6RTY), axis=0)\n",
        "X_RM = np.concatenate((S2RMX, S3RMX, S4RMX, S5RMX, S6RMX), axis=0)\n",
        "y_RM = np.concatenate((S2RMY, S3RMY, S4RMY, S5RMY, S6RMY), axis=0)\n",
        "X_RB = np.concatenate((S2RBX, S3RBX, S4RBX, S5RBX, S6RBX), axis=0)\n",
        "y_RB = np.concatenate((S2RBY, S3RBY, S4RBY, S5RBY, S6RBY), axis=0)\n",
        "\n",
        "len(X_FT)\n",
        "\n",
        "#plt.figure(figsize=(12, 10))\n",
        "#plt.plot(y_FT, \"b\", label=\"y_FT\")\n",
        "#plt.plot(y_MT, \"r\", label=\"y_MT\")\n",
        "#plt.plot(y_RT, \"y\", label=\"y_RT\")\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=y_FT, name=\"y_FT\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_MT, name=\"y_MT\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_RT, name=\"y_RT\", line_shape='linear'))\n",
        "fig.show()\n",
        "\n",
        "fig1 = go.Figure()\n",
        "fig1.add_trace(go.Scatter(y=y_FM, name=\"y_FM\", line_shape='linear'))\n",
        "fig1.add_trace(go.Scatter(y=y_MM, name=\"y_MM\", line_shape='linear'))\n",
        "fig1.add_trace(go.Scatter(y=y_RM, name=\"y_RM\", line_shape='linear'))\n",
        "fig1.show()\n",
        "\n",
        "fig2 = go.Figure()\n",
        "fig2.add_trace(go.Scatter(y=y_FB, name=\"y_FB\", line_shape='linear'))\n",
        "fig2.add_trace(go.Scatter(y=y_MB, name=\"y_MB\", line_shape='linear'))\n",
        "fig2.add_trace(go.Scatter(y=y_RB, name=\"y_RB\", line_shape='linear'))\n",
        "fig2.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"37d1dda7-00fc-49fe-999d-70697d1a12b1\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"37d1dda7-00fc-49fe-999d-70697d1a12b1\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '37d1dda7-00fc-49fe-999d-70697d1a12b1',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_FT\", \"type\": \"scatter\", \"y\": [34.2, 34.5, 34.5, 34.5, 34.5, 34.9, 34.9, 34.5, 34.2, 34.0, 33.8, 33.8, 33.8, 33.8, 34.0, 34.5, 34.3, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.3, 34.2, 34.2, 34.2, 34.0, 33.8, 33.3, 33.1, 34.0, 33.8, 33.4, 33.4, 34.0, 34.2, 35.4, 35.2, 35.2, 35.1, 35.2, 35.2, 35.4, 35.4, 35.6, 35.4, 35.6, 35.4, 35.4, 35.6, 35.8, 36.0, 36.0, 36.3, 36.1, 36.5, 34.5, 34.7, 34.9, 35.1, 35.2, 35.2, 35.8, 36.0, 36.0, 36.1, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.7, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 38.7, 37.9, 37.6, 37.6, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 38.1, 38.3, 38.3, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.6, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.9, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.4, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.9, 39.7, 39.7, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.3, 40.1, 40.3, 40.3, 40.3, 40.1, 40.5, 40.3, 40.1, 40.1, 40.1, 40.1, 39.9, 39.9, 39.9, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.4, 39.4, 39.4, 39.6, 39.4, 39.4, 39.2, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.7, 38.7, 38.7, 38.7, 38.5, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.9, 36.9, 37.8, 37.9, 37.9, 37.8, 37.6, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.4, 37.6, 37.8, 37.8, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.2, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.8, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.9, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 39.9, 39.9, 39.9, 39.9, 39.7, 39.6, 39.6, 39.4, 39.4, 39.2, 39.0, 38.8, 38.7, 38.5, 38.3, 38.3, 38.1, 38.1, 37.9, 37.9, 37.8, 37.6, 37.6, 37.6, 37.4, 37.2, 37.0, 37.0, 37.0, 39.4, 37.9, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.1, 33.6, 33.8, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.8, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 33.4, 33.4, 33.6, 33.6, 33.6, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 35.1, 35.2, 35.6, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 36.0, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_MT\", \"type\": \"scatter\", \"y\": [34.5, 34.7, 34.5, 34.5, 34.9, 34.9, 34.3, 34.0, 33.8, 33.6, 33.8, 33.6, 33.6, 33.8, 34.2, 34.3, 34.3, 34.5, 34.3, 34.5, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.0, 33.8, 33.6, 34.5, 34.2, 34.0, 34.2, 34.3, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 32.4, 32.5, 32.7, 32.9, 33.3, 33.6, 34.0, 34.3, 34.3, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.5, 36.7, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 39.0, 37.4, 36.9, 36.9, 36.9, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.5, 36.3, 36.1, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.6, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 32.7, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.6, 33.8, 34.0, 34.3, 34.7, 34.7, 34.5, 34.5, 34.3, 34.3, 34.2, 34.3, 34.2, 34.2, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.1, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.1, 33.4, 33.3, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.6, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 34.2, 33.8, 33.6, 33.4, 33.8, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 34.0, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.3, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.0, 34.2, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.3, 34.3, 34.7, 34.3, 34.2, 34.2, 34.5, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 32.9, 33.1, 33.4, 33.6, 34.0, 34.2, 34.2, 34.3, 34.5, 34.9, 34.9, 34.5, 34.5, 34.7, 34.9, 34.9, 34.9, 34.9, 35.1, 35.2, 35.4, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.6, 35.4, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.2, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.3, 34.3, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 33.3, 33.4, 33.4, 33.6, 33.8, 34.2, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.9, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.3, 40.3, 40.3, 40.3, 40.3, 40.3, 40.3, 40.3, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.8, 40.8, 40.8, 40.8, 40.8, 40.8, 40.8, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.2, 41.2, 41.2, 41.2, 41.2, 41.2, 41.2, 41.2, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 42.1, 41.9, 41.9, 41.9, 41.9, 41.9, 41.5, 41.2, 41.4, 41.4, 41.4, 41.4, 40.8]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RT\", \"type\": \"scatter\", \"y\": [34.3, 34.7, 34.7, 34.5, 34.5, 34.7, 34.9, 35.1, 34.5, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 34.0, 34.2, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.7, 34.5, 34.3, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 34.5, 34.7, 34.7, 34.9, 35.1, 35.1, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 35.8, 35.8, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 39.7, 39.2, 39.2, 39.0, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.8, 38.7, 38.8, 39.2, 40.5, 39.6, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 39.0, 38.8, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.6, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.5, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 37.9, 37.6, 37.4, 37.6, 37.6, 37.4, 37.4, 37.6, 37.6, 37.4, 36.7, 33.1, 33.3, 33.4, 33.4, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.4, 33.8, 34.2, 34.3, 34.5, 34.7, 34.7, 34.5, 34.5, 34.7, 34.5, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.9, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.7, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.7, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.1, 36.1, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 36.3, 36.7, 36.9, 37.2, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.0, 36.9, 36.9, 37.0, 37.0, 37.0, 37.2, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.6, 39.7, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.7, 39.6, 39.7, 39.6, 39.6, 39.6, 39.7, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 34.5, 34.7, 35.1, 35.2, 35.4, 35.4, 35.6, 35.6, 35.8, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 37.2, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.3, 36.7, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('37d1dda7-00fc-49fe-999d-70697d1a12b1');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"744ea2cf-cd10-484e-acf4-72c6e8de854a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"744ea2cf-cd10-484e-acf4-72c6e8de854a\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '744ea2cf-cd10-484e-acf4-72c6e8de854a',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_FM\", \"type\": \"scatter\", \"y\": [34.3, 34.5, 34.7, 34.7, 34.7, 35.2, 35.2, 35.1, 34.7, 34.3, 34.3, 34.2, 34.2, 34.0, 34.3, 34.3, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 34.7, 34.3, 34.2, 34.0, 34.0, 34.0, 34.0, 33.4, 32.9, 32.9, 34.3, 33.6, 33.4, 33.4, 33.6, 33.8, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.9, 33.3, 33.4, 33.4, 33.3, 33.3, 33.1, 33.3, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.4, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.6, 35.8, 35.6, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.0, 37.2, 37.2, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.5, 38.5, 38.3, 38.3, 38.3, 38.5, 38.5, 38.3, 38.5, 38.7, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 37.9, 37.6, 37.2, 37.2, 37.2, 37.2, 37.4, 37.8, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 37.8, 37.2, 37.2, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.6, 37.2, 37.4, 37.8, 37.8, 37.2, 37.2, 37.0, 37.2, 37.0, 37.2, 37.4, 37.6, 37.8, 37.8, 37.8, 37.2, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 37.2, 37.6, 37.8, 37.8, 37.8, 37.8, 37.9, 37.8, 37.2, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.4, 36.9, 36.5, 36.3, 36.3, 36.1, 36.1, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 36.0, 36.1, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 36.1, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 36.0, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.3, 36.1, 36.0, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 32.0, 32.0, 32.0, 32.4, 32.0, 32.0, 32.0, 32.2, 32.2, 32.2, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.0, 36.0, 35.8, 35.8, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.7, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.2, 34.2, 34.3, 34.2, 34.2, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 32.2, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.9, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.6, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.4, 33.4, 33.3, 33.3, 33.4, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.8, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.8, 33.6, 33.6, 33.4, 33.6, 33.6, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 34.2, 34.0, 34.0, 33.8, 34.0, 33.8, 33.8, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.1, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 34.7, 34.9, 34.9, 35.1, 35.1, 35.2, 35.4, 35.4, 35.6, 35.6, 35.6, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.3, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.9, 36.7, 37.2, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.6, 38.1, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.4, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_MM\", \"type\": \"scatter\", \"y\": [34.5, 34.9, 34.9, 34.7, 34.7, 35.2, 35.2, 34.9, 34.5, 34.2, 34.2, 34.2, 34.2, 34.0, 34.2, 34.3, 34.5, 34.5, 34.5, 34.5, 34.7, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.3, 33.8, 33.6, 34.5, 34.3, 34.0, 34.0, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 30.9, 31.3, 31.3, 31.6, 31.6, 31.8, 32.0, 32.0, 32.2, 32.0, 32.0, 32.0, 32.0, 31.8, 31.8, 31.8, 31.8, 32.0, 32.0, 32.2, 32.0, 32.0, 32.2, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 37.9, 37.0, 36.9, 36.7, 36.7, 36.5, 36.7, 36.7, 36.9, 36.7, 36.5, 36.5, 36.5, 36.5, 37.2, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 36.9, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.0, 37.0, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.2, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.4, 35.2, 35.1, 35.1, 35.2, 35.2, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 31.6, 31.6, 31.8, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 33.1, 33.1, 32.9, 33.1, 32.9, 32.9, 33.1, 32.9, 33.1, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.3, 33.6, 33.4, 33.3, 33.3, 33.3, 33.1, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.6, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.6, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.8, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 34.2, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 32.7, 32.9, 32.7, 32.9, 33.1, 33.8, 34.0, 34.2, 34.3, 34.5, 34.3, 33.8, 34.3, 34.7, 34.7, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.4, 35.2, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 35.1, 35.1, 35.2, 35.1, 35.1, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.5, 34.7, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.9, 34.9, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.9, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.9, 34.7, 34.9, 34.7, 34.9, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 33.4, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.7, 38.3, 38.7, 38.5, 38.7, 38.7, 38.5]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RM\", \"type\": \"scatter\", \"y\": [35.1, 35.4, 35.6, 35.2, 35.6, 36.0, 36.0, 35.6, 35.2, 35.1, 35.2, 35.2, 35.1, 35.1, 35.4, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.6, 35.6, 35.6, 35.4, 35.1, 34.7, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 33.6, 33.6, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.4, 39.4, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.8, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.7, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.3, 38.3, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.8, 37.9, 38.3, 38.1, 38.1, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.7, 36.7, 36.7, 36.9, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.5, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.1, 36.1, 35.8, 35.6, 35.6, 35.6, 35.4, 35.2, 35.2, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 32.9, 32.7, 32.7, 32.7, 32.7, 32.7, 33.1, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 35.4, 34.7, 34.3, 34.3, 34.7, 34.5, 34.5, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.2, 34.2, 34.3, 34.3, 34.2, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.8, 35.8, 35.6, 35.4, 35.4, 35.4, 35.2, 35.2, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.2, 35.4, 35.2, 35.4, 35.2, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 35.1, 34.9, 34.7, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.2, 34.2, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 35.8, 36.0, 36.1, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.0, 35.8, 35.8, 36.1, 36.3, 36.5, 36.5, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.8, 37.4, 37.4, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 33.8, 33.8, 34.0, 34.0, 34.2, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 39.9, 38.3, 37.9, 37.9, 37.8, 37.6, 37.4, 37.4, 37.2, 36.9, 36.9, 36.9, 36.9]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('744ea2cf-cd10-484e-acf4-72c6e8de854a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"52315108-d041-4d02-9bd9-cb10336ab56f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"52315108-d041-4d02-9bd9-cb10336ab56f\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '52315108-d041-4d02-9bd9-cb10336ab56f',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_FB\", \"type\": \"scatter\", \"y\": [34.3, 34.5, 34.5, 34.3, 34.5, 34.7, 34.9, 34.5, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.5, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.7, 34.7, 34.5, 34.5, 34.5, 34.3, 34.2, 33.8, 34.0, 34.5, 34.2, 34.0, 34.0, 34.0, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 33.6, 33.8, 33.8, 34.0, 34.0, 34.2, 34.3, 34.2, 34.3, 34.3, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 36.3, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.9, 37.8, 37.8, 37.6, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.3, 38.3, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.3, 38.1, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.3, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.7, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.5, 38.7, 38.5, 38.5, 38.7, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.5, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 31.6, 31.8, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 35.8, 35.6, 35.4, 35.2, 35.2, 35.2, 35.2, 35.1, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.3, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 35.1, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.9, 34.9, 35.2, 35.1, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 36.0, 36.0, 35.6, 35.6, 35.6, 35.8, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.6, 35.6, 35.6, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_MB\", \"type\": \"scatter\", \"y\": [34.3, 34.5, 34.7, 34.5, 34.5, 34.9, 35.1, 34.7, 34.3, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.5, 34.3, 34.0, 34.0, 34.0, 34.0, 33.6, 33.1, 32.9, 34.2, 33.8, 33.4, 33.4, 33.8, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 31.1, 31.1, 31.3, 31.5, 32.0, 32.2, 32.2, 32.2, 32.0, 31.8, 31.8, 32.0, 32.0, 32.0, 31.8, 31.8, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.5, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.6, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.1, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.2, 35.2, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.3, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.2, 34.2, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.8, 33.8, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.2, 34.3, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 32.4, 32.5, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.2, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 33.8, 33.8, 34.0, 33.8, 33.8, 34.0, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.7, 34.5, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 33.8, 34.0, 34.0, 34.0, 34.0, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.2, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.2, 35.2, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 32.4, 32.4, 32.5, 32.5, 32.5, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RB\", \"type\": \"scatter\", \"y\": [35.6, 36.0, 36.1, 35.8, 36.0, 36.3, 36.1, 35.6, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.4, 35.6, 35.6, 35.6, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 36.0, 36.0, 35.8, 35.6, 35.4, 35.4, 35.2, 35.2, 35.1, 34.7, 34.5, 34.9, 34.9, 34.7, 34.9, 34.9, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 33.4, 33.6, 33.8, 33.8, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 37.0, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.8, 37.4, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.8, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.8, 37.8, 37.8, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.8, 37.9, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.8, 37.8, 37.6, 37.6, 37.4, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.2, 37.4, 37.2, 37.4, 37.2, 37.2, 37.2, 37.0, 37.0, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 31.8, 31.8, 31.8, 31.8, 31.8, 32.0, 31.8, 32.0, 32.4, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 33.1, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.8, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 36.0, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.8, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 36.3, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.6, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 35.8, 35.6, 35.6, 35.8, 36.0, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.7, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 34.3, 34.5, 34.5, 34.7, 34.5, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.2, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.6, 35.6, 35.4, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.5, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('52315108-d041-4d02-9bd9-cb10336ab56f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrWvDeTvBuN1",
        "outputId": "a3c47ae7-76d8-4fb9-8eed-51b045a6c4c5"
      },
      "source": [
        "X_FT[10:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[34.5, 34.2, 34. , 33.8, 33.8, 33.8, 33.8],\n",
              "       [34.2, 34. , 33.8, 33.8, 33.8, 33.8, 34. ],\n",
              "       [34. , 33.8, 33.8, 33.8, 33.8, 34. , 34.5],\n",
              "       [33.8, 33.8, 33.8, 33.8, 34. , 34.5, 34.3],\n",
              "       [33.8, 33.8, 33.8, 34. , 34.5, 34.3, 34.5],\n",
              "       [33.8, 33.8, 34. , 34.5, 34.3, 34.5, 34.5],\n",
              "       [33.8, 34. , 34.5, 34.3, 34.5, 34.5, 34.5],\n",
              "       [34. , 34.5, 34.3, 34.5, 34.5, 34.5, 34.7],\n",
              "       [34.5, 34.3, 34.5, 34.5, 34.5, 34.7, 34.5],\n",
              "       [34.3, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUar_pv0B69U",
        "outputId": "f617bde0-d8cc-4726-b275-b5ce655128cf"
      },
      "source": [
        "y_FT[10:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([33.8, 33.8, 33.8, 33.8, 34. , 34.5, 34.3, 34.5, 34.5, 34.5])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XhBtO6ACRj"
      },
      "source": [
        "**Predict Rear Bottom Sensor Values by the values given for the front top sensor**\n",
        "\n",
        "**INPUT IS ASSIGNED TO MODEL INPUT VARIABLE**\n",
        "\n",
        "**ADD THE THIRD DIMENSION FOR NUMBER OF FEATURES TO THE TRAIN INPUT.**\n",
        "\n",
        "**THIS IS NECESSARY FOR CONV1D MODEL**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq51Cs1jQl-h"
      },
      "source": [
        "# define baseline model 1\n",
        "# create model\n",
        "n_features = 1\n",
        "model1 = Sequential()\n",
        "model1.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))\n",
        "model1.add(MaxPooling1D(pool_size=2))\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(50, activation='relu'))\n",
        "model1.add(Dense(1))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "model1.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "#model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ev6TwMNRjN"
      },
      "source": [
        "n_features = 1\n",
        "def datagenerator(X_in, Y_in):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "  \n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,0:7]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,0:7]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,0:7]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], n_features))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], n_features))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], n_features))\n",
        "  \n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n",
        "#X_train_Input.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nItmhieXwgdb"
      },
      "source": [
        "def evaldata(X_in, Y_in, traindata, testdata):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(X_in, Y_in)\n",
        "  \n",
        "  history = model1.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model1.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model1.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  #print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  #print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q41wARCq_Iac",
        "outputId": "8543cb46-4cdd-4615-8469-9e041aeefa25"
      },
      "source": [
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "\n",
        "model1.save_weights('model1.h5')\n",
        "\n",
        "my_dict = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for kX, vX  in TrainDataSet.items():\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX[-2:]:\n",
        "      continue\n",
        "    \n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      results = evaldata(vX, vy, kX, ky)\n",
        "      TestLossTotal = results[2] + TestLossTotal\n",
        "      TrainLossTotal = results[0] + TrainLossTotal\n",
        "      ValLossTotal = results[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dict[\"DATA_X\"].append(kX)\n",
        "    my_dict[\"DATA_y\"].append(ky)\n",
        "    my_dict[\"Test Loss\"].append(TestLossAvg)\n",
        "    \n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model1.load_weights('model1.h5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5169789910316467, and Avg Validation Loss (mae) is 1.5902438163757324\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5704614400863648\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7853699922561646, and Avg Validation Loss (mae) is 1.912024450302124\n",
            "After 10 runs; Avg Test Loss (mae) is 1.866813600063324\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.790232491493225, and Avg Validation Loss (mae) is 1.7582393169403077\n",
            "After 10 runs; Avg Test Loss (mae) is 1.744213092327118\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4321849822998047, and Avg Validation Loss (mae) is 1.4277374804019929\n",
            "After 10 runs; Avg Test Loss (mae) is 1.432744574546814\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9706547260284424, and Avg Validation Loss (mae) is 1.8947350025177\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9034070372581482\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8616731286048889, and Avg Validation Loss (mae) is 1.849907600879669\n",
            "After 10 runs; Avg Test Loss (mae) is 1.833047664165497\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4011224269866944, and Avg Validation Loss (mae) is 1.28176451921463\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2657583355903625\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5440622210502624, and Avg Validation Loss (mae) is 1.4911309838294984\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4765338182449341\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.393594992160797, and Avg Validation Loss (mae) is 1.5299463033676148\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5529813647270203\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9132217168807983, and Avg Validation Loss (mae) is 1.0061458468437194\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0235851943492889\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0504067361354827, and Avg Validation Loss (mae) is 1.1562770664691926\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1400260090827943\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5042016506195068, and Avg Validation Loss (mae) is 1.509474015235901\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5324951767921449\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2520326018333434, and Avg Validation Loss (mae) is 1.207902979850769\n",
            "After 10 runs; Avg Test Loss (mae) is 1.231633722782135\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8976302325725556, and Avg Validation Loss (mae) is 0.867903345823288\n",
            "After 10 runs; Avg Test Loss (mae) is 0.897314041852951\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 2.0777622938156126, and Avg Validation Loss (mae) is 2.1184725522994996\n",
            "After 10 runs; Avg Test Loss (mae) is 2.0610945224761963\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3575746059417724, and Avg Validation Loss (mae) is 1.35702942609787\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3468348979949951\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9978583455085754, and Avg Validation Loss (mae) is 1.1055091381073\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1085930287837982\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9852643787860871, and Avg Validation Loss (mae) is 1.0118125081062317\n",
            "After 10 runs; Avg Test Loss (mae) is 1.010577392578125\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 2.051825249195099, and Avg Validation Loss (mae) is 2.1092097401618957\n",
            "After 10 runs; Avg Test Loss (mae) is 2.1444306135177613\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.796972143650055, and Avg Validation Loss (mae) is 1.7841964840888977\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7479025363922118\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.412139070034027, and Avg Validation Loss (mae) is 1.3466724276542663\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3591997981071473\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 2.0227977991104127, and Avg Validation Loss (mae) is 2.015114665031433\n",
            "After 10 runs; Avg Test Loss (mae) is 2.010815382003784\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9778531670570374, and Avg Validation Loss (mae) is 1.0462469398975371\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0319195091724396\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1631365180015565, and Avg Validation Loss (mae) is 1.0895534574985504\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0917924344539642\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6895201206207275, and Avg Validation Loss (mae) is 0.6724089294672012\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6755547791719436\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5481189250946046, and Avg Validation Loss (mae) is 1.6450369954109192\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6188682556152343\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.071237713098526, and Avg Validation Loss (mae) is 1.205885797739029\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1973502397537232\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9266919374465943, and Avg Validation Loss (mae) is 0.8897168874740601\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8782661318778991\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.950981080532074, and Avg Validation Loss (mae) is 2.0729599475860594\n",
            "After 10 runs; Avg Test Loss (mae) is 2.084445631504059\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1887850403785705, and Avg Validation Loss (mae) is 1.3741538047790527\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3548957467079163\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.315949761867523, and Avg Validation Loss (mae) is 1.4224841475486756\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4439360976219178\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8225864171981812, and Avg Validation Loss (mae) is 0.7970552027225495\n",
            "After 10 runs; Avg Test Loss (mae) is 0.764987051486969\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6021594882011414, and Avg Validation Loss (mae) is 1.5689403057098388\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5706859946250915\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.271722149848938, and Avg Validation Loss (mae) is 1.3149488806724547\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3166308879852295\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0942402541637422, and Avg Validation Loss (mae) is 1.08725466132164\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0834053456783295\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9256919145584106, and Avg Validation Loss (mae) is 1.8660570502281189\n",
            "After 10 runs; Avg Test Loss (mae) is 1.925004255771637\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5267383813858033, and Avg Validation Loss (mae) is 1.4967678427696227\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4523160576820373\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9022425770759583, and Avg Validation Loss (mae) is 2.0488394856452943\n",
            "After 10 runs; Avg Test Loss (mae) is 2.061838483810425\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.389727807044983, and Avg Validation Loss (mae) is 1.374569833278656\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4118431687355042\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1877443671226502, and Avg Validation Loss (mae) is 1.2177211046218872\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2043949961662292\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7748696506023407, and Avg Validation Loss (mae) is 0.7482605397701263\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7629525661468506\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8880950689315796, and Avg Validation Loss (mae) is 0.9677961528301239\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9760849952697754\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8787264943122863, and Avg Validation Loss (mae) is 1.908370041847229\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9992240428924561\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1748454689979553, and Avg Validation Loss (mae) is 1.1881033182144165\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1860286176204682\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7113365530967712, and Avg Validation Loss (mae) is 1.6623144268989563\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6849857687950134\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9648769736289978, and Avg Validation Loss (mae) is 1.0079627752304077\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0100289702415466\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0123270392417907, and Avg Validation Loss (mae) is 1.1269668281078338\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1108511567115784\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8100621461868286, and Avg Validation Loss (mae) is 0.906132024526596\n",
            "After 10 runs; Avg Test Loss (mae) is 0.897250872850418\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7272981762886047, and Avg Validation Loss (mae) is 0.9504575848579406\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9793159604072571\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5905021190643311, and Avg Validation Loss (mae) is 1.6835664510726929\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6161444902420044\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0058288872241974, and Avg Validation Loss (mae) is 1.067289310693741\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0898670256137848\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4758078575134277, and Avg Validation Loss (mae) is 1.6056397914886475\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5887526273727417\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9788161158561707, and Avg Validation Loss (mae) is 0.9498442649841309\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9512104511260986\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9975300312042237, and Avg Validation Loss (mae) is 1.0044010519981383\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0017855286598205\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.054223245382309, and Avg Validation Loss (mae) is 1.0992669701576232\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0858678579330445\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8273846447467804, and Avg Validation Loss (mae) is 0.7663458287715912\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7747808277606965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "kgncoI3oyMKg",
        "outputId": "6f4ed9d4-2c99-45a3-9836-a77208801d97"
      },
      "source": [
        "CombResults1 = pd.DataFrame.from_dict(my_dict)\n",
        "CombResultsSorted1 = CombResults1.sort_values(by=['Test Loss'])\n",
        "CombResultsSorted1.head(15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA_X</th>\n",
              "      <th>DATA_y</th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>X_MM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.675555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>X_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.762953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>X_MB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.764987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>X_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.774781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>X_MM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.878266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>X_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.897251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>X_FM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.897314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>X_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.951210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>X_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.976085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>X_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.979316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>X_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>1.001786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>X_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>1.010029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>X_MT</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>1.010577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>X_FM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>1.023585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>X_MM</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>1.031920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   DATA_X DATA_y  Test Loss\n",
              "24   X_MM   y_MB   0.675555\n",
              "40   X_RT   y_RM   0.762953\n",
              "31   X_MB   y_MM   0.764987\n",
              "55   X_RB   y_RM   0.774781\n",
              "27   X_MM   y_RB   0.878266\n",
              "47   X_RM   y_RT   0.897251\n",
              "13   X_FM   y_RB   0.897314\n",
              "52   X_RB   y_MM   0.951210\n",
              "41   X_RT   y_RB   0.976085\n",
              "48   X_RM   y_RB   0.979316\n",
              "53   X_RB   y_MB   1.001786\n",
              "45   X_RM   y_MM   1.010029\n",
              "17   X_MT   y_MB   1.010577\n",
              "9    X_FM   y_MM   1.023585\n",
              "22   X_MM   y_FM   1.031920"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "Rny1g3a9_bTz",
        "outputId": "b5f26c3d-b184-4691-8ed3-0a2f4b11fc22"
      },
      "source": [
        "CombResultsSortedgrouped1 = CombResultsSorted1.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedgroupedsortedMF1 = CombResultsSortedgrouped1.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedgroupedsortedMF1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_RB</th>\n",
              "      <td>1.158344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MM</th>\n",
              "      <td>1.214938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FM</th>\n",
              "      <td>1.264938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RM</th>\n",
              "      <td>1.266812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MB</th>\n",
              "      <td>1.374141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RT</th>\n",
              "      <td>1.399205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MT</th>\n",
              "      <td>1.539805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FT</th>\n",
              "      <td>1.659492</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Test Loss\n",
              "DATA_X           \n",
              "X_RB     1.158344\n",
              "X_MM     1.214938\n",
              "X_FM     1.264938\n",
              "X_RM     1.266812\n",
              "X_MB     1.374141\n",
              "X_RT     1.399205\n",
              "X_MT     1.539805\n",
              "X_FT     1.659492"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "YbtkvbWxrmT1",
        "outputId": "b8574cb8-674d-41a8-ccbc-19afa1c22ee1"
      },
      "source": [
        "CombResultsSortedgroupedsortedMF1.to_csv('CombResultsSortedgroupedsortedMF1.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedgroupedsortedMF1.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_07ddf376-4bb6-4c28-ac7c-12c1344354e8\", \"CombResultsSortedgroupedsortedMF1.csv\", 207)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "xHajEhA0W5kk",
        "outputId": "2256f8c4-465a-45e1-dcf1-60d2f9cb1160"
      },
      "source": [
        "CombResultsSorted1.to_csv('CombResultsSorted1.csv')\n",
        "files.download(\"CombResultsSorted1.csv\")\n",
        "\n",
        "#fig = px.box(CombResultsSorted1, x=\"DATA_X\", y=\"Test Loss\", hover_data=[\"DATA_y\"], notched=True, color=\"DATA_X\")\n",
        "fig = px.box(CombResultsSorted1, x=\"DATA_X\", y=\"Test Loss\", hover_data=[\"DATA_y\"])\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_761ad410-1e05-438c-9093-b93e6d313f37\", \"CombResultsSorted1.csv\", 1794)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"8be4e9af-23a8-43aa-803a-83a3b8e2ef73\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"8be4e9af-23a8-43aa-803a-83a3b8e2ef73\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '8be4e9af-23a8-43aa-803a-83a3b8e2ef73',\n",
              "                        [{\"alignmentgroup\": \"True\", \"customdata\": [[\"y_MB\"], [\"y_RM\"], [\"y_MM\"], [\"y_RM\"], [\"y_RB\"], [\"y_RT\"], [\"y_RB\"], [\"y_MM\"], [\"y_RB\"], [\"y_RB\"], [\"y_MB\"], [\"y_MM\"], [\"y_MB\"], [\"y_MM\"], [\"y_FM\"], [\"y_RB\"], [\"y_RT\"], [\"y_FM\"], [\"y_MT\"], [\"y_MM\"], [\"y_MB\"], [\"y_MB\"], [\"y_FM\"], [\"y_RM\"], [\"y_MB\"], [\"y_RM\"], [\"y_RB\"], [\"y_RM\"], [\"y_FM\"], [\"y_FM\"], [\"y_RB\"], [\"y_MM\"], [\"y_MB\"], [\"y_MT\"], [\"y_FM\"], [\"y_FT\"], [\"y_RT\"], [\"y_MT\"], [\"y_FM\"], [\"y_RT\"], [\"y_MT\"], [\"y_FT\"], [\"y_RT\"], [\"y_MT\"], [\"y_MM\"], [\"y_RM\"], [\"y_RM\"], [\"y_MT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_FT\"], [\"y_MT\"], [\"y_FT\"], [\"y_RT\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}<br>DATA_y=%{customdata[0]}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_MM\", \"X_RT\", \"X_MB\", \"X_RB\", \"X_MM\", \"X_RM\", \"X_FM\", \"X_RB\", \"X_RT\", \"X_RM\", \"X_RB\", \"X_RM\", \"X_MT\", \"X_FM\", \"X_MM\", \"X_MB\", \"X_RB\", \"X_RB\", \"X_MM\", \"X_MT\", \"X_RM\", \"X_FM\", \"X_RM\", \"X_MM\", \"X_RT\", \"X_FM\", \"X_FT\", \"X_MB\", \"X_MT\", \"X_MB\", \"X_MT\", \"X_RT\", \"X_FT\", \"X_MB\", \"X_RT\", \"X_FM\", \"X_FM\", \"X_FM\", \"X_FT\", \"X_MB\", \"X_RB\", \"X_RB\", \"X_MM\", \"X_RM\", \"X_FT\", \"X_MT\", \"X_FT\", \"X_FT\", \"X_FT\", \"X_RT\", \"X_RM\", \"X_MM\", \"X_MT\", \"X_RT\", \"X_MB\", \"X_MT\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.6755547791719436, 0.7629525661468506, 0.764987051486969, 0.7747808277606965, 0.8782661318778991, 0.897250872850418, 0.897314041852951, 0.9512104511260986, 0.9760849952697754, 0.9793159604072571, 1.0017855286598205, 1.0100289702415466, 1.010577392578125, 1.0235851943492889, 1.0319195091724396, 1.0834053456783295, 1.0858678579330445, 1.0898670256137848, 1.0917924344539642, 1.1085930287837982, 1.1108511567115784, 1.1400260090827943, 1.1860286176204682, 1.1973502397537232, 1.2043949961662292, 1.231633722782135, 1.2657583355903625, 1.3166308879852295, 1.3468348979949951, 1.3548957467079163, 1.3591997981071473, 1.4118431687355042, 1.432744574546814, 1.4439360976219178, 1.4523160576820373, 1.4765338182449341, 1.5324951767921449, 1.5529813647270203, 1.5704614400863648, 1.5706859946250915, 1.5887526273727417, 1.6161444902420044, 1.6188682556152343, 1.6849857687950134, 1.744213092327118, 1.7479025363922118, 1.833047664165497, 1.866813600063324, 1.9034070372581482, 1.925004255771637, 1.9992240428924561, 2.010815382003784, 2.0610945224761963, 2.061838483810425, 2.084445631504059, 2.1444306135177613], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8be4e9af-23a8-43aa-803a-83a3b8e2ef73');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHCi-CUyt1xF"
      },
      "source": [
        "# define baseline model 2\n",
        "# create model\n",
        "n_features = 1\n",
        "model2 = Sequential()\n",
        "model2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(14, n_features)))\n",
        "model2.add(MaxPooling1D(pool_size=2))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(50, activation='relu'))\n",
        "model2.add(Dense(1))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "model2.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "#model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9dgHFmuuAal"
      },
      "source": [
        "n_features = 1\n",
        "def datageneratorMF2(X_in1, X_in2, Y_in):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  X_in = np.concatenate((X_in1, X_in2), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "\n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], n_features))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], n_features))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], n_features))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbyCU9dvuUQe"
      },
      "source": [
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMS(X_MT, X_MM, y_RM)\n",
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(X_MT, y_RM)\n",
        "#X_train_Input[0:5]\n",
        "#X_train_Input.shape\n",
        "#y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRZ9Y_qKuZ2r"
      },
      "source": [
        "def evaldataMF(X_in1, X_in2, Y_in, traindata1, traindata2, testdata):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMF2(X_in1, X_in2, Y_in)\n",
        "  \n",
        "  history = model2.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model2.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model2.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # PLOTS LOSS VS EPOCH\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  # print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  # print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # PLOTS Y ORIGINAL VS PREDICTED\n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata1)+ str(traindata2)  + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18gXL9lLuxfT",
        "outputId": "7ab44026-35fd-4b36-8331-a0d0b41dce1f"
      },
      "source": [
        "TrainDataSet1 = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TrainDataSet2 = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "\n",
        "model2.save_weights('model2.h5')\n",
        "\n",
        "my_dictMF = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for kX1, vX1  in TrainDataSet1.items():\n",
        "  #TrainDataSet2.popitem()\n",
        "  TrainDataSet2.pop(next(iter(TrainDataSet2)))\n",
        "  for kX2, vX2  in TrainDataSet2.items():\n",
        "    if kX1 == kX2:\n",
        "      continue\n",
        "    for ky, vy  in TestDataSet.items():\n",
        "      if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:]:\n",
        "        continue\n",
        "      \n",
        "      TestLossTotal = 0\n",
        "      TrainLossTotal = 0\n",
        "      ValLossTotal = 0\n",
        "      runs = 10\n",
        "\n",
        "      for i in range(runs):\n",
        "        resultsMF = evaldataMF(vX1, vX2, vy, kX1, kX2, ky)\n",
        "        TestLossTotal = resultsMF[2] + TestLossTotal\n",
        "        TrainLossTotal = resultsMF[0] + TrainLossTotal\n",
        "        ValLossTotal = resultsMF[1] + ValLossTotal\n",
        "      \n",
        "      TestLossAvg = TestLossTotal / runs\n",
        "      TrainLossAvg = TrainLossTotal / runs\n",
        "      ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "      print(\"*****************************************************************************************************************************\")\n",
        "      print(f'Evaluate model for Train Data: {kX1}_{kX2} and Test Data: {ky}')\n",
        "      print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "      print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "      my_dictMF[\"DATA_X\"].append(kX1 + kX2)\n",
        "      my_dictMF[\"DATA_y\"].append(ky)\n",
        "      my_dictMF[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "      # for k, v in my_dict.items():\n",
        "      #   print(k, v)\n",
        "      model2.load_weights('model2.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.391389584541321, and Avg Validation Loss (mae) is 1.230438768863678\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2299752354621887\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9599877297878265, and Avg Validation Loss (mae) is 1.132307583093643\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1190181851387024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.116794717311859, and Avg Validation Loss (mae) is 1.040455585718155\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0458348989486694\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5273920893669128, and Avg Validation Loss (mae) is 1.6219436287879945\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6232594847679138\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2186012864112854, and Avg Validation Loss (mae) is 1.2414862632751464\n",
            "After 10 runs; Avg Test Loss (mae) is 1.206766664981842\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8698525667190552, and Avg Validation Loss (mae) is 0.8706401109695434\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8854544997215271\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.024528193473816, and Avg Validation Loss (mae) is 1.1083925127983094\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1373327672481537\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9903467953205108, and Avg Validation Loss (mae) is 1.063586300611496\n",
            "After 10 runs; Avg Test Loss (mae) is 1.021770864725113\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9053654193878173, and Avg Validation Loss (mae) is 0.837713074684143\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8361704468727111\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7736247777938843, and Avg Validation Loss (mae) is 1.8914172053337097\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8705768227577209\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.398836362361908, and Avg Validation Loss (mae) is 1.446316647529602\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4334052562713624\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1591405868530273, and Avg Validation Loss (mae) is 1.1084375739097596\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1040704488754272\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8033971130847931, and Avg Validation Loss (mae) is 0.949075722694397\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9332331717014313\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.050102210044861, and Avg Validation Loss (mae) is 1.1043922424316406\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1115025222301482\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7778534710407257, and Avg Validation Loss (mae) is 0.9007589995861054\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9001662850379943\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4126904964447022, and Avg Validation Loss (mae) is 1.6769867658615112\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6931620240211487\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0662620186805725, and Avg Validation Loss (mae) is 1.363578552007675\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3536684572696687\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8374463856220246, and Avg Validation Loss (mae) is 0.901999294757843\n",
            "After 10 runs; Avg Test Loss (mae) is 0.913791024684906\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.063610553741455, and Avg Validation Loss (mae) is 1.0821325600147247\n",
            "After 10 runs; Avg Test Loss (mae) is 1.062389862537384\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3511184453964233, and Avg Validation Loss (mae) is 1.375299084186554\n",
            "After 10 runs; Avg Test Loss (mae) is 1.361753559112549\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7992617726325989, and Avg Validation Loss (mae) is 1.0858204007148742\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0807103216648102\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.613420581817627, and Avg Validation Loss (mae) is 1.4579586625099181\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4520147442817688\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2749434471130372, and Avg Validation Loss (mae) is 1.309244978427887\n",
            "After 10 runs; Avg Test Loss (mae) is 1.291906201839447\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9909141540527344, and Avg Validation Loss (mae) is 1.0573950350284576\n",
            "After 10 runs; Avg Test Loss (mae) is 1.037264096736908\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.260865294933319, and Avg Validation Loss (mae) is 1.3689509272575378\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3763815641403199\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6146242499351502, and Avg Validation Loss (mae) is 1.5633288264274596\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5658524751663208\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2762863755226135, and Avg Validation Loss (mae) is 1.344117248058319\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3313405394554139\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2172975182533263, and Avg Validation Loss (mae) is 1.2153980374336242\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1945786774158478\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8225415170192718, and Avg Validation Loss (mae) is 0.7642428517341614\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7629807949066162\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7519656419754028, and Avg Validation Loss (mae) is 0.7170200854539871\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6986924290657044\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.059209305047989, and Avg Validation Loss (mae) is 1.073560208082199\n",
            "After 10 runs; Avg Test Loss (mae) is 1.081166833639145\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5820322513580323, and Avg Validation Loss (mae) is 1.5986545443534852\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5928719520568848\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9144165277481079, and Avg Validation Loss (mae) is 0.9470493614673614\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9548518240451813\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0457716584205627, and Avg Validation Loss (mae) is 1.0427095234394073\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0857297897338867\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8383387684822082, and Avg Validation Loss (mae) is 0.8336822390556335\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8234956443309784\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6794507026672363, and Avg Validation Loss (mae) is 0.8689484357833862\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8909062206745147\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9387370824813843, and Avg Validation Loss (mae) is 0.9341085612773895\n",
            "After 10 runs; Avg Test Loss (mae) is 0.939322966337204\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4119841814041139, and Avg Validation Loss (mae) is 1.5376827120780945\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5275405526161194\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0059331357479095, and Avg Validation Loss (mae) is 1.1919601440429688\n",
            "After 10 runs; Avg Test Loss (mae) is 1.188001698255539\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0143604099750518, and Avg Validation Loss (mae) is 0.9375476121902466\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9432799518108368\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0570845782756806, and Avg Validation Loss (mae) is 1.0370576322078704\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0535849690437318\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8275245547294616, and Avg Validation Loss (mae) is 0.7811111032962799\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7925959706306458\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5365472674369811, and Avg Validation Loss (mae) is 1.4453147649765015\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4509004473686218\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7658979833126068, and Avg Validation Loss (mae) is 0.7267397880554199\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7160346567630768\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9492051720619201, and Avg Validation Loss (mae) is 1.2025580525398254\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1703509628772735\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.493317997455597, and Avg Validation Loss (mae) is 1.3797109603881836\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4001413345336915\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3311084985733033, and Avg Validation Loss (mae) is 1.3775451540946961\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3728007912635802\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9469299018383026, and Avg Validation Loss (mae) is 0.9448684751987457\n",
            "After 10 runs; Avg Test Loss (mae) is 0.960929948091507\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.507483923435211, and Avg Validation Loss (mae) is 1.7309996247291566\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7755598306655884\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1432108402252197, and Avg Validation Loss (mae) is 1.0637668311595916\n",
            "After 10 runs; Avg Test Loss (mae) is 1.083646035194397\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6530823171138763, and Avg Validation Loss (mae) is 0.7483531415462494\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7535640805959701\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.485332155227661, and Avg Validation Loss (mae) is 1.5102134227752686\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5319352626800538\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.09300696849823, and Avg Validation Loss (mae) is 1.0802919089794158\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1324189841747283\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8826834261417389, and Avg Validation Loss (mae) is 0.9153248250484467\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9096692442893982\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.616103208065033, and Avg Validation Loss (mae) is 1.70850670337677\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6996631026268005\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2401886940002442, and Avg Validation Loss (mae) is 1.2762890815734864\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2904144644737243\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6580094635486603, and Avg Validation Loss (mae) is 0.6136687248945236\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6086981356143951\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.490978467464447, and Avg Validation Loss (mae) is 1.4621925711631776\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4616323113441467\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2171221733093263, and Avg Validation Loss (mae) is 1.1488123655319213\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1319179296493531\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8919846713542938, and Avg Validation Loss (mae) is 0.8142842471599578\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7886802971363067\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5095314145088197, and Avg Validation Loss (mae) is 1.6798734545707703\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6610089540481567\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3495563745498658, and Avg Validation Loss (mae) is 1.3514223575592041\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3118329405784608\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9594655215740204, and Avg Validation Loss (mae) is 1.0802826046943665\n",
            "After 10 runs; Avg Test Loss (mae) is 1.090717500448227\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0538129150867461, and Avg Validation Loss (mae) is 1.1666229248046875\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1328436851501464\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6694754779338836, and Avg Validation Loss (mae) is 0.6506750375032425\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6581975162029267\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6462474882602691, and Avg Validation Loss (mae) is 0.7041068643331527\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6980075359344482\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5831249833106995, and Avg Validation Loss (mae) is 1.6578885793685914\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6500413417816162\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.430013155937195, and Avg Validation Loss (mae) is 1.4102837800979615\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3948794245719909\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7679003357887269, and Avg Validation Loss (mae) is 0.8067651510238647\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8001804411411285\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0194464921951294, and Avg Validation Loss (mae) is 1.0847797095775604\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0833235681056976\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8477549493312836, and Avg Validation Loss (mae) is 0.8724031448364258\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8920376002788544\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.638949853181839, and Avg Validation Loss (mae) is 0.8337971299886704\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8309011310338974\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5027255415916443, and Avg Validation Loss (mae) is 1.6153194665908814\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5896948218345641\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3385517716407775, and Avg Validation Loss (mae) is 1.4363690137863159\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4074299454689025\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.868360185623169, and Avg Validation Loss (mae) is 0.8302071452140808\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8664347946643829\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0213213920593263, and Avg Validation Loss (mae) is 1.0423957049846648\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0548807740211488\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0914209842681886, and Avg Validation Loss (mae) is 1.2321350574493408\n",
            "After 10 runs; Avg Test Loss (mae) is 1.226647013425827\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8554641366004944, and Avg Validation Loss (mae) is 1.0087218046188355\n",
            "After 10 runs; Avg Test Loss (mae) is 1.001711755990982\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9820541858673095, and Avg Validation Loss (mae) is 1.9071131467819213\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9838356494903564\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0044005811214447, and Avg Validation Loss (mae) is 0.9180746793746948\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9167017221450806\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6733797907829284, and Avg Validation Loss (mae) is 0.7913389950990677\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8004172056913376\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.426742434501648, and Avg Validation Loss (mae) is 1.593898558616638\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5792065382003784\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0327422618865967, and Avg Validation Loss (mae) is 1.0433487713336944\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0496914982795715\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9498606383800506, and Avg Validation Loss (mae) is 1.0176295936107635\n",
            "After 10 runs; Avg Test Loss (mae) is 0.969048821926117\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9152354836463927, and Avg Validation Loss (mae) is 1.9174423813819885\n",
            "After 10 runs; Avg Test Loss (mae) is 1.978384506702423\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1509122252464294, and Avg Validation Loss (mae) is 1.1301970601081848\n",
            "After 10 runs; Avg Test Loss (mae) is 1.102862399816513\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7939332723617554, and Avg Validation Loss (mae) is 0.797360759973526\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8004257142543793\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.610394501686096, and Avg Validation Loss (mae) is 1.7145115733146667\n",
            "After 10 runs; Avg Test Loss (mae) is 1.763371968269348\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3918057441711427, and Avg Validation Loss (mae) is 1.3645058631896974\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3519313216209412\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0941993772983551, and Avg Validation Loss (mae) is 1.0579221963882446\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0954893052577972\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.817112672328949, and Avg Validation Loss (mae) is 1.8152219653129578\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8213908672332764\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9777408301830292, and Avg Validation Loss (mae) is 1.0470234513282777\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0415588974952699\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6954464256763458, and Avg Validation Loss (mae) is 0.7925474107265472\n",
            "After 10 runs; Avg Test Loss (mae) is 0.760698202252388\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7874493837356568, and Avg Validation Loss (mae) is 0.7664185285568237\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7672264695167541\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.669187068939209, and Avg Validation Loss (mae) is 0.5195994198322296\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5261743068695068\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6053569138050079, and Avg Validation Loss (mae) is 0.58228297829628\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5776149064302445\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6301044464111327, and Avg Validation Loss (mae) is 1.6065617203712463\n",
            "After 10 runs; Avg Test Loss (mae) is 1.613330078125\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9701694428920746, and Avg Validation Loss (mae) is 1.0709863007068634\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0645675480365753\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5830368459224701, and Avg Validation Loss (mae) is 0.6987232595682145\n",
            "After 10 runs; Avg Test Loss (mae) is 0.701207572221756\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7519242286682128, and Avg Validation Loss (mae) is 0.7829837024211883\n",
            "After 10 runs; Avg Test Loss (mae) is 0.773972886800766\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7143133997917175, and Avg Validation Loss (mae) is 0.8306514501571656\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8197830617427826\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7878128707408905, and Avg Validation Loss (mae) is 0.8408216774463654\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8300277829170227\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6769189357757568, and Avg Validation Loss (mae) is 1.6599263191223144\n",
            "After 10 runs; Avg Test Loss (mae) is 1.641461229324341\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9041439592838287, and Avg Validation Loss (mae) is 0.8214825093746185\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8136477887630462\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.614687466621399, and Avg Validation Loss (mae) is 0.6630378544330597\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6639798164367676\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8524296283721924, and Avg Validation Loss (mae) is 1.1106149673461914\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1115440607070923\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9999970018863678, and Avg Validation Loss (mae) is 0.8968933284282684\n",
            "After 10 runs; Avg Test Loss (mae) is 0.887170821428299\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.851966142654419, and Avg Validation Loss (mae) is 0.9187076568603516\n",
            "After 10 runs; Avg Test Loss (mae) is 0.904131805896759\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9400158286094666, and Avg Validation Loss (mae) is 1.9573705315589904\n",
            "After 10 runs; Avg Test Loss (mae) is 1.944037663936615\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0178367614746093, and Avg Validation Loss (mae) is 0.9948230922222138\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0126550197601318\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1431342720985413, and Avg Validation Loss (mae) is 1.0968639373779296\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1166556537151338\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4953926801681519, and Avg Validation Loss (mae) is 1.5566691398620605\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4977857232093812\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0923895955085754, and Avg Validation Loss (mae) is 1.124039602279663\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1349341690540313\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9265424609184265, and Avg Validation Loss (mae) is 1.382769387960434\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4106548428535461\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.887746226787567, and Avg Validation Loss (mae) is 1.885549008846283\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9010727643966674\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9797735154628754, and Avg Validation Loss (mae) is 0.934894198179245\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9182459473609924\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9886486887931824, and Avg Validation Loss (mae) is 1.098919415473938\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0779197812080383\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.777837747335434, and Avg Validation Loss (mae) is 0.9124305486679077\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8971044421195984\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5130195945501328, and Avg Validation Loss (mae) is 0.5202856481075286\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5378136694431305\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6764601171016693, and Avg Validation Loss (mae) is 0.689050841331482\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6985749959945678\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9178409337997437, and Avg Validation Loss (mae) is 1.9015987873077393\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9615407586097717\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9987368643283844, and Avg Validation Loss (mae) is 0.9951646447181701\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0284330368041992\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9264804601669312, and Avg Validation Loss (mae) is 0.9504441618919373\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9555743575096131\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7141832709312439, and Avg Validation Loss (mae) is 0.6733115047216416\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6646197676658631\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6736284255981445, and Avg Validation Loss (mae) is 0.7787017792463302\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7742363810539246\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7676007509231567, and Avg Validation Loss (mae) is 0.8598769068717956\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8567206025123596\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6327662944793702, and Avg Validation Loss (mae) is 1.5130458235740663\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5049411416053773\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7991573691368103, and Avg Validation Loss (mae) is 0.8924403488636017\n",
            "After 10 runs; Avg Test Loss (mae) is 0.865886652469635\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1139782905578612, and Avg Validation Loss (mae) is 1.083027708530426\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0813504576683044\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.69896702170372, and Avg Validation Loss (mae) is 0.6307761430740356\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6039110660552979\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1239145338535308, and Avg Validation Loss (mae) is 1.119420725107193\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1357545971870422\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7680195868015289, and Avg Validation Loss (mae) is 0.8679175436496734\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8451085209846496\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8121222138404847, and Avg Validation Loss (mae) is 1.848001527786255\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8140723347663879\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1803900718688964, and Avg Validation Loss (mae) is 1.2724724054336547\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2869935929775238\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2793504238128661, and Avg Validation Loss (mae) is 1.3283642888069154\n",
            "After 10 runs; Avg Test Loss (mae) is 1.335911786556244\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8036789953708648, and Avg Validation Loss (mae) is 1.1549903333187104\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1511807918548584\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6178394317626953, and Avg Validation Loss (mae) is 0.7547874271869659\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7389368057250977\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8126779139041901, and Avg Validation Loss (mae) is 0.7329549610614776\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7398850679397583\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9397618651390076, and Avg Validation Loss (mae) is 1.9301352500915527\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9056068658828735\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1195440411567688, and Avg Validation Loss (mae) is 1.1350812375545503\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1725226104259492\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3742188453674316, and Avg Validation Loss (mae) is 1.422149693965912\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4132540464401244\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7043822467327118, and Avg Validation Loss (mae) is 0.775974577665329\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8010396420955658\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8350765168666839, and Avg Validation Loss (mae) is 0.819880199432373\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8246380746364593\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7441744327545166, and Avg Validation Loss (mae) is 0.7012666642665863\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6933618664741517\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6184321880340575, and Avg Validation Loss (mae) is 1.6750251054763794\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6574836015701293\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9523015081882477, and Avg Validation Loss (mae) is 1.0458826303482056\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0622965753078462\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3207649827003478, and Avg Validation Loss (mae) is 1.4416965961456298\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4324962735176086\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7272256970405578, and Avg Validation Loss (mae) is 0.8786960780620575\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8983653247356415\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0322875916957854, and Avg Validation Loss (mae) is 1.0894110321998596\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0719576001167297\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8428398966789246, and Avg Validation Loss (mae) is 0.8226497888565063\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8200337409973144\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9005096673965454, and Avg Validation Loss (mae) is 1.9419887900352477\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9192139744758605\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2420831203460694, and Avg Validation Loss (mae) is 1.3993661105632782\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4085989713668823\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.683824110031128, and Avg Validation Loss (mae) is 1.7658690452575683\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7023163795471192\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9735962510108948, and Avg Validation Loss (mae) is 1.1932670295238494\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1749932765960693\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0907948672771455, and Avg Validation Loss (mae) is 1.1165321946144104\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1400836765766145\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7771710634231568, and Avg Validation Loss (mae) is 0.804110985994339\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8012693703174592\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6284595847129821, and Avg Validation Loss (mae) is 1.651524293422699\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6359298825263977\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9712170362472534, and Avg Validation Loss (mae) is 1.1029961943626403\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1279848396778107\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.463102388381958, and Avg Validation Loss (mae) is 1.5028290390968322\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4953739285469054\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9758389592170715, and Avg Validation Loss (mae) is 0.9591714024543763\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9616184711456299\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0592182576656342, and Avg Validation Loss (mae) is 0.983073616027832\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9903261005878449\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6596315264701843, and Avg Validation Loss (mae) is 0.759156996011734\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7545866787433624\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.590747058391571, and Avg Validation Loss (mae) is 1.5871030688285828\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5550794839859008\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.033601212501526, and Avg Validation Loss (mae) is 0.999549126625061\n",
            "After 10 runs; Avg Test Loss (mae) is 0.992265808582306\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4725909948348999, and Avg Validation Loss (mae) is 1.4992040276527405\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4615373849868774\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9171772122383117, and Avg Validation Loss (mae) is 0.9007975101470947\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9207487881183625\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9331274569034577, and Avg Validation Loss (mae) is 1.0193598926067353\n",
            "After 10 runs; Avg Test Loss (mae) is 1.008250743150711\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8952709376811981, and Avg Validation Loss (mae) is 0.9110427618026733\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8973917067050934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tyIy-WjRvFm_",
        "outputId": "a0f91f0c-93e9-4326-cefb-54cff80e075f"
      },
      "source": [
        "CombResultsMF2 = pd.DataFrame.from_dict(my_dictMF)\n",
        "print(CombResultsMF2.shape)\n",
        "CombResultsSortedMF2 = CombResultsMF2.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMF2.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA_X</th>\n",
              "      <th>DATA_y</th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>X_MTX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.526174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>X_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.537814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>X_MTX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.577615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>X_MMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.603911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>X_FMX_MB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.608698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>X_FMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.658198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>X_MTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.663980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>X_MMX_RM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.664620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>X_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.693362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>X_FMX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.698008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>X_MMX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.698575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>X_FTX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.698692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>X_MTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.701208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>X_FMX_MT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.716035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>X_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.738937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>X_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.739885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>X_FMX_MM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.753564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>X_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.754587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>X_MTX_RT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.760698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>X_FTX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.762981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>X_MTX_RT</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.767226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>X_MTX_RM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.773973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>X_MMX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.774236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>X_FMX_MB</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.788680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>X_FTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.792596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>X_FMX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.800180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>X_MTX_MM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.800417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>X_MTX_MB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.800426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>X_MBX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.801040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>X_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.801269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>X_MTX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.813648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>X_MTX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.819783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>X_MBX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.820034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>X_FTX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.823496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>X_MBX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.824638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>X_MTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.830028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>X_FMX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.830901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>X_FTX_MT</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.836170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>X_MMX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.845109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>X_MMX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.856721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>X_MMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.865887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>X_FMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.866435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>X_FTX_FM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.885454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>X_MTX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.887171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>X_FTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.890906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>X_FMX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.892038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>X_MMX_RT</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.897104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>X_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.897392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>X_MBX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.898365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>X_FTX_MM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.900166</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       DATA_X DATA_y  Test Loss\n",
              "94   X_MTX_RT   y_RM   0.526174\n",
              "118  X_MMX_RT   y_RM   0.537814\n",
              "95   X_MTX_RT   y_RB   0.577615\n",
              "129  X_MMX_RB   y_MB   0.603911\n",
              "56   X_FMX_MB   y_MM   0.608698\n",
              "64   X_FMX_RT   y_RM   0.658198\n",
              "104  X_MTX_RB   y_MM   0.663980\n",
              "123  X_MMX_RM   y_MB   0.664620\n",
              "143  X_MBX_RM   y_RB   0.693362\n",
              "65   X_FMX_RT   y_RB   0.698008\n",
              "119  X_MMX_RT   y_RB   0.698575\n",
              "29   X_FTX_RT   y_RB   0.698692\n",
              "98   X_MTX_RM   y_MM   0.701208\n",
              "43   X_FMX_MT   y_MM   0.716035\n",
              "136  X_MBX_RT   y_RM   0.738937\n",
              "137  X_MBX_RT   y_RB   0.739885\n",
              "50   X_FMX_MM   y_MB   0.753564\n",
              "161  X_RTX_RB   y_RM   0.754587\n",
              "92   X_MTX_RT   y_MM   0.760698\n",
              "28   X_FTX_RT   y_RM   0.762981\n",
              "93   X_MTX_RT   y_MB   0.767226\n",
              "99   X_MTX_RM   y_MB   0.773973\n",
              "124  X_MMX_RM   y_RT   0.774236\n",
              "59   X_FMX_MB   y_RB   0.788680\n",
              "41   X_FTX_RB   y_RM   0.792596\n",
              "68   X_FMX_RM   y_MM   0.800180\n",
              "80   X_MTX_MM   y_MB   0.800417\n",
              "86   X_MTX_MB   y_MM   0.800426\n",
              "141  X_MBX_RM   y_MM   0.801040\n",
              "155  X_RTX_RM   y_RB   0.801269\n",
              "103  X_MTX_RB   y_FM   0.813648\n",
              "100  X_MTX_RM   y_RT   0.819783\n",
              "149  X_MBX_RB   y_RM   0.820034\n",
              "34   X_FTX_RM   y_RT   0.823496\n",
              "142  X_MBX_RM   y_RT   0.824638\n",
              "101  X_MTX_RM   y_RB   0.830028\n",
              "71   X_FMX_RM   y_RB   0.830901\n",
              "8    X_FTX_MT   y_MB   0.836170\n",
              "131  X_MMX_RB   y_RM   0.845109\n",
              "125  X_MMX_RM   y_RB   0.856721\n",
              "127  X_MMX_RB   y_FM   0.865887\n",
              "74   X_FMX_RB   y_MM   0.866435\n",
              "5    X_FTX_FM   y_RB   0.885454\n",
              "106  X_MTX_RB   y_RT   0.887171\n",
              "35   X_FTX_RM   y_RB   0.890906\n",
              "70   X_FMX_RM   y_RT   0.892038\n",
              "117  X_MMX_RT   y_MB   0.897104\n",
              "167  X_RMX_RB   y_RT   0.897392\n",
              "147  X_MBX_RB   y_MM   0.898365\n",
              "14   X_FTX_MM   y_MB   0.900166"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "D7JUwzZ1vMDq",
        "outputId": "553ff368-afd5-4b7d-d813-91b630eabbb8"
      },
      "source": [
        "CombResultsSortedMFgrouped = CombResultsSortedMF2.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedMFgroupedsortedMF2 = CombResultsSortedMFgrouped.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMFgroupedsortedMF2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_MTX_RT</th>\n",
              "      <td>0.915777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RM</th>\n",
              "      <td>0.967148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RB</th>\n",
              "      <td>1.003656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RT</th>\n",
              "      <td>1.005122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RB</th>\n",
              "      <td>1.006159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RM</th>\n",
              "      <td>1.040187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RM</th>\n",
              "      <td>1.071504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RB</th>\n",
              "      <td>1.074054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RT</th>\n",
              "      <td>1.092101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RM</th>\n",
              "      <td>1.108561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RM</th>\n",
              "      <td>1.135071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RMX_RB</th>\n",
              "      <td>1.139212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MM</th>\n",
              "      <td>1.150921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RT</th>\n",
              "      <td>1.154971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RB</th>\n",
              "      <td>1.157106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RTX_RB</th>\n",
              "      <td>1.160970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MB</th>\n",
              "      <td>1.163501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RT</th>\n",
              "      <td>1.177830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MT</th>\n",
              "      <td>1.178526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FM</th>\n",
              "      <td>1.185051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RB</th>\n",
              "      <td>1.191133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MM</th>\n",
              "      <td>1.197799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MB</th>\n",
              "      <td>1.214340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MM</th>\n",
              "      <td>1.216484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MT</th>\n",
              "      <td>1.233888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MB</th>\n",
              "      <td>1.348744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MB</th>\n",
              "      <td>1.352787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RTX_RM</th>\n",
              "      <td>1.357746</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Test Loss\n",
              "DATA_X             \n",
              "X_MTX_RT   0.915777\n",
              "X_MTX_RM   0.967148\n",
              "X_MTX_RB   1.003656\n",
              "X_MMX_RT   1.005122\n",
              "X_MMX_RB   1.006159\n",
              "X_MMX_RM   1.040187\n",
              "X_FTX_RM   1.071504\n",
              "X_FTX_RB   1.074054\n",
              "X_FMX_RT   1.092101\n",
              "X_FMX_RM   1.108561\n",
              "X_MBX_RM   1.135071\n",
              "X_RMX_RB   1.139212\n",
              "X_FTX_MM   1.150921\n",
              "X_FTX_RT   1.154971\n",
              "X_MBX_RB   1.157106\n",
              "X_RTX_RB   1.160970\n",
              "X_FMX_MB   1.163501\n",
              "X_MBX_RT   1.177830\n",
              "X_FMX_MT   1.178526\n",
              "X_FTX_FM   1.185051\n",
              "X_FMX_RB   1.191133\n",
              "X_FMX_MM   1.197799\n",
              "X_FTX_MB   1.214340\n",
              "X_MTX_MM   1.216484\n",
              "X_FTX_MT   1.233888\n",
              "X_MTX_MB   1.348744\n",
              "X_MMX_MB   1.352787\n",
              "X_RTX_RM   1.357746"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "PBkDAjxWhYY0",
        "outputId": "47dd2cc8-7b44-465c-b17c-efdfec6b02d1"
      },
      "source": [
        "CombResultsSortedMFgroupedsortedMF2.to_csv('CombResultsSortedMFgroupedsortedMF2.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedMFgroupedsortedMF2.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cb3fa0a6-a9d3-456d-ba03-0914d57a7b34\", \"CombResultsSortedMFgroupedsortedMF2.csv\", 794)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "sBeBEnv2X-EQ",
        "outputId": "3775c8bd-e448-475e-a6b8-6cc4821738dc"
      },
      "source": [
        "CombResultsSortedMF2.to_csv('CombResultsSortedMF2.csv')\n",
        "files.download(\"CombResultsSortedMF2.csv\")\n",
        "\n",
        "fig = px.box(CombResultsSortedMF2, x=\"DATA_X\", y=\"Test Loss\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_70acd70d-491b-472d-b12a-8aecaf42df92\", \"CombResultsSortedMF2.csv\", 6097)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4af31b9d-9028-41a5-afa0-cc47d3dfe5e2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4af31b9d-9028-41a5-afa0-cc47d3dfe5e2\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4af31b9d-9028-41a5-afa0-cc47d3dfe5e2',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_MTX_RT\", \"X_MMX_RT\", \"X_MTX_RT\", \"X_MMX_RB\", \"X_FMX_MB\", \"X_FMX_RT\", \"X_MTX_RB\", \"X_MMX_RM\", \"X_MBX_RM\", \"X_FMX_RT\", \"X_MMX_RT\", \"X_FTX_RT\", \"X_MTX_RM\", \"X_FMX_MT\", \"X_MBX_RT\", \"X_MBX_RT\", \"X_FMX_MM\", \"X_RTX_RB\", \"X_MTX_RT\", \"X_FTX_RT\", \"X_MTX_RT\", \"X_MTX_RM\", \"X_MMX_RM\", \"X_FMX_MB\", \"X_FTX_RB\", \"X_FMX_RM\", \"X_MTX_MM\", \"X_MTX_MB\", \"X_MBX_RM\", \"X_RTX_RM\", \"X_MTX_RB\", \"X_MTX_RM\", \"X_MBX_RB\", \"X_FTX_RM\", \"X_MBX_RM\", \"X_MTX_RM\", \"X_FMX_RM\", \"X_FTX_MT\", \"X_MMX_RB\", \"X_MMX_RM\", \"X_MMX_RB\", \"X_FMX_RB\", \"X_FTX_FM\", \"X_MTX_RB\", \"X_FTX_RM\", \"X_FMX_RM\", \"X_MMX_RT\", \"X_RMX_RB\", \"X_MBX_RB\", \"X_FTX_MM\", \"X_MTX_RB\", \"X_FMX_MM\", \"X_FTX_MM\", \"X_MTX_MM\", \"X_MMX_RT\", \"X_RMX_RB\", \"X_FTX_MM\", \"X_FTX_RB\", \"X_FTX_RB\", \"X_FTX_RM\", \"X_MMX_RM\", \"X_FMX_MT\", \"X_RTX_RB\", \"X_MTX_MM\", \"X_RTX_RB\", \"X_RMX_RB\", \"X_FMX_RB\", \"X_RMX_RB\", \"X_MMX_MB\", \"X_FTX_MT\", \"X_MMX_RM\", \"X_FTX_MB\", \"X_MTX_RT\", \"X_FTX_FM\", \"X_MTX_MM\", \"X_FTX_RB\", \"X_FMX_RB\", \"X_MBX_RB\", \"X_FTX_MB\", \"X_MTX_RM\", \"X_MBX_RB\", \"X_MMX_RT\", \"X_FTX_MB\", \"X_FTX_RM\", \"X_MMX_RB\", \"X_FMX_RM\", \"X_FMX_MM\", \"X_FTX_RM\", \"X_FMX_RT\", \"X_MTX_MB\", \"X_MTX_MB\", \"X_FTX_MT\", \"X_FTX_MM\", \"X_MTX_RB\", \"X_MMX_MB\", \"X_FTX_FM\", \"X_RTX_RB\", \"X_FMX_MB\", \"X_FMX_MM\", \"X_FMX_RT\", \"X_MMX_MB\", \"X_MMX_RB\", \"X_FTX_MT\", \"X_RTX_RM\", \"X_MBX_RT\", \"X_FMX_MT\", \"X_MBX_RM\", \"X_RTX_RM\", \"X_FTX_RB\", \"X_FTX_RT\", \"X_FTX_FM\", \"X_FMX_RB\", \"X_FTX_FM\", \"X_MBX_RT\", \"X_FMX_MB\", \"X_FTX_MB\", \"X_FMX_RT\", \"X_FTX_RT\", \"X_MBX_RT\", \"X_MTX_MB\", \"X_FTX_MM\", \"X_FTX_MB\", \"X_FMX_MT\", \"X_FTX_RT\", \"X_FMX_RM\", \"X_FMX_MT\", \"X_FMX_RB\", \"X_RTX_RM\", \"X_MMX_MB\", \"X_MBX_RM\", \"X_MBX_RB\", \"X_FTX_MT\", \"X_FMX_MT\", \"X_FTX_MB\", \"X_RMX_RB\", \"X_FMX_MB\", \"X_RTX_RB\", \"X_MMX_MB\", \"X_MMX_RB\", \"X_FTX_RB\", \"X_FMX_MM\", \"X_RMX_RB\", \"X_FTX_RT\", \"X_MTX_MM\", \"X_FMX_RB\", \"X_FTX_RM\", \"X_MTX_RM\", \"X_FTX_FM\", \"X_RTX_RB\", \"X_MTX_RB\", \"X_FMX_RM\", \"X_MBX_RB\", \"X_FMX_RT\", \"X_FTX_MM\", \"X_FMX_MB\", \"X_RTX_RM\", \"X_MTX_MB\", \"X_FMX_MM\", \"X_MBX_RT\", \"X_MTX_RT\", \"X_FTX_MT\", \"X_MMX_RT\", \"X_MBX_RM\", \"X_RTX_RM\", \"X_MMX_MB\", \"X_MMX_RM\", \"X_MTX_MB\", \"X_MTX_MM\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.5261743068695068, 0.5378136694431305, 0.5776149064302445, 0.6039110660552979, 0.6086981356143951, 0.6581975162029267, 0.6639798164367676, 0.6646197676658631, 0.6933618664741517, 0.6980075359344482, 0.6985749959945678, 0.6986924290657044, 0.701207572221756, 0.7160346567630768, 0.7389368057250977, 0.7398850679397583, 0.7535640805959701, 0.7545866787433624, 0.760698202252388, 0.7629807949066162, 0.7672264695167541, 0.773972886800766, 0.7742363810539246, 0.7886802971363067, 0.7925959706306458, 0.8001804411411285, 0.8004172056913376, 0.8004257142543793, 0.8010396420955658, 0.8012693703174592, 0.8136477887630462, 0.8197830617427826, 0.8200337409973144, 0.8234956443309784, 0.8246380746364593, 0.8300277829170227, 0.8309011310338974, 0.8361704468727111, 0.8451085209846496, 0.8567206025123596, 0.865886652469635, 0.8664347946643829, 0.8854544997215271, 0.887170821428299, 0.8909062206745147, 0.8920376002788544, 0.8971044421195984, 0.8973917067050934, 0.8983653247356415, 0.9001662850379943, 0.904131805896759, 0.9096692442893982, 0.913791024684906, 0.9167017221450806, 0.9182459473609924, 0.9207487881183625, 0.9332331717014313, 0.939322966337204, 0.9432799518108368, 0.9548518240451813, 0.9555743575096131, 0.960929948091507, 0.9616184711456299, 0.969048821926117, 0.9903261005878449, 0.992265808582306, 1.001711755990982, 1.008250743150711, 1.0126550197601318, 1.021770864725113, 1.0284330368041992, 1.037264096736908, 1.0415588974952699, 1.0458348989486694, 1.0496914982795715, 1.0535849690437318, 1.0548807740211488, 1.0622965753078462, 1.062389862537384, 1.0645675480365753, 1.0719576001167297, 1.0779197812080383, 1.0807103216648102, 1.081166833639145, 1.0813504576683044, 1.0833235681056976, 1.083646035194397, 1.0857297897338867, 1.090717500448227, 1.0954893052577972, 1.102862399816513, 1.1040704488754272, 1.1115025222301482, 1.1115440607070923, 1.1166556537151338, 1.1190181851387024, 1.1279848396778107, 1.1319179296493531, 1.1324189841747283, 1.1328436851501464, 1.1349341690540313, 1.1357545971870422, 1.1373327672481537, 1.1400836765766145, 1.1511807918548584, 1.1703509628772735, 1.1725226104259492, 1.1749932765960693, 1.188001698255539, 1.1945786774158478, 1.206766664981842, 1.226647013425827, 1.2299752354621887, 1.2869935929775238, 1.2904144644737243, 1.291906201839447, 1.3118329405784608, 1.3313405394554139, 1.335911786556244, 1.3519313216209412, 1.3536684572696687, 1.361753559112549, 1.3728007912635802, 1.3763815641403199, 1.3948794245719909, 1.4001413345336915, 1.4074299454689025, 1.4085989713668823, 1.4106548428535461, 1.4132540464401244, 1.4324962735176086, 1.4334052562713624, 1.4509004473686218, 1.4520147442817688, 1.4615373849868774, 1.4616323113441467, 1.4953739285469054, 1.4977857232093812, 1.5049411416053773, 1.5275405526161194, 1.5319352626800538, 1.5550794839859008, 1.5658524751663208, 1.5792065382003784, 1.5896948218345641, 1.5928719520568848, 1.613330078125, 1.6232594847679138, 1.6359298825263977, 1.641461229324341, 1.6500413417816162, 1.6574836015701293, 1.6610089540481567, 1.6931620240211487, 1.6996631026268005, 1.7023163795471192, 1.763371968269348, 1.7755598306655884, 1.8140723347663879, 1.8213908672332764, 1.8705768227577209, 1.9010727643966674, 1.9056068658828735, 1.9192139744758605, 1.944037663936615, 1.9615407586097717, 1.978384506702423, 1.9838356494903564], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4af31b9d-9028-41a5-afa0-cc47d3dfe5e2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWIFrrJ5zcyM"
      },
      "source": [
        "# define baseline model 3\n",
        "# create model\n",
        "n_features = 1\n",
        "model3 = Sequential()\n",
        "model3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(21, n_features)))\n",
        "model3.add(MaxPooling1D(pool_size=2))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(50, activation='relu'))\n",
        "model3.add(Dense(1))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "model3.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "#model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xeRL4LyzcyN"
      },
      "source": [
        "n_features = 1\n",
        "def datageneratorMF3(X_in1, X_in2, X_in3, Y_in):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  X_in = np.concatenate((X_in1, X_in2, X_in3), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "\n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], n_features))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], n_features))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], n_features))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjYuNqU0zcyN"
      },
      "source": [
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMS(X_MT, X_MM, y_RM)\n",
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(X_MT, y_RM)\n",
        "#X_train_Input[0:5]\n",
        "#X_train_Input.shape\n",
        "#y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUNUAuKKzcyN"
      },
      "source": [
        "def evaldataMF3(X_in1, X_in2, X_in3, Y_in, traindata1, traindata2, traindata3, testdata):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMF3(X_in1, X_in2, X_in3, Y_in)\n",
        "  \n",
        "  history = model3.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model3.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model3.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # PLOTS LOSS VS EPOCH\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  # print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  # print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # PLOTS Y ORIGINAL VS PREDICTED\n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata1)+ str(traindata2)  + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnx0EbtLULQv",
        "outputId": "840e2684-b074-4aa3-84cd-3681b9f39de5"
      },
      "source": [
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "\n",
        "model3.save_weights('model3.h5')\n",
        "\n",
        "my_dictMF3 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), 3):\n",
        "  kX1, kX2, kX3 = combo[0][0], combo[1][0], combo[2][0]\n",
        "  vX1, vX2, vX3 = combo[0][1], combo[1][1], combo[2][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF3 = evaldataMF3(vX1, vX2, vX3, vy, kX1, kX2, kX3, ky)\n",
        "      TestLossTotal = resultsMF3[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF3[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF3[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}_{kX2}_{kX3} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF3[\"DATA_X\"].append(kX1 + kX2 + kX3)\n",
        "    my_dictMF3[\"DATA_y\"].append(ky)\n",
        "    my_dictMF3[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model3.load_weights('model3.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8439918458461761, and Avg Validation Loss (mae) is 0.8905733585357666\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8809112727642059\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8905903398990631, and Avg Validation Loss (mae) is 0.9064926862716675\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9094315767288208\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.498454487323761, and Avg Validation Loss (mae) is 1.4499109745025636\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4146369218826294\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2887068152427674, and Avg Validation Loss (mae) is 1.1283807635307312\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1367985427379608\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9844022452831268, and Avg Validation Loss (mae) is 1.0903772830963134\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0539177179336547\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1621453940868378, and Avg Validation Loss (mae) is 1.177806979417801\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2176247000694276\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7254583895206451, and Avg Validation Loss (mae) is 0.8090100735425949\n",
            "After 10 runs; Avg Test Loss (mae) is 0.80575612783432\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4057013511657714, and Avg Validation Loss (mae) is 1.441427218914032\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3975602030754088\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0388219952583313, and Avg Validation Loss (mae) is 1.2393139839172362\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2007325351238252\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8744056940078735, and Avg Validation Loss (mae) is 0.9716428041458129\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9642712056636811\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.130885624885559, and Avg Validation Loss (mae) is 1.1803382396698\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1780955791473389\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5715388268232345, and Avg Validation Loss (mae) is 0.622262641787529\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6252229958772659\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.471097183227539, and Avg Validation Loss (mae) is 1.547205352783203\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5417815685272216\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2202089548110961, and Avg Validation Loss (mae) is 1.246250069141388\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2501118063926697\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8373961210250854, and Avg Validation Loss (mae) is 0.7692434668540955\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7658763647079467\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3891183733940125, and Avg Validation Loss (mae) is 1.499728000164032\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4959411025047302\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9348151981830597, and Avg Validation Loss (mae) is 1.0080981254577637\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9799071907997131\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0452158570289611, and Avg Validation Loss (mae) is 1.2403916120529175\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2251070320606232\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7141889989376068, and Avg Validation Loss (mae) is 0.7502176612615585\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7492977678775787\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.660597974061966, and Avg Validation Loss (mae) is 0.7288507729768753\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7352758914232254\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.420617437362671, and Avg Validation Loss (mae) is 1.4672378778457642\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4543172240257263\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8122600972652435, and Avg Validation Loss (mae) is 0.8554180026054382\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8671304523944855\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0007320761680603, and Avg Validation Loss (mae) is 0.9796010196208954\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9599252939224243\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7937384903430938, and Avg Validation Loss (mae) is 0.8425170958042145\n",
            "After 10 runs; Avg Test Loss (mae) is 0.852572226524353\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6638727366924286, and Avg Validation Loss (mae) is 0.7927383750677108\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8001748353242875\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2188549041748047, and Avg Validation Loss (mae) is 1.2137017011642457\n",
            "After 10 runs; Avg Test Loss (mae) is 1.191258692741394\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8666915416717529, and Avg Validation Loss (mae) is 0.8299670696258545\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8428997457027435\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9810628771781922, and Avg Validation Loss (mae) is 0.9191516101360321\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9385623633861542\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1381899416446686, and Avg Validation Loss (mae) is 1.0664420545101165\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0606616377830504\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8626913666725159, and Avg Validation Loss (mae) is 0.9949422180652618\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9744688153266907\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8351097702980042, and Avg Validation Loss (mae) is 0.6698903501033783\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6773953020572663\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7037821471691131, and Avg Validation Loss (mae) is 0.7513145923614502\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7492991149425506\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3111115932464599, and Avg Validation Loss (mae) is 1.3209250330924989\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3031520605087281\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8656398773193359, and Avg Validation Loss (mae) is 0.9286645650863647\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9115018427371979\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7629969716072083, and Avg Validation Loss (mae) is 0.794647616147995\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8038605093955994\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9341000318527222, and Avg Validation Loss (mae) is 0.9097791731357574\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8943145573139191\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7551648318767548, and Avg Validation Loss (mae) is 0.6929200232028961\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7057172656059265\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4212314248085023, and Avg Validation Loss (mae) is 1.3828104496002198\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3664314508438111\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3524832487106324, and Avg Validation Loss (mae) is 1.1899022698402404\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1808549404144286\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9467927694320679, and Avg Validation Loss (mae) is 0.8595892727375031\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8510888576507568\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9346317112445831, and Avg Validation Loss (mae) is 0.9047887682914734\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8823894083499908\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6963458776473999, and Avg Validation Loss (mae) is 0.8027712583541871\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7962348163127899\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7569251000881195, and Avg Validation Loss (mae) is 0.9943027496337891\n",
            "After 10 runs; Avg Test Loss (mae) is 1.015767228603363\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6793120861053467, and Avg Validation Loss (mae) is 0.6926444083452225\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6805056035518646\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.647341501712799, and Avg Validation Loss (mae) is 0.7361302614212036\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7327842772006988\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.803855562210083, and Avg Validation Loss (mae) is 0.78491712808609\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7833171129226685\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5686429470777512, and Avg Validation Loss (mae) is 0.7088121324777603\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7166061908006668\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7324223458766937, and Avg Validation Loss (mae) is 0.8392781138420105\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8408981144428254\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7726242363452911, and Avg Validation Loss (mae) is 0.6463779985904694\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6524971723556519\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6885478854179382, and Avg Validation Loss (mae) is 0.6078059554100037\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6005210638046264\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7232231557369232, and Avg Validation Loss (mae) is 0.7357305824756623\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7337491273880005\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.633344316482544, and Avg Validation Loss (mae) is 0.6460575044155121\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6472476184368133\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7830226004123688, and Avg Validation Loss (mae) is 0.6778192818164825\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6894276738166809\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.121976101398468, and Avg Validation Loss (mae) is 1.0518315017223359\n",
            "After 10 runs; Avg Test Loss (mae) is 1.042342060804367\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8237923562526703, and Avg Validation Loss (mae) is 0.8520499050617218\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8438287019729614\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7750332295894623, and Avg Validation Loss (mae) is 0.7843674659729004\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7846168398857116\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0966983020305634, and Avg Validation Loss (mae) is 1.0533097624778747\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0657733380794525\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4771594166755677, and Avg Validation Loss (mae) is 1.4829489707946777\n",
            "After 10 runs; Avg Test Loss (mae) is 1.488307273387909\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.149569296836853, and Avg Validation Loss (mae) is 1.1525127649307252\n",
            "After 10 runs; Avg Test Loss (mae) is 1.139777421951294\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8723493814468384, and Avg Validation Loss (mae) is 0.8811283111572266\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8596225321292877\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7695293605327607, and Avg Validation Loss (mae) is 0.8311750948429107\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8547629058361054\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9582550883293152, and Avg Validation Loss (mae) is 0.8544423580169678\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8556057989597321\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6934854209423065, and Avg Validation Loss (mae) is 0.6424416780471802\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6358593374490737\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5218506157398224, and Avg Validation Loss (mae) is 0.5031656682491302\n",
            "After 10 runs; Avg Test Loss (mae) is 0.48987704515457153\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6691482841968537, and Avg Validation Loss (mae) is 0.8287471860647202\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8375940293073654\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7734145581722259, and Avg Validation Loss (mae) is 0.8826194047927857\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9033761858940125\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0694309532642365, and Avg Validation Loss (mae) is 1.0053774654865264\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9868491172790528\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8110624790191651, and Avg Validation Loss (mae) is 1.0586918264627456\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0775883615016937\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7204552888870239, and Avg Validation Loss (mae) is 0.7694059371948242\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7860630363225937\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6318253517150879, and Avg Validation Loss (mae) is 0.5553746521472931\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5580693989992142\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6918777644634246, and Avg Validation Loss (mae) is 0.7848459780216217\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7948333323001862\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.095543622970581, and Avg Validation Loss (mae) is 1.171647810935974\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1764793276786805\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8253020823001862, and Avg Validation Loss (mae) is 0.7027456641197205\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6794335961341857\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.133668178319931, and Avg Validation Loss (mae) is 1.2718890249729156\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2785745501518249\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7930332839488983, and Avg Validation Loss (mae) is 0.8488775968551636\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8531022071838379\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0852767467498778, and Avg Validation Loss (mae) is 1.1606478810310363\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1766372561454772\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3529056191444397, and Avg Validation Loss (mae) is 1.3717301964759827\n",
            "After 10 runs; Avg Test Loss (mae) is 1.353888201713562\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8157290041446685, and Avg Validation Loss (mae) is 0.779804652929306\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7856266736984253\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7284120082855224, and Avg Validation Loss (mae) is 0.7587217867374421\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7692613661289215\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7712080717086792, and Avg Validation Loss (mae) is 0.6960271775722504\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7042771995067596\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0872043430805207, and Avg Validation Loss (mae) is 1.1147328555583953\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1181313931941985\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3240057229995728, and Avg Validation Loss (mae) is 1.3735522985458375\n",
            "After 10 runs; Avg Test Loss (mae) is 1.359982180595398\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7102816998958588, and Avg Validation Loss (mae) is 0.6911439299583435\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7329189360141755\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8649236977100372, and Avg Validation Loss (mae) is 1.0094528138637542\n",
            "After 10 runs; Avg Test Loss (mae) is 1.020096105337143\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6533421814441681, and Avg Validation Loss (mae) is 0.7421121001243591\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7300049632787704\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9159019708633422, and Avg Validation Loss (mae) is 1.1628848135471344\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1681198358535767\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3171586036682128, and Avg Validation Loss (mae) is 1.332252299785614\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3283859729766845\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7513551652431488, and Avg Validation Loss (mae) is 0.8990219950675964\n",
            "After 10 runs; Avg Test Loss (mae) is 0.899289870262146\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.031198924779892, and Avg Validation Loss (mae) is 1.1500881731510162\n",
            "After 10 runs; Avg Test Loss (mae) is 1.186908060312271\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8069640338420868, and Avg Validation Loss (mae) is 0.707477456331253\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7025822341442108\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0016230642795563, and Avg Validation Loss (mae) is 1.0732187390327455\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0626483619213105\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5139927506446837, and Avg Validation Loss (mae) is 1.5030994296073914\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5288603901863098\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9893787741661072, and Avg Validation Loss (mae) is 1.0067009687423707\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0045916616916657\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0665673673152924, and Avg Validation Loss (mae) is 1.0393365144729614\n",
            "After 10 runs; Avg Test Loss (mae) is 1.031051117181778\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6384767174720765, and Avg Validation Loss (mae) is 0.6381845027208328\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6381431221961975\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9111069917678833, and Avg Validation Loss (mae) is 0.9123786449432373\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9371399283409119\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3645161628723144, and Avg Validation Loss (mae) is 1.2850278973579408\n",
            "After 10 runs; Avg Test Loss (mae) is 1.349532914161682\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0188103437423706, and Avg Validation Loss (mae) is 1.1820249676704406\n",
            "After 10 runs; Avg Test Loss (mae) is 1.186560755968094\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0267696022987365, and Avg Validation Loss (mae) is 1.182277101278305\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1636250078678132\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.755557519197464, and Avg Validation Loss (mae) is 0.7507327795028687\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7671345293521881\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9768341243267059, and Avg Validation Loss (mae) is 0.8972122728824615\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8995300233364105\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4830869674682616, and Avg Validation Loss (mae) is 1.590181565284729\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6074357748031616\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.898845762014389, and Avg Validation Loss (mae) is 0.858200854063034\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8702571988105774\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9805006384849548, and Avg Validation Loss (mae) is 1.0189458012580872\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0228426992893218\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8845726251602173, and Avg Validation Loss (mae) is 1.0058735251426696\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0098844766616821\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.533870553970337, and Avg Validation Loss (mae) is 1.652371621131897\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6711787700653076\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7485364854335785, and Avg Validation Loss (mae) is 0.895721435546875\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9131912231445313\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3883081555366517, and Avg Validation Loss (mae) is 1.4709148287773133\n",
            "After 10 runs; Avg Test Loss (mae) is 1.450066590309143\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9432923674583436, and Avg Validation Loss (mae) is 1.0002369940280915\n",
            "After 10 runs; Avg Test Loss (mae) is 1.009316325187683\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9344416856765747, and Avg Validation Loss (mae) is 1.0680492520332336\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0569895088672638\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4876407384872437, and Avg Validation Loss (mae) is 1.5647563099861146\n",
            "After 10 runs; Avg Test Loss (mae) is 1.555060911178589\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.612198007106781, and Avg Validation Loss (mae) is 0.518487548828125\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5219042509794235\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3771995425224304, and Avg Validation Loss (mae) is 1.3280719637870788\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2973100066184997\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1645566523075104, and Avg Validation Loss (mae) is 1.1012474238872527\n",
            "After 10 runs; Avg Test Loss (mae) is 1.126629936695099\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9499052345752717, and Avg Validation Loss (mae) is 1.0725091338157653\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0861854374408721\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5499261140823364, and Avg Validation Loss (mae) is 1.4198575615882874\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4238465666770934\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6881110012531281, and Avg Validation Loss (mae) is 0.6679247558116913\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6744631499052047\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7641267240047455, and Avg Validation Loss (mae) is 0.8391906976699829\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8392415225505829\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7038623124361039, and Avg Validation Loss (mae) is 0.721397590637207\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7296503037214279\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6711476504802704, and Avg Validation Loss (mae) is 0.8378652662038804\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8389055043458938\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5864641070365906, and Avg Validation Loss (mae) is 1.7524879097938537\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7179574489593505\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5475474447011948, and Avg Validation Loss (mae) is 0.6081213653087616\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6107938617467881\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7508116662502289, and Avg Validation Loss (mae) is 0.8949646532535553\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8860688805580139\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7489375352859498, and Avg Validation Loss (mae) is 0.8853414416313171\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8944894850254059\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6337796688079834, and Avg Validation Loss (mae) is 0.659176105260849\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6615889370441437\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5544456958770752, and Avg Validation Loss (mae) is 1.4852051854133606\n",
            "After 10 runs; Avg Test Loss (mae) is 1.458773136138916\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6281074404716491, and Avg Validation Loss (mae) is 0.6090926676988602\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6148225545883179\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7943391084671021, and Avg Validation Loss (mae) is 0.8381827414035797\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8104053676128388\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0501993775367737, and Avg Validation Loss (mae) is 1.1838673055171967\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1861085891723633\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8951397895812988, and Avg Validation Loss (mae) is 1.004668229818344\n",
            "After 10 runs; Avg Test Loss (mae) is 1.009265238046646\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4950844764709472, and Avg Validation Loss (mae) is 1.5069830179214478\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4867220878601075\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.121898078918457, and Avg Validation Loss (mae) is 1.1852408170700073\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1405730545520782\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4634472727775574, and Avg Validation Loss (mae) is 1.5310852766036986\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5546702861785888\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.100217628479004, and Avg Validation Loss (mae) is 1.148867231607437\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1492115139961243\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8069226443767548, and Avg Validation Loss (mae) is 1.015073651075363\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0256270945072175\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6550570726394653, and Avg Validation Loss (mae) is 1.5424792170524597\n",
            "After 10 runs; Avg Test Loss (mae) is 1.522441565990448\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0375893533229827, and Avg Validation Loss (mae) is 0.9387462556362152\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9175775408744812\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7263572156429291, and Avg Validation Loss (mae) is 0.8971731513738632\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9029195725917816\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6376429349184036, and Avg Validation Loss (mae) is 0.527774554491043\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5328955680131913\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6570945501327514, and Avg Validation Loss (mae) is 0.6190581351518631\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6195835143327713\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5648488640785216, and Avg Validation Loss (mae) is 1.8213647961616517\n",
            "After 10 runs; Avg Test Loss (mae) is 1.81847425699234\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0410789370536804, and Avg Validation Loss (mae) is 1.0499886751174927\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0515564858913422\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7161592900753021, and Avg Validation Loss (mae) is 0.7996225118637085\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7899444013833999\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.725091952085495, and Avg Validation Loss (mae) is 0.7460423111915588\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7367230862379074\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7000585079193116, and Avg Validation Loss (mae) is 0.662650391459465\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6708262920379638\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.376550543308258, and Avg Validation Loss (mae) is 1.3779135465621948\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3620210886001587\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1294697165489196, and Avg Validation Loss (mae) is 1.2072762548923492\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2454916477203368\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7213506639003754, and Avg Validation Loss (mae) is 0.7015823751688004\n",
            "After 10 runs; Avg Test Loss (mae) is 0.705900713801384\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1052764356136322, and Avg Validation Loss (mae) is 1.1184994757175446\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1813489496707916\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7753138065338134, and Avg Validation Loss (mae) is 0.8989990651607513\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8907848596572876\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5486846089363098, and Avg Validation Loss (mae) is 1.5352839350700378\n",
            "After 10 runs; Avg Test Loss (mae) is 1.54054137468338\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.137682443857193, and Avg Validation Loss (mae) is 1.2157679617404937\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2351956367492676\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.705510425567627, and Avg Validation Loss (mae) is 0.7197876155376435\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7344087481498718\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6140401601791382, and Avg Validation Loss (mae) is 0.7100753933191299\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7139562517404556\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.641486519575119, and Avg Validation Loss (mae) is 0.6160765290260315\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6193124920129776\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6158717155456543, and Avg Validation Loss (mae) is 1.788749623298645\n",
            "After 10 runs; Avg Test Loss (mae) is 1.858415699005127\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1769030451774598, and Avg Validation Loss (mae) is 1.147387546300888\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1517080962657928\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6438425600528717, and Avg Validation Loss (mae) is 0.7904411792755127\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7715728044509887\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9309117734432221, and Avg Validation Loss (mae) is 0.8423024594783783\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8698420941829681\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.644596540927887, and Avg Validation Loss (mae) is 0.6330551624298095\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6364879667758941\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.610030746459961, and Avg Validation Loss (mae) is 1.6610451817512513\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6404146552085876\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.204525852203369, and Avg Validation Loss (mae) is 1.2849501371383667\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2954607725143432\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6687093317508698, and Avg Validation Loss (mae) is 0.6720050483942032\n",
            "After 10 runs; Avg Test Loss (mae) is 0.658775681257248\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0465737342834474, and Avg Validation Loss (mae) is 1.1424121975898742\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0946678638458252\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8199812531471252, and Avg Validation Loss (mae) is 0.9264241397380829\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9394492506980896\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.610430932044983, and Avg Validation Loss (mae) is 1.4415062069892883\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4215776205062867\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3822718977928161, and Avg Validation Loss (mae) is 1.3698944091796874\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3405157208442688\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7626875400543213, and Avg Validation Loss (mae) is 0.862434321641922\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8355158269405365\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0173066318035127, and Avg Validation Loss (mae) is 0.9550267696380615\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9496770381927491\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5969836056232453, and Avg Validation Loss (mae) is 0.7770998686552048\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7886918157339096\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6263458967208861, and Avg Validation Loss (mae) is 1.9589346289634704\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9589437246322632\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2931688904762269, and Avg Validation Loss (mae) is 1.3051066517829895\n",
            "After 10 runs; Avg Test Loss (mae) is 1.309616756439209\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9412961602210999, and Avg Validation Loss (mae) is 1.0173424184322357\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0062951624393464\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0348955154418946, and Avg Validation Loss (mae) is 1.0185182511806488\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0149496376514435\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6787616372108459, and Avg Validation Loss (mae) is 0.6932447671890258\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6934183895587921\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5612990856170654, and Avg Validation Loss (mae) is 1.5241342663764954\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5322506427764893\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.362715446949005, and Avg Validation Loss (mae) is 1.3449205040931702\n",
            "After 10 runs; Avg Test Loss (mae) is 1.331775426864624\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8095341086387634, and Avg Validation Loss (mae) is 0.8179752707481385\n",
            "After 10 runs; Avg Test Loss (mae) is 0.812739509344101\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9614823222160339, and Avg Validation Loss (mae) is 1.0330314993858338\n",
            "After 10 runs; Avg Test Loss (mae) is 1.010002326965332\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8058092713356018, and Avg Validation Loss (mae) is 0.8333773910999298\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8275168597698211\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9226847410202026, and Avg Validation Loss (mae) is 2.0182974338531494\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9715250372886657\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0283690571784974, and Avg Validation Loss (mae) is 1.028192138671875\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0395734369754792\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.518180787563324, and Avg Validation Loss (mae) is 1.5509369015693664\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5536993503570558\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.980673736333847, and Avg Validation Loss (mae) is 1.2012922704219817\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1770451605319976\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9390897989273072, and Avg Validation Loss (mae) is 0.8590625762939453\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8394899427890777\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9778353810310363, and Avg Validation Loss (mae) is 2.022988224029541\n",
            "After 10 runs; Avg Test Loss (mae) is 2.05085608959198\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9220520615577698, and Avg Validation Loss (mae) is 0.8980519831180572\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8910372853279114\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7361974596977234, and Avg Validation Loss (mae) is 0.7765924155712127\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7952092111110687\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5346685916185379, and Avg Validation Loss (mae) is 0.5411816954612731\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5480785936117172\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7109087705612183, and Avg Validation Loss (mae) is 0.7708406209945678\n",
            "After 10 runs; Avg Test Loss (mae) is 0.778819328546524\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8805717825889587, and Avg Validation Loss (mae) is 1.9344725608825684\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9361400723457336\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9495270550251007, and Avg Validation Loss (mae) is 0.9081822812557221\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8978036880493164\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7730324149131775, and Avg Validation Loss (mae) is 0.7928578078746795\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7748239874839783\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7295881271362304, and Avg Validation Loss (mae) is 0.8438893020153045\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8342605173587799\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7206100702285767, and Avg Validation Loss (mae) is 0.8851980686187744\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8702048122882843\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6784653067588806, and Avg Validation Loss (mae) is 1.5586092948913575\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5534899711608887\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8039319574832916, and Avg Validation Loss (mae) is 0.8283881783485413\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8016478955745697\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6783145427703857, and Avg Validation Loss (mae) is 0.7049685031175613\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7048095881938934\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9350102961063385, and Avg Validation Loss (mae) is 0.9269835591316223\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9346120893955231\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7137417614459991, and Avg Validation Loss (mae) is 0.681733250617981\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6653080195188522\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.807255208492279, and Avg Validation Loss (mae) is 1.7575917482376098\n",
            "After 10 runs; Avg Test Loss (mae) is 1.771574115753174\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9812567234039307, and Avg Validation Loss (mae) is 1.1034003376960755\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0857521831989287\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6801091432571411, and Avg Validation Loss (mae) is 0.6406645894050598\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6528412789106369\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.707633551955223, and Avg Validation Loss (mae) is 0.8484424442052841\n",
            "After 10 runs; Avg Test Loss (mae) is 0.820217627286911\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7897846400737762, and Avg Validation Loss (mae) is 0.7805761098861694\n",
            "After 10 runs; Avg Test Loss (mae) is 0.780651432275772\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9308817982673645, and Avg Validation Loss (mae) is 1.789454710483551\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8465567111968995\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0195300579071045, and Avg Validation Loss (mae) is 1.0703897178173065\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0573965072631837\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5636888086795807, and Avg Validation Loss (mae) is 0.6050113022327424\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5943840026855469\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7343375623226166, and Avg Validation Loss (mae) is 0.8211941659450531\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8216029524803161\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7471967756748199, and Avg Validation Loss (mae) is 0.8587872684001923\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8395727336406708\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8118193864822387, and Avg Validation Loss (mae) is 1.843728280067444\n",
            "After 10 runs; Avg Test Loss (mae) is 1.901343083381653\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8697014331817627, and Avg Validation Loss (mae) is 0.9911422550678253\n",
            "After 10 runs; Avg Test Loss (mae) is 0.993206799030304\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6924924552440643, and Avg Validation Loss (mae) is 0.6849602818489074\n",
            "After 10 runs; Avg Test Loss (mae) is 0.697810709476471\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.982351940870285, and Avg Validation Loss (mae) is 0.8811417996883393\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8739601373672485\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.837238198518753, and Avg Validation Loss (mae) is 0.8101943373680115\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7954454958438874\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5462803602218629, and Avg Validation Loss (mae) is 1.4678233861923218\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4969469547271728\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9506755530834198, and Avg Validation Loss (mae) is 1.181960016489029\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1789022147655488\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5529546141624451, and Avg Validation Loss (mae) is 0.541180682182312\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5431760519742965\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7716791749000549, and Avg Validation Loss (mae) is 0.7880420625209809\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7650764882564545\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6928693115711212, and Avg Validation Loss (mae) is 0.703885966539383\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7102600932121277\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6431108236312866, and Avg Validation Loss (mae) is 1.5277310729026794\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5568724155426026\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.844759863615036, and Avg Validation Loss (mae) is 0.919538700580597\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9141891539096832\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6996100604534149, and Avg Validation Loss (mae) is 0.6935986846685409\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7110741764307023\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7815155327320099, and Avg Validation Loss (mae) is 0.7983615577220917\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8033157527446747\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6566738545894623, and Avg Validation Loss (mae) is 0.7491622030735016\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7388627588748932\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6638553023338318, and Avg Validation Loss (mae) is 1.6643167734146118\n",
            "After 10 runs; Avg Test Loss (mae) is 1.663049054145813\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7932657241821289, and Avg Validation Loss (mae) is 0.8771896719932556\n",
            "After 10 runs; Avg Test Loss (mae) is 0.861609423160553\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6010076314210892, and Avg Validation Loss (mae) is 0.5859808623790741\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5782956898212432\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7389712989330292, and Avg Validation Loss (mae) is 0.7579510152339936\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7497205555438995\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6907341718673706, and Avg Validation Loss (mae) is 0.9488995373249054\n",
            "After 10 runs; Avg Test Loss (mae) is 0.955156335234642\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8856784224510192, and Avg Validation Loss (mae) is 1.8946411252021789\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8992505192756652\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9388852298259736, and Avg Validation Loss (mae) is 0.9483635365962982\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9434275209903717\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9132972836494446, and Avg Validation Loss (mae) is 0.9733736217021942\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9931180357933045\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.522186890244484, and Avg Validation Loss (mae) is 0.5918604791164398\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5960397303104401\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8272524118423462, and Avg Validation Loss (mae) is 0.9340619385242462\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9500215947628021\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9254266381263734, and Avg Validation Loss (mae) is 1.8451525449752808\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8339879631996154\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.02825688123703, and Avg Validation Loss (mae) is 1.0565224468708039\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0697776317596435\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.021996521949768, and Avg Validation Loss (mae) is 0.9955520331859589\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0020512521266938\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7763310611248017, and Avg Validation Loss (mae) is 0.7239509701728821\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7276834487915039\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.79688281416893, and Avg Validation Loss (mae) is 0.8648661673069\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8390851497650147\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6666576385498046, and Avg Validation Loss (mae) is 1.724691104888916\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7371329426765443\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.816386365890503, and Avg Validation Loss (mae) is 0.8537569582462311\n",
            "After 10 runs; Avg Test Loss (mae) is 0.843631488084793\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.237829613685608, and Avg Validation Loss (mae) is 1.2049538552761079\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2296674013137818\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0309573352336883, and Avg Validation Loss (mae) is 1.0610441267490387\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0521787762641908\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8039838612079621, and Avg Validation Loss (mae) is 0.8145678699016571\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8213133752346039\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8320939898490907, and Avg Validation Loss (mae) is 1.8348327279090881\n",
            "After 10 runs; Avg Test Loss (mae) is 1.850548470020294\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9190498769283295, and Avg Validation Loss (mae) is 1.0952519297599792\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1053870737552642\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9417123377323151, and Avg Validation Loss (mae) is 0.9614395380020142\n",
            "After 10 runs; Avg Test Loss (mae) is 0.96457639336586\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7201491177082062, and Avg Validation Loss (mae) is 0.7419635742902756\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7362071454524994\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7442082941532135, and Avg Validation Loss (mae) is 0.6564702212810516\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6695847332477569\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.683936870098114, and Avg Validation Loss (mae) is 1.594653272628784\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6345489859580993\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8432649374008179, and Avg Validation Loss (mae) is 0.689863657951355\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6749199748039245\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.005205935239792, and Avg Validation Loss (mae) is 1.0172027826309205\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0301107227802277\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6933809638023376, and Avg Validation Loss (mae) is 0.6974494367837906\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6924344390630722\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5005680620670319, and Avg Validation Loss (mae) is 0.5408861815929413\n",
            "After 10 runs; Avg Test Loss (mae) is 0.536593559384346\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.618707299232483, and Avg Validation Loss (mae) is 1.5435969829559326\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5412175893783568\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8429771661758423, and Avg Validation Loss (mae) is 1.0942819714546204\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0672823071479798\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.00765221118927, and Avg Validation Loss (mae) is 1.1140174508094787\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1149709165096282\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7780895113945008, and Avg Validation Loss (mae) is 0.721835657954216\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7153297334909439\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7247503399848938, and Avg Validation Loss (mae) is 0.6077221482992172\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6060028195381164\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8789237141609192, and Avg Validation Loss (mae) is 1.8859928727149964\n",
            "After 10 runs; Avg Test Loss (mae) is 1.836331021785736\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1300913631916045, and Avg Validation Loss (mae) is 1.1721971392631532\n",
            "After 10 runs; Avg Test Loss (mae) is 1.198355758190155\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3197015523910522, and Avg Validation Loss (mae) is 1.2614565372467041\n",
            "After 10 runs; Avg Test Loss (mae) is 1.252750027179718\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6096730500459671, and Avg Validation Loss (mae) is 0.6938922524452209\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7106129884719848\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6800793766975403, and Avg Validation Loss (mae) is 0.7908946931362152\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8055537283420563\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6649017453193664, and Avg Validation Loss (mae) is 1.67341810464859\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6454256892204284\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9529553830623627, and Avg Validation Loss (mae) is 0.9630773663520813\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9728973209857941\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2172903776168824, and Avg Validation Loss (mae) is 1.1690211713314056\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1681439459323884\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8008922517299653, and Avg Validation Loss (mae) is 0.7243778467178345\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7153578221797943\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6222914040088654, and Avg Validation Loss (mae) is 0.5760435670614242\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5748584300279618\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6648754239082337, and Avg Validation Loss (mae) is 1.6419647693634034\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6355172514915466\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0014934599399568, and Avg Validation Loss (mae) is 0.943540358543396\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9549917161464692\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3391663193702699, and Avg Validation Loss (mae) is 1.3139050722122192\n",
            "After 10 runs; Avg Test Loss (mae) is 1.297171413898468\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7638890385627747, and Avg Validation Loss (mae) is 0.7337924540042877\n",
            "After 10 runs; Avg Test Loss (mae) is 0.728352153301239\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8401313781738281, and Avg Validation Loss (mae) is 0.8733475565910339\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8761255323886872\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6747604727745056, and Avg Validation Loss (mae) is 1.618015956878662\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6210727572441102\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9880089044570923, and Avg Validation Loss (mae) is 1.0769387423992156\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0889596223831177\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4501282811164855, and Avg Validation Loss (mae) is 1.8386289119720458\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8439070463180542\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8901683270931244, and Avg Validation Loss (mae) is 0.7900677025318146\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7431118667125702\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9597873568534852, and Avg Validation Loss (mae) is 0.9547590553760529\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9699355900287628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5ltrKV-szcyO",
        "outputId": "04895b8e-595f-4cb6-8b7e-96606d39976e"
      },
      "source": [
        "CombResultsMF3 = pd.DataFrame.from_dict(my_dictMF3)\n",
        "print(CombResultsMF3.shape)\n",
        "CombResultsSortedMF3 = CombResultsMF3.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMF3.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(280, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA_X</th>\n",
              "      <th>DATA_y</th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>X_FTX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.489877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>X_FMX_MTX_MB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.521904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>X_FMX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.532896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>X_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.536594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>X_MTX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.543176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>X_MTX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.548079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>X_FTX_MMX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.558069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>X_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.574858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>X_MTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.578296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>X_MTX_MBX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.594384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>X_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.596040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>X_FTX_MTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.600521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>X_MMX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.606003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>X_FMX_MTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.610794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>X_FMX_MTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.614823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>X_FMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.619312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>X_FMX_MMX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.619584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>X_FTX_FMX_MB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.625223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>X_FTX_MMX_RT</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.635859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>X_FMX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.636488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>X_FTX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.638143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>X_FTX_MTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.647248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>X_FTX_MTX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.652497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>X_MTX_MBX_RT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.652841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>X_FMX_MBX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.658776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>X_FMX_MTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.661589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>X_MTX_MMX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.665308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>X_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.669585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>X_FMX_MMX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.670826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>X_FMX_MTX_RT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.674463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>X_MMX_RTX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.674920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>X_FTX_MTX_MM</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.677395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>X_FTX_MMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.679434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>X_FTX_MTX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.680506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>X_FTX_MTX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.689428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>X_MMX_RTX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.692434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>X_FMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.693418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>X_MTX_MBX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.697811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>X_FTX_MBX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.702582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>X_FTX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.704277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>X_MTX_MMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.704810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>X_FTX_MTX_MB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.705717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>X_FMX_MMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.705901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>X_MTX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.710260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>X_MBX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.710613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>X_MTX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.711074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>X_FMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.713956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>X_MMX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.715330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>X_MBX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.715358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>X_FTX_MTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.716606</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           DATA_X DATA_y  Test Loss\n",
              "63   X_FTX_MMX_RT   y_RM   0.489877\n",
              "111  X_FMX_MTX_MB   y_MM   0.521904\n",
              "138  X_FMX_MMX_RT   y_RM   0.532896\n",
              "254  X_MMX_RTX_RB   y_RM   0.536594\n",
              "217  X_MTX_RTX_RM   y_MM   0.543176\n",
              "188  X_MTX_MMX_RT   y_RM   0.548079\n",
              "69   X_FTX_MMX_RM   y_RB   0.558069\n",
              "269  X_MBX_RTX_RB   y_RM   0.574858\n",
              "227  X_MTX_RMX_RB   y_MM   0.578296\n",
              "207  X_MTX_MBX_RM   y_MM   0.594384\n",
              "233  X_MMX_MBX_RT   y_RM   0.596040\n",
              "49   X_FTX_MTX_RM   y_RB   0.600521\n",
              "259  X_MMX_RMX_RB   y_RT   0.606003\n",
              "121  X_FMX_MTX_RM   y_MM   0.610794\n",
              "126  X_FMX_MTX_RB   y_MM   0.614823\n",
              "154  X_FMX_MBX_RT   y_RB   0.619312\n",
              "139  X_FMX_MMX_RT   y_RB   0.619584\n",
              "11   X_FTX_FMX_MB   y_MM   0.625223\n",
              "62   X_FTX_MMX_RT   y_MB   0.635859\n",
              "159  X_FMX_MBX_RM   y_RB   0.636488\n",
              "94   X_FTX_RTX_RM   y_RB   0.638143\n",
              "51   X_FTX_MTX_RB   y_MM   0.647248\n",
              "48   X_FTX_MTX_RM   y_RT   0.652497\n",
              "202  X_MTX_MBX_RT   y_MM   0.652841\n",
              "162  X_FMX_MBX_RB   y_MM   0.658776\n",
              "124  X_FMX_MTX_RM   y_RB   0.661589\n",
              "199  X_MTX_MMX_RB   y_RM   0.665308\n",
              "249  X_MMX_RTX_RM   y_RB   0.669585\n",
              "144  X_FMX_MMX_RM   y_RB   0.670826\n",
              "116  X_FMX_MTX_RT   y_MM   0.674463\n",
              "251  X_MMX_RTX_RB   y_FM   0.674920\n",
              "30   X_FTX_MTX_MM   y_FM   0.677395\n",
              "72   X_FTX_MMX_RB   y_MB   0.679434\n",
              "43   X_FTX_MTX_RT   y_RM   0.680506\n",
              "52   X_FTX_MTX_RB   y_MB   0.689428\n",
              "253  X_MMX_RTX_RB   y_MB   0.692434\n",
              "174  X_FMX_RTX_RB   y_RM   0.693418\n",
              "212  X_MTX_MBX_RB   y_MM   0.697811\n",
              "89   X_FTX_MBX_RB   y_RM   0.702582\n",
              "79   X_FTX_MBX_RT   y_RB   0.704277\n",
              "197  X_MTX_MMX_RB   y_MB   0.704810\n",
              "36   X_FTX_MTX_MB   y_MM   0.705717\n",
              "147  X_FMX_MMX_RB   y_MB   0.705901\n",
              "219  X_MTX_RTX_RM   y_RB   0.710260\n",
              "263  X_MBX_RTX_RM   y_MM   0.710613\n",
              "222  X_MTX_RTX_RB   y_MM   0.711074\n",
              "153  X_FMX_MBX_RT   y_RM   0.713956\n",
              "258  X_MMX_RMX_RB   y_MB   0.715330\n",
              "268  X_MBX_RTX_RB   y_MM   0.715358\n",
              "46   X_FTX_MTX_RM   y_MM   0.716606"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RK_3NvelzcyO",
        "outputId": "b51aa01f-f55c-4627-bfaa-3225ee527c19"
      },
      "source": [
        "CombResultsSortedMFgrouped3 = CombResultsSortedMF3.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedMFgroupedsortedMF3 = CombResultsSortedMFgrouped3.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMFgroupedsortedMF3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RM</th>\n",
              "      <td>0.718768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RT</th>\n",
              "      <td>0.734740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RB</th>\n",
              "      <td>0.791319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RT</th>\n",
              "      <td>0.821536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RM</th>\n",
              "      <td>0.862389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MM</th>\n",
              "      <td>0.889042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RT</th>\n",
              "      <td>0.899084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RT</th>\n",
              "      <td>0.901221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RTX_RB</th>\n",
              "      <td>0.913722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RB</th>\n",
              "      <td>0.931974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RTX_RM</th>\n",
              "      <td>0.938872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RTX_RB</th>\n",
              "      <td>0.944863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RM</th>\n",
              "      <td>0.954180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RB</th>\n",
              "      <td>0.956485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RT</th>\n",
              "      <td>0.957938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RMX_RB</th>\n",
              "      <td>0.961566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RT</th>\n",
              "      <td>0.968683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RM</th>\n",
              "      <td>0.986824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RM</th>\n",
              "      <td>0.992227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MB</th>\n",
              "      <td>0.999681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RB</th>\n",
              "      <td>1.001570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RMX_RB</th>\n",
              "      <td>1.008961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RT</th>\n",
              "      <td>1.012800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RM</th>\n",
              "      <td>1.013505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RTX_RB</th>\n",
              "      <td>1.015337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RB</th>\n",
              "      <td>1.015875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RT</th>\n",
              "      <td>1.022207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RM</th>\n",
              "      <td>1.031903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RT</th>\n",
              "      <td>1.037106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RB</th>\n",
              "      <td>1.052353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RTX_RM</th>\n",
              "      <td>1.053059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RB</th>\n",
              "      <td>1.057057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RM</th>\n",
              "      <td>1.057605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RM</th>\n",
              "      <td>1.062647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RTX_RM</th>\n",
              "      <td>1.065261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RTX_RM</th>\n",
              "      <td>1.067196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MB</th>\n",
              "      <td>1.067619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MB</th>\n",
              "      <td>1.072218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RT</th>\n",
              "      <td>1.076371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RB</th>\n",
              "      <td>1.077109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MT</th>\n",
              "      <td>1.079139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RTX_RB</th>\n",
              "      <td>1.080799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RMX_RB</th>\n",
              "      <td>1.081990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RM</th>\n",
              "      <td>1.094517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RMX_RB</th>\n",
              "      <td>1.098432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RMX_RB</th>\n",
              "      <td>1.102857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MM</th>\n",
              "      <td>1.117189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MB</th>\n",
              "      <td>1.117418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RB</th>\n",
              "      <td>1.125754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RB</th>\n",
              "      <td>1.136785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RTX_RM</th>\n",
              "      <td>1.160721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RTX_RB</th>\n",
              "      <td>1.196645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MM</th>\n",
              "      <td>1.220148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RTX_RMX_RB</th>\n",
              "      <td>1.253397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MB</th>\n",
              "      <td>1.271361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MB</th>\n",
              "      <td>1.316267</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Test Loss\n",
              "DATA_X                 \n",
              "X_FTX_MTX_RM   0.718768\n",
              "X_FTX_MMX_RT   0.734740\n",
              "X_FTX_MTX_RB   0.791319\n",
              "X_FTX_MTX_RT   0.821536\n",
              "X_FTX_MMX_RM   0.862389\n",
              "X_FTX_MTX_MM   0.889042\n",
              "X_FMX_MMX_RT   0.899084\n",
              "X_FMX_MTX_RT   0.901221\n",
              "X_MMX_RTX_RB   0.913722\n",
              "X_MTX_MMX_RB   0.931974\n",
              "X_MTX_RTX_RM   0.938872\n",
              "X_MTX_RTX_RB   0.944863\n",
              "X_FMX_MTX_RM   0.954180\n",
              "X_FTX_MMX_RB   0.956485\n",
              "X_FTX_MBX_RT   0.957938\n",
              "X_MTX_RMX_RB   0.961566\n",
              "X_FMX_MBX_RT   0.968683\n",
              "X_FTX_FMX_RM   0.986824\n",
              "X_FTX_MBX_RM   0.992227\n",
              "X_FTX_MTX_MB   0.999681\n",
              "X_FTX_FMX_RB   1.001570\n",
              "X_MMX_RMX_RB   1.008961\n",
              "X_MTX_MMX_RT   1.012800\n",
              "X_FMX_MMX_RM   1.013505\n",
              "X_MBX_RTX_RB   1.015337\n",
              "X_FMX_MTX_RB   1.015875\n",
              "X_MTX_MBX_RT   1.022207\n",
              "X_MTX_MBX_RM   1.031903\n",
              "X_FTX_FMX_RT   1.037106\n",
              "X_MTX_MBX_RB   1.052353\n",
              "X_FTX_RTX_RM   1.053059\n",
              "X_FTX_MBX_RB   1.057057\n",
              "X_FMX_MBX_RM   1.057605\n",
              "X_MTX_MMX_RM   1.062647\n",
              "X_MMX_RTX_RM   1.065261\n",
              "X_FMX_RTX_RM   1.067196\n",
              "X_FTX_MMX_MB   1.067619\n",
              "X_FTX_FMX_MB   1.072218\n",
              "X_MMX_MBX_RT   1.076371\n",
              "X_FMX_MMX_RB   1.077109\n",
              "X_FTX_FMX_MT   1.079139\n",
              "X_FTX_RTX_RB   1.080799\n",
              "X_FTX_RMX_RB   1.081990\n",
              "X_MMX_MBX_RM   1.094517\n",
              "X_MBX_RMX_RB   1.098432\n",
              "X_FMX_RMX_RB   1.102857\n",
              "X_FTX_FMX_MM   1.117189\n",
              "X_FMX_MTX_MB   1.117418\n",
              "X_FMX_MBX_RB   1.125754\n",
              "X_MMX_MBX_RB   1.136785\n",
              "X_MBX_RTX_RM   1.160721\n",
              "X_FMX_RTX_RB   1.196645\n",
              "X_FMX_MTX_MM   1.220148\n",
              "X_RTX_RMX_RB   1.253397\n",
              "X_FMX_MMX_MB   1.271361\n",
              "X_MTX_MMX_MB   1.316267"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "NKeM-y6RRiZt",
        "outputId": "0093e1a6-3e5e-4435-f0fd-48b0d50a31bd"
      },
      "source": [
        "CombResultsSortedMFgroupedsortedMF3.to_csv('CombResultsSortedMFgroupedsortedMF3.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedMFgroupedsortedMF3.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d44d8106-4db0-42f9-82fa-05ae35b21ade\", \"CombResultsSortedMFgroupedsortedMF3.csv\", 1794)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "YOwzdxFUYg50",
        "outputId": "bba9fdff-2d5d-4f11-98f2-85f54fc4f6c1"
      },
      "source": [
        "CombResultsSortedMF3.to_csv('CombResultsSortedMF3.csv')\n",
        "files.download(\"CombResultsSortedMF3.csv\")\n",
        "\n",
        "fig = px.box(CombResultsSortedMF3, x=\"DATA_X\", y=\"Test Loss\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_7e875604-eef6-437a-9b16-95840f51acb3\", \"CombResultsSortedMF3.csv\", 11340)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4fa79aa9-7d00-4e91-bae0-08276c42ea1d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4fa79aa9-7d00-4e91-bae0-08276c42ea1d\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4fa79aa9-7d00-4e91-bae0-08276c42ea1d',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_FTX_MMX_RT\", \"X_FMX_MTX_MB\", \"X_FMX_MMX_RT\", \"X_MMX_RTX_RB\", \"X_MTX_RTX_RM\", \"X_MTX_MMX_RT\", \"X_FTX_MMX_RM\", \"X_MBX_RTX_RB\", \"X_MTX_RMX_RB\", \"X_MTX_MBX_RM\", \"X_MMX_MBX_RT\", \"X_FTX_MTX_RM\", \"X_MMX_RMX_RB\", \"X_FMX_MTX_RM\", \"X_FMX_MTX_RB\", \"X_FMX_MBX_RT\", \"X_FMX_MMX_RT\", \"X_FTX_FMX_MB\", \"X_FTX_MMX_RT\", \"X_FMX_MBX_RM\", \"X_FTX_RTX_RM\", \"X_FTX_MTX_RB\", \"X_FTX_MTX_RM\", \"X_MTX_MBX_RT\", \"X_FMX_MBX_RB\", \"X_FMX_MTX_RM\", \"X_MTX_MMX_RB\", \"X_MMX_RTX_RM\", \"X_FMX_MMX_RM\", \"X_FMX_MTX_RT\", \"X_MMX_RTX_RB\", \"X_FTX_MTX_MM\", \"X_FTX_MMX_RB\", \"X_FTX_MTX_RT\", \"X_FTX_MTX_RB\", \"X_MMX_RTX_RB\", \"X_FMX_RTX_RB\", \"X_MTX_MBX_RB\", \"X_FTX_MBX_RB\", \"X_FTX_MBX_RT\", \"X_MTX_MMX_RB\", \"X_FTX_MTX_MB\", \"X_FMX_MMX_RB\", \"X_MTX_RTX_RM\", \"X_MBX_RTX_RM\", \"X_MTX_RTX_RB\", \"X_FMX_MBX_RT\", \"X_MMX_RMX_RB\", \"X_MBX_RTX_RB\", \"X_FTX_MTX_RM\", \"X_MMX_MBX_RM\", \"X_MBX_RMX_RB\", \"X_FMX_MTX_RT\", \"X_FTX_MBX_RM\", \"X_FTX_MTX_RT\", \"X_FTX_MBX_RM\", \"X_FTX_MTX_RB\", \"X_FMX_MBX_RT\", \"X_FTX_FMX_RT\", \"X_MMX_RTX_RM\", \"X_FMX_MMX_RM\", \"X_MTX_RTX_RB\", \"X_RTX_RMX_RB\", \"X_FTX_FMX_RT\", \"X_FTX_MTX_MM\", \"X_MTX_RMX_RB\", \"X_MTX_RTX_RM\", \"X_FTX_FMX_MB\", \"X_FTX_RTX_RB\", \"X_FTX_MBX_RT\", \"X_FMX_MBX_RM\", \"X_MTX_MMX_RM\", \"X_MTX_MMX_RT\", \"X_MTX_MBX_RT\", \"X_FTX_MTX_RM\", \"X_FTX_MMX_MB\", \"X_FTX_MBX_RT\", \"X_FTX_MMX_RM\", \"X_FMX_RTX_RM\", \"X_FMX_MMX_RM\", \"X_FTX_MMX_RB\", \"X_MTX_MMX_RT\", \"X_MTX_MBX_RB\", \"X_FTX_MTX_RT\", \"X_FTX_FMX_RM\", \"X_MTX_MMX_RB\", \"X_MTX_RTX_RB\", \"X_FTX_MTX_MM\", \"X_MBX_RTX_RM\", \"X_FTX_FMX_MM\", \"X_FMX_MTX_RB\", \"X_FMX_RMX_RB\", \"X_MTX_MBX_RT\", \"X_MMX_MBX_RB\", \"X_MTX_MBX_RM\", \"X_FMX_RMX_RB\", \"X_MTX_MMX_RM\", \"X_FMX_RTX_RM\", \"X_FTX_MMX_RT\", \"X_FMX_MTX_RT\", \"X_MMX_MBX_RM\", \"X_FMX_MTX_RT\", \"X_MTX_MMX_MB\", \"X_MTX_MBX_RM\", \"X_FTX_MTX_RM\", \"X_FTX_FMX_RB\", \"X_MMX_MBX_RB\", \"X_FTX_MTX_RB\", \"X_FTX_MTX_MB\", \"X_FTX_FMX_RM\", \"X_FTX_MMX_RB\", \"X_FTX_MMX_RT\", \"X_FTX_MMX_RT\", \"X_FTX_MMX_MB\", \"X_MTX_RMX_RB\", \"X_FTX_FMX_RM\", \"X_FMX_MBX_RM\", \"X_MTX_MMX_RM\", \"X_FTX_RMX_RB\", \"X_MTX_MBX_RB\", \"X_MBX_RMX_RB\", \"X_FTX_FMX_MT\", \"X_FTX_MTX_RT\", \"X_FMX_MTX_RM\", \"X_FMX_MMX_RB\", \"X_MTX_MMX_RT\", \"X_FTX_MTX_MB\", \"X_FMX_MTX_RM\", \"X_MTX_MMX_RM\", \"X_FTX_MBX_RB\", \"X_FTX_RMX_RB\", \"X_FMX_MMX_RT\", \"X_FTX_MMX_RM\", \"X_FTX_FMX_MT\", \"X_FTX_MTX_MM\", \"X_FMX_MTX_MM\", \"X_MTX_RTX_RB\", \"X_FMX_MMX_RT\", \"X_MTX_MMX_RB\", \"X_FTX_RTX_RB\", \"X_FTX_FMX_RB\", \"X_FMX_MBX_RB\", \"X_MMX_MBX_RT\", \"X_FMX_RTX_RM\", \"X_MMX_MBX_RT\", \"X_MBX_RMX_RB\", \"X_MTX_RMX_RB\", \"X_FTX_FMX_RM\", \"X_FTX_FMX_MM\", \"X_MMX_RTX_RM\", \"X_RTX_RMX_RB\", \"X_MBX_RTX_RB\", \"X_FTX_FMX_RB\", \"X_FTX_FMX_RT\", \"X_FTX_MMX_RM\", \"X_MMX_MBX_RT\", \"X_MTX_MBX_RB\", \"X_MMX_MBX_RM\", \"X_FTX_RTX_RM\", \"X_FMX_RTX_RB\", \"X_FMX_MTX_RB\", \"X_FMX_MTX_MM\", \"X_FTX_RMX_RB\", \"X_FMX_RMX_RB\", \"X_FMX_RTX_RB\", \"X_FTX_MTX_RT\", \"X_FTX_MBX_RM\", \"X_FTX_RMX_RB\", \"X_FMX_MMX_MB\", \"X_MMX_RTX_RB\", \"X_FTX_RTX_RM\", \"X_MTX_MMX_MB\", \"X_FTX_MTX_RB\", \"X_FMX_MMX_RM\", \"X_MMX_MBX_RB\", \"X_FTX_FMX_MT\", \"X_FMX_MTX_MM\", \"X_MTX_MBX_RM\", \"X_FTX_FMX_RB\", \"X_FTX_RTX_RM\", \"X_FTX_MMX_MB\", \"X_MMX_RMX_RB\", \"X_MMX_MBX_RM\", \"X_FTX_MMX_RM\", \"X_MTX_MBX_RT\", \"X_FMX_MTX_MB\", \"X_RTX_RMX_RB\", \"X_FMX_MBX_RB\", \"X_MMX_RTX_RM\", \"X_MMX_RMX_RB\", \"X_FTX_MBX_RM\", \"X_FMX_MTX_MB\", \"X_FTX_FMX_MT\", \"X_FTX_MMX_MB\", \"X_FMX_MMX_MB\", \"X_FMX_MMX_MB\", \"X_FMX_MBX_RM\", \"X_FTX_RTX_RB\", \"X_FTX_MBX_RB\", \"X_MBX_RTX_RB\", \"X_FTX_MMX_RB\", \"X_FTX_MBX_RT\", \"X_MTX_MMX_MB\", \"X_FTX_FMX_MB\", \"X_MTX_RTX_RM\", \"X_FTX_MTX_MB\", \"X_FMX_MMX_RB\", \"X_FMX_MTX_RB\", \"X_FTX_RTX_RB\", \"X_FTX_MBX_RB\", \"X_FTX_FMX_RB\", \"X_MBX_RTX_RM\", \"X_FTX_FMX_MM\", \"X_FTX_FMX_MM\", \"X_FTX_FMX_RT\", \"X_MMX_MBX_RB\", \"X_FMX_MBX_RT\", \"X_FMX_MMX_RB\", \"X_FTX_FMX_MB\", \"X_MBX_RTX_RM\", \"X_FTX_MMX_RB\", \"X_FMX_MBX_RB\", \"X_MBX_RMX_RB\", \"X_FMX_MTX_MB\", \"X_FTX_MTX_MM\", \"X_FMX_RTX_RB\", \"X_FTX_MBX_RB\", \"X_FMX_RMX_RB\", \"X_FMX_RTX_RM\", \"X_FTX_RTX_RB\", \"X_FTX_MBX_RT\", \"X_FTX_MBX_RM\", \"X_FMX_MMX_RB\", \"X_FTX_MTX_MB\", \"X_FTX_FMX_MM\", \"X_FTX_FMX_MT\", \"X_FMX_RTX_RM\", \"X_FMX_MTX_RT\", \"X_FMX_MTX_MM\", \"X_FTX_FMX_RM\", \"X_FMX_MTX_RB\", \"X_FMX_MMX_MB\", \"X_FTX_MMX_MB\", \"X_FTX_FMX_RT\", \"X_MTX_RTX_RM\", \"X_FMX_MMX_RT\", \"X_FTX_RTX_RM\", \"X_FMX_RMX_RB\", \"X_FMX_MBX_RT\", \"X_MMX_RMX_RB\", \"X_FTX_FMX_MB\", \"X_MTX_MMX_RB\", \"X_MTX_MMX_MB\", \"X_FMX_MMX_MB\", \"X_FMX_MTX_MB\", \"X_MTX_RTX_RB\", \"X_FTX_RMX_RB\", \"X_RTX_RMX_RB\", \"X_MMX_RTX_RB\", \"X_MBX_RMX_RB\", \"X_FMX_MBX_RB\", \"X_MBX_RTX_RB\", \"X_MTX_RMX_RB\", \"X_FMX_MTX_MM\", \"X_FMX_MTX_RM\", \"X_MMX_MBX_RB\", \"X_MTX_MBX_RT\", \"X_FMX_MMX_RM\", \"X_MMX_MBX_RM\", \"X_MBX_RTX_RM\", \"X_RTX_RMX_RB\", \"X_MTX_MBX_RM\", \"X_MMX_RTX_RM\", \"X_FMX_MBX_RM\", \"X_MMX_MBX_RT\", \"X_MTX_MBX_RB\", \"X_MTX_MMX_RM\", \"X_FMX_RTX_RB\", \"X_MTX_MMX_MB\", \"X_MTX_MMX_RT\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.48987704515457153, 0.5219042509794235, 0.5328955680131913, 0.536593559384346, 0.5431760519742965, 0.5480785936117172, 0.5580693989992142, 0.5748584300279618, 0.5782956898212432, 0.5943840026855469, 0.5960397303104401, 0.6005210638046264, 0.6060028195381164, 0.6107938617467881, 0.6148225545883179, 0.6193124920129776, 0.6195835143327713, 0.6252229958772659, 0.6358593374490737, 0.6364879667758941, 0.6381431221961975, 0.6472476184368133, 0.6524971723556519, 0.6528412789106369, 0.658775681257248, 0.6615889370441437, 0.6653080195188522, 0.6695847332477569, 0.6708262920379638, 0.6744631499052047, 0.6749199748039245, 0.6773953020572663, 0.6794335961341857, 0.6805056035518646, 0.6894276738166809, 0.6924344390630722, 0.6934183895587921, 0.697810709476471, 0.7025822341442108, 0.7042771995067596, 0.7048095881938934, 0.7057172656059265, 0.705900713801384, 0.7102600932121277, 0.7106129884719848, 0.7110741764307023, 0.7139562517404556, 0.7153297334909439, 0.7153578221797943, 0.7166061908006668, 0.7276834487915039, 0.728352153301239, 0.7296503037214279, 0.7300049632787704, 0.7327842772006988, 0.7329189360141755, 0.7337491273880005, 0.7344087481498718, 0.7352758914232254, 0.7362071454524994, 0.7367230862379074, 0.7388627588748932, 0.7431118667125702, 0.7492977678775787, 0.7492991149425506, 0.7497205555438995, 0.7650764882564545, 0.7658763647079467, 0.7671345293521881, 0.7692613661289215, 0.7715728044509887, 0.7748239874839783, 0.778819328546524, 0.780651432275772, 0.7833171129226685, 0.7846168398857116, 0.7856266736984253, 0.7860630363225937, 0.7886918157339096, 0.7899444013833999, 0.7948333323001862, 0.7952092111110687, 0.7954454958438874, 0.7962348163127899, 0.8001748353242875, 0.8016478955745697, 0.8033157527446747, 0.8038605093955994, 0.8055537283420563, 0.80575612783432, 0.8104053676128388, 0.812739509344101, 0.820217627286911, 0.8213133752346039, 0.8216029524803161, 0.8275168597698211, 0.8342605173587799, 0.8355158269405365, 0.8375940293073654, 0.8389055043458938, 0.8390851497650147, 0.8392415225505829, 0.8394899427890777, 0.8395727336406708, 0.8408981144428254, 0.8428997457027435, 0.843631488084793, 0.8438287019729614, 0.8510888576507568, 0.852572226524353, 0.8531022071838379, 0.8547629058361054, 0.8556057989597321, 0.8596225321292877, 0.861609423160553, 0.8671304523944855, 0.8698420941829681, 0.8702048122882843, 0.8702571988105774, 0.8739601373672485, 0.8761255323886872, 0.8809112727642059, 0.8823894083499908, 0.8860688805580139, 0.8907848596572876, 0.8910372853279114, 0.8943145573139191, 0.8944894850254059, 0.8978036880493164, 0.899289870262146, 0.8995300233364105, 0.9029195725917816, 0.9033761858940125, 0.9094315767288208, 0.9115018427371979, 0.9131912231445313, 0.9141891539096832, 0.9175775408744812, 0.9346120893955231, 0.9371399283409119, 0.9385623633861542, 0.9394492506980896, 0.9434275209903717, 0.9496770381927491, 0.9500215947628021, 0.9549917161464692, 0.955156335234642, 0.9599252939224243, 0.9642712056636811, 0.96457639336586, 0.9699355900287628, 0.9728973209857941, 0.9744688153266907, 0.9799071907997131, 0.9868491172790528, 0.9931180357933045, 0.993206799030304, 1.0020512521266938, 1.0045916616916657, 1.0062951624393464, 1.009265238046646, 1.009316325187683, 1.0098844766616821, 1.010002326965332, 1.0149496376514435, 1.015767228603363, 1.020096105337143, 1.0228426992893218, 1.0256270945072175, 1.0301107227802277, 1.031051117181778, 1.0395734369754792, 1.042342060804367, 1.0515564858913422, 1.0521787762641908, 1.0539177179336547, 1.0569895088672638, 1.0573965072631837, 1.0606616377830504, 1.0626483619213105, 1.0657733380794525, 1.0672823071479798, 1.0697776317596435, 1.0775883615016937, 1.0857521831989287, 1.0861854374408721, 1.0889596223831177, 1.0946678638458252, 1.1053870737552642, 1.1149709165096282, 1.1181313931941985, 1.126629936695099, 1.1367985427379608, 1.139777421951294, 1.1405730545520782, 1.1492115139961243, 1.1517080962657928, 1.1636250078678132, 1.1681198358535767, 1.1681439459323884, 1.1764793276786805, 1.1766372561454772, 1.1770451605319976, 1.1780955791473389, 1.1789022147655488, 1.1808549404144286, 1.1813489496707916, 1.1861085891723633, 1.186560755968094, 1.186908060312271, 1.191258692741394, 1.198355758190155, 1.2007325351238252, 1.2176247000694276, 1.2251070320606232, 1.2296674013137818, 1.2351956367492676, 1.2454916477203368, 1.2501118063926697, 1.252750027179718, 1.2785745501518249, 1.2954607725143432, 1.297171413898468, 1.2973100066184997, 1.3031520605087281, 1.309616756439209, 1.3283859729766845, 1.331775426864624, 1.3405157208442688, 1.349532914161682, 1.353888201713562, 1.359982180595398, 1.3620210886001587, 1.3664314508438111, 1.3975602030754088, 1.4146369218826294, 1.4215776205062867, 1.4238465666770934, 1.450066590309143, 1.4543172240257263, 1.458773136138916, 1.4867220878601075, 1.488307273387909, 1.4959411025047302, 1.4969469547271728, 1.522441565990448, 1.5288603901863098, 1.5322506427764893, 1.54054137468338, 1.5412175893783568, 1.5417815685272216, 1.5534899711608887, 1.5536993503570558, 1.5546702861785888, 1.555060911178589, 1.5568724155426026, 1.6074357748031616, 1.6210727572441102, 1.6345489859580993, 1.6355172514915466, 1.6404146552085876, 1.6454256892204284, 1.663049054145813, 1.6711787700653076, 1.7179574489593505, 1.7371329426765443, 1.771574115753174, 1.81847425699234, 1.8339879631996154, 1.836331021785736, 1.8439070463180542, 1.8465567111968995, 1.850548470020294, 1.858415699005127, 1.8992505192756652, 1.901343083381653, 1.9361400723457336, 1.9589437246322632, 1.9715250372886657, 2.05085608959198], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4fa79aa9-7d00-4e91-bae0-08276c42ea1d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N87IAQSzcAr"
      },
      "source": [
        "# define baseline model 4\n",
        "# create model\n",
        "n_features = 1\n",
        "model4 = Sequential()\n",
        "model4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(28, n_features)))\n",
        "model4.add(MaxPooling1D(pool_size=2))\n",
        "model4.add(Flatten())\n",
        "model4.add(Dense(50, activation='relu'))\n",
        "model4.add(Dense(1))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "model4.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "#model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hl8XS_eMJn_"
      },
      "source": [
        "n_features = 1\n",
        "def datageneratorMF4(X_in1, X_in2, X_in3, X_in4, Y_in):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  X_in = np.concatenate((X_in1, X_in2, X_in3, X_in4), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "\n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], n_features))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], n_features))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], n_features))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIB_dyaoMJoH"
      },
      "source": [
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMS(X_MT, X_MM, y_RM)\n",
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(X_MT, y_RM)\n",
        "#X_train_Input[0:5]\n",
        "#X_train_Input.shape\n",
        "#y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go6TpsmiMJoH"
      },
      "source": [
        "def evaldataMF4(X_in1, X_in2, X_in3, X_in4, Y_in, traindata1, traindata2, traindata3, traindata4, testdata):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMF4(X_in1, X_in2, X_in3, X_in4, Y_in)\n",
        "  \n",
        "  history = model4.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model4.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model4.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # PLOTS LOSS VS EPOCH\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  # print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  # print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # PLOTS Y ORIGINAL VS PREDICTED\n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata1)+ str(traindata2)  + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ8bGnlGMJoL",
        "outputId": "2eb7aaeb-8226-4ab0-ab86-7fd5d58cada0"
      },
      "source": [
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "\n",
        "model4.save_weights('model4.h5')\n",
        "\n",
        "my_dictMF4 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), 4):\n",
        "  kX1, kX2, kX3, kX4 = combo[0][0], combo[1][0], combo[2][0], combo[3][0]\n",
        "  vX1, vX2, vX3, vX4 = combo[0][1], combo[1][1], combo[2][1], combo[3][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:] or ky[-2:] == kX4[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, kx4 = {kX4}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF4 = evaldataMF4(vX1, vX2, vX3, vX4, vy, kX1, kX2, kX3, kX4, ky)\n",
        "      TestLossTotal = resultsMF4[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF4[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF4[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}_{kX2}_{kX3}_{kX4} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF4[\"DATA_X\"].append(kX1 + kX2 + kX3 + kX4)\n",
        "    my_dictMF4[\"DATA_y\"].append(ky)\n",
        "    my_dictMF4[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model4.load_weights('model4.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7831853449344635, and Avg Validation Loss (mae) is 0.9164369761943817\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9031723260879516\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.38702290058136, and Avg Validation Loss (mae) is 1.5308344960212708\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5055761694908143\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9876692235469818, and Avg Validation Loss (mae) is 1.0770114064216614\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0846634745597838\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8310053646564484, and Avg Validation Loss (mae) is 0.9681647837162017\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9754471600055694\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6168127000331879, and Avg Validation Loss (mae) is 0.6069714486598968\n",
            "After 10 runs; Avg Test Loss (mae) is 0.595270323753357\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4413798451423645, and Avg Validation Loss (mae) is 1.5174983382225036\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5204498291015625\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.196589469909668, and Avg Validation Loss (mae) is 1.348422759771347\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3508085787296296\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8838198065757752, and Avg Validation Loss (mae) is 0.8098913967609406\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8200599431991578\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6919079840183258, and Avg Validation Loss (mae) is 0.6474847555160522\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6510933667421341\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7673289716243744, and Avg Validation Loss (mae) is 0.7918132066726684\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7946194410324097\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.667582780122757, and Avg Validation Loss (mae) is 0.7266179144382476\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7134125351905822\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.611797672510147, and Avg Validation Loss (mae) is 0.7324481576681137\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7474124372005463\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4998683363199234, and Avg Validation Loss (mae) is 0.5957714438438415\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5855235636234284\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7944178700447082, and Avg Validation Loss (mae) is 0.8410403966903687\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8306457042694092\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8195726275444031, and Avg Validation Loss (mae) is 0.9358147442340851\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9238629639148712\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6499260008335114, and Avg Validation Loss (mae) is 0.7836663275957108\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7937292993068695\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6454103708267211, and Avg Validation Loss (mae) is 0.6683733940124512\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6719425618648529\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.831845486164093, and Avg Validation Loss (mae) is 0.8335695624351501\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8411434173583985\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0759179294109344, and Avg Validation Loss (mae) is 1.182902890443802\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1854198276996613\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8381096065044403, and Avg Validation Loss (mae) is 0.9337519407272339\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9251620173454285\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0680467307567596, and Avg Validation Loss (mae) is 1.0751705586910247\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0789916396141053\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4550275444984435, and Avg Validation Loss (mae) is 1.593346083164215\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5684947848320008\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0932889342308045, and Avg Validation Loss (mae) is 1.1578657925128937\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1353368520736695\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8485277652740478, and Avg Validation Loss (mae) is 0.9230437755584717\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9291528463363647\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9949109315872192, and Avg Validation Loss (mae) is 0.9842381119728089\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9971264243125916\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6932495296001434, and Avg Validation Loss (mae) is 0.6790214598178863\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6811392217874527\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5098837703466416, and Avg Validation Loss (mae) is 0.6466339230537415\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6445459663867951\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6806365251541138, and Avg Validation Loss (mae) is 0.9024427354335784\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8946038782596588\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9777173161506653, and Avg Validation Loss (mae) is 1.1350460767745971\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1177762508392335\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7459325015544891, and Avg Validation Loss (mae) is 0.758693066239357\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7598480582237244\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8095575332641601, and Avg Validation Loss (mae) is 0.8607370138168335\n",
            "After 10 runs; Avg Test Loss (mae) is 0.882144284248352\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6088659226894378, and Avg Validation Loss (mae) is 0.6482570230960846\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6638850212097168\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0913521885871886, and Avg Validation Loss (mae) is 1.0888320565223695\n",
            "After 10 runs; Avg Test Loss (mae) is 1.071741485595703\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6817666888237, and Avg Validation Loss (mae) is 0.6658777981996536\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6494135320186615\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0565584003925323, and Avg Validation Loss (mae) is 1.2766669034957885\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2695529222488404\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7890848338603973, and Avg Validation Loss (mae) is 0.8834492266178131\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8716757833957672\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0260052144527436, and Avg Validation Loss (mae) is 1.0948271751403809\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0889474987983703\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6438338160514832, and Avg Validation Loss (mae) is 0.748814481496811\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7411780506372452\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6296377778053284, and Avg Validation Loss (mae) is 0.6658883422613144\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6657209485769272\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6466042280197144, and Avg Validation Loss (mae) is 0.6807986557483673\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6830884367227554\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2296449482440948, and Avg Validation Loss (mae) is 1.3024146020412446\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2538671314716339\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.58336923122406, and Avg Validation Loss (mae) is 0.745818418264389\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7605031251907348\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8733407914638519, and Avg Validation Loss (mae) is 0.8229759633541107\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8455284893512726\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6280328452587127, and Avg Validation Loss (mae) is 0.5823958933353424\n",
            "After 10 runs; Avg Test Loss (mae) is 0.578582513332367\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.151823627948761, and Avg Validation Loss (mae) is 1.0985207617282868\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0829204857349395\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6315036386251449, and Avg Validation Loss (mae) is 0.5947866827249527\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6000216245651245\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.077216774225235, and Avg Validation Loss (mae) is 1.1105985283851623\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1274255394935608\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8732975840568542, and Avg Validation Loss (mae) is 0.9846624553203582\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9977567493915558\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4228362321853638, and Avg Validation Loss (mae) is 1.4914901971817016\n",
            "After 10 runs; Avg Test Loss (mae) is 1.522480857372284\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7649365901947022, and Avg Validation Loss (mae) is 0.9201690196990967\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9206372797489166\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0200020730495454, and Avg Validation Loss (mae) is 1.0313408732414246\n",
            "After 10 runs; Avg Test Loss (mae) is 1.043584442138672\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6088625192642212, and Avg Validation Loss (mae) is 0.6053051859140396\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5989613682031631\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3252463698387147, and Avg Validation Loss (mae) is 1.2767002701759338\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3188342452049255\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8517445087432861, and Avg Validation Loss (mae) is 0.9787668824195862\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0164918541908263\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.010993778705597, and Avg Validation Loss (mae) is 1.0793147206306457\n",
            "After 10 runs; Avg Test Loss (mae) is 1.072948509454727\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6179248452186584, and Avg Validation Loss (mae) is 0.5664612025022506\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5625763773918152\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3933627963066102, and Avg Validation Loss (mae) is 1.461121106147766\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4853735446929932\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7947984218597413, and Avg Validation Loss (mae) is 0.7662319898605346\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7515765190124511\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9798887372016907, and Avg Validation Loss (mae) is 1.0907809734344482\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0794279754161835\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8522128105163574, and Avg Validation Loss (mae) is 0.8437813103199006\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8376691222190857\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8158748686313629, and Avg Validation Loss (mae) is 0.8201408386230469\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8173675894737243\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3246928453445435, and Avg Validation Loss (mae) is 1.242493212223053\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2503778457641601\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9557942807674408, and Avg Validation Loss (mae) is 0.9920692324638367\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0210439562797546\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8750814378261567, and Avg Validation Loss (mae) is 0.8392983317375183\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8421301782131195\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8215044677257538, and Avg Validation Loss (mae) is 0.8147889196872711\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8001988053321838\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7550572752952576, and Avg Validation Loss (mae) is 0.7446354150772094\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7552183926105499\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5190267533063888, and Avg Validation Loss (mae) is 0.5817939549684524\n",
            "After 10 runs; Avg Test Loss (mae) is 0.581775176525116\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6449737429618836, and Avg Validation Loss (mae) is 0.7307767838239669\n",
            "After 10 runs; Avg Test Loss (mae) is 0.710794597864151\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7748767852783203, and Avg Validation Loss (mae) is 0.8437467634677887\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8503449857234955\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7546846568584442, and Avg Validation Loss (mae) is 0.7049326241016388\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7147448539733887\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7211308360099793, and Avg Validation Loss (mae) is 0.8295082807540893\n",
            "After 10 runs; Avg Test Loss (mae) is 0.832785227894783\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6780922710895538, and Avg Validation Loss (mae) is 0.8484433114528656\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8495836228132247\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6926963031291962, and Avg Validation Loss (mae) is 0.7509936392307281\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7258809506893158\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7183027982711792, and Avg Validation Loss (mae) is 0.8810030281543731\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9066274344921113\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.955904883146286, and Avg Validation Loss (mae) is 0.951376450061798\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9491433739662171\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7130731999874115, and Avg Validation Loss (mae) is 0.8211975693702698\n",
            "After 10 runs; Avg Test Loss (mae) is 0.812053307890892\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.868863332271576, and Avg Validation Loss (mae) is 0.9078481793403625\n",
            "After 10 runs; Avg Test Loss (mae) is 0.914964097738266\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7319912672042846, and Avg Validation Loss (mae) is 0.9078787267208099\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8878775149583816\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7297814577817917, and Avg Validation Loss (mae) is 0.6462148517370224\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6399941504001617\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7673397421836853, and Avg Validation Loss (mae) is 0.6370043218135834\n",
            "After 10 runs; Avg Test Loss (mae) is 0.640024608373642\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.845091599225998, and Avg Validation Loss (mae) is 0.7856126666069031\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7904233276844025\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6206210911273956, and Avg Validation Loss (mae) is 0.6558024436235428\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6614878088235855\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8249273240566254, and Avg Validation Loss (mae) is 0.7133081555366516\n",
            "After 10 runs; Avg Test Loss (mae) is 0.714645916223526\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7027436852455139, and Avg Validation Loss (mae) is 0.6544778019189834\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6577032715082168\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7564655423164368, and Avg Validation Loss (mae) is 0.8044197082519531\n",
            "After 10 runs; Avg Test Loss (mae) is 0.815443480014801\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.689711457490921, and Avg Validation Loss (mae) is 0.8005781143903732\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8005081653594971\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9932158410549163, and Avg Validation Loss (mae) is 0.883081179857254\n",
            "After 10 runs; Avg Test Loss (mae) is 0.89574134349823\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7854762732982635, and Avg Validation Loss (mae) is 0.8216458261013031\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8082094073295594\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8805267989635468, and Avg Validation Loss (mae) is 0.930072408914566\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9345882356166839\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5483444303274154, and Avg Validation Loss (mae) is 0.5530385673046112\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5754513621330262\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7940090000629425, and Avg Validation Loss (mae) is 0.7460655927658081\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7553229033946991\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6667469680309296, and Avg Validation Loss (mae) is 0.6812409996986389\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7027208030223846\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8105071127414704, and Avg Validation Loss (mae) is 0.763477212190628\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7801692128181458\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5966813027858734, and Avg Validation Loss (mae) is 0.5815673738718032\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5666598826646805\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7163972198963166, and Avg Validation Loss (mae) is 0.7692009687423706\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7621240615844727\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6266726791858673, and Avg Validation Loss (mae) is 0.586492532491684\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5906214416027069\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8031620681285858, and Avg Validation Loss (mae) is 0.7782826066017151\n",
            "After 10 runs; Avg Test Loss (mae) is 0.799152547121048\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6287244588136673, and Avg Validation Loss (mae) is 0.6766030937433243\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6772949516773223\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7420036613941192, and Avg Validation Loss (mae) is 0.7998933017253875\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7971456289291382\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6846847295761108, and Avg Validation Loss (mae) is 0.8120036154985428\n",
            "After 10 runs; Avg Test Loss (mae) is 0.820230457186699\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8762141227722168, and Avg Validation Loss (mae) is 0.8571432113647461\n",
            "After 10 runs; Avg Test Loss (mae) is 0.883879441022873\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9616630494594574, and Avg Validation Loss (mae) is 1.1580255508422852\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1445922374725341\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6139179527759552, and Avg Validation Loss (mae) is 0.7458913713693619\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7557681381702424\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6665457904338836, and Avg Validation Loss (mae) is 0.7621348410844803\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7579279601573944\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8025192737579345, and Avg Validation Loss (mae) is 0.8243897378444671\n",
            "After 10 runs; Avg Test Loss (mae) is 0.829956340789795\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0558026611804963, and Avg Validation Loss (mae) is 1.0980590283870697\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0887119591236114\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7824012875556946, and Avg Validation Loss (mae) is 0.7632449418306351\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7351144015789032\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6749669909477234, and Avg Validation Loss (mae) is 0.681650772690773\n",
            "After 10 runs; Avg Test Loss (mae) is 0.690717589855194\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.764313155412674, and Avg Validation Loss (mae) is 0.7235406279563904\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7364757478237152\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1717555284500123, and Avg Validation Loss (mae) is 1.1770152807235719\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2092278063297273\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0664637625217437, and Avg Validation Loss (mae) is 1.0247731804847717\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0222411513328553\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7685143291950226, and Avg Validation Loss (mae) is 0.827955138683319\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8362796902656555\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8024288713932037, and Avg Validation Loss (mae) is 0.79194495677948\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8082313239574432\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.921774172782898, and Avg Validation Loss (mae) is 1.0248406052589416\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0380515933036805\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.67832453250885, and Avg Validation Loss (mae) is 0.869416868686676\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8801316261291504\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6483126282691956, and Avg Validation Loss (mae) is 0.6611922889947891\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6611201286315918\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7166061520576477, and Avg Validation Loss (mae) is 0.7672226071357727\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7696822941303253\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9877294719219207, and Avg Validation Loss (mae) is 1.0529038190841675\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0464218080043792\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6561277896165848, and Avg Validation Loss (mae) is 0.698313695192337\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7046156644821167\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5650690227746964, and Avg Validation Loss (mae) is 0.5357277065515518\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5389934092760086\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7546674013137817, and Avg Validation Loss (mae) is 0.8192323684692383\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8365200579166412\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0526887357234955, and Avg Validation Loss (mae) is 1.0653495967388154\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0435486257076263\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7086292386054993, and Avg Validation Loss (mae) is 0.6559810131788254\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6437096059322357\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6936443686485291, and Avg Validation Loss (mae) is 0.7134260028600693\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7120513379573822\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0555965542793273, and Avg Validation Loss (mae) is 0.957332968711853\n",
            "After 10 runs; Avg Test Loss (mae) is 0.96690132021904\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.253781831264496, and Avg Validation Loss (mae) is 1.2812264561653137\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2738427639007568\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7185543894767761, and Avg Validation Loss (mae) is 0.8143799334764481\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8043825387954712\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7315543830394745, and Avg Validation Loss (mae) is 0.5926847994327545\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5894904226064682\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0397425651550294, and Avg Validation Loss (mae) is 1.196840226650238\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2044025003910064\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2795788884162902, and Avg Validation Loss (mae) is 1.5459879398345948\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4793835699558258\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7813231527805329, and Avg Validation Loss (mae) is 0.7097618579864502\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6964779317378997\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5913434535264969, and Avg Validation Loss (mae) is 0.6112200081348419\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6116054654121399\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9940056443214417, and Avg Validation Loss (mae) is 1.0718270540237427\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0407486855983734\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3035211086273193, and Avg Validation Loss (mae) is 1.4452506065368653\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4195578813552856\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8410138309001922, and Avg Validation Loss (mae) is 0.7064063012599945\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7027892708778382\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.853865110874176, and Avg Validation Loss (mae) is 0.8611146628856658\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8675486803054809\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9332319557666778, and Avg Validation Loss (mae) is 0.9483331561088562\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9447169423103332\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4831875205039977, and Avg Validation Loss (mae) is 1.6270000338554382\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6418753385543823\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8586687266826629, and Avg Validation Loss (mae) is 0.9053933441638946\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8966112911701203\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.994758415222168, and Avg Validation Loss (mae) is 1.0458967447280885\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0456037819385529\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.59072242975235, and Avg Validation Loss (mae) is 1.5007795929908752\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4882815718650817\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3425756216049194, and Avg Validation Loss (mae) is 1.2943592190742492\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3245244860649108\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9641738414764405, and Avg Validation Loss (mae) is 0.8793780028820037\n",
            "After 10 runs; Avg Test Loss (mae) is 0.871476811170578\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.797791701555252, and Avg Validation Loss (mae) is 0.8102824866771698\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8098717749118804\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7077759861946107, and Avg Validation Loss (mae) is 1.7692665815353394\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7439912557601929\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6883678555488586, and Avg Validation Loss (mae) is 0.6234034359455108\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6144496887922287\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5783291429281234, and Avg Validation Loss (mae) is 0.525555145740509\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5288140565156937\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6104195892810822, and Avg Validation Loss (mae) is 0.7157333254814148\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7222394287586212\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5556333661079407, and Avg Validation Loss (mae) is 1.76757652759552\n",
            "After 10 runs; Avg Test Loss (mae) is 1.735303556919098\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7347637295722962, and Avg Validation Loss (mae) is 0.717976513504982\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7072295695543289\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7426124274730682, and Avg Validation Loss (mae) is 0.7514724910259247\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7261843323707581\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6480469763278961, and Avg Validation Loss (mae) is 0.815838885307312\n",
            "After 10 runs; Avg Test Loss (mae) is 0.838022118806839\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5323654651641845, and Avg Validation Loss (mae) is 1.6918273091316223\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6479520082473755\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7251137733459473, and Avg Validation Loss (mae) is 0.7759650707244873\n",
            "After 10 runs; Avg Test Loss (mae) is 0.767251405119896\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9546256363391876, and Avg Validation Loss (mae) is 0.9701092481613159\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9784148275852204\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7255145847797394, and Avg Validation Loss (mae) is 0.7240095973014832\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7307227551937103\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5633909702301025, and Avg Validation Loss (mae) is 1.5841822743415832\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6252970933914184\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6001340270042419, and Avg Validation Loss (mae) is 0.6078034400939941\n",
            "After 10 runs; Avg Test Loss (mae) is 0.588164547085762\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.682887077331543, and Avg Validation Loss (mae) is 0.741809195280075\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7428317219018936\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6780517280101777, and Avg Validation Loss (mae) is 0.6412467688322068\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6428664028644562\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6193062782287597, and Avg Validation Loss (mae) is 1.7436344742774963\n",
            "After 10 runs; Avg Test Loss (mae) is 1.757822859287262\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5235251903533935, and Avg Validation Loss (mae) is 0.4887329161167145\n",
            "After 10 runs; Avg Test Loss (mae) is 0.4931711435317993\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7232645511627197, and Avg Validation Loss (mae) is 0.6784732580184937\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7001205742359161\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6565116941928864, and Avg Validation Loss (mae) is 0.6110783994197846\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6149511456489563\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.459285521507263, and Avg Validation Loss (mae) is 1.4495582938194276\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4105515718460082\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6077520549297333, and Avg Validation Loss (mae) is 0.6225233942270278\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6206890255212784\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0029843389987945, and Avg Validation Loss (mae) is 1.0641821205615998\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0523074865341187\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8168029904365539, and Avg Validation Loss (mae) is 0.9667006731033325\n",
            "After 10 runs; Avg Test Loss (mae) is 0.96524977684021\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5240335702896117, and Avg Validation Loss (mae) is 1.502420675754547\n",
            "After 10 runs; Avg Test Loss (mae) is 1.514040434360504\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5931850850582123, and Avg Validation Loss (mae) is 0.6154879659414292\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6210460126399994\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8026958465576172, and Avg Validation Loss (mae) is 0.827919089794159\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8024623215198516\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6144223749637604, and Avg Validation Loss (mae) is 0.5521799683570862\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5617314875125885\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5890093803405763, and Avg Validation Loss (mae) is 1.5674471616744996\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5558860778808594\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6565922260284424, and Avg Validation Loss (mae) is 0.6124403953552247\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6186796844005584\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7842456579208374, and Avg Validation Loss (mae) is 0.7321510553359986\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7308360457420349\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6952004730701447, and Avg Validation Loss (mae) is 0.6484913110733033\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6339463621377945\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4042991638183593, and Avg Validation Loss (mae) is 1.433710789680481\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4600336074829101\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5475575447082519, and Avg Validation Loss (mae) is 0.5394760370254517\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5374155402183532\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7454039812088012, and Avg Validation Loss (mae) is 0.8273926019668579\n",
            "After 10 runs; Avg Test Loss (mae) is 0.844562566280365\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6905249297618866, and Avg Validation Loss (mae) is 0.6301366448402405\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6421148121356964\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5194939613342284, and Avg Validation Loss (mae) is 1.7751892447471618\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7134063839912415\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9527417898178101, and Avg Validation Loss (mae) is 1.0031290829181672\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0122862994670867\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5082944303750991, and Avg Validation Loss (mae) is 0.6494101256132125\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6545455396175385\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6233410000801086, and Avg Validation Loss (mae) is 0.5896336883306503\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5938299000263214\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.665052318572998, and Avg Validation Loss (mae) is 1.6240321755409242\n",
            "After 10 runs; Avg Test Loss (mae) is 1.623496985435486\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9236254155635834, and Avg Validation Loss (mae) is 0.9709862470626831\n",
            "After 10 runs; Avg Test Loss (mae) is 0.975106930732727\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7529253363609314, and Avg Validation Loss (mae) is 0.8462150633335114\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8305558353662491\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6938418686389923, and Avg Validation Loss (mae) is 0.702882245182991\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6939826220273971\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4654637575149536, and Avg Validation Loss (mae) is 1.4655978798866272\n",
            "After 10 runs; Avg Test Loss (mae) is 1.452764594554901\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0649342358112335, and Avg Validation Loss (mae) is 1.020360243320465\n",
            "After 10 runs; Avg Test Loss (mae) is 1.035676747560501\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0940744996070861, and Avg Validation Loss (mae) is 1.154119861125946\n",
            "After 10 runs; Avg Test Loss (mae) is 1.164870709180832\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8037134170532226, and Avg Validation Loss (mae) is 0.8983004689216614\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8950881421566009\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5501083612442017, and Avg Validation Loss (mae) is 1.6694005846977233\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5878061652183533\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.03539577126503, and Avg Validation Loss (mae) is 1.0180128812789917\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9983943819999694\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.728756046295166, and Avg Validation Loss (mae) is 0.7180958151817322\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7188370138406753\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6133722990751267, and Avg Validation Loss (mae) is 0.6347854018211365\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6333947151899337\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4677231550216674, and Avg Validation Loss (mae) is 1.397045338153839\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4809793829917908\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0165412425994873, and Avg Validation Loss (mae) is 1.2629952490329743\n",
            "After 10 runs; Avg Test Loss (mae) is 1.250667005777359\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7422483801841736, and Avg Validation Loss (mae) is 0.6916202038526535\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7025327265262604\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5707437992095947, and Avg Validation Loss (mae) is 0.6027360051870346\n",
            "After 10 runs; Avg Test Loss (mae) is 0.581719845533371\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5154070496559142, and Avg Validation Loss (mae) is 1.4364900231361388\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4618986248970032\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0240761399269105, and Avg Validation Loss (mae) is 0.9343877077102661\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9500336349010468\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8218952000141144, and Avg Validation Loss (mae) is 0.7137730479240417\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7027186632156373\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7614981770515442, and Avg Validation Loss (mae) is 0.7300534546375275\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7119886904954911\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6542140007019044, and Avg Validation Loss (mae) is 1.8518649101257325\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8696189045906066\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0621655464172364, and Avg Validation Loss (mae) is 1.2000810742378234\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1924015522003173\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5875485926866532, and Avg Validation Loss (mae) is 0.6416162729263306\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6365107297897339\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6445148050785064, and Avg Validation Loss (mae) is 0.7190988719463348\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7097242444753646\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5797497034072876, and Avg Validation Loss (mae) is 1.6865219354629517\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6542407751083374\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0676522076129913, and Avg Validation Loss (mae) is 1.158380913734436\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1891742587089538\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7471375465393066, and Avg Validation Loss (mae) is 0.7435168206691742\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7318953394889831\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5821165472269059, and Avg Validation Loss (mae) is 0.6811780065298081\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6946804314851761\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5069570541381836, and Avg Validation Loss (mae) is 1.478106713294983\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4936340212821961\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2233996748924256, and Avg Validation Loss (mae) is 1.2582538366317748\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2593324899673461\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6409268736839294, and Avg Validation Loss (mae) is 0.6191828280687333\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6310675352811813\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8304576992988586, and Avg Validation Loss (mae) is 0.8221632897853851\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8336580574512482\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.482122004032135, and Avg Validation Loss (mae) is 1.587965977191925\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6027301907539369\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3508980989456176, and Avg Validation Loss (mae) is 1.3463256716728211\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3288918256759643\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7672945022583008, and Avg Validation Loss (mae) is 0.8032488524913788\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7967227101325989\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9515496492385864, and Avg Validation Loss (mae) is 1.0367812991142273\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0344728648662567\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8585385441780091, and Avg Validation Loss (mae) is 1.8618102550506592\n",
            "After 10 runs; Avg Test Loss (mae) is 1.897109067440033\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8932501375675201, and Avg Validation Loss (mae) is 0.8599138021469116\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8870902776718139\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6286992371082306, and Avg Validation Loss (mae) is 0.475234842300415\n",
            "After 10 runs; Avg Test Loss (mae) is 0.46100509762763975\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7340501189231873, and Avg Validation Loss (mae) is 0.7156269252300262\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7127123057842255\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9060242772102356, and Avg Validation Loss (mae) is 1.8210224986076355\n",
            "After 10 runs; Avg Test Loss (mae) is 1.824454164505005\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9514218747615815, and Avg Validation Loss (mae) is 0.9836432576179505\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9839734375476837\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.744803249835968, and Avg Validation Loss (mae) is 0.6989697098731995\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7080335289239883\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7931574165821076, and Avg Validation Loss (mae) is 1.0529906034469605\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0449564933776856\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6422432661056519, and Avg Validation Loss (mae) is 1.5691762566566467\n",
            "After 10 runs; Avg Test Loss (mae) is 1.586927056312561\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8258174419403076, and Avg Validation Loss (mae) is 0.7711594879627228\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7787872672080993\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9998818397521972, and Avg Validation Loss (mae) is 0.9385516703128814\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9402568280696869\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7178121924400329, and Avg Validation Loss (mae) is 0.8862548410892487\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8655375301837921\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8790091037750245, and Avg Validation Loss (mae) is 1.8247770071029663\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8223390579223633\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8649518609046936, and Avg Validation Loss (mae) is 0.8218801081180572\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8257194340229035\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7647731721401214, and Avg Validation Loss (mae) is 0.7024567067623139\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7112566590309143\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7347317039966583, and Avg Validation Loss (mae) is 0.7776492238044739\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7897050142288208\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6769453167915345, and Avg Validation Loss (mae) is 1.6986249327659606\n",
            "After 10 runs; Avg Test Loss (mae) is 1.71911940574646\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8061999499797821, and Avg Validation Loss (mae) is 0.784274673461914\n",
            "After 10 runs; Avg Test Loss (mae) is 0.80241579413414\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7093429148197175, and Avg Validation Loss (mae) is 0.7412850022315979\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7371816992759704\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5460473090410233, and Avg Validation Loss (mae) is 0.41763947904109955\n",
            "After 10 runs; Avg Test Loss (mae) is 0.4127637356519699\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6270236492156982, and Avg Validation Loss (mae) is 1.8335329413414\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8339412927627563\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8510791838169098, and Avg Validation Loss (mae) is 0.7885550141334534\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7927405297756195\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8086193084716797, and Avg Validation Loss (mae) is 0.8714204847812652\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8774644136428833\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6872547805309296, and Avg Validation Loss (mae) is 0.6361536860466004\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6368489593267441\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9112322807312012, and Avg Validation Loss (mae) is 1.9432548046112061\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9040242433547974\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9999184966087341, and Avg Validation Loss (mae) is 0.9552918791770935\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9970828056335449\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5260254830121994, and Avg Validation Loss (mae) is 0.7299372136592865\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7269042819738388\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7726890325546265, and Avg Validation Loss (mae) is 0.7990979015827179\n",
            "After 10 runs; Avg Test Loss (mae) is 0.78412424325943\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6861114978790284, and Avg Validation Loss (mae) is 1.618547749519348\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6424522042274474\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8316865801811218, and Avg Validation Loss (mae) is 0.9753438413143158\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9808901846408844\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5906507521867752, and Avg Validation Loss (mae) is 0.6883516162633896\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6858241528272628\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.619227385520935, and Avg Validation Loss (mae) is 0.7195544689893723\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7219550520181656\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7793835639953612, and Avg Validation Loss (mae) is 1.7404940605163575\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7928370475769042\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8852171063423157, and Avg Validation Loss (mae) is 0.7909503281116486\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8112910151481628\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6552053660154342, and Avg Validation Loss (mae) is 0.5294095695018768\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5259494870901108\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6996421456336975, and Avg Validation Loss (mae) is 0.7209807395935058\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7424564898014069\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6946339011192322, and Avg Validation Loss (mae) is 1.9154647588729858\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8458290815353393\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.901241272687912, and Avg Validation Loss (mae) is 0.9177862465381622\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9015669226646423\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5996519058942795, and Avg Validation Loss (mae) is 0.7979498028755188\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7896629810333252\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7870292901992798, and Avg Validation Loss (mae) is 0.8001717686653137\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8108003556728363\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9085269212722777, and Avg Validation Loss (mae) is 2.182427501678467\n",
            "After 10 runs; Avg Test Loss (mae) is 2.182398736476898\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9766041755676269, and Avg Validation Loss (mae) is 1.0453195571899414\n",
            "After 10 runs; Avg Test Loss (mae) is 1.063833999633789\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9200676500797271, and Avg Validation Loss (mae) is 1.032676202058792\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0306084513664246\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6801563739776612, and Avg Validation Loss (mae) is 0.7413901507854461\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7403877377510071\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6780722618103028, and Avg Validation Loss (mae) is 1.6980600595474242\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7281222939491272\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8445441782474518, and Avg Validation Loss (mae) is 0.8088159203529358\n",
            "After 10 runs; Avg Test Loss (mae) is 0.814286595582962\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0112828850746154, and Avg Validation Loss (mae) is 1.0633175671100616\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0571388185024262\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4736372768878937, and Avg Validation Loss (mae) is 0.5349415302276611\n",
            "After 10 runs; Avg Test Loss (mae) is 0.532750454545021\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6625808000564575, and Avg Validation Loss (mae) is 1.7446388840675353\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7214571714401246\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8382030189037323, and Avg Validation Loss (mae) is 1.1086418807506562\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1065994679927826\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0431769132614135, and Avg Validation Loss (mae) is 1.1842330873012543\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1839335024356843\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7152458012104035, and Avg Validation Loss (mae) is 0.7741463750600814\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7937247574329376\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6731439471244811, and Avg Validation Loss (mae) is 1.7240403652191163\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7554096698760986\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7985629558563232, and Avg Validation Loss (mae) is 0.8490874290466308\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8799227833747864\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0382147431373596, and Avg Validation Loss (mae) is 1.2704027593135834\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2705976903438567\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7519878268241882, and Avg Validation Loss (mae) is 0.8056088089942932\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7980920732021332\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6719141244888305, and Avg Validation Loss (mae) is 1.6353500604629516\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6669735550880431\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8972643315792084, and Avg Validation Loss (mae) is 0.9949556291103363\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9933142900466919\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.201099932193756, and Avg Validation Loss (mae) is 1.123299640417099\n",
            "After 10 runs; Avg Test Loss (mae) is 1.142539644241333\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6617547929286957, and Avg Validation Loss (mae) is 0.6836602538824081\n",
            "After 10 runs; Avg Test Loss (mae) is 0.690594545006752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XihPNGJcMJoM",
        "outputId": "be2747a9-7781-4c9d-d1fc-bd31a05a40c8"
      },
      "source": [
        "CombResultsMF4 = pd.DataFrame.from_dict(my_dictMF4)\n",
        "print(CombResultsMF4.shape)\n",
        "CombResultsSortedMF4 = CombResultsMF4.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMF4.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(280, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA_X</th>\n",
              "      <th>DATA_y</th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>X_MTX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.412764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>X_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.461005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>X_FMX_MTX_MBX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.493171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>X_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.525949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>X_FMX_MTX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.528814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>X_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.532750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>X_FMX_MTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.537416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>X_FTX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.538993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>X_FMX_MTX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.561731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>X_FTX_FMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.562576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>X_FTX_MTX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.566660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>X_FTX_MTX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.575451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>X_FTX_FMX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.578583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>X_FMX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.581720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>X_FTX_MTX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.581775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>X_FTX_FMX_MTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.585524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>X_FMX_MTX_MBX_RT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.588165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>X_FTX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.589490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>X_FTX_MTX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.590621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>X_FMX_MMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.593830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>X_FTX_FMX_MTX_MB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.595270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>X_FTX_FMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.598961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>X_FTX_FMX_MBX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.600022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>X_FTX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.611605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>X_FMX_MTX_MMX_RT</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.614450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>X_FMX_MTX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.614951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>X_FMX_MTX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.618680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>X_FMX_MTX_MBX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.620689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>X_FMX_MTX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.621046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>X_FMX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.631068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>X_FMX_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.633395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>X_FMX_MTX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.633946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>X_FMX_MBX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.636511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>X_MTX_MMX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.636849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>X_FTX_MTX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.639994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>X_FTX_MTX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.640025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>X_FMX_MTX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.642115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>X_FMX_MTX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.642866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>X_FTX_MMX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.643710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>X_FTX_FMX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.644546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>X_FTX_FMX_MMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.649414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>X_FTX_FMX_MTX_RT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.651093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>X_FMX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.654546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>X_FTX_MTX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.657703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>X_FTX_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.661120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>X_FTX_MTX_MBX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.661488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>X_FTX_FMX_MMX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.663885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>X_FTX_FMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.665721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>X_FTX_FMX_MTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.671943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>X_FTX_MTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.677295</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               DATA_X DATA_y  Test Loss\n",
              "239  X_MTX_MMX_RTX_RB   y_RM   0.412764\n",
              "222  X_MTX_MMX_MBX_RT   y_RM   0.461005\n",
              "161  X_FMX_MTX_MBX_RM   y_MM   0.493171\n",
              "254  X_MTX_MBX_RMX_RB   y_MM   0.525949\n",
              "146  X_FMX_MTX_MMX_RT   y_RM   0.528814\n",
              "267  X_MMX_MBX_RTX_RB   y_RM   0.532750\n",
              "177  X_FMX_MTX_RMX_RB   y_MM   0.537416\n",
              "119  X_FTX_MMX_RTX_RB   y_RM   0.538993\n",
              "171  X_FMX_MTX_RTX_RM   y_RB   0.561731\n",
              "55   X_FTX_FMX_RTX_RB   y_RM   0.562576\n",
              "93   X_FTX_MTX_RTX_RB   y_MM   0.566660\n",
              "89   X_FTX_MTX_RTX_RM   y_MM   0.575451\n",
              "43   X_FTX_FMX_MBX_RM   y_RB   0.578583\n",
              "199  X_FMX_MMX_RTX_RB   y_RM   0.581720\n",
              "66   X_FTX_MTX_MMX_RT   y_RM   0.581775\n",
              "12   X_FTX_FMX_MTX_RM   y_MM   0.585524\n",
              "157  X_FMX_MTX_MBX_RT   y_MM   0.588165\n",
              "127  X_FTX_MBX_RTX_RM   y_RB   0.589490\n",
              "95   X_FTX_MTX_RTX_RB   y_RM   0.590621\n",
              "183  X_FMX_MMX_MBX_RT   y_RB   0.593830\n",
              "4    X_FTX_FMX_MTX_MB   y_MM   0.595270\n",
              "51   X_FTX_FMX_RTX_RM   y_RB   0.598961\n",
              "45   X_FTX_FMX_MBX_RB   y_MM   0.600022\n",
              "131  X_FTX_MBX_RTX_RB   y_RM   0.611605\n",
              "145  X_FMX_MTX_MMX_RT   y_MB   0.614450\n",
              "163  X_FMX_MTX_MBX_RM   y_RB   0.614951\n",
              "173  X_FMX_MTX_RTX_RB   y_MM   0.618680\n",
              "165  X_FMX_MTX_MBX_RB   y_MM   0.620689\n",
              "169  X_FMX_MTX_RTX_RM   y_MM   0.621046\n",
              "214  X_FMX_MBX_RMX_RB   y_MM   0.631068\n",
              "195  X_FMX_MMX_RTX_RM   y_RB   0.633395\n",
              "175  X_FMX_MTX_RTX_RB   y_RM   0.633946\n",
              "206  X_FMX_MBX_RTX_RM   y_MM   0.636511\n",
              "243  X_MTX_MMX_RMX_RB   y_RT   0.636849\n",
              "78   X_FTX_MTX_MBX_RT   y_RM   0.639994\n",
              "79   X_FTX_MTX_MBX_RT   y_RB   0.640025\n",
              "179  X_FMX_MTX_RMX_RB   y_RT   0.642115\n",
              "159  X_FMX_MTX_MBX_RT   y_RB   0.642866\n",
              "122  X_FTX_MMX_RMX_RB   y_MB   0.643710\n",
              "26   X_FTX_FMX_MMX_RT   y_RM   0.644546\n",
              "33   X_FTX_FMX_MMX_RB   y_MB   0.649414\n",
              "8    X_FTX_FMX_MTX_RT   y_MM   0.651093\n",
              "182  X_FMX_MMX_MBX_RT   y_RM   0.654546\n",
              "83   X_FTX_MTX_MBX_RM   y_RB   0.657703\n",
              "115  X_FTX_MMX_RTX_RM   y_RB   0.661120\n",
              "81   X_FTX_MTX_MBX_RM   y_MM   0.661488\n",
              "31   X_FTX_FMX_MMX_RM   y_RB   0.663885\n",
              "38   X_FTX_FMX_MBX_RT   y_RM   0.665721\n",
              "16   X_FTX_FMX_MTX_RB   y_MM   0.671943\n",
              "97   X_FTX_MTX_RMX_RB   y_MM   0.677295"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "mAeOw0XyMJoM",
        "outputId": "88c727ec-1a73-4752-d5e0-3a64eee5b5ca"
      },
      "source": [
        "CombResultsSortedMFgrouped4 = CombResultsSortedMF4.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedMFgroupedsortedMF4 = CombResultsSortedMFgrouped4.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMFgroupedsortedMF4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RTX_RB</th>\n",
              "      <td>0.674894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RM</th>\n",
              "      <td>0.706065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RT</th>\n",
              "      <td>0.711997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RT</th>\n",
              "      <td>0.726634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RTX_RM</th>\n",
              "      <td>0.742021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RTX_RMX_RB</th>\n",
              "      <td>1.176006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MB</th>\n",
              "      <td>1.177994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RTX_RMX_RB</th>\n",
              "      <td>1.190704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RMX_RB</th>\n",
              "      <td>1.201429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RTX_RM</th>\n",
              "      <td>1.254307</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows  1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Test Loss\n",
              "DATA_X                     \n",
              "X_FTX_MTX_RTX_RB   0.674894\n",
              "X_FTX_MTX_MBX_RM   0.706065\n",
              "X_FTX_MTX_MMX_RT   0.711997\n",
              "X_FTX_FMX_MTX_RT   0.726634\n",
              "X_FTX_MTX_RTX_RM   0.742021\n",
              "...                     ...\n",
              "X_MMX_RTX_RMX_RB   1.176006\n",
              "X_FTX_FMX_MMX_MB   1.177994\n",
              "X_FMX_RTX_RMX_RB   1.190704\n",
              "X_MMX_MBX_RMX_RB   1.201429\n",
              "X_MMX_MBX_RTX_RM   1.254307\n",
              "\n",
              "[70 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "r8JL-9aVmWd6",
        "outputId": "f9687da5-38da-432b-e398-8c63b569f6fa"
      },
      "source": [
        "CombResultsSortedMFgroupedsortedMF4.to_csv('CombResultsSortedMFgroupedsortedMF4.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedMFgroupedsortedMF4.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0d1c61d9-9541-4169-bfe9-1f17e7c41d6d\", \"CombResultsSortedMFgroupedsortedMF4.csv\", 2519)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "X6UIC9iInIVT",
        "outputId": "e4d47426-3328-4ecd-e967-14c733468563"
      },
      "source": [
        "CombResultsSortedMF4.to_csv('CombResultsSortedMF4.csv')\n",
        "files.download(\"CombResultsSortedMF4.csv\")\n",
        "\n",
        "fig = px.box(CombResultsSortedMF4, x=\"DATA_X\", y=\"Test Loss\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_01b3c53f-d871-4419-9713-0c1e115c3799\", \"CombResultsSortedMF4.csv\", 12460)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"16332097-63c2-449c-9cb1-55fcaee5d6f2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"16332097-63c2-449c-9cb1-55fcaee5d6f2\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '16332097-63c2-449c-9cb1-55fcaee5d6f2',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_MTX_MMX_RTX_RB\", \"X_MTX_MMX_MBX_RT\", \"X_FMX_MTX_MBX_RM\", \"X_MTX_MBX_RMX_RB\", \"X_FMX_MTX_MMX_RT\", \"X_MMX_MBX_RTX_RB\", \"X_FMX_MTX_RMX_RB\", \"X_FTX_MMX_RTX_RB\", \"X_FMX_MTX_RTX_RM\", \"X_FTX_FMX_RTX_RB\", \"X_FTX_MTX_RTX_RB\", \"X_FTX_MTX_RTX_RM\", \"X_FTX_FMX_MBX_RM\", \"X_FMX_MMX_RTX_RB\", \"X_FTX_MTX_MMX_RT\", \"X_FTX_FMX_MTX_RM\", \"X_FMX_MTX_MBX_RT\", \"X_FTX_MBX_RTX_RM\", \"X_FTX_MTX_RTX_RB\", \"X_FMX_MMX_MBX_RT\", \"X_FTX_FMX_MTX_MB\", \"X_FTX_FMX_RTX_RM\", \"X_FTX_FMX_MBX_RB\", \"X_FTX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RT\", \"X_FMX_MTX_MBX_RM\", \"X_FMX_MTX_RTX_RB\", \"X_FMX_MTX_MBX_RB\", \"X_FMX_MTX_RTX_RM\", \"X_FMX_MBX_RMX_RB\", \"X_FMX_MMX_RTX_RM\", \"X_FMX_MTX_RTX_RB\", \"X_FMX_MBX_RTX_RM\", \"X_MTX_MMX_RMX_RB\", \"X_FTX_MTX_MBX_RT\", \"X_FTX_MTX_MBX_RT\", \"X_FMX_MTX_RMX_RB\", \"X_FMX_MTX_MBX_RT\", \"X_FTX_MMX_RMX_RB\", \"X_FTX_FMX_MMX_RT\", \"X_FTX_FMX_MMX_RB\", \"X_FTX_FMX_MTX_RT\", \"X_FMX_MMX_MBX_RT\", \"X_FTX_MTX_MBX_RM\", \"X_FTX_MMX_RTX_RM\", \"X_FTX_MTX_MBX_RM\", \"X_FTX_FMX_MMX_RM\", \"X_FTX_FMX_MBX_RT\", \"X_FTX_FMX_MTX_RB\", \"X_FTX_MTX_RMX_RB\", \"X_FTX_FMX_MMX_RT\", \"X_FTX_FMX_MBX_RT\", \"X_MTX_MBX_RTX_RB\", \"X_MBX_RTX_RMX_RB\", \"X_FTX_MMX_MBX_RM\", \"X_FMX_MMX_MBX_RM\", \"X_FMX_MBX_RTX_RB\", \"X_FTX_MBX_RTX_RB\", \"X_FMX_MTX_MBX_RM\", \"X_FMX_MMX_RTX_RB\", \"X_FMX_MMX_RMX_RB\", \"X_FTX_MTX_RTX_RM\", \"X_FTX_MBX_RMX_RB\", \"X_FTX_MMX_RTX_RB\", \"X_FMX_MTX_MMX_RM\", \"X_MTX_MMX_MBX_RM\", \"X_FMX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_RT\", \"X_MTX_MMX_RTX_RM\", \"X_FMX_MMX_RMX_RB\", \"X_FTX_MMX_RMX_RB\", \"X_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MTX_RT\", \"X_FTX_MTX_MBX_RM\", \"X_FTX_MTX_MMX_RM\", \"X_FMX_MMX_RTX_RM\", \"X_MTX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RT\", \"X_FTX_MTX_MMX_RB\", \"X_FMX_MTX_MMX_RM\", \"X_MTX_MBX_RTX_RM\", \"X_FMX_MTX_MMX_RB\", \"X_FMX_MTX_RTX_RB\", \"X_FMX_MBX_RTX_RB\", \"X_FTX_MMX_MBX_RM\", \"X_FTX_MMX_MBX_RB\", \"X_MTX_MMX_RTX_RB\", \"X_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MBX_RT\", \"X_MTX_MBX_RMX_RB\", \"X_FMX_MTX_MBX_RT\", \"X_FTX_FMX_MTX_RT\", \"X_FTX_FMX_RMX_RB\", \"X_FTX_MTX_MMX_RT\", \"X_FTX_MTX_RTX_RM\", \"X_FTX_MMX_MBX_RT\", \"X_FTX_MMX_MBX_RT\", \"X_FTX_FMX_MMX_RM\", \"X_FTX_FMX_MBX_RM\", \"X_FTX_MTX_RTX_RB\", \"X_FMX_MTX_MMX_RB\", \"X_FTX_MMX_RTX_RB\", \"X_MTX_MMX_MBX_RB\", \"X_FTX_MTX_RTX_RB\", \"X_MTX_MBX_RTX_RM\", \"X_MTX_RTX_RMX_RB\", \"X_MTX_MMX_RTX_RM\", \"X_FTX_MTX_MBX_RM\", \"X_MTX_MMX_RMX_RB\", \"X_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_RM\", \"X_FTX_FMX_MTX_RT\", \"X_FMX_RTX_RMX_RB\", \"X_FTX_MTX_RMX_RB\", \"X_MMX_RTX_RMX_RB\", \"X_FTX_MTX_RMX_RB\", \"X_FTX_MTX_MMX_RT\", \"X_FTX_MTX_MBX_RB\", \"X_MTX_MMX_RTX_RB\", \"X_FMX_MTX_RTX_RM\", \"X_FTX_MBX_RTX_RM\", \"X_FTX_MTX_MBX_RB\", \"X_FTX_MMX_RTX_RM\", \"X_FMX_MTX_MMX_MB\", \"X_MTX_RTX_RMX_RB\", \"X_MTX_MBX_RMX_RB\", \"X_FTX_MTX_MMX_RB\", \"X_MMX_MBX_RTX_RB\", \"X_FTX_MTX_MBX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_FTX_FMX_MTX_MB\", \"X_FTX_MTX_RMX_RB\", \"X_MTX_MMX_RTX_RM\", \"X_FTX_MMX_MBX_RM\", \"X_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_RM\", \"X_FTX_MTX_MMX_RM\", \"X_FMX_MBX_RMX_RB\", \"X_FTX_MMX_MBX_RB\", \"X_FTX_MMX_RMX_RB\", \"X_FTX_FMX_RMX_RB\", \"X_FMX_MTX_MMX_RM\", \"X_FTX_FMX_MTX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_FMX_MTX_RMX_RB\", \"X_FTX_FMX_MBX_RM\", \"X_FTX_MTX_MMX_RM\", \"X_FTX_MTX_MMX_RM\", \"X_MTX_MMX_MBX_RB\", \"X_FTX_MBX_RMX_RB\", \"X_FMX_MTX_MMX_MB\", \"X_FTX_FMX_MMX_RB\", \"X_MTX_MMX_RMX_RB\", \"X_MMX_RTX_RMX_RB\", \"X_FTX_MMX_RTX_RM\", \"X_FTX_FMX_MMX_RM\", \"X_FTX_MMX_MBX_RT\", \"X_MTX_MMX_MBX_RT\", \"X_FTX_MTX_MBX_RT\", \"X_FTX_FMX_MMX_RT\", \"X_FMX_MMX_MBX_RB\", \"X_FTX_MTX_MBX_RB\", \"X_FTX_RTX_RMX_RB\", \"X_MTX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MM\", \"X_FTX_MTX_MMX_RB\", \"X_FTX_MTX_MBX_RT\", \"X_FTX_FMX_RTX_RM\", \"X_FTX_FMX_MTX_RM\", \"X_FTX_FMX_MTX_RB\", \"X_FTX_FMX_MMX_MB\", \"X_FTX_MTX_RTX_RM\", \"X_MTX_MMX_MBX_RB\", \"X_FTX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RB\", \"X_FMX_MMX_RMX_RB\", \"X_FMX_MTX_MBX_RB\", \"X_FTX_MBX_RTX_RM\", \"X_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_MM\", \"X_FMX_MTX_MMX_RB\", \"X_MTX_MBX_RTX_RB\", \"X_MTX_MMX_MBX_RM\", \"X_MBX_RTX_RMX_RB\", \"X_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MMX_RT\", \"X_FTX_FMX_MBX_RB\", \"X_FMX_MMX_RTX_RM\", \"X_FMX_MMX_MBX_RT\", \"X_FTX_FMX_RTX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_FTX_MMX_MBX_RB\", \"X_MMX_MBX_RTX_RM\", \"X_FMX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RB\", \"X_FTX_MMX_RTX_RM\", \"X_FTX_MBX_RMX_RB\", \"X_FTX_MMX_RMX_RB\", \"X_FTX_FMX_RTX_RM\", \"X_MTX_MMX_MBX_RM\", \"X_FTX_RTX_RMX_RB\", \"X_FTX_MMX_RTX_RB\", \"X_FMX_MTX_MBX_RB\", \"X_MMX_MBX_RTX_RB\", \"X_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MMX_RB\", \"X_FTX_FMX_RTX_RB\", \"X_FTX_FMX_MMX_MB\", \"X_FTX_FMX_RMX_RB\", \"X_FTX_FMX_MBX_RB\", \"X_FTX_FMX_MTX_MM\", \"X_FTX_MMX_MBX_RM\", \"X_FTX_FMX_MBX_RT\", \"X_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RM\", \"X_FTX_FMX_MBX_RB\", \"X_FTX_FMX_MMX_MB\", \"X_MBX_RTX_RMX_RB\", \"X_FTX_MMX_MBX_RT\", \"X_FMX_MMX_MBX_RB\", \"X_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_RB\", \"X_FMX_MBX_RTX_RB\", \"X_FMX_MBX_RTX_RM\", \"X_FTX_MBX_RTX_RB\", \"X_FTX_MMX_MBX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_FMX_MMX_RTX_RB\", \"X_FTX_FMX_MBX_RM\", \"X_FMX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RB\", \"X_MMX_RTX_RMX_RB\", \"X_FTX_MBX_RTX_RM\", \"X_FTX_FMX_RTX_RB\", \"X_FMX_MTX_MMX_MB\", \"X_FMX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MB\", \"X_FMX_MTX_MBX_RB\", \"X_FTX_MBX_RMX_RB\", \"X_FMX_MMX_MBX_RB\", \"X_FMX_MTX_RMX_RB\", \"X_FMX_MMX_RMX_RB\", \"X_FTX_MBX_RTX_RB\", \"X_FMX_MMX_RTX_RB\", \"X_FTX_FMX_RMX_RB\", \"X_FMX_MTX_MMX_MB\", \"X_FMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MM\", \"X_FMX_MTX_RTX_RM\", \"X_FTX_FMX_MTX_MB\", \"X_FTX_FMX_RTX_RM\", \"X_FMX_MTX_RTX_RB\", \"X_FTX_FMX_MMX_MB\", \"X_MTX_MMX_MBX_RB\", \"X_FMX_MMX_RTX_RM\", \"X_FMX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RM\", \"X_FMX_MTX_MBX_RT\", \"X_FTX_RTX_RMX_RB\", \"X_MTX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RB\", \"X_FMX_MBX_RTX_RB\", \"X_MBX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RT\", \"X_MTX_MMX_RTX_RB\", \"X_MMX_MBX_RMX_RB\", \"X_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RM\", \"X_FMX_MTX_MMX_RT\", \"X_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MBX_RM\", \"X_MTX_MBX_RMX_RB\", \"X_MTX_MMX_RTX_RM\", \"X_MTX_MMX_MBX_RM\", \"X_MTX_MMX_RMX_RB\", \"X_MTX_RTX_RMX_RB\", \"X_FMX_MBX_RTX_RM\", \"X_MTX_MMX_MBX_RT\", \"X_MTX_MBX_RTX_RM\", \"X_MMX_MBX_RTX_RM\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.4127637356519699, 0.46100509762763975, 0.4931711435317993, 0.5259494870901108, 0.5288140565156937, 0.532750454545021, 0.5374155402183532, 0.5389934092760086, 0.5617314875125885, 0.5625763773918152, 0.5666598826646805, 0.5754513621330262, 0.578582513332367, 0.581719845533371, 0.581775176525116, 0.5855235636234284, 0.588164547085762, 0.5894904226064682, 0.5906214416027069, 0.5938299000263214, 0.595270323753357, 0.5989613682031631, 0.6000216245651245, 0.6116054654121399, 0.6144496887922287, 0.6149511456489563, 0.6186796844005584, 0.6206890255212784, 0.6210460126399994, 0.6310675352811813, 0.6333947151899337, 0.6339463621377945, 0.6365107297897339, 0.6368489593267441, 0.6399941504001617, 0.640024608373642, 0.6421148121356964, 0.6428664028644562, 0.6437096059322357, 0.6445459663867951, 0.6494135320186615, 0.6510933667421341, 0.6545455396175385, 0.6577032715082168, 0.6611201286315918, 0.6614878088235855, 0.6638850212097168, 0.6657209485769272, 0.6719425618648529, 0.6772949516773223, 0.6811392217874527, 0.6830884367227554, 0.6858241528272628, 0.690594545006752, 0.690717589855194, 0.6939826220273971, 0.6946804314851761, 0.6964779317378997, 0.7001205742359161, 0.7025327265262604, 0.7027186632156373, 0.7027208030223846, 0.7027892708778382, 0.7046156644821167, 0.7072295695543289, 0.7080335289239883, 0.7097242444753646, 0.710794597864151, 0.7112566590309143, 0.7119886904954911, 0.7120513379573822, 0.7127123057842255, 0.7134125351905822, 0.714645916223526, 0.7147448539733887, 0.7188370138406753, 0.7219550520181656, 0.7222394287586212, 0.7258809506893158, 0.7261843323707581, 0.7269042819738388, 0.7307227551937103, 0.7308360457420349, 0.7318953394889831, 0.7351144015789032, 0.7364757478237152, 0.7371816992759704, 0.7403877377510071, 0.7411780506372452, 0.7424564898014069, 0.7428317219018936, 0.7474124372005463, 0.7515765190124511, 0.7552183926105499, 0.7553229033946991, 0.7557681381702424, 0.7579279601573944, 0.7598480582237244, 0.7605031251907348, 0.7621240615844727, 0.767251405119896, 0.7696822941303253, 0.7787872672080993, 0.7801692128181458, 0.78412424325943, 0.7896629810333252, 0.7897050142288208, 0.7904233276844025, 0.7927405297756195, 0.7937247574329376, 0.7937292993068695, 0.7946194410324097, 0.7967227101325989, 0.7971456289291382, 0.7980920732021332, 0.799152547121048, 0.8001988053321838, 0.8005081653594971, 0.80241579413414, 0.8024623215198516, 0.8043825387954712, 0.8082094073295594, 0.8082313239574432, 0.8098717749118804, 0.8108003556728363, 0.8112910151481628, 0.812053307890892, 0.814286595582962, 0.815443480014801, 0.8173675894737243, 0.8200599431991578, 0.820230457186699, 0.8257194340229035, 0.829956340789795, 0.8305558353662491, 0.8306457042694092, 0.832785227894783, 0.8336580574512482, 0.8362796902656555, 0.8365200579166412, 0.8376691222190857, 0.838022118806839, 0.8411434173583985, 0.8421301782131195, 0.844562566280365, 0.8455284893512726, 0.8495836228132247, 0.8503449857234955, 0.8655375301837921, 0.8675486803054809, 0.871476811170578, 0.8716757833957672, 0.8774644136428833, 0.8799227833747864, 0.8801316261291504, 0.882144284248352, 0.883879441022873, 0.8870902776718139, 0.8878775149583816, 0.8946038782596588, 0.8950881421566009, 0.89574134349823, 0.8966112911701203, 0.9015669226646423, 0.9031723260879516, 0.9066274344921113, 0.914964097738266, 0.9206372797489166, 0.9238629639148712, 0.9251620173454285, 0.9291528463363647, 0.9345882356166839, 0.9402568280696869, 0.9447169423103332, 0.9491433739662171, 0.9500336349010468, 0.96524977684021, 0.96690132021904, 0.975106930732727, 0.9754471600055694, 0.9784148275852204, 0.9808901846408844, 0.9839734375476837, 0.9933142900466919, 0.9970828056335449, 0.9971264243125916, 0.9977567493915558, 0.9983943819999694, 1.0122862994670867, 1.0164918541908263, 1.0210439562797546, 1.0222411513328553, 1.0306084513664246, 1.0344728648662567, 1.035676747560501, 1.0380515933036805, 1.0407486855983734, 1.0435486257076263, 1.043584442138672, 1.0449564933776856, 1.0456037819385529, 1.0464218080043792, 1.0523074865341187, 1.0571388185024262, 1.063833999633789, 1.071741485595703, 1.072948509454727, 1.0789916396141053, 1.0794279754161835, 1.0829204857349395, 1.0846634745597838, 1.0887119591236114, 1.0889474987983703, 1.1065994679927826, 1.1177762508392335, 1.1274255394935608, 1.1353368520736695, 1.142539644241333, 1.1445922374725341, 1.164870709180832, 1.1839335024356843, 1.1854198276996613, 1.1891742587089538, 1.1924015522003173, 1.2044025003910064, 1.2092278063297273, 1.2503778457641601, 1.250667005777359, 1.2538671314716339, 1.2593324899673461, 1.2695529222488404, 1.2705976903438567, 1.2738427639007568, 1.3188342452049255, 1.3245244860649108, 1.3288918256759643, 1.3508085787296296, 1.4105515718460082, 1.4195578813552856, 1.452764594554901, 1.4600336074829101, 1.4618986248970032, 1.4793835699558258, 1.4809793829917908, 1.4853735446929932, 1.4882815718650817, 1.4936340212821961, 1.5055761694908143, 1.514040434360504, 1.5204498291015625, 1.522480857372284, 1.5558860778808594, 1.5684947848320008, 1.586927056312561, 1.5878061652183533, 1.6027301907539369, 1.623496985435486, 1.6252970933914184, 1.6418753385543823, 1.6424522042274474, 1.6479520082473755, 1.6542407751083374, 1.6669735550880431, 1.7134063839912415, 1.71911940574646, 1.7214571714401246, 1.7281222939491272, 1.735303556919098, 1.7439912557601929, 1.7554096698760986, 1.757822859287262, 1.7928370475769042, 1.8223390579223633, 1.824454164505005, 1.8339412927627563, 1.8458290815353393, 1.8696189045906066, 1.897109067440033, 1.9040242433547974, 2.182398736476898], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('16332097-63c2-449c-9cb1-55fcaee5d6f2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9bABRGTnKVp"
      },
      "source": [
        "# define baseline model 5\n",
        "# create model\n",
        "n_features = 1\n",
        "model5 = Sequential()\n",
        "model5.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(35, n_features)))\n",
        "model5.add(MaxPooling1D(pool_size=2))\n",
        "model5.add(Flatten())\n",
        "model5.add(Dense(50, activation='relu'))\n",
        "model5.add(Dense(1))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "model5.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "#model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h68hbp59nKVq"
      },
      "source": [
        "n_features = 1\n",
        "def datageneratorMF5(X_in1, X_in2, X_in3, X_in4, X_in5, Y_in):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  X_in = np.concatenate((X_in1, X_in2, X_in3, X_in4, X_in5), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "\n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], n_features))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], n_features))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], n_features))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcrwvrknnKVq"
      },
      "source": [
        "def evaldataMF5(X_in1, X_in2, X_in3, X_in4, X_in5, Y_in, traindata1, traindata2, traindata3, traindata4, traindata5, testdata):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMF5(X_in1, X_in2, X_in3, X_in4, X_in5, Y_in)\n",
        "  \n",
        "  history = model5.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model5.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model5.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # PLOTS LOSS VS EPOCH\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  # print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  # print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # PLOTS Y ORIGINAL VS PREDICTED\n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata1)+ str(traindata2)  + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMj4oSI3nKVq",
        "outputId": "71180348-05cb-4fa7-eba7-e252d587cf98"
      },
      "source": [
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "\n",
        "model5.save_weights('model5.h5')\n",
        "\n",
        "my_dictMF5 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), 5):\n",
        "  kX1, kX2, kX3, kX4, kX5 = combo[0][0], combo[1][0], combo[2][0], combo[3][0], combo[4][0]\n",
        "  vX1, vX2, vX3, vX4, vX5 = combo[0][1], combo[1][1], combo[2][1], combo[3][1], combo[4][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:] or ky[-2:] == kX4[-2:] or ky[-2:] == kX5[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, kx4 = {kX4}, kx5 = {kX5}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF5 = evaldataMF5(vX1, vX2, vX3, vX4, vX5, vy, kX1, kX2, kX3, kX4, kX5, ky)\n",
        "      TestLossTotal = resultsMF5[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF5[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF5[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}_{kX2}_{kX3}_{kX4}_{kX5} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF5[\"DATA_X\"].append(kX1 + kX2 + kX3 + kX4 + kX5)\n",
        "    my_dictMF5[\"DATA_y\"].append(ky)\n",
        "    my_dictMF5[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model5.load_weights('model5.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3200432121753694, and Avg Validation Loss (mae) is 1.2342382729053498\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2037676274776459\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9219467461109161, and Avg Validation Loss (mae) is 0.9372364699840545\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9571249604225158\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7435882449150085, and Avg Validation Loss (mae) is 0.8135351479053498\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8201122939586639\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6291895627975463, and Avg Validation Loss (mae) is 0.6603876411914825\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6695978134870529\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5893002390861511, and Avg Validation Loss (mae) is 0.5382329553365708\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5447245329618454\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5967609465122223, and Avg Validation Loss (mae) is 0.7318939983844757\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7403128743171692\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7044037222862244, and Avg Validation Loss (mae) is 0.8264419674873352\n",
            "After 10 runs; Avg Test Loss (mae) is 0.821193951368332\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7297820925712586, and Avg Validation Loss (mae) is 0.6905987650156021\n",
            "After 10 runs; Avg Test Loss (mae) is 0.683251827955246\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6806114912033081, and Avg Validation Loss (mae) is 0.7197974562644959\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7360159456729889\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7271349608898163, and Avg Validation Loss (mae) is 0.766979557275772\n",
            "After 10 runs; Avg Test Loss (mae) is 0.75755635201931\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0511201441287994, and Avg Validation Loss (mae) is 1.2263967156410218\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2143423020839692\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7415220379829407, and Avg Validation Loss (mae) is 0.887241679430008\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8707434564828873\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5892566323280335, and Avg Validation Loss (mae) is 0.5714721411466599\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5860203176736831\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6289970993995666, and Avg Validation Loss (mae) is 0.6880136609077454\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6868196874856949\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6428837180137634, and Avg Validation Loss (mae) is 0.519656366109848\n",
            "After 10 runs; Avg Test Loss (mae) is 0.534953448176384\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5127233475446701, and Avg Validation Loss (mae) is 0.5878366947174072\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5663093835115433\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7636947393417358, and Avg Validation Loss (mae) is 0.6982540309429168\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6690142393112183\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6527710586786271, and Avg Validation Loss (mae) is 0.6780120491981506\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6876676887273788\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6815367221832276, and Avg Validation Loss (mae) is 0.9799261152744293\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9579869002103806\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9970089673995972, and Avg Validation Loss (mae) is 1.1051006674766541\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1161822855472565\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8446315288543701, and Avg Validation Loss (mae) is 1.1291063725948334\n",
            "After 10 runs; Avg Test Loss (mae) is 1.120361989736557\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6739500850439072, and Avg Validation Loss (mae) is 0.5776392370462418\n",
            "After 10 runs; Avg Test Loss (mae) is 0.568505248427391\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7702372670173645, and Avg Validation Loss (mae) is 0.818953275680542\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8254526078701019\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6383229047060013, and Avg Validation Loss (mae) is 0.6737939119338989\n",
            "After 10 runs; Avg Test Loss (mae) is 0.676747339963913\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6617928147315979, and Avg Validation Loss (mae) is 0.6934041470289231\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6876679807901382\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.834026426076889, and Avg Validation Loss (mae) is 0.7968123376369476\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7937827169895172\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6220429569482804, and Avg Validation Loss (mae) is 0.5167291820049286\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5205554097890854\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5408269673585892, and Avg Validation Loss (mae) is 0.5055735021829605\n",
            "After 10 runs; Avg Test Loss (mae) is 0.523774042725563\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7811449289321899, and Avg Validation Loss (mae) is 0.7304211556911469\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7264602363109589\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7049900650978088, and Avg Validation Loss (mae) is 0.897276645898819\n",
            "After 10 runs; Avg Test Loss (mae) is 0.925607168674469\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9785193562507629, and Avg Validation Loss (mae) is 1.0131892621517182\n",
            "After 10 runs; Avg Test Loss (mae) is 1.014307087659836\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5806235909461975, and Avg Validation Loss (mae) is 0.6956115126609802\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6824948072433472\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6447633981704712, and Avg Validation Loss (mae) is 0.6454755425453186\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6572823137044906\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.972747141122818, and Avg Validation Loss (mae) is 1.002729058265686\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0120429813861846\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8628961324691773, and Avg Validation Loss (mae) is 0.9557465612888336\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9572633981704712\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7123929738998414, and Avg Validation Loss (mae) is 0.8504927337169648\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8642674863338471\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0837490379810333, and Avg Validation Loss (mae) is 1.0249719977378846\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0009860038757323\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1633759558200836, and Avg Validation Loss (mae) is 1.1947186827659606\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1708573520183563\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8563571035861969, and Avg Validation Loss (mae) is 0.8029439210891723\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7809270441532135\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0074008703231812, and Avg Validation Loss (mae) is 1.2161241471767426\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2020801961421967\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.739872133731842, and Avg Validation Loss (mae) is 0.7002626717090606\n",
            "After 10 runs; Avg Test Loss (mae) is 0.702626422047615\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5852676212787629, and Avg Validation Loss (mae) is 0.6728052645921707\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6671377897262574\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0450275003910066, and Avg Validation Loss (mae) is 0.9613332927227021\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9748745799064636\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.69008469581604, and Avg Validation Loss (mae) is 0.9093592941761017\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8984594762325286\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4977502375841141, and Avg Validation Loss (mae) is 0.6395107269287109\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6271134555339813\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0484813153743744, and Avg Validation Loss (mae) is 1.078258740901947\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0464823603630067\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6951841413974762, and Avg Validation Loss (mae) is 0.7248370617628097\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7521167784929276\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7823135256767273, and Avg Validation Loss (mae) is 1.0168296098709106\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0150964319705964\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9908586084842682, and Avg Validation Loss (mae) is 1.2813290119171143\n",
            "After 10 runs; Avg Test Loss (mae) is 1.270822960138321\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5637276858091355, and Avg Validation Loss (mae) is 0.6471650630235672\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6529379516839982\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5734357535839081, and Avg Validation Loss (mae) is 0.6599893450737\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6609559535980225\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0174355626106262, and Avg Validation Loss (mae) is 1.048004412651062\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0450845062732697\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6487378001213073, and Avg Validation Loss (mae) is 0.7486970663070679\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7617838025093079\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5943260908126831, and Avg Validation Loss (mae) is 0.51938516497612\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5293367594480515\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1598608493804932, and Avg Validation Loss (mae) is 1.3827145159244538\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3624408543109894\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6566777884960174, and Avg Validation Loss (mae) is 0.6870039016008377\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6996959805488586\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8091870725154877, and Avg Validation Loss (mae) is 0.9663768410682678\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9662828326225281\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3002270221710206, and Avg Validation Loss (mae) is 1.3765116214752198\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3759739756584168\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8024003684520722, and Avg Validation Loss (mae) is 0.9082742631435394\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9113840162754059\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9971939980983734, and Avg Validation Loss (mae) is 0.9184741139411926\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9088384449481964\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7761647284030915, and Avg Validation Loss (mae) is 0.7617583215236664\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7727800667285919\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6666549682617188, and Avg Validation Loss (mae) is 0.6607094675302505\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6652423948049545\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5506611645221711, and Avg Validation Loss (mae) is 0.6347503513097763\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6535855591297149\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7337062537670136, and Avg Validation Loss (mae) is 0.707870364189148\n",
            "After 10 runs; Avg Test Loss (mae) is 0.706611156463623\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7236825793981552, and Avg Validation Loss (mae) is 0.7828900903463364\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7904301673173905\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6701736330986023, and Avg Validation Loss (mae) is 0.7802827090024949\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7731788277626037\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7229247748851776, and Avg Validation Loss (mae) is 0.6242432594299316\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6178241193294525\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9988035202026367, and Avg Validation Loss (mae) is 1.159078848361969\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1804408490657807\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8616952121257782, and Avg Validation Loss (mae) is 0.8854235827922821\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8814655900001526\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7781689703464508, and Avg Validation Loss (mae) is 0.8198851406574249\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8215929210186005\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7524265885353089, and Avg Validation Loss (mae) is 0.646038830280304\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6477336645126343\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5980286419391632, and Avg Validation Loss (mae) is 0.7536452293395997\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7279122561216355\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7829183995723724, and Avg Validation Loss (mae) is 0.8011413156986237\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8085051834583282\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7718131899833679, and Avg Validation Loss (mae) is 0.7173572599887847\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7466978669166565\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5728424489498138, and Avg Validation Loss (mae) is 0.5779311269521713\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5749097615480423\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8092540442943573, and Avg Validation Loss (mae) is 0.782386589050293\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7686871767044068\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7372551739215851, and Avg Validation Loss (mae) is 0.8927127122879028\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8818337202072144\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8152804493904113, and Avg Validation Loss (mae) is 0.9461792171001434\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9446096301078797\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8709723591804505, and Avg Validation Loss (mae) is 0.8192005753517151\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7932888209819794\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6169308841228485, and Avg Validation Loss (mae) is 0.7234485477209092\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7268843621015548\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6550973862409591, and Avg Validation Loss (mae) is 0.8089237868785858\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8176390022039414\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8099449276924133, and Avg Validation Loss (mae) is 1.0013423323631288\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0064898550510406\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6361641347408294, and Avg Validation Loss (mae) is 0.7639622002840042\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7721850126981735\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6243634283542633, and Avg Validation Loss (mae) is 0.569554191827774\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5722467541694641\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8049218773841857, and Avg Validation Loss (mae) is 0.8387478470802308\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8562291860580444\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6871269166469574, and Avg Validation Loss (mae) is 0.5194786489009857\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5194498151540756\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7119284212589264, and Avg Validation Loss (mae) is 0.8656939476728439\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8885425060987473\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7750816464424133, and Avg Validation Loss (mae) is 0.8373150944709777\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8483352661132812\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6483899086713791, and Avg Validation Loss (mae) is 0.6159198641777038\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6184046119451523\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.754954320192337, and Avg Validation Loss (mae) is 0.871948653459549\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8569511890411377\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8364180028438568, and Avg Validation Loss (mae) is 0.770922714471817\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7923787593841553\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9488189935684204, and Avg Validation Loss (mae) is 0.940294373035431\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9262053310871124\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7074283003807068, and Avg Validation Loss (mae) is 0.6572691857814789\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6493793308734894\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7827467679977417, and Avg Validation Loss (mae) is 0.7239248096942902\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7077098965644837\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0228658258914947, and Avg Validation Loss (mae) is 0.9593535602092743\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9206148207187652\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5197433084249496, and Avg Validation Loss (mae) is 0.8216166913509368\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8107947200536728\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7599138975143432, and Avg Validation Loss (mae) is 0.6754622161388397\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6850207865238189\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1854864835739136, and Avg Validation Loss (mae) is 1.0213437139987946\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0387152969837188\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7914988994598389, and Avg Validation Loss (mae) is 0.8807191550731659\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8835650324821472\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7958835542201996, and Avg Validation Loss (mae) is 0.8848042726516724\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9041616141796112\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0366641581058502, and Avg Validation Loss (mae) is 1.1084493160247804\n",
            "After 10 runs; Avg Test Loss (mae) is 1.08059783577919\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7426836311817169, and Avg Validation Loss (mae) is 0.7869915038347244\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8019292294979096\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.897613775730133, and Avg Validation Loss (mae) is 1.0220877408981324\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0283564507961274\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.236196720600128, and Avg Validation Loss (mae) is 1.3365966439247132\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3179348945617675\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8508919060230256, and Avg Validation Loss (mae) is 0.878921702504158\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8716294556856156\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5335955500602723, and Avg Validation Loss (mae) is 1.759936797618866\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7186411261558532\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5402106463909149, and Avg Validation Loss (mae) is 0.5993800342082978\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5920722365379334\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6287408649921418, and Avg Validation Loss (mae) is 0.6270804882049561\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6154503554105759\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5556188941001892, and Avg Validation Loss (mae) is 1.6235225558280946\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6201168656349183\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8530597627162934, and Avg Validation Loss (mae) is 0.907095217704773\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9067461341619492\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6794235646724701, and Avg Validation Loss (mae) is 0.6423286378383637\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6490008056163787\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.500529158115387, and Avg Validation Loss (mae) is 1.4534944295883179\n",
            "After 10 runs; Avg Test Loss (mae) is 1.399537777900696\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0989846289157867, and Avg Validation Loss (mae) is 1.0467086732387543\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0289480268955231\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7459596455097198, and Avg Validation Loss (mae) is 0.7690590888261795\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7758831471204758\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5731669187545776, and Avg Validation Loss (mae) is 1.7164541721343993\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7355593681335448\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7996826171875, and Avg Validation Loss (mae) is 0.7375070750713348\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7499327600002289\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6182416200637817, and Avg Validation Loss (mae) is 0.6144275784492492\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6175020217895508\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4826111793518066, and Avg Validation Loss (mae) is 1.5577282428741455\n",
            "After 10 runs; Avg Test Loss (mae) is 1.58767329454422\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.756538587808609, and Avg Validation Loss (mae) is 1.0155574440956117\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0351037502288818\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5945754617452621, and Avg Validation Loss (mae) is 0.7251329630613327\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7218288630247116\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5297208428382874, and Avg Validation Loss (mae) is 1.502449381351471\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5494090914726257\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7348233759403229, and Avg Validation Loss (mae) is 0.6961371928453446\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7006945908069611\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6158488094806671, and Avg Validation Loss (mae) is 0.6209985733032226\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6342916995286941\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.619351291656494, and Avg Validation Loss (mae) is 1.7424392819404602\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7157914996147157\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5035817563533783, and Avg Validation Loss (mae) is 0.6043847918510437\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6036746889352799\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5785094708204269, and Avg Validation Loss (mae) is 0.5940567523241043\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6077757924795151\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.591105008125305, and Avg Validation Loss (mae) is 1.8386289715766906\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8535438537597657\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6011777967214584, and Avg Validation Loss (mae) is 0.6985482960939408\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6805864214897156\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6809480875730515, and Avg Validation Loss (mae) is 0.6911576867103577\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7083380967378616\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5699546098709107, and Avg Validation Loss (mae) is 1.7966354727745055\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7757896661758423\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.559338527917862, and Avg Validation Loss (mae) is 0.6555840581655502\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6607230365276336\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7079955577850342, and Avg Validation Loss (mae) is 0.6392769277095794\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6182780176401138\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4109760165214538, and Avg Validation Loss (mae) is 1.4246920943260193\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4335633397102356\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7125426918268204, and Avg Validation Loss (mae) is 0.5828006446361542\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5705945581197739\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7649656713008881, and Avg Validation Loss (mae) is 0.8143133819103241\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8212356626987457\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.463207471370697, and Avg Validation Loss (mae) is 1.650541365146637\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6422390937805176\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0196340262889863, and Avg Validation Loss (mae) is 1.0279547333717347\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0263186037540435\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5899729251861572, and Avg Validation Loss (mae) is 0.6237943798303605\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6263241797685624\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.492358958721161, and Avg Validation Loss (mae) is 1.537666881084442\n",
            "After 10 runs; Avg Test Loss (mae) is 1.521420168876648\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0222213447093964, and Avg Validation Loss (mae) is 0.8604043185710907\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8444447696208954\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5283860862255096, and Avg Validation Loss (mae) is 0.6099665820598602\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6083805859088898\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4946648240089417, and Avg Validation Loss (mae) is 1.506151306629181\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5142107844352721\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0050821602344513, and Avg Validation Loss (mae) is 1.1856746792793273\n",
            "After 10 runs; Avg Test Loss (mae) is 1.195470243692398\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7749391198158264, and Avg Validation Loss (mae) is 0.87497219145298\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8761414736509323\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4404402375221252, and Avg Validation Loss (mae) is 1.44171804189682\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3955662369728088\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0423363864421844, and Avg Validation Loss (mae) is 1.0353708803653716\n",
            "After 10 runs; Avg Test Loss (mae) is 1.010916829109192\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.748887550830841, and Avg Validation Loss (mae) is 0.738764888048172\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7412774384021759\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5782134294509889, and Avg Validation Loss (mae) is 1.650182342529297\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6154344439506532\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.17463059425354, and Avg Validation Loss (mae) is 1.1448150992393493\n",
            "After 10 runs; Avg Test Loss (mae) is 1.135063773393631\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6037672311067581, and Avg Validation Loss (mae) is 0.6775905966758728\n",
            "After 10 runs; Avg Test Loss (mae) is 0.673886439204216\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8513050198554992, and Avg Validation Loss (mae) is 1.8867359161376953\n",
            "After 10 runs; Avg Test Loss (mae) is 1.873452115058899\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9214436292648316, and Avg Validation Loss (mae) is 0.9255023717880249\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9040434122085571\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7293478727340699, and Avg Validation Loss (mae) is 0.8340410351753235\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8287821233272552\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7151884078979491, and Avg Validation Loss (mae) is 1.8381817102432252\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8716179132461548\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.819833368062973, and Avg Validation Loss (mae) is 0.7569329380989075\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7647003471851349\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.49709696173667905, and Avg Validation Loss (mae) is 0.5132486939430236\n",
            "After 10 runs; Avg Test Loss (mae) is 0.506665951013565\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7943065643310547, and Avg Validation Loss (mae) is 1.6807139039039611\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6932968735694884\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8070100843906403, and Avg Validation Loss (mae) is 0.9128987550735473\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9362431168556213\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8443661689758301, and Avg Validation Loss (mae) is 0.8755082815885544\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8739004045724869\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6679284572601318, and Avg Validation Loss (mae) is 1.6506307005882264\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6803820610046387\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8063002765178681, and Avg Validation Loss (mae) is 0.7678840160369873\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7744557738304139\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.741264808177948, and Avg Validation Loss (mae) is 0.8384483218193054\n",
            "After 10 runs; Avg Test Loss (mae) is 0.821572208404541\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6548907279968261, and Avg Validation Loss (mae) is 1.6128825545310974\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5987466335296632\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8814609050750732, and Avg Validation Loss (mae) is 1.071738713979721\n",
            "After 10 runs; Avg Test Loss (mae) is 1.059808224439621\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5669212758541107, and Avg Validation Loss (mae) is 0.5826317697763443\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5817748755216599\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6506418943405152, and Avg Validation Loss (mae) is 1.7321259140968324\n",
            "After 10 runs; Avg Test Loss (mae) is 1.768338131904602\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8099815607070923, and Avg Validation Loss (mae) is 0.8023740768432617\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8077674567699432\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.005869245529175, and Avg Validation Loss (mae) is 0.9793323338031769\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9966343402862549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ULPDHbotnKVr",
        "outputId": "76fdc5ef-a0fc-4e43-87c1-e33d8fe33ad8"
      },
      "source": [
        "CombResultsMF5 = pd.DataFrame.from_dict(my_dictMF5)\n",
        "print(CombResultsMF5.shape)\n",
        "CombResultsSortedMF5 = CombResultsMF5.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMF5.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA_X</th>\n",
              "      <th>DATA_y</th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>X_MTX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.506666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>X_FTX_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.519450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.520555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>X_FTX_FMX_MTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.523774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>X_FTX_FMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.529337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.534953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.544725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.566309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.568505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>X_FMX_MTX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.570595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>X_FTX_MTX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.572247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>X_FTX_MTX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.574910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>X_MTX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.581775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.586020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.592072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>X_FMX_MTX_MBX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.603675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>X_FMX_MTX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.607776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>X_FMX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.608381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.615450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>X_FMX_MTX_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.617502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.617824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>X_FMX_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.618278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>X_FTX_MTX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.618405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>X_FMX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.626324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>X_FTX_FMX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.627113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>X_FMX_MTX_MMX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.634292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>X_FTX_MTX_MMX_RTX_RM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.647734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.649001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>X_FTX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.649379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>X_FTX_FMX_MBX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.652938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.653586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.657282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>X_FMX_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.660723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>X_FTX_FMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.660956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.665242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>X_FTX_FMX_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.667138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.669014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RT</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.669598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>X_FMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.673886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.676747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>X_FMX_MTX_MBX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.680586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.682495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.683252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>X_FTX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.685021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.686820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.687668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.687668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>X_FTX_FMX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.699696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>X_FMX_MTX_MMX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.700695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>X_FTX_FMX_MMX_RTX_RM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.702626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   DATA_X DATA_y  Test Loss\n",
              "155  X_MTX_MMX_MBX_RTX_RB   y_RM   0.506666\n",
              "85   X_FTX_MTX_MBX_RMX_RB   y_MM   0.519450\n",
              "26   X_FTX_FMX_MTX_RTX_RB   y_RM   0.520555\n",
              "27   X_FTX_FMX_MTX_RMX_RB   y_MM   0.523774\n",
              "53   X_FTX_FMX_MBX_RTX_RB   y_RM   0.529337\n",
              "14   X_FTX_FMX_MTX_MBX_RT   y_RB   0.534953\n",
              "4    X_FTX_FMX_MTX_MMX_RT   y_RM   0.544725\n",
              "15   X_FTX_FMX_MTX_MBX_RM   y_MM   0.566309\n",
              "21   X_FTX_FMX_MTX_RTX_RM   y_MM   0.568505\n",
              "133  X_FMX_MTX_RTX_RMX_RB   y_MM   0.570595\n",
              "83   X_FTX_MTX_MBX_RTX_RB   y_RM   0.572247\n",
              "74   X_FTX_MTX_MMX_RTX_RB   y_RM   0.574910\n",
              "164  X_MTX_MBX_RTX_RMX_RB   y_MM   0.581775\n",
              "12   X_FTX_FMX_MTX_MBX_RT   y_MM   0.586020\n",
              "106  X_FMX_MTX_MMX_MBX_RT   y_RM   0.592072\n",
              "124  X_FMX_MTX_MBX_RTX_RM   y_MM   0.603675\n",
              "125  X_FMX_MTX_MBX_RTX_RM   y_RB   0.607776\n",
              "140  X_FMX_MMX_MBX_RTX_RB   y_RM   0.608381\n",
              "107  X_FMX_MTX_MMX_MBX_RT   y_RB   0.615450\n",
              "116  X_FMX_MTX_MMX_RTX_RM   y_RB   0.617502\n",
              "66   X_FTX_MTX_MMX_MBX_RB   y_FM   0.617824\n",
              "131  X_FMX_MTX_MBX_RMX_RB   y_RT   0.618278\n",
              "88   X_FTX_MTX_RTX_RMX_RB   y_MM   0.618405\n",
              "137  X_FMX_MMX_MBX_RTX_RM   y_RB   0.626324\n",
              "44   X_FTX_FMX_MMX_RTX_RB   y_RM   0.627113\n",
              "122  X_FMX_MTX_MMX_RMX_RB   y_RT   0.634292\n",
              "70   X_FTX_MTX_MMX_RTX_RM   y_MB   0.647734\n",
              "110  X_FMX_MTX_MMX_MBX_RM   y_RB   0.649001\n",
              "92   X_FTX_MMX_MBX_RTX_RM   y_RB   0.649379\n",
              "49   X_FTX_FMX_MBX_RTX_RM   y_MM   0.652938\n",
              "62   X_FTX_MTX_MMX_MBX_RT   y_RB   0.653586\n",
              "32   X_FTX_FMX_MMX_MBX_RT   y_RB   0.657282\n",
              "130  X_FMX_MTX_MBX_RMX_RB   y_MM   0.660723\n",
              "50   X_FTX_FMX_MBX_RTX_RM   y_RB   0.660956\n",
              "61   X_FTX_MTX_MMX_MBX_RT   y_RM   0.665242\n",
              "41   X_FTX_FMX_MMX_RTX_RM   y_RB   0.667138\n",
              "16   X_FTX_FMX_MTX_MBX_RM   y_RT   0.669014\n",
              "3    X_FTX_FMX_MTX_MMX_RT   y_MB   0.669598\n",
              "149  X_FMX_MBX_RTX_RMX_RB   y_MM   0.673886\n",
              "23   X_FTX_FMX_MTX_RTX_RM   y_RB   0.676747\n",
              "127  X_FMX_MTX_MBX_RTX_RB   y_MM   0.680586\n",
              "31   X_FTX_FMX_MMX_MBX_RT   y_RM   0.682495\n",
              "7    X_FTX_FMX_MTX_MMX_RM   y_RT   0.683252\n",
              "96   X_FTX_MMX_MBX_RMX_RB   y_FM   0.685021\n",
              "13   X_FTX_FMX_MTX_MBX_RT   y_RM   0.686820\n",
              "17   X_FTX_FMX_MTX_MBX_RM   y_RB   0.687668\n",
              "24   X_FTX_FMX_MTX_RTX_RB   y_MM   0.687668\n",
              "55   X_FTX_FMX_MBX_RMX_RB   y_MM   0.699696\n",
              "121  X_FMX_MTX_MMX_RMX_RB   y_MB   0.700695\n",
              "40   X_FTX_FMX_MMX_RTX_RM   y_MB   0.702626"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LxD7gkY-nKVr",
        "outputId": "3816011d-5057-49de-d33e-42e2dde37061"
      },
      "source": [
        "CombResultsSortedMFgrouped5 = CombResultsSortedMF5.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedMFgroupedsortedMF5 = CombResultsSortedMFgrouped5.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMFgroupedsortedMF5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RT</th>\n",
              "      <td>0.602598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RM</th>\n",
              "      <td>0.640997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RT</th>\n",
              "      <td>0.651545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RTX_RB</th>\n",
              "      <td>0.667335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RTX_RM</th>\n",
              "      <td>0.690235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RT</th>\n",
              "      <td>0.697203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RTX_RB</th>\n",
              "      <td>0.710038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RMX_RB</th>\n",
              "      <td>0.725280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RTX_RM</th>\n",
              "      <td>0.732413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RM</th>\n",
              "      <td>0.746821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RMX_RB</th>\n",
              "      <td>0.754741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RM</th>\n",
              "      <td>0.756740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RTX_RMX_RB</th>\n",
              "      <td>0.774564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RTX_RB</th>\n",
              "      <td>0.778735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RTX_RM</th>\n",
              "      <td>0.779271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RTX_RB</th>\n",
              "      <td>0.783641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RT</th>\n",
              "      <td>0.784695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>0.789321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.813040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RTX_RB</th>\n",
              "      <td>0.833483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RTX_RM</th>\n",
              "      <td>0.857281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RTX_RM</th>\n",
              "      <td>0.861572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RMX_RB</th>\n",
              "      <td>0.865044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>0.869100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RB</th>\n",
              "      <td>0.893244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>0.928896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RMX_RB</th>\n",
              "      <td>0.937899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RTX_RMX_RB</th>\n",
              "      <td>0.941798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RM</th>\n",
              "      <td>0.944525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RB</th>\n",
              "      <td>0.947547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RMX_RB</th>\n",
              "      <td>0.961465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RT</th>\n",
              "      <td>0.975388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RTX_RM</th>\n",
              "      <td>0.975747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RB</th>\n",
              "      <td>0.984257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.991415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MB</th>\n",
              "      <td>0.993668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RMX_RB</th>\n",
              "      <td>1.009473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RMX_RB</th>\n",
              "      <td>1.018264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RTX_RM</th>\n",
              "      <td>1.034331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>1.047661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RTX_RMX_RB</th>\n",
              "      <td>1.049254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RM</th>\n",
              "      <td>1.058621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RB</th>\n",
              "      <td>1.064844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RTX_RMX_RB</th>\n",
              "      <td>1.065399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RB</th>\n",
              "      <td>1.068123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.072640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.080110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RTX_RB</th>\n",
              "      <td>1.080823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>1.092137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RTX_RM</th>\n",
              "      <td>1.098294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RTX_RB</th>\n",
              "      <td>1.114869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.141462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>1.167813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.190913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RMX_RB</th>\n",
              "      <td>1.195274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>1.202093</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Test Loss\n",
              "DATA_X                         \n",
              "X_FTX_FMX_MTX_MBX_RT   0.602598\n",
              "X_FTX_FMX_MTX_MBX_RM   0.640997\n",
              "X_FTX_FMX_MTX_MMX_RT   0.651545\n",
              "X_FTX_FMX_MTX_RTX_RB   0.667335\n",
              "X_FTX_FMX_MTX_RTX_RM   0.690235\n",
              "X_FTX_MTX_MMX_MBX_RT   0.697203\n",
              "X_FTX_MTX_MMX_RTX_RB   0.710038\n",
              "X_FTX_FMX_MTX_RMX_RB   0.725280\n",
              "X_FTX_MTX_MMX_RTX_RM   0.732413\n",
              "X_FTX_FMX_MTX_MMX_RM   0.746821\n",
              "X_FTX_MTX_MBX_RMX_RB   0.754741\n",
              "X_FTX_MTX_MMX_MBX_RM   0.756740\n",
              "X_FTX_MTX_RTX_RMX_RB   0.774564\n",
              "X_FTX_FMX_MBX_RTX_RB   0.778735\n",
              "X_FTX_MTX_MBX_RTX_RM   0.779271\n",
              "X_FTX_MTX_MBX_RTX_RB   0.783641\n",
              "X_FTX_FMX_MMX_MBX_RT   0.784695\n",
              "X_FTX_MMX_MBX_RTX_RM   0.789321\n",
              "X_FTX_MMX_MBX_RTX_RB   0.813040\n",
              "X_FTX_FMX_MMX_RTX_RB   0.833483\n",
              "X_FTX_FMX_MMX_RTX_RM   0.857281\n",
              "X_FTX_FMX_MBX_RTX_RM   0.861572\n",
              "X_FTX_MTX_MMX_RMX_RB   0.865044\n",
              "X_FTX_MMX_MBX_RMX_RB   0.869100\n",
              "X_FTX_MTX_MMX_MBX_RB   0.893244\n",
              "X_FTX_MMX_RTX_RMX_RB   0.928896\n",
              "X_FTX_FMX_MMX_RMX_RB   0.937899\n",
              "X_FMX_MTX_RTX_RMX_RB   0.941798\n",
              "X_FTX_FMX_MMX_MBX_RM   0.944525\n",
              "X_FTX_FMX_MTX_MMX_RB   0.947547\n",
              "X_FMX_MTX_MMX_RMX_RB   0.961465\n",
              "X_FMX_MTX_MMX_MBX_RT   0.975388\n",
              "X_FMX_MTX_MBX_RTX_RM   0.975747\n",
              "X_FTX_FMX_MMX_MBX_RB   0.984257\n",
              "X_FMX_MMX_MBX_RTX_RB   0.991415\n",
              "X_FTX_FMX_MTX_MMX_MB   0.993668\n",
              "X_FTX_FMX_MBX_RMX_RB   1.009473\n",
              "X_FMX_MTX_MBX_RMX_RB   1.018264\n",
              "X_FMX_MTX_MMX_RTX_RM   1.034331\n",
              "X_MTX_MMX_MBX_RTX_RB   1.047661\n",
              "X_FMX_MMX_RTX_RMX_RB   1.049254\n",
              "X_FMX_MTX_MMX_MBX_RM   1.058621\n",
              "X_FTX_FMX_MTX_MBX_RB   1.064844\n",
              "X_FTX_FMX_RTX_RMX_RB   1.065399\n",
              "X_FMX_MTX_MMX_MBX_RB   1.068123\n",
              "X_FTX_MBX_RTX_RMX_RB   1.072640\n",
              "X_MTX_MBX_RTX_RMX_RB   1.080110\n",
              "X_FMX_MTX_MBX_RTX_RB   1.080823\n",
              "X_MTX_MMX_RTX_RMX_RB   1.092137\n",
              "X_FMX_MMX_MBX_RTX_RM   1.098294\n",
              "X_FMX_MTX_MMX_RTX_RB   1.114869\n",
              "X_FMX_MBX_RTX_RMX_RB   1.141462\n",
              "X_MTX_MMX_MBX_RMX_RB   1.167813\n",
              "X_MMX_MBX_RTX_RMX_RB   1.190913\n",
              "X_FMX_MMX_MBX_RMX_RB   1.195274\n",
              "X_MTX_MMX_MBX_RTX_RM   1.202093"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "oQooIBxSnKVr",
        "outputId": "f908990f-a70f-41d9-ee9e-205e3a9a84d0"
      },
      "source": [
        "CombResultsSortedMFgroupedsortedMF5.to_csv('CombResultsSortedMFgroupedsortedMF5.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedMFgroupedsortedMF5.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_451b5671-912a-4602-a2bd-cadd82be08e0\", \"CombResultsSortedMFgroupedsortedMF5.csv\", 2246)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "wo3dCd9L9zyI",
        "outputId": "479080a9-6bff-4659-b35b-82a266fa3a54"
      },
      "source": [
        "CombResultsSortedMF5.to_csv('CombResultsSortedMF5.csv')\n",
        "files.download(\"CombResultsSortedMF5.csv\")\n",
        "\n",
        "fig = px.box(CombResultsSortedMF5, x=\"DATA_X\", y=\"Test Loss\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_732c9904-c954-42b8-975b-45366b7bc981\", \"CombResultsSortedMF5.csv\", 8118)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9691068b-d695-4f4b-95ed-3c881b9f53e5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9691068b-d695-4f4b-95ed-3c881b9f53e5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9691068b-d695-4f4b-95ed-3c881b9f53e5',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_MTX_MMX_MBX_RTX_RB\", \"X_FTX_MTX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_RTX_RB\", \"X_FTX_FMX_MTX_RMX_RB\", \"X_FTX_FMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MBX_RT\", \"X_FTX_FMX_MTX_MMX_RT\", \"X_FTX_FMX_MTX_MBX_RM\", \"X_FTX_FMX_MTX_RTX_RM\", \"X_FMX_MTX_RTX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_RTX_RB\", \"X_MTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RT\", \"X_FMX_MTX_MMX_MBX_RT\", \"X_FMX_MTX_MBX_RTX_RM\", \"X_FMX_MTX_MBX_RTX_RM\", \"X_FMX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_MBX_RT\", \"X_FMX_MTX_MMX_RTX_RM\", \"X_FTX_MTX_MMX_MBX_RB\", \"X_FMX_MTX_MBX_RMX_RB\", \"X_FTX_MTX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MMX_RTX_RB\", \"X_FMX_MTX_MMX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RM\", \"X_FMX_MTX_MMX_MBX_RM\", \"X_FTX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MMX_MBX_RT\", \"X_FMX_MTX_MBX_RMX_RB\", \"X_FTX_FMX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RM\", \"X_FTX_FMX_MTX_MMX_RT\", \"X_FMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_RTX_RM\", \"X_FMX_MTX_MBX_RTX_RB\", \"X_FTX_FMX_MMX_MBX_RT\", \"X_FTX_FMX_MTX_MMX_RM\", \"X_FTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RT\", \"X_FTX_FMX_MTX_MBX_RM\", \"X_FTX_FMX_MTX_RTX_RB\", \"X_FTX_FMX_MBX_RMX_RB\", \"X_FMX_MTX_MMX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RM\", \"X_FTX_MTX_MMX_MBX_RM\", \"X_FTX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RTX_RB\", \"X_FTX_FMX_MTX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RM\", \"X_FTX_FMX_MTX_MMX_RT\", \"X_FMX_MMX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RB\", \"X_FMX_MTX_MMX_RTX_RM\", \"X_FTX_FMX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_RB\", \"X_FTX_FMX_MBX_RTX_RB\", \"X_MTX_MMX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_MBX_RT\", \"X_FTX_MTX_MMX_MBX_RM\", \"X_MTX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RB\", \"X_FTX_FMX_MMX_MBX_RB\", \"X_FTX_MTX_MMX_MBX_RM\", \"X_FTX_MMX_MBX_RTX_RM\", \"X_FTX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_RTX_RB\", \"X_FTX_MMX_RTX_RMX_RB\", \"X_MMX_MBX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RB\", \"X_FTX_MMX_MBX_RTX_RB\", \"X_FTX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_MB\", \"X_FTX_FMX_MTX_MMX_RM\", \"X_FMX_MTX_RTX_RMX_RB\", \"X_MTX_MMX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_RTX_RM\", \"X_MTX_MMX_MBX_RTX_RM\", \"X_FMX_MMX_MBX_RTX_RB\", \"X_FTX_MTX_RTX_RMX_RB\", \"X_FTX_MTX_MBX_RMX_RB\", \"X_FTX_MTX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_MMX_RB\", \"X_FTX_MBX_RTX_RMX_RB\", \"X_MTX_MMX_MBX_RMX_RB\", \"X_FMX_MMX_MBX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RB\", \"X_FTX_MTX_MMX_RMX_RB\", \"X_FTX_MMX_MBX_RMX_RB\", \"X_FTX_MTX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RB\", \"X_MTX_MMX_MBX_RTX_RM\", \"X_FTX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RM\", \"X_FTX_FMX_RTX_RMX_RB\", \"X_FTX_FMX_RTX_RMX_RB\", \"X_FTX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_RMX_RB\", \"X_FTX_MMX_MBX_RTX_RM\", \"X_MTX_MMX_MBX_RMX_RB\", \"X_FTX_MTX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_MB\", \"X_FTX_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_MBX_RB\", \"X_FTX_FMX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RB\", \"X_MMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RB\", \"X_FTX_MTX_MBX_RTX_RB\", \"X_FMX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MMX_MBX_RT\", \"X_FTX_FMX_MMX_RMX_RB\", \"X_FMX_MMX_MBX_RTX_RM\", \"X_FTX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RB\", \"X_FMX_MTX_MMX_RTX_RB\", \"X_FTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MBX_RTX_RB\", \"X_FTX_FMX_MMX_RMX_RB\", \"X_MTX_MBX_RTX_RMX_RB\", \"X_FTX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RB\", \"X_FTX_FMX_MTX_MBX_RB\", \"X_FMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RB\", \"X_FTX_MTX_MMX_MBX_RB\", \"X_FMX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_MB\", \"X_FTX_FMX_MTX_MMX_RB\", \"X_FTX_FMX_MBX_RTX_RM\", \"X_FTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MBX_RMX_RB\", \"X_FTX_FMX_RTX_RMX_RB\", \"X_FMX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RB\", \"X_FMX_MTX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RMX_RB\", \"X_FMX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RMX_RB\", \"X_FMX_MTX_MMX_RTX_RB\", \"X_MTX_MBX_RTX_RMX_RB\", \"X_FMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RM\", \"X_FMX_MMX_MBX_RTX_RM\", \"X_MTX_MMX_RTX_RMX_RB\", \"X_MTX_MMX_MBX_RMX_RB\", \"X_FMX_MTX_MBX_RTX_RM\", \"X_FMX_MTX_MMX_MBX_RT\", \"X_FMX_MTX_MMX_RTX_RM\", \"X_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MBX_RMX_RB\", \"X_FMX_MTX_MBX_RTX_RB\", \"X_MTX_MMX_MBX_RTX_RB\", \"X_MTX_MMX_MBX_RTX_RM\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.506665951013565, 0.5194498151540756, 0.5205554097890854, 0.523774042725563, 0.5293367594480515, 0.534953448176384, 0.5447245329618454, 0.5663093835115433, 0.568505248427391, 0.5705945581197739, 0.5722467541694641, 0.5749097615480423, 0.5817748755216599, 0.5860203176736831, 0.5920722365379334, 0.6036746889352799, 0.6077757924795151, 0.6083805859088898, 0.6154503554105759, 0.6175020217895508, 0.6178241193294525, 0.6182780176401138, 0.6184046119451523, 0.6263241797685624, 0.6271134555339813, 0.6342916995286941, 0.6477336645126343, 0.6490008056163787, 0.6493793308734894, 0.6529379516839982, 0.6535855591297149, 0.6572823137044906, 0.6607230365276336, 0.6609559535980225, 0.6652423948049545, 0.6671377897262574, 0.6690142393112183, 0.6695978134870529, 0.673886439204216, 0.676747339963913, 0.6805864214897156, 0.6824948072433472, 0.683251827955246, 0.6850207865238189, 0.6868196874856949, 0.6876676887273788, 0.6876679807901382, 0.6996959805488586, 0.7006945908069611, 0.702626422047615, 0.706611156463623, 0.7077098965644837, 0.7083380967378616, 0.7218288630247116, 0.7264602363109589, 0.7268843621015548, 0.7279122561216355, 0.7360159456729889, 0.7403128743171692, 0.7412774384021759, 0.7466978669166565, 0.7499327600002289, 0.7521167784929276, 0.75755635201931, 0.7617838025093079, 0.7647003471851349, 0.7686871767044068, 0.7721850126981735, 0.7727800667285919, 0.7731788277626037, 0.7744557738304139, 0.7758831471204758, 0.7809270441532135, 0.7904301673173905, 0.7923787593841553, 0.7932888209819794, 0.7937827169895172, 0.8019292294979096, 0.8077674567699432, 0.8085051834583282, 0.8107947200536728, 0.8176390022039414, 0.8201122939586639, 0.821193951368332, 0.8212356626987457, 0.821572208404541, 0.8215929210186005, 0.8254526078701019, 0.8287821233272552, 0.8444447696208954, 0.8483352661132812, 0.8562291860580444, 0.8569511890411377, 0.8642674863338471, 0.8707434564828873, 0.8716294556856156, 0.8739004045724869, 0.8761414736509323, 0.8814655900001526, 0.8818337202072144, 0.8835650324821472, 0.8885425060987473, 0.8984594762325286, 0.9040434122085571, 0.9041616141796112, 0.9067461341619492, 0.9088384449481964, 0.9113840162754059, 0.9206148207187652, 0.925607168674469, 0.9262053310871124, 0.9362431168556213, 0.9446096301078797, 0.9571249604225158, 0.9572633981704712, 0.9579869002103806, 0.9662828326225281, 0.9748745799064636, 0.9966343402862549, 1.0009860038757323, 1.0064898550510406, 1.010916829109192, 1.0120429813861846, 1.014307087659836, 1.0150964319705964, 1.0263186037540435, 1.0283564507961274, 1.0289480268955231, 1.0351037502288818, 1.0387152969837188, 1.0450845062732697, 1.0464823603630067, 1.059808224439621, 1.08059783577919, 1.1161822855472565, 1.120361989736557, 1.135063773393631, 1.1708573520183563, 1.1804408490657807, 1.195470243692398, 1.2020801961421967, 1.2037676274776459, 1.2143423020839692, 1.270822960138321, 1.3179348945617675, 1.3624408543109894, 1.3759739756584168, 1.3955662369728088, 1.399537777900696, 1.4335633397102356, 1.5142107844352721, 1.521420168876648, 1.5494090914726257, 1.58767329454422, 1.5987466335296632, 1.6154344439506532, 1.6201168656349183, 1.6422390937805176, 1.6803820610046387, 1.6932968735694884, 1.7157914996147157, 1.7186411261558532, 1.7355593681335448, 1.768338131904602, 1.7757896661758423, 1.8535438537597657, 1.8716179132461548, 1.873452115058899], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9691068b-d695-4f4b-95ed-3c881b9f53e5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71S6Zan390mE"
      },
      "source": [
        "# define baseline model 6\n",
        "# create model\n",
        "n_features = 1\n",
        "model6 = Sequential()\n",
        "model6.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(42, n_features)))\n",
        "model6.add(MaxPooling1D(pool_size=2))\n",
        "model6.add(Flatten())\n",
        "model6.add(Dense(50, activation='relu'))\n",
        "model6.add(Dense(1))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "model6.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "#model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdjGNlaj90mE"
      },
      "source": [
        "n_features = 1\n",
        "def datageneratorMF6(X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, Y_in):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  X_in = np.concatenate((X_in1, X_in2, X_in3, X_in4, X_in5, X_in6), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "\n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], n_features))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], n_features))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], n_features))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bulC2l-T90mE"
      },
      "source": [
        "def evaldataMF6(X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, Y_in, traindata1, traindata2, traindata3, traindata4, traindata5, traindata6, testdata):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMF6(X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, Y_in)\n",
        "  \n",
        "  history = model6.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model6.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model6.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # PLOTS LOSS VS EPOCH\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  # print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  # print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # PLOTS Y ORIGINAL VS PREDICTED\n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata1)+ str(traindata2)  + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywYKZ43D90mF",
        "outputId": "8124168a-0d0b-4e8e-8dc5-51b9fb6d0ab0"
      },
      "source": [
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "\n",
        "model6.save_weights('model6.h5')\n",
        "\n",
        "my_dictMF6 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), 6):\n",
        "  kX1, kX2, kX3, kX4, kX5, kX6 = combo[0][0], combo[1][0], combo[2][0], combo[3][0], combo[4][0], combo[5][0]\n",
        "  vX1, vX2, vX3, vX4, vX5, vX6 = combo[0][1], combo[1][1], combo[2][1], combo[3][1], combo[4][1], combo[5][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:] or ky[-2:] == kX4[-2:] or ky[-2:] == kX5[-2:] or ky[-2:] == kX6[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, kx4 = {kX4}, kx5 = {kX5}, kx6 = {kX6}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF6 = evaldataMF6(vX1, vX2, vX3, vX4, vX5, vX6, vy, kX1, kX2, kX3, kX4, kX5, kX6, ky)\n",
        "      TestLossTotal = resultsMF6[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF6[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF6[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}_{kX2}_{kX3}_{kX4}_{kX5}_{kX6} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF6[\"DATA_X\"].append(kX1 + kX2 + kX3 + kX4 + kX5 + kX6)\n",
        "    my_dictMF6[\"DATA_y\"].append(ky)\n",
        "    my_dictMF6[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model6.load_weights('model6.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.77279734313488, and Avg Validation Loss (mae) is 0.6670779943466186\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6719517081975936\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6411697924137115, and Avg Validation Loss (mae) is 0.561491721868515\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5607773184776306\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8216216802597046, and Avg Validation Loss (mae) is 0.843624472618103\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8798842549324035\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7133135974407196, and Avg Validation Loss (mae) is 0.6015431493520736\n",
            "After 10 runs; Avg Test Loss (mae) is 0.584307712316513\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0985134601593018, and Avg Validation Loss (mae) is 1.2214823961257935\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2426615357398987\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8407521665096283, and Avg Validation Loss (mae) is 0.6914427727460861\n",
            "After 10 runs; Avg Test Loss (mae) is 0.688959801197052\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7042907297611236, and Avg Validation Loss (mae) is 0.8229497313499451\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8333911240100861\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5365205526351928, and Avg Validation Loss (mae) is 0.6187895864248276\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6048680394887924\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.719013112783432, and Avg Validation Loss (mae) is 0.6615360707044602\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6645403385162354\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5056878238916397, and Avg Validation Loss (mae) is 0.6217894077301025\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6046994656324387\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7160328686237335, and Avg Validation Loss (mae) is 0.7146202981472015\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7189769119024276\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6833497524261475, and Avg Validation Loss (mae) is 0.6246087193489075\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6233897715806961\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5855263382196426, and Avg Validation Loss (mae) is 0.5008409351110459\n",
            "After 10 runs; Avg Test Loss (mae) is 0.4961110830307007\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7384768307209015, and Avg Validation Loss (mae) is 0.5418456763029098\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5217514365911484\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5863152951002121, and Avg Validation Loss (mae) is 0.7230722963809967\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7230144262313842\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5955146819353103, and Avg Validation Loss (mae) is 0.5344132006168365\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5366623282432557\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5065715491771698, and Avg Validation Loss (mae) is 0.5130334854125976\n",
            "After 10 runs; Avg Test Loss (mae) is 0.508341309428215\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7511391997337341, and Avg Validation Loss (mae) is 0.7553286910057068\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7271753907203674\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5256078034639359, and Avg Validation Loss (mae) is 0.5873008042573928\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5888545632362365\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7717780590057373, and Avg Validation Loss (mae) is 0.8188381612300872\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7969058990478516\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9525642156600952, and Avg Validation Loss (mae) is 1.0530806124210357\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0559014976024628\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6017502099275589, and Avg Validation Loss (mae) is 0.5938538312911987\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6106295675039292\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9123253166675568, and Avg Validation Loss (mae) is 1.0922289669513703\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0689674973487855\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5550614953041076, and Avg Validation Loss (mae) is 0.6634684264659881\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6783225536346436\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0063424944877624, and Avg Validation Loss (mae) is 0.9267925679683685\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9551758229732513\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7274925291538239, and Avg Validation Loss (mae) is 0.8907397478818894\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8737967818975448\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1366213023662568, and Avg Validation Loss (mae) is 1.158558452129364\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1730066061019897\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7663483262062073, and Avg Validation Loss (mae) is 0.7578514456748963\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7679475486278534\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.128751963376999, and Avg Validation Loss (mae) is 1.290661132335663\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2699739336967468\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5463902235031128, and Avg Validation Loss (mae) is 0.759946808218956\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7667256176471711\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7540203869342804, and Avg Validation Loss (mae) is 0.8108002245426178\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8033052742481231\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.614471685886383, and Avg Validation Loss (mae) is 0.6044879347085953\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5951890796422958\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7498919427394867, and Avg Validation Loss (mae) is 0.7147040009498596\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7088811039924622\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5134344547986984, and Avg Validation Loss (mae) is 0.6376852810382843\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6518845081329345\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7112331449985504, and Avg Validation Loss (mae) is 0.7363190412521362\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7232519924640656\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.676491504907608, and Avg Validation Loss (mae) is 0.8015824556350708\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7888930261135101\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8101574718952179, and Avg Validation Loss (mae) is 0.7384260833263397\n",
            "After 10 runs; Avg Test Loss (mae) is 0.74239262342453\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.693831992149353, and Avg Validation Loss (mae) is 0.695836728811264\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6733625054359436\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8586259007453918, and Avg Validation Loss (mae) is 0.7265279412269592\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7382008194923401\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5846782177686691, and Avg Validation Loss (mae) is 0.6495327234268189\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6705285876989364\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7335352778434754, and Avg Validation Loss (mae) is 0.8803340673446656\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8688406109809875\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.025480979681015, and Avg Validation Loss (mae) is 1.027120006084442\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0253298044204713\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5642086267471313, and Avg Validation Loss (mae) is 1.6120816707611083\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6385905981063842\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6515841007232666, and Avg Validation Loss (mae) is 0.7028522312641143\n",
            "After 10 runs; Avg Test Loss (mae) is 0.704246562719345\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5857311248779298, and Avg Validation Loss (mae) is 1.585286283493042\n",
            "After 10 runs; Avg Test Loss (mae) is 1.584087324142456\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5615503162145614, and Avg Validation Loss (mae) is 0.68747578561306\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6955746531486511\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5153998613357544, and Avg Validation Loss (mae) is 1.5016406655311585\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5005632996559144\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.686877292394638, and Avg Validation Loss (mae) is 0.7817332744598389\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7881002724170685\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5980255842208861, and Avg Validation Loss (mae) is 1.8968071937561035\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9136064052581787\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7408090651035308, and Avg Validation Loss (mae) is 0.7377983212471009\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7433415502309799\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5713826656341552, and Avg Validation Loss (mae) is 1.7183814644813538\n",
            "After 10 runs; Avg Test Loss (mae) is 1.739976668357849\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.48324345946311953, and Avg Validation Loss (mae) is 0.6096194833517075\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6093438506126404\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5225081324577332, and Avg Validation Loss (mae) is 1.6481477260589599\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6506933569908142\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9731314599514007, and Avg Validation Loss (mae) is 0.9162154912948608\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9180058240890503\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6703500747680664, and Avg Validation Loss (mae) is 1.8288312792778014\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7668201327323914\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7848660409450531, and Avg Validation Loss (mae) is 0.8709991097450256\n",
            "After 10 runs; Avg Test Loss (mae) is 0.872760671377182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "316H8Rh_90mF",
        "outputId": "d4d27f82-e2d2-4bcd-9c29-87cbce8a6a95"
      },
      "source": [
        "CombResultsMF6 = pd.DataFrame.from_dict(my_dictMF6)\n",
        "print(CombResultsMF6.shape)\n",
        "CombResultsSortedMF6 = CombResultsMF6.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMF6.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(56, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA_X</th>\n",
              "      <th>DATA_y</th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.496111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.508341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.521751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.536662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.560777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.584308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.588855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.595189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.604699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.604868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>X_FMX_MTX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.609344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.610630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.623390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.651885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RTX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.664540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>X_FTX_MTX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.670529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.671952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>X_FTX_MTX_MMX_RTX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.673363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.678323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.688960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.695575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.704247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.708881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.718977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.723014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.723252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.727175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>X_FTX_MTX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.738201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>X_FTX_MTX_MMX_RTX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.742393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>X_FMX_MTX_MMX_RTX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.743342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>X_FTX_FMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.766726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>X_FTX_FMX_MMX_RTX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.767948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.788100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.788893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.796906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.803305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RTX_RM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.833391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>X_FTX_MMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.868841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>X_MTX_MMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.872761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.873797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.879884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>X_FMX_MMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>0.918006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>0.955176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>X_FTX_MMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>1.025330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>1.055901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>1.068967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>X_FTX_FMX_MMX_RTX_RMX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>1.173007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>1.242662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>X_FTX_FMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>1.269974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_FT</td>\n",
              "      <td>1.500563</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      DATA_X DATA_y  Test Loss\n",
              "12  X_FTX_FMX_MTX_MBX_RTX_RM   y_MM   0.496111\n",
              "16  X_FTX_FMX_MTX_MBX_RMX_RB   y_MM   0.508341\n",
              "13  X_FTX_FMX_MTX_MBX_RTX_RM   y_RB   0.521751\n",
              "15  X_FTX_FMX_MTX_MBX_RTX_RB   y_RM   0.536662\n",
              "1   X_FTX_FMX_MTX_MMX_MBX_RT   y_RB   0.560777\n",
              "3   X_FTX_FMX_MTX_MMX_MBX_RM   y_RB   0.584308\n",
              "18  X_FTX_FMX_MTX_RTX_RMX_RB   y_MM   0.588855\n",
              "31  X_FTX_MTX_MMX_MBX_RTX_RM   y_RB   0.595189\n",
              "9   X_FTX_FMX_MTX_MMX_RTX_RB   y_RM   0.604699\n",
              "7   X_FTX_FMX_MTX_MMX_RTX_RM   y_RB   0.604868\n",
              "51  X_FMX_MTX_MBX_RTX_RMX_RB   y_MM   0.609344\n",
              "21  X_FTX_FMX_MMX_MBX_RTX_RM   y_RB   0.610630\n",
              "11  X_FTX_FMX_MTX_MMX_RMX_RB   y_RT   0.623390\n",
              "33  X_FTX_MTX_MMX_MBX_RTX_RB   y_RM   0.651885\n",
              "8   X_FTX_FMX_MTX_MMX_RTX_RB   y_MB   0.664540\n",
              "39  X_FTX_MTX_MBX_RTX_RMX_RB   y_MM   0.670529\n",
              "0   X_FTX_FMX_MTX_MMX_MBX_RT   y_RM   0.671952\n",
              "37  X_FTX_MTX_MMX_RTX_RMX_RB   y_MB   0.673363\n",
              "23  X_FTX_FMX_MMX_MBX_RTX_RB   y_RM   0.678323\n",
              "5   X_FTX_FMX_MTX_MMX_MBX_RB   y_RM   0.688960\n",
              "45  X_FMX_MTX_MMX_MBX_RTX_RB   y_RM   0.695575\n",
              "43  X_FMX_MTX_MMX_MBX_RTX_RM   y_RB   0.704247\n",
              "32  X_FTX_MTX_MMX_MBX_RTX_RB   y_FM   0.708881\n",
              "10  X_FTX_FMX_MTX_MMX_RMX_RB   y_MB   0.718977\n",
              "14  X_FTX_FMX_MTX_MBX_RTX_RB   y_MM   0.723014\n",
              "34  X_FTX_MTX_MMX_MBX_RMX_RB   y_FM   0.723252\n",
              "17  X_FTX_FMX_MTX_MBX_RMX_RB   y_RT   0.727175\n",
              "38  X_FTX_MTX_MBX_RTX_RMX_RB   y_FM   0.738201\n",
              "36  X_FTX_MTX_MMX_RTX_RMX_RB   y_FM   0.742393\n",
              "49  X_FMX_MTX_MMX_RTX_RMX_RB   y_MB   0.743342\n",
              "29  X_FTX_FMX_MBX_RTX_RMX_RB   y_MM   0.766726\n",
              "27  X_FTX_FMX_MMX_RTX_RMX_RB   y_MB   0.767948\n",
              "47  X_FMX_MTX_MMX_MBX_RMX_RB   y_RT   0.788100\n",
              "35  X_FTX_MTX_MMX_MBX_RMX_RB   y_RT   0.788893\n",
              "19  X_FTX_FMX_MTX_RTX_RMX_RB   y_MB   0.796906\n",
              "30  X_FTX_MTX_MMX_MBX_RTX_RM   y_FM   0.803305\n",
              "6   X_FTX_FMX_MTX_MMX_RTX_RM   y_MB   0.833391\n",
              "40  X_FTX_MMX_MBX_RTX_RMX_RB   y_FM   0.868841\n",
              "55  X_MTX_MMX_MBX_RTX_RMX_RB   y_FM   0.872761\n",
              "25  X_FTX_FMX_MMX_MBX_RMX_RB   y_RT   0.873797\n",
              "2   X_FTX_FMX_MTX_MMX_MBX_RM   y_RT   0.879884\n",
              "53  X_FMX_MMX_MBX_RTX_RMX_RB   y_MT   0.918006\n",
              "24  X_FTX_FMX_MMX_MBX_RMX_RB   y_MT   0.955176\n",
              "41  X_FTX_MMX_MBX_RTX_RMX_RB   y_MT   1.025330\n",
              "20  X_FTX_FMX_MMX_MBX_RTX_RM   y_MT   1.055901\n",
              "22  X_FTX_FMX_MMX_MBX_RTX_RB   y_MT   1.068967\n",
              "26  X_FTX_FMX_MMX_RTX_RMX_RB   y_MT   1.173007\n",
              "4   X_FTX_FMX_MTX_MMX_MBX_RB   y_RT   1.242662\n",
              "28  X_FTX_FMX_MBX_RTX_RMX_RB   y_MT   1.269974\n",
              "46  X_FMX_MTX_MMX_MBX_RMX_RB   y_FT   1.500563"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "Kogpu-7P90mF",
        "outputId": "ceb4ac99-317d-47de-df20-d9b05001f5ad"
      },
      "source": [
        "CombResultsSortedMFgrouped6 = CombResultsSortedMF6.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedMFgroupedsortedMF6 = CombResultsSortedMFgrouped6.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMFgroupedsortedMF6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RTX_RM</th>\n",
              "      <td>0.508931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RT</th>\n",
              "      <td>0.616365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RMX_RB</th>\n",
              "      <td>0.617758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RTX_RB</th>\n",
              "      <td>0.629838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RTX_RB</th>\n",
              "      <td>0.634620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RMX_RB</th>\n",
              "      <td>0.671183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.680383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RTX_RMX_RB</th>\n",
              "      <td>0.692880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>0.699247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>0.704365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>0.707878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RTX_RM</th>\n",
              "      <td>0.719130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RM</th>\n",
              "      <td>0.732096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>0.756073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RTX_RM</th>\n",
              "      <td>0.833266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.873645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RMX_RB</th>\n",
              "      <td>0.914486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>0.947085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RB</th>\n",
              "      <td>0.965811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RTX_RMX_RB</th>\n",
              "      <td>0.970477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.018350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>1.139831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>1.144332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>1.171419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.174660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.284350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.319790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>1.328474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          Test Loss\n",
              "DATA_X                             \n",
              "X_FTX_FMX_MTX_MBX_RTX_RM   0.508931\n",
              "X_FTX_FMX_MTX_MMX_MBX_RT   0.616365\n",
              "X_FTX_FMX_MTX_MBX_RMX_RB   0.617758\n",
              "X_FTX_FMX_MTX_MBX_RTX_RB   0.629838\n",
              "X_FTX_FMX_MTX_MMX_RTX_RB   0.634620\n",
              "X_FTX_FMX_MTX_MMX_RMX_RB   0.671183\n",
              "X_FTX_MTX_MMX_MBX_RTX_RB   0.680383\n",
              "X_FTX_FMX_MTX_RTX_RMX_RB   0.692880\n",
              "X_FTX_MTX_MMX_MBX_RTX_RM   0.699247\n",
              "X_FTX_MTX_MBX_RTX_RMX_RB   0.704365\n",
              "X_FTX_MTX_MMX_RTX_RMX_RB   0.707878\n",
              "X_FTX_FMX_MTX_MMX_RTX_RM   0.719130\n",
              "X_FTX_FMX_MTX_MMX_MBX_RM   0.732096\n",
              "X_FTX_MTX_MMX_MBX_RMX_RB   0.756073\n",
              "X_FTX_FMX_MMX_MBX_RTX_RM   0.833266\n",
              "X_FTX_FMX_MMX_MBX_RTX_RB   0.873645\n",
              "X_FTX_FMX_MMX_MBX_RMX_RB   0.914486\n",
              "X_FTX_MMX_MBX_RTX_RMX_RB   0.947085\n",
              "X_FTX_FMX_MTX_MMX_MBX_RB   0.965811\n",
              "X_FTX_FMX_MMX_RTX_RMX_RB   0.970477\n",
              "X_FTX_FMX_MBX_RTX_RMX_RB   1.018350\n",
              "X_FMX_MTX_MMX_MBX_RTX_RB   1.139831\n",
              "X_FMX_MTX_MMX_MBX_RMX_RB   1.144332\n",
              "X_FMX_MTX_MMX_MBX_RTX_RM   1.171419\n",
              "X_FMX_MTX_MBX_RTX_RMX_RB   1.174660\n",
              "X_FMX_MMX_MBX_RTX_RMX_RB   1.284350\n",
              "X_MTX_MMX_MBX_RTX_RMX_RB   1.319790\n",
              "X_FMX_MTX_MMX_RTX_RMX_RB   1.328474"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "f6oILgq190mG",
        "outputId": "72a4e50a-4917-4417-baad-8f27f3ac2aac"
      },
      "source": [
        "CombResultsSortedMFgroupedsortedMF6.to_csv('CombResultsSortedMFgroupedsortedMF6.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedMFgroupedsortedMF6.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e335eef7-d02a-450d-b1a4-9bd1759c82b7\", \"CombResultsSortedMFgroupedsortedMF6.csv\", 1246)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "z0kTGYAvxngl",
        "outputId": "23849fcc-57da-4c2d-f9b1-a277a71806b7"
      },
      "source": [
        "CombResultsSortedMF6.to_csv('CombResultsSortedMF6.csv')\n",
        "files.download(\"CombResultsSortedMF6.csv\")\n",
        "\n",
        "fig = px.box(CombResultsSortedMF6, x=\"DATA_X\", y=\"Test Loss\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_58f4a01a-2a56-4bab-811b-e57bd588394d\", \"CombResultsSortedMF6.csv\", 2918)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e9ce287f-4a12-4d07-accd-9fe8c67a4c8b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e9ce287f-4a12-4d07-accd-9fe8c67a4c8b\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e9ce287f-4a12-4d07-accd-9fe8c67a4c8b',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_FTX_FMX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MTX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_RTX_RM\", \"X_FMX_MTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_RTX_RB\", \"X_FTX_MTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RT\", \"X_FTX_MTX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RTX_RM\", \"X_FTX_MMX_MBX_RTX_RMX_RB\", \"X_MTX_MMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RM\", \"X_FMX_MMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RMX_RB\", \"X_FTX_MMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RB\", \"X_FTX_FMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RM\", \"X_FMX_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MBX_RTX_RMX_RB\", \"X_MTX_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_RTX_RMX_RB\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.4961110830307007, 0.508341309428215, 0.5217514365911484, 0.5366623282432557, 0.5607773184776306, 0.584307712316513, 0.5888545632362365, 0.5951890796422958, 0.6046994656324387, 0.6048680394887924, 0.6093438506126404, 0.6106295675039292, 0.6233897715806961, 0.6518845081329345, 0.6645403385162354, 0.6705285876989364, 0.6719517081975936, 0.6733625054359436, 0.6783225536346436, 0.688959801197052, 0.6955746531486511, 0.704246562719345, 0.7088811039924622, 0.7189769119024276, 0.7230144262313842, 0.7232519924640656, 0.7271753907203674, 0.7382008194923401, 0.74239262342453, 0.7433415502309799, 0.7667256176471711, 0.7679475486278534, 0.7881002724170685, 0.7888930261135101, 0.7969058990478516, 0.8033052742481231, 0.8333911240100861, 0.8688406109809875, 0.872760671377182, 0.8737967818975448, 0.8798842549324035, 0.9180058240890503, 0.9551758229732513, 1.0253298044204713, 1.0559014976024628, 1.0689674973487855, 1.1730066061019897, 1.2426615357398987, 1.2699739336967468, 1.5005632996559144, 1.584087324142456, 1.6385905981063842, 1.6506933569908142, 1.739976668357849, 1.7668201327323914, 1.9136064052581787], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e9ce287f-4a12-4d07-accd-9fe8c67a4c8b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLNnKgJTD4G7"
      },
      "source": [
        "**ITERATIVE TESTION MODEL**\n",
        "\n",
        "*   **SCALE TEST X DATA**\n",
        "*   **GRAB A SAMPLE, GIVE THE ADDITIONAL DIMENSION (REQ FOR CONV1D)**\n",
        "*   **GET THE PREDICTION RESULT OF THE SAMPLE**\n",
        "*   **INPUT THIS PREDICTION INTO NEXT SAMPLE, LAST FEATURE**\n",
        "*   **APPEND PREDICTION TO THE ITERATIVE RESULTS ARRAY**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6JiGfg2cBs0"
      },
      "source": [
        "def Iterative_Test(ITERATIVE_Test_Input):\n",
        "  ITERATIVE_Test_Output = []\n",
        "  yhat = 0\n",
        "  for i in range(len(ITERATIVE_Test_Input)):\n",
        "    \n",
        "    #z = max(i-3, 0)\n",
        "    #print(ITERATIVE_Test_Input[z: i+3])\n",
        "\n",
        "    #SINGLE SEQUENCE (SAMPLE,ROW) IS LOADED INTO X_Input, TO GET THE PREDICTION ROW BY ROW THROUGHOUT THE DATASET\n",
        "    X_input = ITERATIVE_Test_Input[i:i+1]\n",
        "    X_nextinput = ITERATIVE_Test_Input[i+1:i+2]\n",
        "    \n",
        "    print(\"ITERATIVE_Test_Input[i:i+1]__:\", X_input)\n",
        "    print(\"ITERATIVE_Test_Input[i+1:i+2]:\", X_nextinput)\n",
        "    \n",
        "    if i < (len(ITERATIVE_Test_Input)-2):\n",
        "      print(\"ITERATIVE_Test_Input[i]__: \",ITERATIVE_Test_Input[i])\n",
        "      print(\"ITERATIVE_Test_Input[i+1]: \",ITERATIVE_Test_Input[i+1])\n",
        "\n",
        "    yhat = model.predict(X_input, verbose=0)\n",
        "    print(\"Current yhat_______________:\", yhat)\n",
        "\n",
        "    if i < (len(ITERATIVE_Test_Input)-1):\n",
        "      yhat2 = model.predict(X_nextinput, verbose=0)\n",
        "      print(\"Next yhat, before iteration:\", yhat2)\n",
        "    \n",
        "    linesleft = len(ITERATIVE_Test_Input) - i -1\n",
        "    print(\"Linesleft_:\", linesleft)\n",
        "    linesleft1 = min(n_steps, linesleft)\n",
        "    print(\"Linesleft1:\", linesleft1)\n",
        "\n",
        "    #yhatshaped = yhat.reshape((len(yhat), 1))\n",
        "    #print(\"yhatshaped:\", yhatshaped)\n",
        "    #yhatnormalized = scaler.transform(yhat)\n",
        "    #print(\"yhatnormalized:\", yhatnormalized)\n",
        "    \n",
        "    if i < (len(ITERATIVE_Test_Input)-1):\n",
        "      for j in range(linesleft1):\n",
        "        ITERATIVE_Test_Input[i+j+1,n_steps-1-j] = yhat\n",
        "        print(\"Next Unscaled Input Iterated: \", ITERATIVE_Test_Input[i+j+1])\n",
        "    \n",
        "  \n",
        "    ITERATIVE_Test_Output.append(yhat)\n",
        "    print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
        "    #s = input('Enter something : ')\n",
        "\n",
        "  ITERATIVE_Test_Output = np.ravel(ITERATIVE_Test_Output)\n",
        "  print(ITERATIVE_Test_Output)\n",
        "  return(ITERATIVE_Test_Output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q55KpLpzvqK"
      },
      "source": [
        "**CREATE SEQUENCE FROM NONSCALED TEST DATA TO BE USED IN THE ITERATIVE TEST MODEL**\n",
        "\n",
        "**CALL ITERATIVE TEST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H7yjxEAsCRL"
      },
      "source": [
        "X_test_RT_ITERATIVE = np.concatenate((S2RTX, S3RTX, S4RTX, S5RTX, S6RTX), axis=0)\n",
        "print(\"X_test_RT_ITERATIVE:\", X_test_RT_ITERATIVE[0:5])\n",
        "\n",
        "X_test_RT_ITERATIVE = X_test_RT_ITERATIVE.reshape((X_test_RT_ITERATIVE.shape[0], X_test_RT_ITERATIVE.shape[1], n_features))\n",
        "\n",
        "print(\"X_test_RT_ITERATIVE_scaled\", X_test_RT_ITERATIVE[0])\n",
        "\n",
        "y_test_RT_ITERATIVE = Iterative_Test(X_test_RT_ITERATIVE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENIIpSgcFp0E"
      },
      "source": [
        "**DISPLAY ACTUAL, NONITERATIVE AND ITERATIVE PREDICTION RESULTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Gr82rfj80AGc",
        "outputId": "29a7e488-b832-49ae-b61e-16530a388cd8"
      },
      "source": [
        "#plt.figure(figsize=(8, 8))\n",
        "#plt.plot(y_test_RT_NONITERATIVE, \"y\", label=\"y_test_RT_NONITERATIVE\")\n",
        "#plt.plot(y_test_RT_ITERATIVE, \"b\", label=\"y_test_RT_ITERATIVE\")\n",
        "#plt.plot(y_RT, \"r\", label=\"y_RT\")\n",
        "#plt.title(\"Single Sclaer Iterative/Noniterative- Filters=64, Dense=50\")\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=y_test_RT_NONITERATIVE, name=\"y_test_RT_NONITERATIVE\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_RT, name=\"y_RT\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_test_RT_ITERATIVE, name=\"y_test_RT_ITERATIVE\", line_shape='linear'))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"a3c0ced5-19d4-43e9-9106-6fd5e0d1cd84\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"a3c0ced5-19d4-43e9-9106-6fd5e0d1cd84\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'a3c0ced5-19d4-43e9-9106-6fd5e0d1cd84',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_test_RT_NONITERATIVE\", \"type\": \"scatter\", \"y\": [34.53227615356445, 34.50122833251953, 34.67282485961914, 34.889610290527344, 35.07686233520508, 34.741607666015625, 34.49224853515625, 34.186729431152344, 34.24262237548828, 34.18865203857422, 34.06352233886719, 33.92122268676758, 34.000946044921875, 34.2012939453125, 34.28612518310547, 34.4476432800293, 34.462364196777344, 34.58159637451172, 34.66510772705078, 34.6922721862793, 34.722068786621094, 34.73305130004883, 34.83683776855469, 34.75959396362305, 34.72385025024414, 34.76549530029297, 34.85511016845703, 35.09502410888672, 35.083309173583984, 35.10070037841797, 35.11542510986328, 35.110565185546875, 35.13362121582031, 35.13732147216797, 34.936912536621094, 34.740264892578125, 34.647151947021484, 34.53158187866211, 34.649688720703125, 34.5858039855957, 34.669219970703125, 34.70089340209961, 34.705169677734375, 34.72576904296875, 34.73305130004883, 34.736656188964844, 34.840538024902344, 34.89268493652344, 34.90735626220703, 34.92424774169922, 34.9315299987793, 34.93513870239258, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.03901672363281, 35.09116744995117, 35.105838775634766, 35.12273406982422, 35.18010330200195, 35.20978546142578, 35.220821380615234, 34.75363540649414, 35.0327262878418, 35.033077239990234, 35.147090911865234, 35.19158935546875, 35.294246673583984, 35.377899169921875, 35.39990997314453, 35.418609619140625, 35.427738189697266, 35.43135070800781, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.53522491455078, 35.58737564086914, 35.60204315185547, 35.61894226074219, 35.626220703125, 35.62982940673828, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.733707427978516, 35.785858154296875, 35.80052947998047, 35.81742477416992, 35.82470703125, 35.82831573486328, 35.83201599121094, 35.83201599121094, 35.93218994140625, 35.98434066772461, 35.99901580810547, 35.915706634521484, 35.839683532714844, 35.982078552246094, 35.87313461303711, 35.849952697753906, 35.98095703125, 35.99891662597656, 36.02013397216797, 36.0395393371582, 36.023189544677734, 36.02679443359375, 36.03049850463867, 36.03049850463867, 36.080589294433594, 36.10666275024414, 36.11399841308594, 36.1224479675293, 36.12608337402344, 36.12788772583008, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.229915618896484, 36.282066345214844, 36.29673767089844, 36.313636779785156, 36.32091522216797, 36.32452392578125, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.428401947021484, 36.480552673339844, 36.49522399902344, 36.512115478515625, 36.51939392089844, 36.52300262451172, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.62688446044922, 36.67903137207031, 36.693702697753906, 36.710601806640625, 36.71788024902344, 36.72148895263672, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.82536697387695, 36.74442672729492, 36.775978088378906, 36.902652740478516, 36.89289855957031, 36.93021011352539, 36.93036651611328, 36.91997528076172, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 37.07394027709961, 37.152164459228516, 37.07396697998047, 37.01600646972656, 37.06553649902344, 37.03924560546875, 37.0573616027832, 37.046539306640625, 37.022911071777344, 37.022911071777344, 37.12309265136719, 37.17523956298828, 37.08970642089844, 37.0233039855957, 37.069190979003906, 37.041099548339844, 37.0573616027832, 37.046539306640625, 37.12309265136719, 37.17523956298828, 37.18991470336914, 37.206809997558594, 37.214088439941406, 37.317874908447266, 37.37372589111328, 37.388397216796875, 37.405296325683594, 37.412574768066406, 37.41618347167969, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.52006149291992, 37.572208404541016, 37.586875915527344, 37.60377502441406, 37.611053466796875, 37.614662170410156, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.718536376953125, 37.77069091796875, 37.785362243652344, 37.80226135253906, 37.809539794921875, 37.81314468383789, 37.81684875488281, 37.81684875488281, 37.86693572998047, 37.89301300048828, 37.90034866333008, 37.90879821777344, 37.912437438964844, 37.91423797607422, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 38.016265869140625, 38.068416595458984, 38.08308792114258, 38.09998321533203, 38.107261657714844, 38.21105194091797, 38.26689910888672, 38.181365966796875, 38.114959716796875, 38.160850524902344, 38.13275146484375, 38.14902114868164, 38.13819885253906, 38.214752197265625, 38.13380813598633, 38.16535568237305, 38.29203796386719, 38.28227996826172, 38.31958770751953, 38.31974792480469, 38.30935287475586, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.413230895996094, 38.33229064941406, 38.29654312133789, 38.33819580078125, 38.32762908935547, 38.33417892456055, 38.3366813659668, 38.413230895996094, 38.46538162231445, 38.48005294799805, 38.4969482421875, 38.50423049926758, 38.50783920288086, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.611717224121094, 38.5307731628418, 38.495025634765625, 38.53667449951172, 38.526119232177734, 38.63283920288086, 38.687496185302734, 38.67853546142578, 38.6954345703125, 38.70271301269531, 38.706321716308594, 38.71002197265625, 38.71002197265625, 39.210914611816406, 39.10077667236328, 39.08625030517578, 39.11927795410156, 39.024017333984375, 38.91579818725586, 38.953407287597656, 38.815494537353516, 38.75926971435547, 38.76044464111328, 38.771053314208984, 38.73686599731445, 38.747230529785156, 39.0367317199707, 39.8082389831543, 39.49644470214844, 39.387569427490234, 39.28089141845703, 39.36650085449219, 39.22809600830078, 39.169090270996094, 39.10858917236328, 39.029632568359375, 39.042198181152344, 39.0313720703125, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 38.90753936767578, 38.89153289794922, 38.88208770751953, 38.868064880371094, 39.02117919921875, 38.99097442626953, 39.014278411865234, 38.91423416137695, 38.82053756713867, 38.96302795410156, 38.98347473144531, 39.010711669921875, 39.01678466796875, 39.00043487548828, 39.00404357910156, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 38.90753936767578, 38.82423782348633, 38.862850189208984, 38.831146240234375, 38.843711853027344, 38.832889556884766, 38.809261322021484, 38.809261322021484, 38.80926513671875, 38.80926513671875, 38.809261322021484, 38.809261322021484, 38.90943908691406, 38.82849884033203, 38.792755126953125, 38.83439636230469, 38.92401885986328, 38.98271179199219, 38.99988555908203, 38.99315643310547, 39.00043487548828, 39.00404357910156, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.1079216003418, 39.026981353759766, 38.99123764038086, 39.03288269042969, 39.02232360839844, 39.02886962890625, 39.0313720703125, 39.00774383544922, 39.1079216003418, 39.026981353759766, 38.99123764038086, 39.133060455322266, 39.174652099609375, 39.19586944580078, 39.21527099609375, 39.19892501831055, 39.20252990722656, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.3064079284668, 39.35855484008789, 39.373226165771484, 39.49030685424805, 39.41664123535156, 39.38450622558594, 39.429847717285156, 39.419288635253906, 39.42583465576172, 39.42833709716797, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.30450439453125, 39.20433044433594, 39.07631301879883, 39.192996978759766, 39.21641540527344, 39.21650695800781, 39.21527099609375, 39.19892501831055, 39.20252990722656, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.10602569580078, 39.02272415161133, 39.06133270263672, 39.029632568359375, 39.042198181152344, 39.0313720703125, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.1079216003418, 39.16007614135742, 39.174747467041016, 39.191646575927734, 39.19892501831055, 39.20252990722656, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.3064079284668, 39.35855484008789, 39.373226165771484, 39.39012145996094, 39.397403717041016, 39.30080795288086, 39.22120666503906, 39.25981903076172, 39.228118896484375, 39.24068069458008, 39.2298583984375, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.10602569580078, 39.02272415161133, 39.16151428222656, 39.18196105957031, 39.209197998046875, 39.21527099609375, 39.19892501831055, 39.20252990722656, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.10602569580078, 39.02272415161133, 39.16151428222656, 39.18196105957031, 39.209197998046875, 39.21527099609375, 39.19892501831055, 39.20252990722656, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.10602569580078, 39.02272415161133, 39.06133270263672, 39.029632568359375, 38.94198989868164, 38.84786605834961, 38.862850189208984, 38.781044006347656, 38.75196075439453, 38.76044464111328, 38.72096633911133, 38.72724914550781, 38.721839904785156, 38.71002197265625, 38.71002197265625, 38.60981750488281, 38.526512145996094, 38.665306091308594, 38.685752868652344, 38.61278533935547, 38.60284423828125, 38.577056884765625, 38.49934768676758, 38.571128845214844, 38.54011535644531, 38.53266143798828, 38.5351676940918, 38.41133117675781, 38.32802963256836, 38.36664581298828, 38.334938049316406, 38.347503662109375, 38.3366813659668, 38.21284866333008, 38.129547119140625, 38.16815948486328, 38.13645553588867, 38.24919891357422, 38.29052734375, 38.28157043457031, 38.29846954345703, 38.30574417114258, 38.20914840698242, 38.129547119140625, 38.16815948486328, 38.13645553588867, 38.24919891357422, 38.157440185546875, 38.09806442260742, 38.13970947265625, 38.129146575927734, 38.13569259643555, 38.13819885253906, 38.214752197265625, 38.26689910888672, 38.28157043457031, 38.29846954345703, 38.30574417114258, 38.30935287475586, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.112648010253906, 37.91229248046875, 37.722801208496094, 37.640785217285156, 37.73716354370117, 37.57099914550781, 37.47563934326172, 37.58033752441406, 37.59039306640625, 37.52112579345703, 37.16973114013672, 36.823890686035156, 36.93989944458008, 36.73603057861328, 33.35908508300781, 33.36296463012695, 33.28388977050781, 33.179752349853516, 33.21991729736328, 33.19159698486328, 33.19131088256836, 33.176116943359375, 33.302757263183594, 33.649269104003906, 33.96833038330078, 34.15753173828125, 34.3675422668457, 34.60173416137695, 34.62836456298828, 34.584808349609375, 34.525665283203125, 34.669036865234375, 34.57929611206055, 34.54294204711914, 34.40713119506836, 34.4211540222168, 34.3362922668457, 34.29339599609375, 34.29457092285156, 34.25509262084961, 34.261375427246094, 34.25596237182617, 34.24414825439453, 34.29423522949219, 34.25376510620117, 34.23589324951172, 34.256717681884766, 34.25143814086914, 34.25471115112305, 34.25596237182617, 34.24414825439453, 34.24414825439453, 34.29423904418945, 34.25376892089844, 34.23589324951172, 34.256717681884766, 34.15122985839844, 34.07120132446289, 34.11106872558594, 34.06755065917969, 34.080116271972656, 34.06929397583008, 34.145843505859375, 34.064903259277344, 34.02915954589844, 34.070804595947266, 34.06024169921875, 34.06678771972656, 34.169471740722656, 34.197994232177734, 34.112457275390625, 34.046058654785156, 34.091949462890625, 34.06385040283203, 34.080116271972656, 34.06929397583008, 34.145843505859375, 34.064903259277344, 34.09645080566406, 34.2231330871582, 34.213375091552734, 34.30077362060547, 34.327003479003906, 34.323944091796875, 34.336097717285156, 34.3397331237793, 34.34153747558594, 34.343387603759766, 34.343387603759766, 34.443565368652344, 34.4957160949707, 34.5103874206543, 34.52728271484375, 34.53456115722656, 34.538169860839844, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.642051696777344, 34.972145080566406, 34.86517333984375, 34.88556671142578, 34.93682861328125, 34.928245544433594, 34.95402526855469, 34.96247100830078, 35.03901672363281, 35.09116744995117, 35.105838775634766, 35.12273406982422, 35.18010330200195, 35.20978546142578, 35.220821380615234, 35.32944869995117, 35.38523864746094, 35.40171432495117, 35.42046356201172, 35.527915954589844, 35.58367156982422, 35.5018424987793, 35.50273132324219, 35.63365173339844, 35.60636520385742, 35.65338897705078, 35.640220642089844, 35.73000717163086, 35.785858154296875, 35.80052947998047, 35.81742858886719, 35.92488479614258, 35.98064041137695, 35.99901580810547, 36.01591110229492, 36.023189544677734, 36.02679443359375, 36.03049850463867, 36.080589294433594, 36.106658935546875, 36.11399841308594, 36.1224479675293, 36.226261138916016, 36.280216217041016, 36.29673767089844, 36.41381072998047, 36.473243713378906, 36.491519927978516, 36.512115478515625, 36.61957550048828, 36.67533493041992, 36.693702697753906, 36.710601806640625, 36.71788024902344, 36.72148895263672, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.62498474121094, 36.541683197021484, 36.580291748046875, 36.54859161376953, 36.5611572265625, 36.75068664550781, 36.683013916015625, 36.66032409667969, 36.5469856262207, 36.59716033935547, 36.563777923583984, 36.56846618652344, 36.550331115722656, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.62688446044922, 36.54594421386719, 36.51020050048828, 36.652015686035156, 36.693607330322266, 36.6146240234375, 36.550724029541016, 36.57298278808594, 36.544891357421875, 36.5611572265625, 36.550331115722656, 36.52670669555664, 36.52670669555664, 36.4265022277832, 36.34320068359375, 36.381813049316406, 36.35010528564453, 36.3626708984375, 36.35185241699219, 36.328224182128906, 36.328224182128906, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.428401947021484, 36.66405487060547, 36.51445770263672, 36.48833465576172, 36.36102294921875, 36.38597106933594, 36.37123107910156, 36.36997985839844, 36.35185241699219, 36.32822036743164, 36.428401947021484, 36.34745788574219, 36.31171417236328, 36.353363037109375, 36.342796325683594, 36.34934616088867, 36.25164794921875, 36.212005615234375, 36.615943908691406, 36.4610595703125, 36.49116134643555, 36.355194091796875, 36.38022994995117, 36.36529541015625, 36.36997985839844, 36.35185241699219, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.428401947021484, 36.480552673339844, 36.49522399902344, 36.512115478515625, 36.51939392089844, 36.52300262451172, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.4265022277832, 36.34320068359375, 36.481990814208984, 36.36934280395508, 36.346160888671875, 36.47716522216797, 36.36203384399414, 36.33283233642578, 36.37698745727539, 36.342796325683594, 36.34934616088867, 36.35185241699219, 36.428401947021484, 36.34745788574219, 36.31171417236328, 36.25315475463867, 36.15929412841797, 36.20445251464844, 36.17525100708008, 36.264366149902344, 36.305694580078125, 36.29673767089844, 36.41381072998047, 36.473243713378906, 36.391319274902344, 36.32861328125, 36.37450408935547, 36.346405029296875, 36.26246643066406, 36.235633850097656, 36.335655212402344, 36.30475616455078, 36.34808349609375, 36.234710693359375, 36.14101791381836, 36.183326721191406, 36.15162658691406, 36.164188385009766, 36.15336608886719, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.07963562011719, 36.037986755371094, 36.057289123535156, 36.09152603149414, 36.12388610839844, 36.22598648071289, 36.27477264404297, 36.29308319091797, 36.211578369140625, 36.13740539550781, 36.17962646484375, 36.1516227722168, 36.164188385009766, 36.15336608886719, 36.129737854003906, 36.07963562011719, 36.037986755371094, 36.057289123535156, 36.04143524169922, 36.04772186279297, 36.04231262207031, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 35.930294036865234, 35.84699249267578, 35.88560485839844, 35.85389709472656, 35.86646270751953, 35.85564422607422, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.731807708740234, 35.64850616455078, 35.68711853027344, 35.65541076660156, 35.6679801940918, 35.65715789794922, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.5333251953125, 35.45002365112305, 35.48863983154297, 35.456932067871094, 35.46949768066406, 35.45867919921875, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 36.97102355957031, 37.06221389770508, 37.23649597167969, 37.18973159790039, 37.1747932434082, 37.24098205566406, 37.2359733581543, 37.142311096191406, 37.04650115966797, 36.9847526550293, 37.0267219543457, 37.04522705078125, 37.040870666503906, 37.12761306762695, 37.35509490966797, 37.340389251708984, 37.483585357666016, 37.55031204223633, 37.55955123901367, 37.600074768066406, 37.611053466796875, 37.614662170410156, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.51816177368164, 37.43486022949219, 37.473472595214844, 37.54193878173828, 37.606658935546875, 37.610504150390625, 37.60377502441406, 37.611053466796875, 37.614662170410156, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.718536376953125, 37.77069091796875, 37.785362243652344, 37.80226135253906, 37.809539794921875, 37.81314468383789, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.86693572998047, 37.89301300048828, 37.90034866333008, 37.90879821777344, 37.912437438964844, 37.91423797607422, 37.91609191894531, 37.91609191894531, 37.91609191894531, 38.016265869140625, 38.068416595458984, 38.08308792114258, 38.09998321533203, 38.107261657714844, 38.110870361328125, 38.11457061767578, 38.11457061767578, 38.214752197265625, 38.26689910888672, 38.28157043457031, 38.29846954345703, 38.30574417114258, 38.30935287475586, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.313053131103516, 38.41323471069336, 38.46538162231445, 38.48005294799805, 38.4969482421875, 38.50423049926758, 38.50783920288086, 38.511539459228516, 38.511539459228516, 38.611717224121094, 38.66386413574219, 38.67853546142578, 38.6954345703125, 38.70271301269531, 38.706321716308594, 38.71002197265625, 38.71002197265625, 38.71002197265625, 38.71002197265625, 38.71002197265625, 38.76011276245117, 38.78618621826172, 38.79351806640625, 38.80196762084961, 38.805606842041016, 38.807411193847656, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.80926513671875, 38.90943908691406, 38.82849884033203, 38.86004638671875, 38.98672866821289, 38.97697448730469, 39.014278411865234, 39.014434814453125, 39.00404357910156, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.1079216003418, 39.026981353759766, 39.058528900146484, 39.185211181640625, 39.17545700073242, 39.2127685546875, 39.21292495727539, 39.20252990722656, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.3064079284668, 39.358558654785156, 39.373226165771484, 39.39012145996094, 39.397403717041016, 39.4010124206543, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.5048942565918, 39.557037353515625, 39.57170867919922, 39.58860778808594, 39.59588623046875, 39.59949493408203, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.65328598022461, 39.679359436035156, 39.68669891357422, 39.69514465332031, 39.698787689208984, 39.700592041015625, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.652339935302734, 39.61068344116211, 39.62998962402344, 39.6141357421875, 39.670509338378906, 39.62462615966797, 39.594940185546875, 39.61576461791992, 39.61048126220703, 39.61375427246094, 39.61500930786133, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.65328598022461, 39.612815856933594, 39.594940185546875, 39.61576461791992, 39.61048126220703, 39.61375427246094, 39.61500930786133, 39.60319519042969, 39.65328598022461, 39.679359436035156, 39.68669891357422, 39.69514465332031, 39.698787689208984, 39.700592041015625, 39.652339935302734, 39.61068344116211, 39.680076599121094, 39.690303802490234, 39.703922271728516, 39.70695877075195, 39.648681640625, 39.642478942871094, 39.63960647583008, 39.632598876953125, 39.70915222167969, 39.694053649902344, 39.70570755004883, 39.705787658691406, 39.700592041015625, 39.70243835449219, 39.652339935302734, 39.64432907104492, 39.706153869628906, 39.690704345703125, 39.712371826171875, 39.705787658691406, 39.700592041015625, 39.652339935302734, 39.61068344116211, 39.62998962402344, 39.66422653198242, 39.69658660888672, 39.69851303100586, 39.69514465332031, 39.648681640625, 39.60883331298828, 39.62998962402344, 39.6141357421875, 39.670509338378906, 39.62462615966797, 39.62858581542969, 39.62538146972656, 39.628944396972656, 39.63594055175781, 39.602298736572266, 39.62632751464844, 39.66757583618164, 39.623374938964844, 39.64039993286133, 39.62538146972656, 39.595298767089844, 39.62632751464844, 39.61748504638672, 39.61375427246094, 39.61500930786133, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.65328598022461, 39.612815856933594, 39.594940185546875, 39.61576461791992, 39.61048126220703, 39.61375427246094, 39.61500930786133, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.65328598022461, 39.612815856933594, 39.594940185546875, 39.61576461791992, 39.61048126220703, 39.61375427246094, 39.61500930786133, 39.60319519042969, 39.65328598022461, 39.679359436035156, 39.68669891357422, 39.69514465332031, 39.698787689208984, 39.700592041015625, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 35.02783966064453, 35.25115966796875, 35.31551742553711, 35.468719482421875, 35.554100036621094, 35.68562316894531, 35.950103759765625, 35.94554901123047, 35.99026107788086, 36.05868911743164, 36.07957458496094, 36.11029815673828, 36.222625732421875, 36.278411865234375, 36.294883728027344, 36.313636779785156, 36.42109298706055, 36.47685241699219, 36.49522399902344, 36.512115478515625, 36.51939392089844, 36.52300262451172, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.62688446044922, 36.54594421386719, 36.51020050048828, 36.551841735839844, 36.541282653808594, 36.547828674316406, 36.550331115722656, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.4265022277832, 36.34320068359375, 36.381813049316406, 36.35010528564453, 36.3626708984375, 36.35185241699219, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.328224182128906, 36.328224182128906, 36.32822036743164, 36.32822036743164, 36.22801971435547, 36.14471435546875, 36.183326721191406, 36.1516227722168, 36.164188385009766, 36.15336608886719, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.229915618896484, 36.282066345214844, 36.29673767089844, 36.313636779785156, 36.32091522216797, 36.32452392578125, 36.428401947021484, 36.480552673339844, 36.84584426879883, 36.68886184692383, 36.612884521484375, 36.57774353027344, 36.61681365966797, 36.59259033203125, 36.60567855834961, 36.550331115722656, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.62688446044922, 36.54594421386719, 36.51020050048828, 36.551841735839844, 36.541282653808594, 36.547828674316406, 36.550331115722656, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.4265022277832, 36.34320068359375, 36.481990814208984, 36.36934280395508, 36.346160888671875, 36.376983642578125, 36.342796325683594, 36.34934616088867, 36.35185241699219, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.22801971435547, 36.14471435546875, 36.183326721191406, 36.1516227722168, 36.164188385009766, 36.15336608886719, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.229915618896484, 36.14897537231445, 36.11323165893555, 36.154876708984375, 36.14431381225586, 36.15086364746094, 36.15336608886719, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.33009338378906, 36.286041259765625, 36.28022766113281, 36.23331832885742, 36.15198516845703, 36.198516845703125, 36.17525100708008, 36.264366149902344, 36.58598327636719, 36.453041076660156, 36.47358703613281, 36.52470016479492, 36.41531753540039, 36.42567825317383, 36.55776596069336, 36.403038024902344, 36.43035125732422, 36.54083251953125, 36.499542236328125, 36.54656982421875, 36.53340148925781, 36.52300262451172, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.4265022277832, 36.41048812866211, 36.53413772583008, 36.50324249267578, 36.54656982421875, 36.53340148925781, 36.52300262451172, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.4265022277832, 36.326324462890625]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RT\", \"type\": \"scatter\", \"y\": [34.5, 34.7, 34.9, 35.1, 34.5, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 34.0, 34.2, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.7, 34.5, 34.3, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 35.8, 35.8, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 39.7, 39.2, 39.2, 39.0, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.8, 38.7, 38.8, 39.2, 40.5, 39.6, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 39.0, 38.8, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.6, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.5, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 37.9, 37.6, 37.4, 37.6, 37.6, 37.4, 37.4, 37.6, 37.6, 37.4, 36.7, 36.7, 36.7, 36.5, 36.3, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.4, 33.8, 34.2, 34.3, 34.5, 34.7, 34.7, 34.5, 34.5, 34.7, 34.5, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.9, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.7, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.7, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.1, 36.1, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.0, 36.9, 36.9, 37.0, 37.0, 37.0, 37.2, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.6, 39.7, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.7, 39.6, 39.7, 39.6, 39.6, 39.6, 39.7, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 35.4, 35.4, 35.6, 35.6, 35.8, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 37.2, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.3, 36.7, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.1, 36.1]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_test_RT_ITERATIVE\", \"type\": \"scatter\", \"y\": [34.53227233886719, 34.5089225769043, 34.58062744140625, 34.5903434753418, 34.64912414550781, 34.67234420776367, 34.6910285949707, 34.713584899902344, 34.73662567138672, 34.762962341308594, 34.78651809692383, 34.811405181884766, 34.83570861816406, 34.859825134277344, 34.883792877197266, 34.90768814086914, 34.93146896362305, 34.95506286621094, 34.97855758666992, 35.00197219848633, 35.02527618408203, 35.048465728759766, 35.071556091308594, 35.09453582763672, 35.11740493774414, 35.140167236328125, 35.16282653808594, 35.18538284301758, 35.20783233642578, 35.23017883300781, 35.252418518066406, 35.27455520629883, 35.29658889770508, 35.31852722167969, 35.340362548828125, 35.362091064453125, 35.38372039794922, 35.40524673461914, 35.426673889160156, 35.448001861572266, 35.469234466552734, 35.4903678894043, 35.51140213012695, 35.5323371887207, 35.55317687988281, 35.573917388916016, 35.59456253051758, 35.615108489990234, 35.635562896728516, 35.65592575073242, 35.67619323730469, 35.69636535644531, 35.71643829345703, 35.73642349243164, 35.756317138671875, 35.77611541748047, 35.79581832885742, 35.815433502197266, 35.8349609375, 35.85439682006836, 35.87373733520508, 35.89299011230469, 35.91215515136719, 35.93123245239258, 35.95022201538086, 35.969120025634766, 35.98793029785156, 36.00665283203125, 36.02528762817383, 36.04384231567383, 36.06230545043945, 36.080684661865234, 36.09897994995117, 36.1171875, 36.135311126708984, 36.15335464477539, 36.17131042480469, 36.18918228149414, 36.20697021484375, 36.22467803955078, 36.24230194091797, 36.25984191894531, 36.277305603027344, 36.2946891784668, 36.311988830566406, 36.32920837402344, 36.34634780883789, 36.36341094970703, 36.380393981933594, 36.39729309082031, 36.41411590576172, 36.43086242675781, 36.44752883911133, 36.4641227722168, 36.48063278198242, 36.4970703125, 36.513431549072266, 36.52971649169922, 36.545921325683594, 36.56205368041992, 36.57810974121094, 36.59409713745117, 36.61001205444336, 36.625850677490234, 36.6416130065918, 36.65730285644531, 36.67292022705078, 36.6884651184082, 36.70393753051758, 36.719337463378906, 36.73466873168945, 36.74992752075195, 36.765113830566406, 36.78022766113281, 36.79527282714844, 36.81024932861328, 36.825157165527344, 36.839996337890625, 36.85476303100586, 36.86946487426758, 36.884098052978516, 36.89866256713867, 36.91315841674805, 36.92758560180664, 36.941951751708984, 36.95624923706055, 36.97047805786133, 36.98463821411133, 36.99873352050781, 37.01276779174805, 37.026737213134766, 37.0406379699707, 37.05447769165039, 37.06825256347656, 37.08195877075195, 37.095603942871094, 37.10918426513672, 37.122703552246094, 37.13615798950195, 37.14955520629883, 37.16288757324219, 37.1761589050293, 37.18936538696289, 37.202510833740234, 37.21559524536133, 37.22861862182617, 37.241580963134766, 37.254486083984375, 37.267333984375, 37.280120849609375, 37.292842864990234, 37.30550765991211, 37.318115234375, 37.33066177368164, 37.34315490722656, 37.355587005615234, 37.36796188354492, 37.380279541015625, 37.392539978027344, 37.404747009277344, 37.416893005371094, 37.42898178100586, 37.44101333618164, 37.4529914855957, 37.46491622924805, 37.47678756713867, 37.48859786987305, 37.50035858154297, 37.51205825805664, 37.523704528808594, 37.535301208496094, 37.546844482421875, 37.55833053588867, 37.56976318359375, 37.58114242553711, 37.592472076416016, 37.60374450683594, 37.614967346191406, 37.62614059448242, 37.637264251708984, 37.64833068847656, 37.65934371948242, 37.67030715942383, 37.68122482299805, 37.69208908081055, 37.702903747558594, 37.71366500854492, 37.7243766784668, 37.73503875732422, 37.74565505981445, 37.75621795654297, 37.76673126220703, 37.77719497680664, 37.78761291503906, 37.79798126220703, 37.80830383300781, 37.818580627441406, 37.82880783081055, 37.83898162841797, 37.8491096496582, 37.859195709228516, 37.86922836303711, 37.87921905517578, 37.889163970947266, 37.89906311035156, 37.90892028808594, 37.91872787475586, 37.928489685058594, 37.93820571899414, 37.947879791259766, 37.95750427246094, 37.96708297729492, 37.97662353515625, 37.986114501953125, 37.99556350708008, 38.004966735839844, 38.01432418823242, 38.023643493652344, 38.032920837402344, 38.04214859008789, 38.051334381103516, 38.06048583984375, 38.0695915222168, 38.078651428222656, 38.087669372558594, 38.096649169921875, 38.1055793762207, 38.114471435546875, 38.12332534790039, 38.132137298583984, 38.140907287597656, 38.149635314941406, 38.1583251953125, 38.166969299316406, 38.17557907104492, 38.18415069580078, 38.192684173583984, 38.201171875, 38.20962142944336, 38.21803283691406, 38.22639846801758, 38.23473358154297, 38.24302673339844, 38.25128173828125, 38.25950241088867, 38.26768112182617, 38.27582550048828, 38.28392791748047, 38.2919921875, 38.300018310546875, 38.30801010131836, 38.31596374511719, 38.32387924194336, 38.33176040649414, 38.339603424072266, 38.347412109375, 38.35518264770508, 38.3629150390625, 38.37062072753906, 38.37828826904297, 38.38591384887695, 38.39350509643555, 38.40106201171875, 38.40858459472656, 38.416072845458984, 38.423526763916016, 38.430946350097656, 38.438331604003906, 38.4456787109375, 38.4529914855957, 38.46027374267578, 38.46752166748047, 38.4747314453125, 38.481910705566406, 38.48905944824219, 38.49617385864258, 38.50325012207031, 38.51029968261719, 38.51731491088867, 38.524295806884766, 38.531246185302734, 38.53816223144531, 38.5450439453125, 38.55189895629883, 38.55872344970703, 38.565513610839844, 38.57227325439453, 38.579002380371094, 38.58570098876953, 38.59236526489258, 38.598995208740234, 38.6056022644043, 38.6121711730957, 38.618709564208984, 38.625221252441406, 38.6317024230957, 38.638153076171875, 38.64457702636719, 38.65096664428711, 38.65732955932617, 38.66366195678711, 38.66996383666992, 38.676239013671875, 38.68248748779297, 38.68870162963867, 38.69488525390625, 38.701045989990234, 38.70718002319336, 38.713279724121094, 38.71935272216797, 38.72539520263672, 38.73141098022461, 38.73740005493164, 38.74335861206055, 38.749290466308594, 38.75519561767578, 38.761070251464844, 38.76692199707031, 38.77274703979492, 38.778541564941406, 38.7843132019043, 38.79005813598633, 38.7957763671875, 38.80146789550781, 38.807132720947266, 38.81277084350586, 38.818382263183594, 38.8239631652832, 38.82952117919922, 38.83505630493164, 38.84056091308594, 38.84604263305664, 38.85150146484375, 38.85693359375, 38.86233901977539, 38.86771774291992, 38.87307357788086, 38.8784065246582, 38.88370895385742, 38.88899230957031, 38.894248962402344, 38.899478912353516, 38.90468978881836, 38.90987777709961, 38.915035247802734, 38.920169830322266, 38.9252815246582, 38.93037414550781, 38.93544006347656, 38.94047927856445, 38.945499420166016, 38.95049285888672, 38.95545959472656, 38.96040725708008, 38.965335845947266, 38.97024154663086, 38.97512435913086, 38.97998046875, 38.98481750488281, 38.989627838134766, 38.99441146850586, 38.999183654785156, 39.00393295288086, 39.0086555480957, 39.01335525512695, 39.018035888671875, 39.0226936340332, 39.0273323059082, 39.03194808959961, 39.03654098510742, 39.041114807128906, 39.04566192626953, 39.05018997192383, 39.0546989440918, 39.05918884277344, 39.063655853271484, 39.06809997558594, 39.07252502441406, 39.07693099975586, 39.08131790161133, 39.0856819152832, 39.090023040771484, 39.0943489074707, 39.09865188598633, 39.10293960571289, 39.107200622558594, 39.11144256591797, 39.11566925048828, 39.119876861572266, 39.124061584472656, 39.128231048583984, 39.13237380981445, 39.136497497558594, 39.14060974121094, 39.14470291137695, 39.148773193359375, 39.15282440185547, 39.15685272216797, 39.16086959838867, 39.16486358642578, 39.16883850097656, 39.172794342041016, 39.17673873901367, 39.18065643310547, 39.18455505371094, 39.188438415527344, 39.19230651855469, 39.1961555480957, 39.199989318847656, 39.20380401611328, 39.20759963989258, 39.21138000488281, 39.21514129638672, 39.2188835144043, 39.22261047363281, 39.226318359375, 39.230010986328125, 39.23368453979492, 39.237342834472656, 39.24098587036133, 39.244606018066406, 39.248207092285156, 39.251800537109375, 39.25537109375, 39.25893020629883, 39.262474060058594, 39.26599884033203, 39.269508361816406, 39.27299880981445, 39.27647018432617, 39.279930114746094, 39.28337478637695, 39.286800384521484, 39.29021072387695, 39.29360580444336, 39.29698181152344, 39.30034255981445, 39.303688049316406, 39.30702209472656, 39.31033706665039, 39.313636779785156, 39.31692123413086, 39.320194244384766, 39.323448181152344, 39.326690673828125, 39.329917907714844, 39.3331298828125, 39.33632278442383, 39.33950424194336, 39.34267044067383, 39.345821380615234, 39.34895706176758, 39.35207748413086, 39.355186462402344, 39.358280181884766, 39.36135482788086, 39.364418029785156, 39.367469787597656, 39.37051010131836, 39.373531341552734, 39.37653732299805, 39.37953567504883, 39.38251495361328, 39.38547897338867, 39.388427734375, 39.3913688659668, 39.39429473876953, 39.3972053527832, 39.40010070800781, 39.40298843383789, 39.405860900878906, 39.408721923828125, 39.411563873291016, 39.414398193359375, 39.41722106933594, 39.42002487182617, 39.422813415527344, 39.425594329833984, 39.42835998535156, 39.43111038208008, 39.43385314941406, 39.436580657958984, 39.439292907714844, 39.441993713378906, 39.44468688964844, 39.447364807128906, 39.45002746582031, 39.45268249511719, 39.455326080322266, 39.45795440673828, 39.4605712890625, 39.46318054199219, 39.46577072143555, 39.46834945678711, 39.470916748046875, 39.47346878051758, 39.47601318359375, 39.47854995727539, 39.48107147216797, 39.48358154296875, 39.486083984375, 39.48856735229492, 39.49104309082031, 39.49350357055664, 39.49595260620117, 39.49839401245117, 39.500823974609375, 39.50324630737305, 39.50564956665039, 39.50803756713867, 39.51042175292969, 39.51279830932617, 39.51516342163086, 39.517513275146484, 39.51985168457031, 39.522178649902344, 39.524497985839844, 39.52680587768555, 39.52910232543945, 39.53139114379883, 39.53366470336914, 39.535926818847656, 39.53818130493164, 39.54042434692383, 39.54265594482422, 39.54487991333008, 39.547096252441406, 39.54929733276367, 39.551490783691406, 39.553672790527344, 39.555843353271484, 39.55800247192383, 39.56015396118164, 39.562294006347656, 39.56442642211914, 39.56654739379883, 39.56866455078125, 39.570762634277344, 39.572853088378906, 39.5749397277832, 39.5770149230957, 39.57907485961914, 39.58112716674805, 39.58317184448242, 39.585208892822266, 39.58723831176758, 39.589256286621094, 39.59126281738281, 39.593257904052734, 39.59524917602539, 39.59722900390625, 39.59920120239258, 39.60116195678711, 39.603111267089844, 39.60505294799805, 39.60698699951172, 39.608909606933594, 39.61082458496094, 39.61273193359375, 39.61463165283203, 39.616519927978516, 39.61840057373047, 39.62027359008789, 39.62213897705078, 39.62398910522461, 39.625831604003906, 39.62766647338867, 39.629493713378906, 39.631317138671875, 39.63312530517578, 39.634925842285156, 39.636722564697266, 39.638511657714844, 39.640289306640625, 39.642059326171875, 39.643821716308594, 39.64557647705078, 39.64731979370117, 39.64905548095703, 39.65078353881836, 39.65250778198242, 39.65422058105469, 39.65592575073242, 39.65761947631836, 39.65930938720703, 39.66099166870117, 39.66267013549805, 39.664337158203125, 39.665992736816406, 39.667640686035156, 39.66928482055664, 39.670921325683594, 39.672550201416016, 39.67416763305664, 39.675777435302734, 39.6773796081543, 39.67897415161133, 39.680564880371094, 39.68214797973633, 39.68372344970703, 39.6852912902832, 39.686851501464844, 39.68840408325195, 39.6899528503418, 39.69149398803711, 39.69302749633789, 39.69455337524414, 39.69607162475586, 39.69758224487305, 39.6990852355957, 39.70058059692383, 39.70206832885742, 39.70355224609375, 39.70502471923828, 39.70649337768555, 39.70795440673828, 39.70941162109375, 39.71086502075195, 39.71230697631836, 39.713741302490234, 39.71516799926758, 39.716590881347656, 39.71800994873047, 39.719417572021484, 39.720821380615234, 39.72221755981445, 39.72360610961914, 39.7249870300293, 39.72636413574219, 39.72773742675781, 39.729103088378906, 39.7304573059082, 39.731807708740234, 39.733154296875, 39.734493255615234, 39.73582077026367, 39.737152099609375, 39.73847579956055, 39.73978805541992, 39.741092681884766, 39.742393493652344, 39.743690490722656, 39.74497985839844, 39.74626159667969, 39.74754333496094, 39.74881362915039, 39.75007629394531, 39.751338958740234, 39.752593994140625, 39.753841400146484, 39.75508117675781, 39.756317138671875, 39.757545471191406, 39.75876998901367, 39.75999450683594, 39.761207580566406, 39.76241683959961, 39.76361846923828, 39.76482009887695, 39.76601028442383, 39.76719665527344, 39.76838302612305, 39.76955795288086, 39.770729064941406, 39.77189254760742, 39.773048400878906, 39.774200439453125, 39.77534866333008, 39.776493072509766, 39.77763366699219, 39.77876663208008, 39.77989196777344, 39.781009674072266, 39.78212356567383, 39.78322982788086, 39.78433609008789, 39.785438537597656, 39.786537170410156, 39.787628173828125, 39.78871154785156, 39.789791107177734, 39.79086685180664, 39.79193878173828, 39.79300308227539, 39.794063568115234, 39.79512023925781, 39.796173095703125, 39.79722213745117, 39.79826354980469, 39.79930114746094, 39.80033493041992, 39.801361083984375, 39.80238342285156, 39.80339813232422, 39.804412841796875, 39.805423736572266, 39.80642318725586, 39.80742263793945, 39.80842208862305, 39.809410095214844, 39.81039047241211, 39.811370849609375, 39.812339782714844, 39.81331253051758, 39.81428146362305, 39.815242767333984, 39.816200256347656, 39.81715393066406, 39.8181037902832, 39.81904983520508, 39.81998825073242, 39.8209228515625, 39.82185363769531, 39.82278060913086, 39.82370376586914, 39.824623107910156, 39.82553482055664, 39.82644271850586, 39.82734680175781, 39.8282470703125, 39.82914733886719, 39.83004379272461, 39.8309326171875, 39.831817626953125, 39.83269500732422, 39.83356857299805, 39.834434509277344, 39.83530044555664, 39.8361701965332, 39.837032318115234, 39.837886810302734, 39.83873748779297, 39.83958435058594, 39.840431213378906, 39.841270446777344, 39.84210205078125, 39.84293746948242, 39.84376907348633, 39.8445930480957, 39.84541320800781, 39.846229553222656, 39.8470458984375, 39.84785461425781, 39.84865951538086, 39.84946060180664, 39.850257873535156, 39.851051330566406, 39.85184097290039, 39.85262680053711, 39.853416442871094, 39.85420227050781, 39.854976654052734, 39.85574722290039, 39.85651779174805, 39.85728454589844, 39.85804748535156, 39.858802795410156, 39.85955810546875, 39.86031723022461, 39.86106491088867, 39.8618049621582, 39.862545013427734, 39.86328125, 39.864013671875, 39.86474609375, 39.865478515625, 39.86620330810547, 39.86692810058594, 39.86764907836914, 39.86836242675781, 39.86907196044922, 39.86977767944336, 39.8704833984375, 39.871185302734375, 39.87188720703125, 39.872581481933594, 39.87327194213867, 39.873958587646484, 39.87464141845703, 39.87532424926758, 39.87600326538086, 39.87667465209961, 39.877349853515625, 39.87801742553711, 39.87868118286133, 39.87934112548828, 39.8800048828125, 39.88066482543945, 39.881317138671875, 39.8819694519043, 39.88262176513672, 39.883262634277344, 39.8838996887207, 39.88453674316406, 39.88517379760742, 39.88581085205078, 39.886444091796875, 39.88706588745117, 39.88768768310547, 39.888309478759766, 39.8889274597168, 39.88954162597656, 39.890159606933594, 39.890769958496094, 39.89137649536133, 39.8919792175293, 39.892578125, 39.8931770324707, 39.89377212524414, 39.89436340332031, 39.894954681396484, 39.89554214477539, 39.8961296081543, 39.89670944213867, 39.89728927612305, 39.897865295410156, 39.8984375, 39.899009704589844, 39.89958190917969, 39.900150299072266, 39.90071487426758, 39.901275634765625, 39.90182876586914, 39.902381896972656, 39.90293502807617, 39.90348434448242, 39.90403366088867, 39.90458297729492, 39.90512466430664, 39.905662536621094, 39.90620040893555, 39.906734466552734, 39.907264709472656, 39.907798767089844, 39.9083251953125, 39.90884780883789, 39.90937042236328, 39.90989303588867, 39.9104118347168, 39.910926818847656, 39.911441802978516, 39.911949157714844, 39.912452697753906, 39.91295623779297, 39.9134635925293, 39.91396713256836, 39.914466857910156, 39.91496276855469, 39.91545486450195, 39.91594696044922, 39.91643524169922, 39.916927337646484, 39.91741180419922, 39.91789627075195, 39.91838073730469, 39.918861389160156, 39.919334411621094, 39.91980743408203, 39.92028045654297, 39.92074966430664, 39.92122268676758, 39.921688079833984, 39.922149658203125, 39.922607421875, 39.923065185546875, 39.92352294921875, 39.92397689819336, 39.9244270324707, 39.92487716674805, 39.925323486328125, 39.9257698059082, 39.926212310791016, 39.926658630371094, 39.927101135253906, 39.92753982543945, 39.927978515625, 39.92841339111328, 39.9288444519043, 39.92927551269531, 39.92970275878906, 39.93013000488281, 39.93055725097656, 39.93098068237305, 39.931400299072266, 39.93181610107422, 39.93223190307617, 39.932647705078125, 39.93306350708008, 39.9334716796875, 39.93387985229492, 39.93428421020508, 39.934688568115234, 39.93509292602539, 39.93549728393555, 39.93589401245117, 39.9362907409668, 39.936683654785156, 39.937076568603516, 39.937469482421875, 39.937862396240234, 39.93825149536133, 39.938636779785156, 39.939022064208984, 39.93940734863281, 39.93978500366211, 39.940162658691406, 39.9405403137207, 39.94091796875, 39.941287994384766, 39.9416618347168, 39.94203567504883, 39.942405700683594, 39.94276809692383, 39.94313430786133, 39.943504333496094, 39.94386291503906, 39.944217681884766, 39.944576263427734, 39.9449348449707, 39.94529342651367, 39.945648193359375, 39.94599914550781, 39.94635009765625, 39.94670104980469, 39.94704818725586, 39.947391510009766, 39.94773483276367, 39.94807815551758, 39.94841766357422, 39.948753356933594, 39.949092864990234, 39.94942855834961, 39.94976043701172, 39.950096130371094, 39.9504280090332, 39.95075988769531, 39.951087951660156, 39.951416015625, 39.95174026489258, 39.95206069946289, 39.95238494873047, 39.95270538330078, 39.95302963256836, 39.95335006713867, 39.95366287231445, 39.953975677490234, 39.954288482666016, 39.95460510253906, 39.95491409301758, 39.955223083496094, 39.95553207397461, 39.955841064453125, 39.956146240234375, 39.956451416015625, 39.956756591796875, 39.95705795288086, 39.95735549926758, 39.9576530456543, 39.957942962646484, 39.9582405090332, 39.958534240722656, 39.958824157714844, 39.95911407470703, 39.95940399169922, 39.959693908691406, 39.95998001098633, 39.96026611328125, 39.96055221557617, 39.96083450317383, 39.96112060546875, 39.96139907836914, 39.96167755126953, 39.96194839477539, 39.96222686767578, 39.96249771118164, 39.962772369384766, 39.963043212890625, 39.96331787109375, 39.963584899902344, 39.96384811401367, 39.96411895751953, 39.964385986328125, 39.96464920043945, 39.96491622924805, 39.965179443359375, 39.9654426574707, 39.965702056884766, 39.965965270996094, 39.96622085571289, 39.96648025512695, 39.96673583984375, 39.96699142456055, 39.96724319458008, 39.967498779296875, 39.96774673461914, 39.96799850463867, 39.96824645996094, 39.96849060058594, 39.9687385559082, 39.9689826965332, 39.96922302246094, 39.96946716308594, 39.96970748901367, 39.96994400024414, 39.970184326171875, 39.97042465209961, 39.970664978027344, 39.97090148925781, 39.97113800048828, 39.97136688232422, 39.971595764160156, 39.971832275390625, 39.97206115722656, 39.9722900390625, 39.97251892089844, 39.972747802734375, 39.97297668457031, 39.97320556640625, 39.97343063354492, 39.97365188598633, 39.973876953125, 39.97409439086914, 39.97431182861328, 39.97452926635742, 39.97475051879883, 39.97496795654297, 39.97518539428711, 39.97540283203125, 39.975616455078125, 39.975830078125, 39.976043701171875, 39.976253509521484, 39.976463317871094, 39.9766731262207, 39.97687911987305, 39.97708511352539, 39.977291107177734, 39.97749710083008, 39.97770309448242, 39.977909088134766, 39.978111267089844, 39.97831344604492, 39.978511810302734, 39.97871398925781, 39.978912353515625, 39.97910690307617, 39.979305267333984, 39.97949981689453, 39.97969055175781, 39.97988510131836, 39.980079650878906, 39.98027038574219, 39.9804573059082, 39.980648040771484, 39.9808349609375, 39.981021881103516, 39.9812126159668, 39.98139953613281, 39.98158264160156, 39.98176956176758, 39.981956481933594, 39.982139587402344, 39.982322692871094, 39.982505798339844, 39.98268508911133, 39.98286437988281, 39.98304748535156, 39.98322296142578, 39.983402252197266, 39.983577728271484, 39.9837532043457, 39.98392868041992, 39.98410415649414, 39.984275817871094, 39.98445129394531, 39.984622955322266, 39.98479461669922, 39.984962463378906, 39.985130310058594, 39.98530197143555, 39.98546600341797, 39.985633850097656, 39.98580551147461, 39.985965728759766, 39.98612976074219, 39.986297607421875, 39.9864616394043, 39.98662567138672, 39.986785888671875, 39.98694610595703, 39.98710632324219, 39.98727035522461, 39.987430572509766, 39.987586975097656, 39.98774719238281, 39.9879035949707, 39.98805618286133, 39.98820877075195, 39.988365173339844, 39.98851776123047, 39.988670349121094, 39.98882293701172, 39.98897171020508, 39.98912048339844, 39.9892692565918, 39.98942184448242, 39.98957061767578, 39.989715576171875, 39.989864349365234, 39.990013122558594, 39.99015808105469, 39.99030685424805, 39.990447998046875, 39.9905891418457, 39.9907341003418, 39.99087142944336, 39.99100875854492, 39.991153717041016, 39.991294860839844, 39.99143600463867, 39.9915771484375, 39.99171829223633, 39.99185562133789, 39.99199676513672, 39.992130279541016, 39.99226760864258, 39.992401123046875, 39.99253463745117, 39.992671966552734, 39.9928092956543, 39.992942810058594, 39.99307632446289, 39.99320983886719, 39.99333953857422, 39.99346923828125, 39.99360275268555, 39.99372863769531, 39.99385070800781, 39.993980407714844, 39.994110107421875, 39.99423599243164, 39.99436569213867, 39.9944953918457, 39.9946174621582, 39.9947395324707, 39.99486541748047, 39.994991302490234, 39.99510955810547, 39.99523162841797, 39.99535369873047, 39.99547576904297, 39.9955940246582, 39.99571228027344, 39.99583053588867, 39.995948791503906, 39.996063232421875, 39.996185302734375, 39.996299743652344, 39.99641799926758, 39.99653244018555, 39.996646881103516, 39.99676513671875, 39.99687957763672, 39.99699401855469, 39.997108459472656, 39.997222900390625, 39.99733352661133, 39.99744415283203, 39.997554779052734, 39.99766540527344, 39.99777603149414, 39.997886657714844, 39.99799728393555, 39.99810791015625, 39.99821853637695, 39.998329162597656, 39.998435974121094, 39.998538970947266, 39.9986457824707, 39.99875259399414, 39.99885559082031, 39.998958587646484, 39.99906539916992, 39.99917221069336, 39.99927520751953, 39.9993782043457, 39.99948501586914, 39.99958801269531, 39.99968719482422, 39.999786376953125, 39.9998893737793, 39.9999885559082, 40.000091552734375, 40.00019073486328, 40.00028610229492, 40.00038528442383, 40.000484466552734, 40.00058364868164, 40.00067901611328, 40.00077819824219, 40.000877380371094, 40.000972747802734, 40.001068115234375, 40.001163482666016, 40.001258850097656, 40.0013542175293, 40.00144577026367, 40.00154113769531, 40.00163269042969, 40.00172424316406, 40.00181579589844, 40.00190734863281, 40.00199508666992, 40.0020866394043, 40.002174377441406, 40.00226593017578, 40.002357482910156, 40.002445220947266, 40.002532958984375, 40.002620697021484, 40.00270462036133, 40.00279235839844, 40.00287628173828, 40.002967834472656, 40.0030517578125, 40.00313949584961, 40.00322723388672, 40.00331115722656, 40.00339126586914, 40.003475189208984, 40.00355529785156, 40.003639221191406, 40.00372314453125, 40.003807067871094, 40.00389099121094, 40.00397491455078, 40.00405502319336, 40.00413513183594, 40.004215240478516, 40.004295349121094, 40.00437545776367, 40.004451751708984, 40.00453186035156, 40.00461196899414, 40.00468826293945, 40.00476837158203, 40.004844665527344, 40.004920959472656, 40.00499725341797, 40.00507736206055, 40.005149841308594, 40.00522232055664, 40.00529479980469, 40.00537109375, 40.00544357299805, 40.00551986694336, 40.00559616088867, 40.005672454833984, 40.005741119384766, 40.00581741333008, 40.005889892578125, 40.00596618652344, 40.00603485107422, 40.006107330322266, 40.00617980957031, 40.006256103515625, 40.00632095336914, 40.00638961791992, 40.00645446777344, 40.00652313232422, 40.006591796875, 40.00666427612305, 40.006736755371094, 40.006805419921875, 40.00687026977539, 40.00693893432617, 40.00700759887695, 40.007076263427734, 40.00714111328125, 40.00720977783203, 40.00727081298828, 40.0073356628418, 40.00740051269531, 40.007469177246094, 40.00753402709961, 40.007598876953125, 40.007659912109375, 40.007728576660156, 40.00779342651367, 40.00785446166992, 40.00791931152344, 40.00798416137695, 40.00804901123047, 40.008113861083984, 40.008174896240234, 40.008235931396484, 40.008296966552734, 40.008358001708984, 40.0084228515625, 40.008480072021484, 40.008541107177734, 40.008602142333984, 40.008663177490234, 40.008724212646484, 40.008785247802734, 40.008846282958984, 40.00890350341797, 40.00895690917969, 40.00901412963867, 40.009071350097656, 40.009132385253906, 40.00918960571289, 40.009246826171875, 40.00930404663086, 40.009361267089844, 40.00941467285156, 40.00947189331055, 40.00952911376953, 40.00958251953125, 40.00963592529297, 40.00969314575195, 40.00975036621094, 40.009803771972656, 40.009857177734375, 40.00990676879883, 40.00996017456055, 40.010013580322266, 40.010066986083984, 40.0101203918457, 40.01017379760742, 40.010223388671875, 40.01027297973633, 40.01032257080078, 40.0103759765625, 40.01042938232422, 40.01047897338867, 40.01053237915039, 40.01058578491211, 40.01063537597656, 40.010684967041016, 40.010738372802734, 40.01079177856445, 40.010841369628906, 40.010887145996094, 40.01093673706055, 40.010986328125, 40.01103210449219, 40.01108169555664, 40.011131286621094, 40.01118087768555, 40.01123046875, 40.01127624511719, 40.01132583618164, 40.011375427246094, 40.011417388916016, 40.0114631652832, 40.01150894165039, 40.011558532714844, 40.011600494384766, 40.01164627075195, 40.011688232421875, 40.01173782348633, 40.01177978515625, 40.01182556152344, 40.01186752319336, 40.01191329956055, 40.011959075927734, 40.01200485229492, 40.012046813964844, 40.012088775634766, 40.01213073730469, 40.01217269897461, 40.01221466064453, 40.01225662231445, 40.01230239868164, 40.01234817504883, 40.012393951416016, 40.01243209838867, 40.01247024536133, 40.01251220703125, 40.01255416870117, 40.012596130371094, 40.012638092041016, 40.01268005371094, 40.01272201538086, 40.012760162353516, 40.01280212402344, 40.012840270996094, 40.012882232666016, 40.01292419433594, 40.01296615600586, 40.01300811767578, 40.01304626464844, 40.013084411621094, 40.013126373291016, 40.01316452026367, 40.01320266723633, 40.01323699951172, 40.013275146484375, 40.013309478759766, 40.01335144042969, 40.01338577270508, 40.013427734375, 40.013465881347656, 40.01350021362305, 40.0135383605957, 40.01357650756836, 40.013614654541016, 40.013648986816406, 40.0136833190918, 40.01372146606445, 40.01375961303711, 40.0137939453125, 40.013832092285156, 40.01386642456055, 40.01390075683594, 40.01393508911133, 40.01396942138672, 40.014007568359375, 40.0140380859375, 40.01407241821289, 40.014102935791016, 40.014137268066406, 40.0141716003418, 40.01420593261719, 40.01423645019531, 40.0142707824707, 40.014305114746094, 40.014339447021484, 40.01437759399414, 40.014408111572266, 40.014442443847656, 40.01447296142578, 40.01450729370117, 40.0145378112793, 40.01456832885742, 40.01459884643555, 40.01462936401367, 40.01466751098633, 40.01469802856445, 40.01472854614258, 40.0147590637207, 40.01478958129883, 40.01482009887695, 40.01485061645508, 40.01488494873047, 40.01491165161133, 40.01493835449219, 40.01496887207031, 40.01499938964844, 40.01503372192383, 40.01506042480469, 40.01509094238281, 40.01512145996094, 40.01515197753906, 40.01517868041992, 40.01520919799805, 40.01523971557617, 40.01526641845703, 40.01529312133789, 40.01531982421875, 40.015350341796875, 40.015380859375, 40.015411376953125, 40.015438079833984, 40.015464782714844, 40.01549530029297, 40.01552200317383, 40.01554870605469, 40.01557540893555, 40.015602111816406, 40.015628814697266, 40.015655517578125, 40.015682220458984, 40.015708923339844, 40.0157356262207, 40.01576614379883, 40.01578903198242, 40.01581573486328, 40.015838623046875, 40.015865325927734, 40.015892028808594, 40.01591873168945, 40.01594161987305, 40.015968322753906, 40.015995025634766, 40.01601791381836, 40.01604080200195, 40.01606750488281, 40.01609420776367, 40.016117095947266, 40.016143798828125, 40.016170501708984, 40.01619338989258, 40.01621627807617, 40.01624298095703, 40.016265869140625, 40.01628875732422, 40.01631546020508, 40.01633834838867, 40.016361236572266, 40.01638412475586, 40.01641082763672, 40.01643371582031, 40.01646041870117, 40.0164794921875, 40.01649856567383, 40.01652145385742, 40.016544342041016, 40.016571044921875, 40.01659393310547, 40.01661682128906, 40.016639709472656, 40.016658782958984, 40.01668167114258, 40.016700744628906, 40.0167236328125, 40.01673889160156, 40.016761779785156, 40.016780853271484, 40.016807556152344, 40.01683044433594, 40.01685333251953, 40.016876220703125, 40.01689529418945, 40.01691436767578, 40.016937255859375, 40.016963958740234, 40.0169792175293, 40.016998291015625, 40.01701736450195, 40.01704406738281, 40.017066955566406, 40.01708221435547, 40.01710510253906, 40.01712417602539, 40.017147064208984, 40.01716613769531, 40.01718521118164, 40.017208099365234, 40.01722717285156, 40.01724624633789, 40.017269134521484, 40.01728820800781, 40.017311096191406, 40.017330169677734, 40.0173454284668, 40.017364501953125, 40.01738357543945, 40.01740264892578, 40.01742172241211, 40.01744079589844, 40.01746368408203, 40.01748275756836, 40.01750183105469, 40.01751708984375, 40.01753616333008, 40.017555236816406, 40.017574310302734, 40.01759338378906, 40.01761245727539, 40.01762771606445, 40.01764678955078, 40.017662048339844, 40.01768112182617, 40.0177001953125, 40.01771926879883, 40.01773452758789, 40.01775360107422, 40.01776885986328, 40.01778793334961, 40.01780319213867, 40.017818450927734, 40.01783752441406, 40.01785659790039, 40.01787185668945, 40.017887115478516, 40.017906188964844, 40.017921447753906, 40.01793670654297, 40.0179557800293, 40.01797103881836, 40.01799011230469, 40.01800537109375, 40.01802062988281, 40.01803970336914, 40.0180549621582, 40.018070220947266, 40.01808547973633, 40.01810073852539, 40.01811599731445, 40.018131256103516, 40.01814651489258, 40.01816177368164, 40.0181770324707, 40.018192291259766, 40.018211364746094, 40.018226623535156, 40.01824188232422, 40.01825714111328, 40.018272399902344, 40.01828384399414, 40.01829528808594, 40.018314361572266, 40.0183219909668, 40.01833724975586, 40.018348693847656, 40.01836395263672, 40.01837921142578, 40.01839065551758, 40.01840591430664, 40.01841735839844, 40.018436431884766, 40.01844787597656, 40.018463134765625, 40.01847457885742, 40.018489837646484, 40.01850128173828, 40.01852035522461, 40.018531799316406, 40.01854705810547, 40.01856231689453, 40.01857376098633, 40.018585205078125, 40.01860046386719, 40.01861572265625, 40.01863098144531, 40.01864242553711, 40.01865768432617, 40.0186653137207, 40.0186767578125, 40.0186882019043, 40.01870346069336, 40.018714904785156, 40.01872634887695, 40.018741607666016, 40.01875686645508, 40.018768310546875, 40.01878356933594, 40.018795013427734, 40.0188102722168, 40.01881790161133, 40.01883316040039, 40.01884460449219, 40.018856048583984, 40.01886749267578, 40.01887512207031, 40.018890380859375, 40.018898010253906, 40.01891326904297, 40.0189208984375, 40.0189323425293, 40.018943786621094, 40.01895523071289, 40.01897048950195, 40.01898193359375, 40.01899337768555, 40.019004821777344, 40.01901626586914, 40.01902770996094, 40.019039154052734, 40.01905059814453, 40.01906204223633, 40.019073486328125, 40.019081115722656, 40.01909255981445, 40.019100189208984, 40.01911163330078, 40.01912307739258, 40.019134521484375, 40.01914978027344, 40.019161224365234, 40.019168853759766, 40.01918411254883, 40.01919174194336, 40.01919937133789, 40.01921081542969, 40.01921844482422, 40.019229888916016, 40.01924133300781, 40.01925277709961, 40.019264221191406, 40.01927185058594, 40.019287109375, 40.01929473876953, 40.01930618286133, 40.01931381225586, 40.019325256347656, 40.01933670043945, 40.019344329833984, 40.019351959228516, 40.01936340332031, 40.019371032714844, 40.01938247680664, 40.01939010620117, 40.0193977355957, 40.0194091796875, 40.0194206237793, 40.01942825317383, 40.01943588256836, 40.01944351196289, 40.01945114135742, 40.019466400146484, 40.01947021484375, 40.019474029541016, 40.01948547363281, 40.019493103027344, 40.01950454711914, 40.01951217651367, 40.01952362060547, 40.019535064697266, 40.0195426940918, 40.019554138183594, 40.019561767578125, 40.01957321166992, 40.01958084106445, 40.019588470458984, 40.01959991455078, 40.01960754394531, 40.019615173339844, 40.01962661743164, 40.019630432128906, 40.01963806152344, 40.0196418762207, 40.019649505615234, 40.01966094970703, 40.01966857910156, 40.019676208496094, 40.019683837890625, 40.019691467285156, 40.01970291137695, 40.019710540771484, 40.01971435546875, 40.01972198486328, 40.01972961425781, 40.01974105834961, 40.019744873046875, 40.019752502441406, 40.0197639465332, 40.01976776123047, 40.019771575927734, 40.01978302001953, 40.01979064941406, 40.019798278808594, 40.01980972290039, 40.019813537597656, 40.01981735229492, 40.01982498168945, 40.01983642578125, 40.01984405517578, 40.01984786987305, 40.01985549926758, 40.01986312866211, 40.01987075805664, 40.019874572753906, 40.01987838745117, 40.01988983154297, 40.0198974609375, 40.019901275634766, 40.0199089050293, 40.01991653442383, 40.01992416381836, 40.019927978515625, 40.019935607910156, 40.01994323730469, 40.01995086669922, 40.01995849609375, 40.01996994018555, 40.01997756958008, 40.019981384277344, 40.01998519897461, 40.01999282836914, 40.02000045776367, 40.02000427246094, 40.02001190185547, 40.02001953125, 40.02002716064453, 40.02003479003906, 40.02003860473633, 40.020050048828125, 40.020050048828125, 40.020057678222656, 40.02006149291992, 40.02006912231445, 40.02008056640625, 40.020084381103516, 40.02009201049805, 40.02009963989258, 40.020103454589844, 40.02010726928711, 40.02011489868164, 40.02012252807617, 40.0201301574707, 40.02013397216797, 40.020137786865234, 40.020145416259766, 40.0201530456543, 40.02015686035156, 40.020164489746094, 40.02016830444336, 40.020172119140625, 40.020179748535156, 40.02018737792969, 40.02019119262695, 40.02019500732422, 40.020198822021484, 40.020206451416016, 40.02021408081055, 40.02021789550781, 40.02022171020508, 40.020225524902344, 40.020233154296875, 40.020240783691406, 40.02024459838867, 40.0202522277832, 40.020259857177734, 40.020263671875, 40.020267486572266, 40.02027130126953, 40.02027893066406, 40.020286560058594, 40.02029037475586, 40.020294189453125, 40.020301818847656, 40.020301818847656, 40.02030563354492, 40.02030944824219, 40.02031707763672, 40.02032470703125, 40.02032470703125, 40.020328521728516, 40.02033615112305, 40.02034378051758, 40.02035140991211, 40.020355224609375, 40.020362854003906, 40.02036666870117, 40.02037048339844, 40.0203742980957, 40.0203742980957, 40.02037811279297, 40.020381927490234, 40.02039337158203, 40.0203971862793, 40.02040100097656, 40.02040481567383, 40.020408630371094, 40.020416259765625, 40.020423889160156, 40.020423889160156, 40.02042770385742, 40.02043151855469, 40.02043533325195, 40.020442962646484, 40.02044677734375, 40.02045440673828, 40.02045822143555, 40.02046585083008, 40.020469665527344, 40.020477294921875, 40.02048110961914, 40.020484924316406, 40.02048873901367, 40.02049255371094, 40.0204963684082, 40.02050018310547, 40.020503997802734, 40.0205078125, 40.020511627197266, 40.0205192565918, 40.02052688598633, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.02053451538086, 40.02054214477539, 40.020545959472656, 40.02054977416992, 40.02055358886719, 40.02056121826172]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a3c0ced5-19d4-43e9-9106-6fd5e0d1cd84');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSjHFe4A3x6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e434a40-dd06-4024-dd6e-6861bcbe7981"
      },
      "source": [
        "X_test_RB_ITERATIVE = np.concatenate((S2RBX, S3RBX, S4RBX, S5RBX, S6RBX), axis=0)\n",
        "print(\"X_test_RB_ITERATIVE:\", X_test_RB_ITERATIVE[0:5])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_test_RB_ITERATIVE = X_test_RB_ITERATIVE.reshape((X_test_RB_ITERATIVE.shape[0], X_test_RB_ITERATIVE.shape[1], n_features))\n",
        "\n",
        "print(\"X_test_RT_ITERATIVE_scaled\", X_test_RB_ITERATIVE[0])\n",
        "\n",
        "y_test_RB_ITERATIVE = Iterative_Test(X_test_RB_ITERATIVE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020546]]\n",
            "Next yhat, before iteration: [[37.931633]]\n",
            "Linesleft_: 63\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02055]]\n",
            "Next yhat, before iteration: [[37.931633]]\n",
            "Linesleft_: 62\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020554]]\n",
            "Next yhat, before iteration: [[37.93164]]\n",
            "Linesleft_: 61\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02056]]\n",
            "Next yhat, before iteration: [[37.93164]]\n",
            "Linesleft_: 60\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]]\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020565]]\n",
            "Next yhat, before iteration: [[37.931644]]\n",
            "Linesleft_: 59\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]]\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02057]]\n",
            "Next yhat, before iteration: [[37.931644]]\n",
            "Linesleft_: 58\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020576]]\n",
            "Next yhat, before iteration: [[37.931644]]\n",
            "Linesleft_: 57\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020576]]\n",
            "Next yhat, before iteration: [[37.931644]]\n",
            "Linesleft_: 56\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02058]]\n",
            "Next yhat, before iteration: [[37.93165]]\n",
            "Linesleft_: 55\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020584]]\n",
            "Next yhat, before iteration: [[37.93165]]\n",
            "Linesleft_: 54\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02059]]\n",
            "Next yhat, before iteration: [[37.931652]]\n",
            "Linesleft_: 53\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020596]]\n",
            "Next yhat, before iteration: [[37.931656]]\n",
            "Linesleft_: 52\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.0206]]\n",
            "Next yhat, before iteration: [[37.93166]]\n",
            "Linesleft_: 51\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020603]]\n",
            "Next yhat, before iteration: [[37.93166]]\n",
            "Linesleft_: 50\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020607]]\n",
            "Next yhat, before iteration: [[37.931664]]\n",
            "Linesleft_: 49\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02061]]\n",
            "Next yhat, before iteration: [[37.931664]]\n",
            "Linesleft_: 48\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020615]]\n",
            "Next yhat, before iteration: [[37.931664]]\n",
            "Linesleft_: 47\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020615]]\n",
            "Next yhat, before iteration: [[37.931664]]\n",
            "Linesleft_: 46\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020615]]\n",
            "Next yhat, before iteration: [[37.931667]]\n",
            "Linesleft_: 45\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020622]]\n",
            "Next yhat, before iteration: [[37.931667]]\n",
            "Linesleft_: 44\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020626]]\n",
            "Next yhat, before iteration: [[37.931667]]\n",
            "Linesleft_: 43\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02063]]\n",
            "Next yhat, before iteration: [[37.931667]]\n",
            "Linesleft_: 42\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020634]]\n",
            "Next yhat, before iteration: [[37.93167]]\n",
            "Linesleft_: 41\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020638]]\n",
            "Next yhat, before iteration: [[37.931675]]\n",
            "Linesleft_: 40\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02064]]\n",
            "Next yhat, before iteration: [[37.93168]]\n",
            "Linesleft_: 39\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02064]]\n",
            "Next yhat, before iteration: [[37.931683]]\n",
            "Linesleft_: 38\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02064]]\n",
            "Next yhat, before iteration: [[37.931683]]\n",
            "Linesleft_: 37\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02065]]\n",
            "Next yhat, before iteration: [[37.931683]]\n",
            "Linesleft_: 36\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020653]]\n",
            "Next yhat, before iteration: [[37.931683]]\n",
            "Linesleft_: 35\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02065277]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020657]]\n",
            "Next yhat, before iteration: [[37.931683]]\n",
            "Linesleft_: 34\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02065277]\n",
            " [40.02065659]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02065659]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02066]]\n",
            "Next yhat, before iteration: [[37.931686]]\n",
            "Linesleft_: 33\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02065659]\n",
            " [40.0206604 ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206604]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020664]]\n",
            "Next yhat, before iteration: [[37.931686]]\n",
            "Linesleft_: 32\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206604 ]\n",
            " [40.02066422]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02066422]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020668]]\n",
            "Next yhat, before iteration: [[37.931686]]\n",
            "Linesleft_: 31\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02066422]\n",
            " [40.02066803]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02066803]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020676]]\n",
            "Next yhat, before iteration: [[37.93169]]\n",
            "Linesleft_: 30\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]]\n",
            "Next Unscaled Input Iterated:  [[40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02066803]\n",
            " [40.02067566]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02067566]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]\n",
            "  [40.02067566]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]\n",
            "  [40.02067566]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02068]]\n",
            "Next yhat, before iteration: [[37.93169]]\n",
            "Linesleft_: 29\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]]\n",
            "Next Unscaled Input Iterated:  [[40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02067566]\n",
            " [40.02067947]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02067947]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]\n",
            "  [40.02067566]\n",
            "  [40.02067947]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]\n",
            "  [40.02067566]\n",
            "  [40.02067947]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020683]]\n",
            "Next yhat, before iteration: [[37.93169]]\n",
            "Linesleft_: 28\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]]\n",
            "Next Unscaled Input Iterated:  [[40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02067947]\n",
            " [40.02068329]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02068329]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]\n",
            "  [40.02067566]\n",
            "  [40.02067947]\n",
            "  [40.02068329]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]\n",
            "  [40.02067566]\n",
            "  [40.02067947]\n",
            "  [40.02068329]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020687]]\n",
            "Next yhat, before iteration: [[37.93169]]\n",
            "Linesleft_: 27\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]]\n",
            "Next Unscaled Input Iterated:  [[40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02068329]\n",
            " [40.0206871 ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206871]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]\n",
            " [36.1      ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]\n",
            "  [40.02067566]\n",
            "  [40.02067947]\n",
            "  [40.02068329]\n",
            "  [40.0206871 ]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02066422]\n",
            "  [40.02066803]\n",
            "  [40.02067566]\n",
            "  [40.02067947]\n",
            "  [40.02068329]\n",
            "  [40.0206871 ]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02069]]\n",
            "Next yhat, before iteration: [[37.931698]]\n",
            "Linesleft_: 26\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]]\n",
            "Next Unscaled Input Iterated:  [[40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206871 ]\n",
            " [40.02069092]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069092]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02066422]\n",
            "  [40.02066803]\n",
            "  [40.02067566]\n",
            "  [40.02067947]\n",
            "  [40.02068329]\n",
            "  [40.0206871 ]\n",
            "  [40.02069092]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02066803]\n",
            "  [40.02067566]\n",
            "  [40.02067947]\n",
            "  [40.02068329]\n",
            "  [40.0206871 ]\n",
            "  [40.02069092]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020695]]\n",
            "Next yhat, before iteration: [[37.931698]]\n",
            "Linesleft_: 25\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]]\n",
            "Next Unscaled Input Iterated:  [[40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069092]\n",
            " [40.02069473]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069473]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02066803]\n",
            "  [40.02067566]\n",
            "  [40.02067947]\n",
            "  [40.02068329]\n",
            "  [40.0206871 ]\n",
            "  [40.02069092]\n",
            "  [40.02069473]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02067566]\n",
            "  [40.02067947]\n",
            "  [40.02068329]\n",
            "  [40.0206871 ]\n",
            "  [40.02069092]\n",
            "  [40.02069473]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02066803]\n",
            " [40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.0207]]\n",
            "Next yhat, before iteration: [[37.9317]]\n",
            "Linesleft_: 24\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]]\n",
            "Next Unscaled Input Iterated:  [[40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069473]\n",
            " [40.02069855]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069855]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02067566]\n",
            "  [40.02067947]\n",
            "  [40.02068329]\n",
            "  [40.0206871 ]\n",
            "  [40.02069092]\n",
            "  [40.02069473]\n",
            "  [40.02069855]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02067947]\n",
            "  [40.02068329]\n",
            "  [40.0206871 ]\n",
            "  [40.02069092]\n",
            "  [40.02069473]\n",
            "  [40.02069855]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02067566]\n",
            " [40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020702]]\n",
            "Next yhat, before iteration: [[37.931705]]\n",
            "Linesleft_: 23\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]]\n",
            "Next Unscaled Input Iterated:  [[40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069855]\n",
            " [40.02070236]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02070236]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02067947]\n",
            "  [40.02068329]\n",
            "  [40.0206871 ]\n",
            "  [40.02069092]\n",
            "  [40.02069473]\n",
            "  [40.02069855]\n",
            "  [40.02070236]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02068329]\n",
            "  [40.0206871 ]\n",
            "  [40.02069092]\n",
            "  [40.02069473]\n",
            "  [40.02069855]\n",
            "  [40.02070236]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02067947]\n",
            " [40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02071]]\n",
            "Next yhat, before iteration: [[37.931705]]\n",
            "Linesleft_: 22\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]]\n",
            "Next Unscaled Input Iterated:  [[40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02070236]\n",
            " [40.02070999]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02070999]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02068329]\n",
            "  [40.0206871 ]\n",
            "  [40.02069092]\n",
            "  [40.02069473]\n",
            "  [40.02069855]\n",
            "  [40.02070236]\n",
            "  [40.02070999]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0206871 ]\n",
            "  [40.02069092]\n",
            "  [40.02069473]\n",
            "  [40.02069855]\n",
            "  [40.02070236]\n",
            "  [40.02070999]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02068329]\n",
            " [40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020714]]\n",
            "Next yhat, before iteration: [[37.93171]]\n",
            "Linesleft_: 21\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]]\n",
            "Next Unscaled Input Iterated:  [[40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02070999]\n",
            " [40.02071381]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02071381]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0206871 ]\n",
            "  [40.02069092]\n",
            "  [40.02069473]\n",
            "  [40.02069855]\n",
            "  [40.02070236]\n",
            "  [40.02070999]\n",
            "  [40.02071381]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02069092]\n",
            "  [40.02069473]\n",
            "  [40.02069855]\n",
            "  [40.02070236]\n",
            "  [40.02070999]\n",
            "  [40.02071381]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0206871 ]\n",
            " [40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020718]]\n",
            "Next yhat, before iteration: [[37.93171]]\n",
            "Linesleft_: 20\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]]\n",
            "Next Unscaled Input Iterated:  [[40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02071381]\n",
            " [40.02071762]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02071762]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02069092]\n",
            "  [40.02069473]\n",
            "  [40.02069855]\n",
            "  [40.02070236]\n",
            "  [40.02070999]\n",
            "  [40.02071381]\n",
            "  [40.02071762]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02069473]\n",
            "  [40.02069855]\n",
            "  [40.02070236]\n",
            "  [40.02070999]\n",
            "  [40.02071381]\n",
            "  [40.02071762]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02069092]\n",
            " [40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02072]]\n",
            "Next yhat, before iteration: [[37.93171]]\n",
            "Linesleft_: 19\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]]\n",
            "Next Unscaled Input Iterated:  [[40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02071762]\n",
            " [40.02072144]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072144]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02069473]\n",
            "  [40.02069855]\n",
            "  [40.02070236]\n",
            "  [40.02070999]\n",
            "  [40.02071381]\n",
            "  [40.02071762]\n",
            "  [40.02072144]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02069855]\n",
            "  [40.02070236]\n",
            "  [40.02070999]\n",
            "  [40.02071381]\n",
            "  [40.02071762]\n",
            "  [40.02072144]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02069473]\n",
            " [40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020725]]\n",
            "Next yhat, before iteration: [[37.931713]]\n",
            "Linesleft_: 18\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]]\n",
            "Next Unscaled Input Iterated:  [[40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072144]\n",
            " [40.02072525]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072525]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02069855]\n",
            "  [40.02070236]\n",
            "  [40.02070999]\n",
            "  [40.02071381]\n",
            "  [40.02071762]\n",
            "  [40.02072144]\n",
            "  [40.02072525]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02070236]\n",
            "  [40.02070999]\n",
            "  [40.02071381]\n",
            "  [40.02071762]\n",
            "  [40.02072144]\n",
            "  [40.02072525]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02069855]\n",
            " [40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02073]]\n",
            "Next yhat, before iteration: [[37.931717]]\n",
            "Linesleft_: 17\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]]\n",
            "Next Unscaled Input Iterated:  [[40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072525]\n",
            " [40.02072906]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072906]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02070236]\n",
            "  [40.02070999]\n",
            "  [40.02071381]\n",
            "  [40.02071762]\n",
            "  [40.02072144]\n",
            "  [40.02072525]\n",
            "  [40.02072906]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02070999]\n",
            "  [40.02071381]\n",
            "  [40.02071762]\n",
            "  [40.02072144]\n",
            "  [40.02072525]\n",
            "  [40.02072906]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02070236]\n",
            " [40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020733]]\n",
            "Next yhat, before iteration: [[37.931717]]\n",
            "Linesleft_: 16\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]]\n",
            "Next Unscaled Input Iterated:  [[40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072906]\n",
            " [40.02073288]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02073288]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02070999]\n",
            "  [40.02071381]\n",
            "  [40.02071762]\n",
            "  [40.02072144]\n",
            "  [40.02072525]\n",
            "  [40.02072906]\n",
            "  [40.02073288]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02071381]\n",
            "  [40.02071762]\n",
            "  [40.02072144]\n",
            "  [40.02072525]\n",
            "  [40.02072906]\n",
            "  [40.02073288]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02070999]\n",
            " [40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020737]]\n",
            "Next yhat, before iteration: [[37.931717]]\n",
            "Linesleft_: 15\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]]\n",
            "Next Unscaled Input Iterated:  [[40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02073288]\n",
            " [40.02073669]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02073669]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02071381]\n",
            "  [40.02071762]\n",
            "  [40.02072144]\n",
            "  [40.02072525]\n",
            "  [40.02072906]\n",
            "  [40.02073288]\n",
            "  [40.02073669]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02071762]\n",
            "  [40.02072144]\n",
            "  [40.02072525]\n",
            "  [40.02072906]\n",
            "  [40.02073288]\n",
            "  [40.02073669]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02071381]\n",
            " [40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02074]]\n",
            "Next yhat, before iteration: [[37.93172]]\n",
            "Linesleft_: 14\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]]\n",
            "Next Unscaled Input Iterated:  [[40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02073669]\n",
            " [40.02074051]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074051]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02071762]\n",
            "  [40.02072144]\n",
            "  [40.02072525]\n",
            "  [40.02072906]\n",
            "  [40.02073288]\n",
            "  [40.02073669]\n",
            "  [40.02074051]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02072144]\n",
            "  [40.02072525]\n",
            "  [40.02072906]\n",
            "  [40.02073288]\n",
            "  [40.02073669]\n",
            "  [40.02074051]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02071762]\n",
            " [40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020744]]\n",
            "Next yhat, before iteration: [[37.93172]]\n",
            "Linesleft_: 13\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]]\n",
            "Next Unscaled Input Iterated:  [[40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074051]\n",
            " [40.02074432]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074432]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02072144]\n",
            "  [40.02072525]\n",
            "  [40.02072906]\n",
            "  [40.02073288]\n",
            "  [40.02073669]\n",
            "  [40.02074051]\n",
            "  [40.02074432]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02072525]\n",
            "  [40.02072906]\n",
            "  [40.02073288]\n",
            "  [40.02073669]\n",
            "  [40.02074051]\n",
            "  [40.02074432]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02072144]\n",
            " [40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02075]]\n",
            "Next yhat, before iteration: [[37.93172]]\n",
            "Linesleft_: 12\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]]\n",
            "Next Unscaled Input Iterated:  [[40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074432]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02072525]\n",
            "  [40.02072906]\n",
            "  [40.02073288]\n",
            "  [40.02073669]\n",
            "  [40.02074051]\n",
            "  [40.02074432]\n",
            "  [40.02074814]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02072906]\n",
            "  [40.02073288]\n",
            "  [40.02073669]\n",
            "  [40.02074051]\n",
            "  [40.02074432]\n",
            "  [40.02074814]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02072525]\n",
            " [40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02075]]\n",
            "Next yhat, before iteration: [[37.931725]]\n",
            "Linesleft_: 11\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]]\n",
            "Next Unscaled Input Iterated:  [[40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02072906]\n",
            "  [40.02073288]\n",
            "  [40.02073669]\n",
            "  [40.02074051]\n",
            "  [40.02074432]\n",
            "  [40.02074814]\n",
            "  [40.02074814]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02073288]\n",
            "  [40.02073669]\n",
            "  [40.02074051]\n",
            "  [40.02074432]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02072906]\n",
            " [40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02075]]\n",
            "Next yhat, before iteration: [[37.931725]]\n",
            "Linesleft_: 10\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]]\n",
            "Next Unscaled Input Iterated:  [[40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02073288]\n",
            "  [40.02073669]\n",
            "  [40.02074051]\n",
            "  [40.02074432]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02073669]\n",
            "  [40.02074051]\n",
            "  [40.02074432]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02073288]\n",
            " [40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02075]]\n",
            "Next yhat, before iteration: [[37.931725]]\n",
            "Linesleft_: 9\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]]\n",
            "Next Unscaled Input Iterated:  [[40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02073669]\n",
            "  [40.02074051]\n",
            "  [40.02074432]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02074051]\n",
            "  [40.02074432]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02073669]\n",
            " [40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020756]]\n",
            "Next yhat, before iteration: [[37.931725]]\n",
            "Linesleft_: 8\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]]\n",
            "Next Unscaled Input Iterated:  [[40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02075577]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02074051]\n",
            "  [40.02074432]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02075577]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02074432]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02075577]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02074051]\n",
            " [40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020756]]\n",
            "Next yhat, before iteration: [[37.931725]]\n",
            "Linesleft_: 7\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075577]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02074432]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02075577]\n",
            "  [40.02075577]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02075577]\n",
            "  [40.02075577]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02074432]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02076]]\n",
            "Next yhat, before iteration: [[37.93173]]\n",
            "Linesleft_: 6\n",
            "Linesleft1: 6\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075958]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02075577]\n",
            "  [40.02075577]\n",
            "  [40.02075958]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02075577]\n",
            "  [40.02075577]\n",
            "  [40.02075958]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020767]]\n",
            "Next yhat, before iteration: [[37.93173]]\n",
            "Linesleft_: 5\n",
            "Linesleft1: 5\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02075577]\n",
            "  [40.02075577]\n",
            "  [40.02075958]\n",
            "  [40.02076721]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02075577]\n",
            "  [40.02075577]\n",
            "  [40.02075958]\n",
            "  [40.02076721]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02077]]\n",
            "Next yhat, before iteration: [[37.93173]]\n",
            "Linesleft_: 4\n",
            "Linesleft1: 4\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]]\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [36.1       ]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02074814]\n",
            "  [40.02074814]\n",
            "  [40.02075577]\n",
            "  [40.02075577]\n",
            "  [40.02075958]\n",
            "  [40.02076721]\n",
            "  [40.02077103]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02074814]\n",
            "  [40.02075577]\n",
            "  [40.02075577]\n",
            "  [40.02075958]\n",
            "  [40.02076721]\n",
            "  [40.02077103]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02074814]\n",
            " [40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.02077]]\n",
            "Next yhat, before iteration: [[37.931736]]\n",
            "Linesleft_: 3\n",
            "Linesleft1: 3\n",
            "Next Unscaled Input Iterated:  [[40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [40.02077103]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [40.02077103]\n",
            " [36.1       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [40.02077103]\n",
            " [36.1       ]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02074814]\n",
            "  [40.02075577]\n",
            "  [40.02075577]\n",
            "  [40.02075958]\n",
            "  [40.02076721]\n",
            "  [40.02077103]\n",
            "  [40.02077103]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02075577]\n",
            "  [40.02075577]\n",
            "  [40.02075958]\n",
            "  [40.02076721]\n",
            "  [40.02077103]\n",
            "  [40.02077103]\n",
            "  [36.1       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02074814]\n",
            " [40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [40.02077103]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [40.02077103]\n",
            " [36.1       ]]\n",
            "Current yhat_______________: [[40.020775]]\n",
            "Next yhat, before iteration: [[37.931736]]\n",
            "Linesleft_: 2\n",
            "Linesleft1: 2\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [40.02077103]\n",
            " [40.02077484]]\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [40.02077103]\n",
            " [40.02077484]\n",
            " [36.1       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02075577]\n",
            "  [40.02075577]\n",
            "  [40.02075958]\n",
            "  [40.02076721]\n",
            "  [40.02077103]\n",
            "  [40.02077103]\n",
            "  [40.02077484]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02075577]\n",
            "  [40.02075958]\n",
            "  [40.02076721]\n",
            "  [40.02077103]\n",
            "  [40.02077103]\n",
            "  [40.02077484]\n",
            "  [36.1       ]]]\n",
            "Current yhat_______________: [[40.02078]]\n",
            "Next yhat, before iteration: [[37.93174]]\n",
            "Linesleft_: 1\n",
            "Linesleft1: 1\n",
            "Next Unscaled Input Iterated:  [[40.02075577]\n",
            " [40.02075958]\n",
            " [40.02076721]\n",
            " [40.02077103]\n",
            " [40.02077103]\n",
            " [40.02077484]\n",
            " [40.02077866]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02075577]\n",
            "  [40.02075958]\n",
            "  [40.02076721]\n",
            "  [40.02077103]\n",
            "  [40.02077103]\n",
            "  [40.02077484]\n",
            "  [40.02077866]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: []\n",
            "Current yhat_______________: [[40.020782]]\n",
            "Linesleft_: 0\n",
            "Linesleft1: 0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "[35.862465 35.82638  35.90367  ... 40.020775 40.02078  40.020782]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "3Ul6VpBKtIfH",
        "outputId": "f2bd34b5-ca44-4a37-e560-9469dcb1da09"
      },
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=y_test_RB_NONITERATIVE, name=\"y_test_RB_NONITERATIVE\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_RB, name=\"y_RB\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_test_RB_ITERATIVE, name=\"y_test_RB_ITERATIVE\", line_shape='linear'))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"40914322-127c-44c7-8cd8-392be8eed0ca\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"40914322-127c-44c7-8cd8-392be8eed0ca\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '40914322-127c-44c7-8cd8-392be8eed0ca',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_test_RB_NONITERATIVE\", \"type\": \"scatter\", \"y\": [35.862464904785156, 35.8678092956543, 36.26593780517578, 36.07977294921875, 35.96529769897461, 35.659080505371094, 35.310089111328125, 35.36717224121094, 35.33753967285156, 35.25133514404297, 35.221778869628906, 35.3664665222168, 35.555450439453125, 35.563907623291016, 35.60029983520508, 35.70252227783203, 35.75891876220703, 35.89700698852539, 35.969749450683594, 35.991703033447266, 36.01220703125, 36.023189544677734, 36.02679443359375, 35.930294036865234, 35.84699249267578, 35.98577880859375, 36.0062255859375, 35.933258056640625, 35.83915328979492, 35.67790985107422, 35.531402587890625, 35.454368591308594, 35.315185546875, 35.28180694580078, 35.02753448486328, 34.801509857177734, 34.86762237548828, 34.96967697143555, 34.856082916259766, 34.86084747314453, 34.94390869140625, 35.008148193359375, 35.196197509765625, 35.18869400024414, 35.206817626953125, 35.322139739990234, 35.37438201904297, 35.40171432495117, 35.42046356201172, 35.427738189697266, 35.531524658203125, 35.58737564086914, 35.60204315185547, 35.61894226074219, 35.72639846801758, 35.78215789794922, 35.80052947998047, 35.81742858886719, 35.92488479614258, 35.84754943847656, 35.88279724121094, 36.00947570800781, 35.999725341796875, 36.03703308105469, 36.03718948364258, 33.7227897644043, 33.89210891723633, 33.97761535644531, 33.98790740966797, 34.1275520324707, 34.27585220336914, 34.285125732421875, 34.31734085083008, 34.42896270751953, 34.48120880126953, 34.50853729248047, 34.52728271484375, 34.53456115722656, 34.63835144042969, 34.56111145019531, 34.525367736816406, 34.56700897216797, 34.6566276550293, 34.71532440185547, 34.73249816894531, 34.72576904296875, 34.73305130004883, 34.736656188964844, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.840538024902344, 34.89268493652344, 34.90735626220703, 34.924251556396484, 34.9315299987793, 34.93513870239258, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.03901672363281, 34.95807647705078, 34.9896240234375, 35.116302490234375, 35.10654830932617, 35.14385986328125, 35.14401626586914, 35.13362121582031, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.18741226196289, 35.21348571777344, 35.220821380615234, 35.229270935058594, 35.23291015625, 35.23471450805664, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.33673858642578, 35.25579833984375, 35.28734588623047, 35.414031982421875, 35.404273986816406, 35.44158172607422, 35.441741943359375, 35.43135070800781, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.53522491455078, 35.58737564086914, 35.60204315185547, 35.61894226074219, 35.626220703125, 35.62982940673828, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.733707427978516, 35.96839141845703, 35.952857971191406, 35.893760681152344, 35.825096130371094, 35.8585205078125, 35.850196838378906, 35.86646270751953, 35.85564422607422, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.93218994140625, 35.98434066772461, 35.99901580810547, 36.01591110229492, 36.023189544677734, 36.02679443359375, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.080589294433594, 36.10666275024414, 36.06389617919922, 36.03069305419922, 36.05363464355469, 36.039588928222656, 36.04772186279297, 36.09239959716797, 36.10666275024414, 36.11399841308594, 36.1224479675293, 36.12608337402344, 36.228065490722656, 36.282066345214844, 36.396915435791016, 36.33287048339844, 36.304405212402344, 36.44983673095703, 36.36203384399414, 36.33283233642578, 36.37698745727539, 36.44297790527344, 36.36857986450195, 36.3353385925293, 36.353363037109375, 36.342796325683594, 36.34934616088867, 36.35185241699219, 36.32822036743164, 36.428401947021484, 36.480552673339844, 36.49522399902344, 36.512115478515625, 36.51939392089844, 36.52300262451172, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.77714920043945, 36.83118438720703, 36.79248046875, 36.72040939331055, 36.77619552612305, 36.74946594238281, 36.764015197753906, 36.748817443847656, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.82536697387695, 36.74442672729492, 36.775978088378906, 36.902652740478516, 36.89289855957031, 36.93021011352539, 36.93036651611328, 36.91997528076172, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.97376251220703, 36.99983596801758, 37.007171630859375, 37.01561737060547, 37.019256591796875, 37.021060943603516, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022918701171875, 37.022918701171875, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.423622131347656, 37.33551788330078, 37.32389450073242, 37.430484771728516, 37.434417724609375, 37.35005187988281, 37.28363037109375, 37.274986267089844, 37.243282318115234, 37.25584411621094, 37.245025634765625, 37.221397399902344, 37.52192687988281, 37.390254974365234, 37.35538101196289, 37.44767379760742, 37.44172668457031, 37.45421600341797, 37.467140197753906, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.520057678222656, 37.572208404541016, 37.48667907714844, 37.42027282714844, 37.466163635253906, 37.43806457519531, 37.45433044433594, 37.443511962890625, 37.620235443115234, 37.72454071044922, 37.75387954711914, 37.687469482421875, 37.61872100830078, 37.66455078125, 37.64025115966797, 37.652809143066406, 37.641990661621094, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.718536376953125, 37.63759994506836, 37.60185241699219, 37.64350128173828, 37.63294219970703, 37.739662170410156, 37.79431915283203, 37.685157775878906, 37.618751525878906, 37.664642333984375, 37.63654708862305, 37.65281295776367, 37.641990661621094, 37.61836242675781, 37.61836242675781, 37.718536376953125, 37.77069091796875, 37.785362243652344, 37.80226135253906, 37.809539794921875, 37.71294403076172, 37.70063018798828, 37.82427978515625, 37.79338073730469, 37.88679504394531, 37.833160400390625, 37.804893493652344, 37.82941818237305, 37.824134826660156, 37.82740783691406, 37.82865905761719, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.86693572998047, 37.89301300048828, 37.850242614746094, 37.85068893432617, 37.849609375, 37.84440231323242, 37.92280578613281, 37.90770721435547, 37.91935729980469, 37.919437408447266, 37.91423797607422, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91608810424805, 37.91608810424805, 38.016265869140625, 37.935325622558594, 37.89958190917969, 37.94123077392578, 37.930667877197266, 37.93721389770508, 38.039894104003906, 38.068416595458984, 37.98288345336914, 37.916481018066406, 37.962371826171875, 37.93427658081055, 37.950538635253906, 37.939720153808594, 38.016265869140625, 37.935325622558594, 37.89958190917969, 37.94123077392578, 37.930667877197266, 38.037391662597656, 38.092041015625, 38.08308792114258, 38.09998321533203, 38.107261657714844, 38.110870361328125, 38.014366149902344, 37.99835968017578, 37.988914489746094, 37.9076042175293, 37.975677490234375, 37.94466781616211, 37.93721008300781, 37.939720153808594, 38.016265869140625, 38.068416595458984, 37.98288345336914, 37.916481018066406, 37.962371826171875, 37.93427658081055, 37.950538635253906, 37.939720153808594, 38.016265869140625, 38.068416595458984, 38.08308792114258, 38.09998321533203, 38.107261657714844, 38.110870361328125, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.014366149902344, 37.916046142578125, 37.87792205810547, 37.76960372924805, 37.678733825683594, 37.605445861816406, 37.451683044433594, 37.32440948486328, 37.33009719848633, 37.37791061401367, 37.41548538208008, 37.412025451660156, 37.405296325683594, 37.412574768066406, 37.41618347167969, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.52006149291992, 37.572208404541016, 37.586875915527344, 37.60377502441406, 37.611053466796875, 37.614662170410156, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.51816177368164, 37.43486022949219, 37.473472595214844, 37.34156036376953, 37.27082061767578, 37.298614501953125, 37.243282318115234, 37.25584411621094, 37.24502182006836, 37.321571350097656, 37.37372589111328, 37.28819274902344, 37.28908157348633, 37.28691864013672, 37.27650451660156, 37.30022048950195, 37.2196044921875, 37.26765823364258, 37.14977264404297, 37.05901336669922, 37.30048751831055, 37.20109939575195, 37.207855224609375, 37.250328063964844, 37.22866439819336, 37.23658752441406, 37.14482116699219, 37.03789138793945, 37.076499938964844, 37.0447998046875, 37.00726318359375, 36.95478820800781, 36.95046615600586, 36.93461608886719, 36.940895080566406, 36.93548583984375, 36.97376251220703, 36.933292388916016, 36.91542053222656, 36.936241149902344, 36.93096160888672, 36.934234619140625, 36.93548583984375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.82347106933594, 36.740169525146484, 36.778778076171875, 36.747074127197266, 36.75963592529297, 36.748817443847656, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.62498474121094, 36.541683197021484, 36.580291748046875, 36.54859161376953, 36.5611572265625, 36.550331115722656, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.62688446044922, 36.67903137207031, 36.59349822998047, 36.527095794677734, 36.57298278808594, 36.544891357421875, 36.46095275878906, 36.43411636352539, 36.534141540527344, 36.403038024902344, 36.363059997558594, 36.38850402832031, 36.346405029296875, 36.3626708984375, 36.35185241699219, 36.32822036743164, 36.428401947021484, 36.480552673339844, 36.49522399902344, 36.512115478515625, 36.51939392089844, 36.52300262451172, 36.52670669555664, 36.52670669555664, 31.845746994018555, 31.858646392822266, 31.9625244140625, 31.88158416748047, 31.913129806518555, 32.313812255859375, 32.186363220214844, 32.22690963745117, 32.27130889892578, 32.24971389770508, 32.274505615234375, 32.28294372558594, 32.259315490722656, 32.259315490722656, 32.259315490722656, 32.259315490722656, 32.259315490722656, 32.3594970703125, 32.41164779663086, 32.42631530761719, 32.443214416503906, 32.45049285888672, 32.4541015625, 32.457801818847656, 32.457801818847656, 32.50788879394531, 32.46742248535156, 32.483192443847656, 32.546531677246094, 32.541656494140625, 32.56031036376953, 32.560386657714844, 32.55519104003906, 32.55704116821289, 32.55704116821289, 32.55704116821289, 32.55704116821289, 32.65721893310547, 32.57627868652344, 32.607826232910156, 32.73450469970703, 32.72475051879883, 32.76205825805664, 32.76221466064453, 32.75182342529297, 32.755523681640625, 32.755523681640625, 32.8557014465332, 32.90785217285156, 32.922523498535156, 32.939422607421875, 33.046875, 32.96954345703125, 32.9375, 32.97914505004883, 32.96858215332031, 33.07530975341797, 33.12996292114258, 33.121002197265625, 33.137901306152344, 33.145179748535156, 33.14878845214844, 33.152488708496094, 33.152488708496094, 33.152488708496094, 33.152488708496094, 33.152488708496094, 33.25267028808594, 33.17172622680664, 33.203277587890625, 33.329952239990234, 33.32019805908203, 33.357505798339844, 33.357662200927734, 33.34727096557617, 33.350975036621094, 33.350975036621094, 33.350975036621094, 33.40106201171875, 33.66511535644531, 33.47294998168945, 33.466331481933594, 33.516075134277344, 33.435550689697266, 33.51759719848633, 33.500186920166016, 33.471336364746094, 33.57402038574219, 33.469459533691406, 33.433712005615234, 33.4753532409668, 33.46479034423828, 33.57151412963867, 33.62617111206055, 33.61721420288086, 33.63410949707031, 33.64139175415039, 33.644996643066406, 33.64869689941406, 33.64869689941406, 33.64869689941406, 33.64869689941406, 33.64869689941406, 33.74887466430664, 33.801025390625, 33.81569290161133, 33.83259201049805, 33.839874267578125, 33.84347915649414, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.947357177734375, 33.999507904052734, 34.014183044433594, 34.03107452392578, 34.038352966308594, 34.04196548461914, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566955566406, 34.04566955566406, 34.145843505859375, 34.197994232177734, 34.21266174316406, 34.22956085205078, 34.236839294433594, 34.240447998046875, 34.24414825439453, 34.24414825439453, 34.29423904418945, 34.3203125, 34.32764434814453, 34.336097717285156, 34.3397331237793, 34.34153747558594, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.443565368652344, 34.4957160949707, 34.5103874206543, 34.52728271484375, 34.53456115722656, 34.538169860839844, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.642051696777344, 34.6942024230957, 34.7088737487793, 34.72576904296875, 34.73305130004883, 34.736656188964844, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.840538024902344, 34.89268493652344, 34.90735626220703, 34.92424774169922, 34.9315299987793, 34.93513870239258, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.03901672363281, 35.09116744995117, 35.105838775634766, 35.12273406982422, 35.13001251220703, 35.13362121582031, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.18741226196289, 35.21348571777344, 35.220821380615234, 35.229270935058594, 35.23291015625, 35.23471450805664, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.33673858642578, 35.388893127441406, 35.403568267822266, 35.42046356201172, 35.427738189697266, 35.43135070800781, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.63540267944336, 35.591346740722656, 35.58553695678711, 35.638832092285156, 35.64079666137695, 35.64871597290039, 35.65715789794922, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.833885192871094, 35.789833068847656, 35.76714324951172, 35.65380859375, 35.70398712158203, 35.67060089111328, 35.67528533935547, 35.65715789794922, 35.733707427978516, 35.65276336669922, 35.68431091308594, 35.67790222167969, 35.68502426147461, 35.83211898803711, 35.815242767333984, 35.83854675292969, 35.838706970214844, 35.928489685058594, 35.98434066772461, 35.898807525634766, 35.83240509033203, 35.8782958984375, 35.850196838378906, 36.11690902709961, 36.01720428466797, 35.97425079345703, 36.04657745361328, 36.04505157470703, 36.05521774291992, 36.065940856933594, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 35.930294036865234, 35.84699249267578, 35.88560485839844, 35.85389709472656, 35.86646270751953, 35.85564422607422, 35.93218994140625, 35.98434066772461, 35.99901580810547, 35.915706634521484, 35.839683532714844, 35.88190460205078, 35.85389709472656, 35.86646270751953, 35.95581817626953, 35.98434066772461, 35.898807525634766, 35.83240509033203, 35.8782958984375, 35.850196838378906, 35.86646270751953, 35.85564422607422, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.93218994140625, 35.98434066772461, 35.99901580810547, 36.01591110229492, 36.023189544677734, 36.02679443359375, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 35.930294036865234, 35.84699249267578, 35.88560485839844, 35.85389709472656, 35.86646270751953, 35.85564422607422, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.731807708740234, 35.64850616455078, 35.68711853027344, 35.65541076660156, 35.6679801940918, 35.65715789794922, 35.63352966308594, 35.5333251953125, 35.45002365112305, 35.48863983154297, 35.456932067871094, 35.56967544555664, 35.47791290283203, 35.418540954589844, 35.46018600463867, 35.449623107910156, 35.35596466064453, 35.275169372558594, 35.39033508300781, 35.277687072753906, 35.25450897216797, 35.28533172607422, 35.25114059448242, 35.257686614990234, 35.26019287109375, 35.23656463623047, 35.23656463623047, 35.78556823730469, 35.86936950683594, 35.935420989990234, 35.97438430786133, 35.99768829345703, 36.023094177246094, 36.080589294433594, 35.93838882446289, 35.81353759765625, 35.713653564453125, 35.82611083984375, 36.03441619873047, 36.09339141845703, 36.093753814697266, 36.19655990600586, 36.24768829345703, 36.2822265625, 36.31178283691406, 36.42109298706055, 36.47685241699219, 36.49522399902344, 36.512115478515625, 36.51939392089844, 36.6231803894043, 36.54594421386719, 36.51020050048828, 36.551841735839844, 36.541282653808594, 36.547828674316406, 36.550331115722656, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.4265022277832, 36.34320068359375, 36.381813049316406, 36.35010528564453, 36.3626708984375, 36.35185241699219, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.428401947021484, 36.34745788574219, 36.31171417236328, 36.35335922241211, 36.342796325683594, 36.34934616088867, 36.35185241699219, 36.32822036743164, 36.428401947021484, 36.480552673339844, 36.49522399902344, 36.41191101074219, 36.33589172363281, 36.37811279296875, 36.35010528564453, 36.3626708984375, 36.35185241699219, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.328224182128906, 36.328224182128906, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.428401947021484, 36.34745788574219, 36.31171417236328, 36.353363037109375, 36.342796325683594, 36.34934616088867, 36.35185241699219, 36.32822036743164, 36.428401947021484, 36.34745788574219, 36.31171417236328, 36.353363037109375, 36.342796325683594, 36.34934616088867, 36.35185241699219, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.328224182128906, 36.328224182128906, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.22801971435547, 36.212005615234375, 36.335655212402344, 36.204551696777344, 36.16457748413086, 36.19001770019531, 36.14792251586914, 36.164188385009766, 36.15336608886719, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.430274963378906, 36.2985954284668, 36.26372146606445, 36.356014251708984, 36.350067138671875, 36.262351989746094, 36.19197082519531, 36.183326721191406, 36.1516227722168, 36.164188385009766, 36.15336608886719, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.229915618896484, 36.14897537231445, 36.18052291870117, 36.30720520019531, 36.297447204589844, 36.33475875854492, 36.33491516113281, 36.32452392578125, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.328224182128906, 36.328224182128906, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.328224182128906, 36.328224182128906, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.428401947021484, 36.34745788574219, 36.379005432128906, 36.50568389892578, 36.495933532714844, 36.533241271972656, 36.53340148925781, 36.42279815673828, 36.410491943359375, 36.53413772583008, 36.50324249267578, 36.54656982421875, 36.53340148925781, 36.52300262451172, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.62688446044922, 36.54594421386719, 36.51020050048828, 36.551841735839844, 36.541282653808594, 36.547828674316406, 36.550331115722656, 36.52670669555664, 36.62688446044922, 36.54594421386719, 34.57621765136719, 34.52783966064453, 34.67339324951172, 34.71961212158203, 34.758399963378906, 34.91910934448242, 34.91301345825195, 34.93943786621094, 35.04570770263672, 34.954376220703125, 34.9896240234375, 35.201473236083984, 35.182708740234375, 35.18153381347656, 35.1778564453125, 35.22576904296875, 35.222984313964844, 35.24649429321289, 35.23991394042969, 35.23471450805664, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.436920166015625, 35.54121398925781, 35.47035217285156, 35.42085266113281, 35.37381362915039, 35.333316802978516, 35.4769287109375, 35.435211181640625, 35.45491027832031, 35.441741943359375, 35.43135070800781, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.63540267944336, 35.591346740722656, 35.58553695678711, 35.638832092285156, 35.64079666137695, 35.64871597290039, 35.65715789794922, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.733707427978516, 35.785858154296875, 35.80052947998047, 35.71722412109375, 35.641197204589844, 35.68341827392578, 35.65541076660156, 35.6679801940918, 35.65715789794922, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.733707427978516, 35.785858154296875, 35.80052947998047, 35.81742858886719, 35.82470703125, 35.82831573486328, 35.93218994140625, 35.85124969482422, 35.88279724121094, 35.87638854980469, 35.81621551513672, 35.97845458984375, 36.012916564941406, 36.02013397216797, 36.08962631225586, 36.0993537902832, 36.06019592285156, 36.03069305419922, 36.05363464355469, 36.039588928222656, 36.04772186279297, 36.04231262207031, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.080589294433594, 36.106658935546875, 36.06389617919922, 36.06433868408203, 36.129798889160156, 36.11615753173828, 36.139671325683594, 36.082984924316406, 36.036136627197266, 36.057289123535156, 36.04143524169922, 36.04772186279297, 36.04231262207031, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.080589294433594, 36.04011535644531, 36.05589294433594, 36.05268478393555, 36.02259826660156, 36.053627014160156, 36.04478454589844, 36.04106140136719, 36.09239959716797, 36.04011535644531, 36.022247314453125, 36.04306411743164, 36.03778839111328, 36.04106140136719, 36.04231262207031, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.080589294433594, 36.04011535644531, 36.022247314453125, 36.04306411743164, 36.03778839111328, 36.29150390625, 36.2725944519043, 36.24761199951172, 36.14218521118164, 36.19654083251953, 36.16522216796875, 36.1714973449707, 36.15336608886719, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RB\", \"type\": \"scatter\", \"y\": [36.0, 36.3, 36.1, 35.6, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.4, 35.6, 35.6, 35.6, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 36.0, 36.0, 35.8, 35.6, 35.4, 35.4, 35.2, 35.2, 35.1, 34.7, 34.5, 34.9, 34.9, 34.7, 34.9, 34.9, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 37.0, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.8, 37.4, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.8, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.8, 37.8, 37.8, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.8, 37.9, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.8, 37.8, 37.6, 37.6, 37.4, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.2, 37.4, 37.2, 37.4, 37.2, 37.2, 37.2, 37.0, 37.0, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 31.8, 32.0, 31.8, 32.0, 32.4, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 33.1, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.8, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 36.0, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.8, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 36.3, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 35.8, 35.6, 35.6, 35.8, 36.0, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.7, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.7, 34.5, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.2, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.6, 35.6, 35.4, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.5, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_test_RB_ITERATIVE\", \"type\": \"scatter\", \"y\": [35.862464904785156, 35.82638168334961, 35.90367126464844, 35.89115905761719, 35.953392028808594, 35.970924377441406, 35.98359680175781, 35.998775482177734, 36.017311096191406, 36.03718948364258, 36.05527877807617, 36.074241638183594, 36.09284973144531, 36.11125183105469, 36.129207611083984, 36.1472053527832, 36.165225982666016, 36.18312072753906, 36.200927734375, 36.218692779541016, 36.236358642578125, 36.2539176940918, 36.271400451660156, 36.288814544677734, 36.3061408996582, 36.323387145996094, 36.340553283691406, 36.35763931274414, 36.3746452331543, 36.39157485961914, 36.408424377441406, 36.42519760131836, 36.441890716552734, 36.4585075378418, 36.47504806518555, 36.491512298583984, 36.50790023803711, 36.52421188354492, 36.54044723510742, 36.55660629272461, 36.572689056396484, 36.58869552612305, 36.60462951660156, 36.620487213134766, 36.63627624511719, 36.6519889831543, 36.66762924194336, 36.683197021484375, 36.698692321777344, 36.714115142822266, 36.72946548461914, 36.744747161865234, 36.75996017456055, 36.77510070800781, 36.79016876220703, 36.80516815185547, 36.82009506225586, 36.834957122802734, 36.84975051879883, 36.86447525024414, 36.879127502441406, 36.893714904785156, 36.90822982788086, 36.92267990112305, 36.93706512451172, 36.951385498046875, 36.96563720703125, 36.979820251464844, 36.99394226074219, 37.00799560546875, 37.0219841003418, 37.03590774536133, 37.049766540527344, 37.063560485839844, 37.07728958129883, 37.0909538269043, 37.10456085205078, 37.118099212646484, 37.13157653808594, 37.14499282836914, 37.158348083496094, 37.17163848876953, 37.18486404418945, 37.19803237915039, 37.211143493652344, 37.22418975830078, 37.23717498779297, 37.250099182128906, 37.262962341308594, 37.2757682800293, 37.28851318359375, 37.30120086669922, 37.31382751464844, 37.32640075683594, 37.33890914916992, 37.35136032104492, 37.36375427246094, 37.376094818115234, 37.388370513916016, 37.40059280395508, 37.41276168823242, 37.42487335205078, 37.436927795410156, 37.44892501831055, 37.46086502075195, 37.47275161743164, 37.484580993652344, 37.49635696411133, 37.508079528808594, 37.519744873046875, 37.53135681152344, 37.54291915893555, 37.55442810058594, 37.565879821777344, 37.57727813720703, 37.588626861572266, 37.599918365478516, 37.61116027832031, 37.622344970703125, 37.633480072021484, 37.64456558227539, 37.65559768676758, 37.66658020019531, 37.67750930786133, 37.68838882446289, 37.699222564697266, 37.709999084472656, 37.72072219848633, 37.73140335083008, 37.74203109741211, 37.75261306762695, 37.763145446777344, 37.77363204956055, 37.78406524658203, 37.7944450378418, 37.80478286743164, 37.81507873535156, 37.825321197509766, 37.83551788330078, 37.845664978027344, 37.85576248168945, 37.865814208984375, 37.87582015991211, 37.88578414916992, 37.89569854736328, 37.90557098388672, 37.91539001464844, 37.925167083740234, 37.93489456176758, 37.944580078125, 37.9542236328125, 37.96382141113281, 37.9733772277832, 37.982887268066406, 37.992347717285156, 38.001766204833984, 38.011138916015625, 38.02047348022461, 38.029762268066406, 38.039005279541016, 38.04821014404297, 38.057376861572266, 38.06649398803711, 38.07556915283203, 38.084598541259766, 38.09358596801758, 38.102535247802734, 38.111446380615234, 38.12031173706055, 38.12914276123047, 38.1379280090332, 38.146671295166016, 38.155372619628906, 38.164031982421875, 38.17265701293945, 38.181243896484375, 38.18978500366211, 38.19828796386719, 38.20675277709961, 38.21517562866211, 38.22356414794922, 38.23190689086914, 38.240211486816406, 38.24848556518555, 38.256717681884766, 38.26490783691406, 38.27306365966797, 38.28118133544922, 38.28925704956055, 38.29729461669922, 38.305301666259766, 38.313270568847656, 38.321205139160156, 38.329097747802734, 38.336952209472656, 38.34476852416992, 38.3525505065918, 38.36030197143555, 38.368011474609375, 38.37568283081055, 38.383323669433594, 38.39093017578125, 38.39849853515625, 38.40603256225586, 38.41353225708008, 38.42099380493164, 38.42842483520508, 38.43581771850586, 38.443180084228516, 38.45050811767578, 38.45779800415039, 38.46506118774414, 38.472286224365234, 38.47947692871094, 38.48663330078125, 38.49375915527344, 38.500850677490234, 38.507911682128906, 38.51493453979492, 38.52192687988281, 38.52888870239258, 38.53581619262695, 38.5427131652832, 38.54957580566406, 38.5564079284668, 38.563209533691406, 38.56998062133789, 38.576717376708984, 38.58342361450195, 38.5900993347168, 38.59674072265625, 38.603355407714844, 38.60993576049805, 38.61648941040039, 38.62301254272461, 38.6295051574707, 38.63595962524414, 38.64238739013672, 38.6487922668457, 38.65517044067383, 38.6615104675293, 38.667823791503906, 38.67410659790039, 38.680362701416016, 38.68658447265625, 38.692779541015625, 38.69894790649414, 38.7050895690918, 38.71120071411133, 38.717281341552734, 38.72333526611328, 38.729366302490234, 38.7353630065918, 38.7413330078125, 38.747276306152344, 38.75318908691406, 38.75907897949219, 38.76493835449219, 38.77077102661133, 38.77657699584961, 38.78235626220703, 38.788108825683594, 38.79383087158203, 38.799530029296875, 38.80520248413086, 38.810848236083984, 38.81646728515625, 38.822059631347656, 38.8276252746582, 38.83316421508789, 38.83868408203125, 38.84417724609375, 38.849639892578125, 38.85507583618164, 38.86048889160156, 38.865882873535156, 38.87125015258789, 38.876590728759766, 38.88190460205078, 38.8871955871582, 38.892459869384766, 38.897701263427734, 38.90291976928711, 38.90811538696289, 38.91328430175781, 38.91843032836914, 38.92354965209961, 38.92864990234375, 38.933719635009766, 38.93876647949219, 38.94379425048828, 38.94879913330078, 38.95378112792969, 38.958740234375, 38.96367263793945, 38.96857833862305, 38.97346496582031, 38.97833251953125, 38.983177185058594, 38.987998962402344, 38.9927978515625, 38.9975700378418, 39.002323150634766, 39.00705337524414, 39.01176452636719, 39.016456604003906, 39.021121978759766, 39.02576446533203, 39.03038787841797, 39.03498840332031, 39.03956604003906, 39.04412078857422, 39.04865646362305, 39.05317306518555, 39.05767059326172, 39.06214904785156, 39.06660461425781, 39.07103729248047, 39.07544708251953, 39.079833984375, 39.084205627441406, 39.08855438232422, 39.0928840637207, 39.09719467163086, 39.10149002075195, 39.10575866699219, 39.110008239746094, 39.11423873901367, 39.11845016479492, 39.12263870239258, 39.12681198120117, 39.13096618652344, 39.13509750366211, 39.13921356201172, 39.143310546875, 39.14738464355469, 39.15143966674805, 39.155479431152344, 39.15950393676758, 39.16350173950195, 39.167484283447266, 39.171451568603516, 39.17539978027344, 39.17932891845703, 39.1832389831543, 39.1871337890625, 39.191009521484375, 39.194862365722656, 39.198699951171875, 39.202518463134766, 39.20631790161133, 39.210105895996094, 39.213871002197266, 39.217620849609375, 39.22135543823242, 39.22507095336914, 39.22876739501953, 39.232444763183594, 39.236106872558594, 39.23975372314453, 39.243385314941406, 39.24699783325195, 39.25059509277344, 39.254173278808594, 39.25773620605469, 39.26128005981445, 39.26481246948242, 39.26832580566406, 39.271820068359375, 39.27530288696289, 39.278770446777344, 39.28221893310547, 39.28564453125, 39.289058685302734, 39.29246139526367, 39.29584503173828, 39.29920959472656, 39.30256271362305, 39.3058967590332, 39.30921936035156, 39.31252670288086, 39.315818786621094, 39.319095611572266, 39.322357177734375, 39.325599670410156, 39.32883071899414, 39.33204650878906, 39.33524703979492, 39.33843231201172, 39.34160232543945, 39.34476089477539, 39.347900390625, 39.35102844238281, 39.35414505004883, 39.357242584228516, 39.360321044921875, 39.36338806152344, 39.3664436340332, 39.369483947753906, 39.37250900268555, 39.375526428222656, 39.3785285949707, 39.38151168823242, 39.38447952270508, 39.3874397277832, 39.390384674072266, 39.39331817626953, 39.39623260498047, 39.39913558959961, 39.40202331542969, 39.40489959716797, 39.40775680541992, 39.410606384277344, 39.4134407043457, 39.416263580322266, 39.41907501220703, 39.421871185302734, 39.424652099609375, 39.427425384521484, 39.43018341064453, 39.43292999267578, 39.435665130615234, 39.43838882446289, 39.44109344482422, 39.44378662109375, 39.44646453857422, 39.44913101196289, 39.451786041259766, 39.454429626464844, 39.457061767578125, 39.45968246459961, 39.4622917175293, 39.46488571166992, 39.46746826171875, 39.47003936767578, 39.47259521484375, 39.47514724731445, 39.477684020996094, 39.48020553588867, 39.48271942138672, 39.48522186279297, 39.487709045410156, 39.49019241333008, 39.49265670776367, 39.49510955810547, 39.497554779052734, 39.4999885559082, 39.502410888671875, 39.50482177734375, 39.50721740722656, 39.509605407714844, 39.51198196411133, 39.51434326171875, 39.51669692993164, 39.51904296875, 39.52137756347656, 39.52369689941406, 39.52600860595703, 39.5283088684082, 39.53059387207031, 39.53287124633789, 39.53514099121094, 39.53740310668945, 39.539649963378906, 39.54188919067383, 39.54411697387695, 39.546329498291016, 39.54853057861328, 39.55072784423828, 39.552913665771484, 39.555091857910156, 39.557254791259766, 39.559410095214844, 39.56155776977539, 39.563690185546875, 39.56581115722656, 39.567928314208984, 39.57003402709961, 39.57212829589844, 39.574214935302734, 39.576290130615234, 39.5783576965332, 39.580413818359375, 39.58246612548828, 39.584503173828125, 39.58652877807617, 39.58855056762695, 39.5905647277832, 39.592567443847656, 39.59455490112305, 39.596534729003906, 39.598506927490234, 39.60047149658203, 39.6024284362793, 39.604373931884766, 39.6063117980957, 39.60824203491211, 39.61016082763672, 39.61206817626953, 39.61396789550781, 39.61586380004883, 39.61774826049805, 39.61962127685547, 39.62148666381836, 39.62334442138672, 39.62519454956055, 39.62703323364258, 39.62886428833008, 39.63068771362305, 39.63249969482422, 39.63430404663086, 39.63610076904297, 39.63788604736328, 39.63966751098633, 39.64144515991211, 39.643211364746094, 39.64496612548828, 39.64671325683594, 39.64845275878906, 39.650184631347656, 39.65190887451172, 39.65362548828125, 39.65533447265625, 39.65703582763672, 39.65872573852539, 39.66040802001953, 39.662086486816406, 39.663753509521484, 39.66541290283203, 39.66706848144531, 39.66871643066406, 39.670352935791016, 39.6719856262207, 39.67361068725586, 39.67522430419922, 39.67683029174805, 39.678428649902344, 39.680023193359375, 39.68160629272461, 39.68318557739258, 39.68476104736328, 39.68632507324219, 39.6878776550293, 39.68942642211914, 39.69096755981445, 39.6925048828125, 39.69403076171875, 39.695552825927734, 39.69706726074219, 39.698577880859375, 39.700077056884766, 39.70156478881836, 39.70304870605469, 39.704524993896484, 39.705997467041016, 39.707462310791016, 39.70892333984375, 39.71037673950195, 39.71181869506836, 39.713253021240234, 39.71468734741211, 39.71611022949219, 39.717525482177734, 39.718936920166016, 39.720340728759766, 39.72174072265625, 39.7231330871582, 39.72452163696289, 39.72589874267578, 39.72726821899414, 39.7286376953125, 39.72999572753906, 39.73134994506836, 39.73270034790039, 39.73404312133789, 39.73537826538086, 39.7367057800293, 39.73802947998047, 39.73934555053711, 39.74065399169922, 39.74195861816406, 39.743255615234375, 39.74454879760742, 39.74583435058594, 39.74711227416992, 39.748390197753906, 39.749656677246094, 39.750919342041016, 39.752174377441406, 39.75342559814453, 39.754669189453125, 39.75590515136719, 39.757137298583984, 39.758365631103516, 39.75959014892578, 39.760807037353516, 39.762020111083984, 39.76322555541992, 39.76442337036133, 39.76561737060547, 39.76680374145508, 39.767982482910156, 39.769161224365234, 39.77033233642578, 39.77149963378906, 39.772666931152344, 39.77382278442383, 39.77497100830078, 39.77611541748047, 39.77725601196289, 39.77838897705078, 39.779510498046875, 39.780635833740234, 39.78175735473633, 39.78287124633789, 39.78397750854492, 39.78507995605469, 39.78617858886719, 39.78726577758789, 39.788352966308594, 39.78943634033203, 39.79050827026367, 39.79158020019531, 39.79264831542969, 39.7937126159668, 39.794769287109375, 39.79582214355469, 39.796871185302734, 39.79791259765625, 39.798946380615234, 39.799983978271484, 39.80101776123047, 39.80203628540039, 39.80305480957031, 39.804073333740234, 39.805084228515625, 39.80608367919922, 39.80708694458008, 39.80808639526367, 39.80907440185547, 39.810062408447266, 39.8110466003418, 39.8120231628418, 39.812992095947266, 39.813961029052734, 39.81492233276367, 39.81588363647461, 39.816837310791016, 39.817787170410156, 39.81873321533203, 39.819671630859375, 39.82060623168945, 39.82154083251953, 39.822471618652344, 39.82339859008789, 39.82432174682617, 39.825233459472656, 39.82614517211914, 39.82705307006836, 39.82795333862305, 39.82884979248047, 39.829742431640625, 39.83063507080078, 39.831520080566406, 39.832401275634766, 39.83327865600586, 39.83415222167969, 39.83502197265625, 39.83588790893555, 39.83675003051758, 39.837608337402344, 39.838462829589844, 39.83931350708008, 39.84016036987305, 39.84100341796875, 39.84183883666992, 39.84267044067383, 39.84349822998047, 39.844322204589844, 39.84514236450195, 39.8459587097168, 39.846771240234375, 39.84758377075195, 39.848392486572266, 39.84919738769531, 39.849998474121094, 39.85079574584961, 39.85158920288086, 39.852378845214844, 39.8531608581543, 39.853939056396484, 39.85471725463867, 39.85549545288086, 39.85626220703125, 39.85702896118164, 39.85779571533203, 39.85855484008789, 39.85931396484375, 39.86006546020508, 39.860816955566406, 39.86156463623047, 39.8623046875, 39.86304473876953, 39.8637809753418, 39.86451721191406, 39.8652458190918, 39.865970611572266, 39.866695404052734, 39.86741256713867, 39.868125915527344, 39.868839263916016, 39.86954879760742, 39.87025451660156, 39.87095642089844, 39.87165832519531, 39.87235641479492, 39.873050689697266, 39.873741149902344, 39.874427795410156, 39.8751106262207, 39.87579345703125, 39.876468658447266, 39.87714385986328, 39.87781524658203, 39.87847900390625, 39.87914276123047, 39.87980270385742, 39.88045883178711, 39.8811149597168, 39.88176727294922, 39.882415771484375, 39.883056640625, 39.88370132446289, 39.88433837890625, 39.88497543334961, 39.8856086730957, 39.88623809814453, 39.886863708496094, 39.88748550415039, 39.88810729980469, 39.88872528076172, 39.889347076416016, 39.88996124267578, 39.89057159423828, 39.891178131103516, 39.89178466796875, 39.89238739013672, 39.89298629760742, 39.89358139038086, 39.8941764831543, 39.8947639465332, 39.895355224609375, 39.89594268798828, 39.89652633666992, 39.8971061706543, 39.897682189941406, 39.898258209228516, 39.898834228515625, 39.8994026184082, 39.89997100830078, 39.900535583496094, 39.90109634399414, 39.90165710449219, 39.90221405029297, 39.902767181396484, 39.903316497802734, 39.90386199951172, 39.90441131591797, 39.90495681762695, 39.905494689941406, 39.906036376953125, 39.90657424926758, 39.90711212158203, 39.90764236450195, 39.90816879272461, 39.90869903564453, 39.909217834472656, 39.90974044799805, 39.91026306152344, 39.91078186035156, 39.91129684448242, 39.911808013916016, 39.912315368652344, 39.912818908691406, 39.913326263427734, 39.9138298034668, 39.914329528808594, 39.91482925415039, 39.91532516479492, 39.91581344604492, 39.91630554199219, 39.91679382324219, 39.91727828979492, 39.917762756347656, 39.91824722290039, 39.91872787475586, 39.91920852661133, 39.919681549072266, 39.92015075683594, 39.920623779296875, 39.92109298706055, 39.92155838012695, 39.92202377319336, 39.9224853515625, 39.92294692993164, 39.92340087890625, 39.923858642578125, 39.924312591552734, 39.92476272583008, 39.92521286010742, 39.9256591796875, 39.92610168457031, 39.926544189453125, 39.92698669433594, 39.927425384521484, 39.92786407470703, 39.92829895019531, 39.92873001098633, 39.929161071777344, 39.929588317871094, 39.930015563964844, 39.93043899536133, 39.93086242675781, 39.93128204345703, 39.931697845458984, 39.9321174621582, 39.932533264160156, 39.932945251464844, 39.93335723876953, 39.93376541137695, 39.934173583984375, 39.934574127197266, 39.93497848510742, 39.93537902832031, 39.93578338623047, 39.936180114746094, 39.93657302856445, 39.93696594238281, 39.93736267089844, 39.9377555847168, 39.938140869140625, 39.93852615356445, 39.93891143798828, 39.939292907714844, 39.93967819213867, 39.94005584716797, 39.9404296875, 39.9408073425293, 39.94118118286133, 39.94155502319336, 39.94192886352539, 39.94230270385742, 39.94266891479492, 39.943031311035156, 39.943397521972656, 39.943756103515625, 39.94411849975586, 39.94447708129883, 39.9448356628418, 39.9451904296875, 39.9455451965332, 39.945899963378906, 39.946250915527344, 39.946598052978516, 39.94694519042969, 39.94729232788086, 39.94763946533203, 39.94798278808594, 39.94832229614258, 39.94866180419922, 39.948997497558594, 39.94933319091797, 39.94967269897461, 39.95000457763672, 39.95033645629883, 39.9506721496582, 39.95100021362305, 39.951324462890625, 39.95165252685547, 39.95197677612305, 39.95229721069336, 39.95261764526367, 39.95293426513672, 39.95325469970703, 39.95357131958008, 39.95388412475586, 39.95419692993164, 39.95450973510742, 39.9548225402832, 39.95513153076172, 39.955440521240234, 39.95574951171875, 39.956058502197266, 39.95635986328125, 39.956661224365234, 39.95696258544922, 39.9572639465332, 39.95756149291992, 39.957855224609375, 39.958152770996094, 39.95844268798828, 39.958736419677734, 39.95903015136719, 39.95932388305664, 39.95961380004883, 39.95989990234375, 39.960182189941406, 39.96046829223633, 39.960750579833984, 39.961036682128906, 39.9613151550293, 39.96159362792969, 39.96187210083008, 39.96215057373047, 39.962425231933594, 39.96269989013672, 39.962974548339844, 39.9632453918457, 39.96351623535156, 39.96378707885742, 39.964054107666016, 39.964317321777344, 39.96458435058594, 39.964847564697266, 39.965110778808594, 39.965370178222656, 39.96562957763672, 39.96588897705078, 39.96615219116211, 39.96641159057617, 39.9666633605957, 39.9669189453125, 39.96717071533203, 39.9674186706543, 39.96767044067383, 39.967918395996094, 39.96816635131836, 39.968414306640625, 39.96866226196289, 39.96890640258789, 39.969154357910156, 39.969398498535156, 39.96963882446289, 39.969879150390625, 39.970115661621094, 39.97035217285156, 39.9705924987793, 39.970829010009766, 39.971065521240234, 39.9713020324707, 39.971534729003906, 39.97176742553711, 39.97199630737305, 39.972225189208984, 39.97245788574219, 39.97268295288086, 39.97290802001953, 39.97312927246094, 39.97335433959961, 39.973575592041016, 39.97379684448242, 39.97401809692383, 39.9742431640625, 39.97446060180664, 39.97467803955078, 39.97489547729492, 39.9751091003418, 39.97532653808594, 39.97554397583008, 39.97575759887695, 39.97596740722656, 39.97617721557617, 39.97638702392578, 39.97659683227539, 39.976802825927734, 39.977012634277344, 39.97721862792969, 39.977420806884766, 39.97762680053711, 39.97783279418945, 39.97803497314453, 39.97823715209961, 39.97843933105469, 39.9786376953125, 39.97883605957031, 39.97903823852539, 39.9792366027832, 39.979427337646484, 39.9796257019043, 39.979820251464844, 39.98001480102539, 39.98020553588867, 39.98040008544922, 39.980587005615234, 39.98077392578125, 39.98096466064453, 39.98115539550781, 39.981346130371094, 39.98153305053711, 39.98171615600586, 39.98189926147461, 39.98208236694336, 39.98226547241211, 39.98244857788086, 39.98263168334961, 39.982810974121094, 39.98299026489258, 39.9831657409668, 39.98334503173828, 39.9835205078125, 39.983699798583984, 39.98387908935547, 39.98405456542969, 39.984222412109375, 39.984397888183594, 39.98456573486328, 39.98473358154297, 39.98490524291992, 39.985076904296875, 39.98524856567383, 39.985416412353516, 39.9855842590332, 39.985748291015625, 39.98591232299805, 39.98607635498047, 39.98624038696289, 39.98640823364258, 39.986572265625, 39.98673629760742, 39.98689270019531, 39.987056732177734, 39.987213134765625, 39.987369537353516, 39.98753356933594, 39.98768615722656, 39.98783874511719, 39.98799514770508, 39.9881477355957, 39.98830032348633, 39.98845672607422, 39.988609313964844, 39.98876190185547, 39.98891067504883, 39.98906707763672, 39.98921585083008, 39.98936462402344, 39.9895133972168, 39.98965835571289, 39.98980712890625, 39.98995590209961, 39.99010467529297, 39.99024963378906, 39.990394592285156, 39.99053955078125, 39.990684509277344, 39.9908332824707, 39.99097442626953, 39.99111557006836, 39.99125671386719, 39.991397857666016, 39.99153518676758, 39.991676330566406, 39.99181365966797, 39.991947174072266, 39.99208450317383, 39.992218017578125, 39.99235534667969, 39.99249267578125, 39.99262619018555, 39.99276351928711, 39.992897033691406, 39.9930305480957, 39.9931640625, 39.9932975769043, 39.99342727661133, 39.99355697631836, 39.993690490722656, 39.99381637573242, 39.99394226074219, 39.994075775146484, 39.994197845458984, 39.99432373046875, 39.99445343017578, 39.99457931518555, 39.99470520019531, 39.99482727050781, 39.99494934082031, 39.99507141113281, 39.99519348144531, 39.99531936645508, 39.99543762207031, 39.99555969238281, 39.99567794799805, 39.99579620361328, 39.99591827392578, 39.99603271484375, 39.99615478515625, 39.99626922607422, 39.99638748168945, 39.99650192260742, 39.99661636352539, 39.996734619140625, 39.996849060058594, 39.9969596862793, 39.9970703125, 39.99718475341797, 39.99729919433594, 39.99740982055664, 39.99752426147461, 39.99763488769531, 39.997745513916016, 39.99785614013672, 39.99796676635742, 39.998077392578125, 39.99818801879883, 39.99829864501953, 39.99840545654297, 39.998512268066406, 39.99861526489258, 39.998722076416016, 39.99882888793945, 39.998931884765625, 39.99903869628906, 39.999141693115234, 39.99924850463867, 39.999351501464844, 39.99945831298828, 39.99955749511719, 39.99966049194336, 39.999759674072266, 39.99986267089844, 39.999961853027344, 40.00006103515625, 40.00016403198242, 40.00025939941406, 40.00035858154297, 40.000457763671875, 40.000553131103516, 40.000648498535156, 40.0007438659668, 40.00083923339844, 40.00093460083008, 40.001033782958984, 40.00112533569336, 40.001220703125, 40.001312255859375, 40.001407623291016, 40.00149917602539, 40.001590728759766, 40.00168228149414, 40.00177764892578, 40.00186538696289, 40.001956939697266, 40.002044677734375, 40.002132415771484, 40.00222396850586, 40.00231170654297, 40.00239944458008, 40.00248718261719, 40.0025749206543, 40.002662658691406, 40.002750396728516, 40.00283432006836, 40.00292205810547, 40.00300598144531, 40.00309371948242, 40.00318145751953, 40.00326919555664, 40.00335693359375, 40.003440856933594, 40.00352096557617, 40.003604888916016, 40.00368881225586, 40.00376892089844, 40.003849029541016, 40.003936767578125, 40.0040168762207, 40.00409698486328, 40.00417709350586, 40.00425720214844, 40.004337310791016, 40.004417419433594, 40.00449752807617, 40.00457763671875, 40.00465393066406, 40.00473403930664, 40.00481033325195, 40.004886627197266, 40.004966735839844, 40.00504684448242, 40.005123138427734, 40.00519943237305, 40.00527572631836, 40.005348205566406, 40.00542068481445, 40.005496978759766, 40.00557327270508, 40.005645751953125, 40.005714416503906, 40.00579071044922, 40.005863189697266, 40.00593948364258, 40.006011962890625, 40.006080627441406, 40.00615310668945, 40.006229400634766, 40.00629425048828, 40.00636672973633, 40.006431579589844, 40.006500244140625, 40.00657272338867, 40.00664138793945, 40.006710052490234, 40.006778717041016, 40.00684356689453, 40.00691223144531, 40.00697708129883, 40.00704574584961, 40.00711441040039, 40.00718307495117, 40.00724792480469, 40.00731658935547, 40.007381439208984, 40.007450103759766, 40.007511138916016, 40.00757598876953, 40.00764083862305, 40.00770950317383, 40.00777053833008, 40.007835388183594, 40.00790023803711, 40.00796127319336, 40.008026123046875, 40.00808334350586, 40.00814437866211, 40.00820541381836, 40.008270263671875, 40.00832748413086, 40.008392333984375, 40.00844955444336, 40.00851058959961, 40.008567810058594, 40.00863265991211, 40.008689880371094, 40.008750915527344, 40.00880432128906, 40.00886154174805, 40.0089225769043, 40.00897979736328, 40.009033203125, 40.009090423583984, 40.00914764404297, 40.00920867919922, 40.0092658996582, 40.00932312011719, 40.00938034057617, 40.009437561035156, 40.00949478149414, 40.009552001953125, 40.009605407714844, 40.00966262817383, 40.00971984863281, 40.00977325439453, 40.00982666015625, 40.00988006591797, 40.00992965698242, 40.00998306274414, 40.01003646850586, 40.01008987426758, 40.0101432800293, 40.010196685791016, 40.01025390625, 40.01030349731445, 40.010353088378906, 40.010406494140625, 40.01045608520508, 40.0105094909668, 40.010562896728516, 40.01061248779297, 40.01066207885742, 40.010711669921875, 40.01076126098633, 40.01081085205078, 40.010860443115234, 40.01091384887695, 40.010963439941406, 40.01101303100586, 40.01105880737305, 40.011104583740234, 40.01115036010742, 40.011199951171875, 40.01124572753906, 40.011295318603516, 40.01134490966797, 40.01139450073242, 40.011444091796875, 40.01148986816406, 40.01153564453125, 40.01158142089844, 40.01162338256836, 40.01166915893555, 40.011714935302734, 40.01176071166992, 40.01180648803711, 40.0118522644043, 40.011898040771484, 40.011940002441406, 40.011985778808594, 40.01203155517578, 40.0120735168457, 40.012115478515625, 40.01216125488281, 40.012203216552734, 40.012245178222656, 40.012290954589844, 40.012332916259766, 40.01237487792969, 40.012413024902344, 40.012454986572266, 40.01250076293945, 40.012542724609375, 40.0125846862793, 40.01262664794922, 40.01266860961914, 40.0127067565918, 40.012752532958984, 40.012786865234375, 40.01282501220703, 40.01285934448242, 40.01290512084961, 40.012939453125, 40.01298141479492, 40.01301574707031, 40.013057708740234, 40.01309585571289, 40.01313400268555, 40.01317596435547, 40.01321029663086, 40.013248443603516, 40.013282775878906, 40.01332473754883, 40.01335906982422, 40.01340103149414, 40.0134391784668, 40.01347351074219, 40.013511657714844, 40.013545989990234, 40.013587951660156, 40.01362609863281, 40.0136604309082, 40.01369857788086, 40.01373291015625, 40.013771057128906, 40.0138053894043, 40.01383972167969, 40.013877868652344, 40.013912200927734, 40.013946533203125, 40.01398468017578, 40.01401901245117, 40.01405334472656, 40.01408767700195, 40.014122009277344, 40.014156341552734, 40.014190673828125, 40.014225006103516, 40.014259338378906, 40.01428985595703, 40.01432418823242, 40.01435852050781, 40.01438903808594, 40.01442337036133, 40.01445770263672, 40.014488220214844, 40.014522552490234, 40.01455307006836, 40.01458740234375, 40.014617919921875, 40.0146484375, 40.01468276977539, 40.014713287353516, 40.01474380493164, 40.01477813720703, 40.01480484008789, 40.01483917236328, 40.01486587524414, 40.014892578125, 40.014923095703125, 40.01495361328125, 40.014984130859375, 40.0150146484375, 40.015045166015625, 40.015071868896484, 40.01510238647461, 40.015132904052734, 40.01516342163086, 40.015193939208984, 40.01522445678711, 40.015254974365234, 40.01528549194336, 40.01531219482422, 40.015342712402344, 40.0153694152832, 40.01539993286133, 40.01542663574219, 40.01545715332031, 40.01548385620117, 40.0155143737793, 40.01554489135742, 40.01557159423828, 40.01559829711914, 40.015621185302734, 40.01565170288086, 40.01567840576172, 40.01570510864258, 40.01572799682617, 40.01575469970703, 40.015785217285156, 40.01580810546875, 40.01583480834961, 40.01586151123047, 40.01588821411133, 40.01591491699219, 40.01594161987305, 40.015968322753906, 40.015995025634766, 40.01601791381836, 40.01604461669922, 40.01606750488281, 40.01609420776367, 40.016117095947266, 40.016143798828125, 40.016170501708984, 40.01619338989258, 40.01621627807617, 40.01624298095703, 40.016265869140625, 40.01628875732422, 40.01631546020508, 40.01633834838867, 40.016361236572266, 40.01638412475586, 40.01641082763672, 40.01643371582031, 40.01646041870117, 40.0164794921875, 40.01649856567383, 40.01652145385742, 40.016544342041016, 40.016571044921875, 40.01659393310547, 40.01661682128906, 40.016639709472656, 40.016658782958984, 40.01668167114258, 40.016700744628906, 40.0167236328125, 40.01673889160156, 40.016761779785156, 40.016780853271484, 40.016807556152344, 40.01683044433594, 40.01685333251953, 40.016876220703125, 40.01689529418945, 40.01691436767578, 40.016937255859375, 40.016963958740234, 40.0169792175293, 40.016998291015625, 40.01701736450195, 40.01704406738281, 40.017066955566406, 40.01708221435547, 40.01710510253906, 40.01712417602539, 40.017147064208984, 40.01716613769531, 40.01718521118164, 40.017208099365234, 40.01722717285156, 40.01724624633789, 40.017269134521484, 40.01728820800781, 40.017311096191406, 40.017330169677734, 40.0173454284668, 40.017364501953125, 40.01738357543945, 40.01740264892578, 40.01742172241211, 40.01744079589844, 40.01746368408203, 40.01748275756836, 40.01750183105469, 40.01751708984375, 40.01753616333008, 40.017555236816406, 40.017574310302734, 40.01759338378906, 40.01761245727539, 40.01762771606445, 40.01764678955078, 40.017662048339844, 40.01768112182617, 40.0177001953125, 40.01771926879883, 40.01773452758789, 40.01775360107422, 40.01776885986328, 40.01778793334961, 40.01780319213867, 40.017818450927734, 40.01783752441406, 40.01785659790039, 40.01787185668945, 40.017887115478516, 40.017906188964844, 40.017921447753906, 40.01793670654297, 40.0179557800293, 40.01797103881836, 40.01799011230469, 40.01800537109375, 40.01802062988281, 40.01803970336914, 40.0180549621582, 40.018070220947266, 40.01808547973633, 40.01810073852539, 40.01811599731445, 40.018131256103516, 40.01814651489258, 40.01816177368164, 40.0181770324707, 40.018192291259766, 40.018211364746094, 40.018226623535156, 40.01824188232422, 40.01825714111328, 40.018272399902344, 40.01828384399414, 40.01829528808594, 40.018314361572266, 40.0183219909668, 40.01833724975586, 40.018348693847656, 40.01836395263672, 40.01837921142578, 40.01839065551758, 40.01840591430664, 40.01841735839844, 40.018436431884766, 40.01844787597656, 40.018463134765625, 40.01847457885742, 40.018489837646484, 40.01850128173828, 40.01852035522461, 40.018531799316406, 40.01854705810547, 40.01856231689453, 40.01857376098633, 40.018585205078125, 40.01860046386719, 40.01861572265625, 40.01863098144531, 40.01864242553711, 40.01865768432617, 40.0186653137207, 40.0186767578125, 40.0186882019043, 40.01870346069336, 40.018714904785156, 40.01872634887695, 40.018741607666016, 40.01875686645508, 40.018768310546875, 40.01878356933594, 40.018795013427734, 40.0188102722168, 40.01881790161133, 40.01883316040039, 40.01884460449219, 40.018856048583984, 40.01886749267578, 40.01887512207031, 40.018890380859375, 40.018898010253906, 40.01891326904297, 40.0189208984375, 40.0189323425293, 40.018943786621094, 40.01895523071289, 40.01897048950195, 40.01898193359375, 40.01899337768555, 40.019004821777344, 40.01901626586914, 40.01902770996094, 40.019039154052734, 40.01905059814453, 40.01906204223633, 40.019073486328125, 40.019081115722656, 40.01909255981445, 40.019100189208984, 40.01911163330078, 40.01912307739258, 40.019134521484375, 40.01914978027344, 40.019161224365234, 40.019168853759766, 40.01918411254883, 40.01919174194336, 40.01919937133789, 40.01921081542969, 40.01921844482422, 40.019229888916016, 40.01924133300781, 40.01925277709961, 40.019264221191406, 40.01927185058594, 40.019287109375, 40.01929473876953, 40.01930618286133, 40.01931381225586, 40.019325256347656, 40.01933670043945, 40.019344329833984, 40.019351959228516, 40.01936340332031, 40.019371032714844, 40.01938247680664, 40.01939010620117, 40.0193977355957, 40.0194091796875, 40.0194206237793, 40.01942825317383, 40.01943588256836, 40.01944351196289, 40.01945114135742, 40.019466400146484, 40.01947021484375, 40.019474029541016, 40.01948547363281, 40.019493103027344, 40.01950454711914, 40.01951217651367, 40.01952362060547, 40.019535064697266, 40.0195426940918, 40.019554138183594, 40.019561767578125, 40.01957321166992, 40.01958084106445, 40.019588470458984, 40.01959991455078, 40.01960754394531, 40.019615173339844, 40.01962661743164, 40.019630432128906, 40.01963806152344, 40.0196418762207, 40.019649505615234, 40.01966094970703, 40.01966857910156, 40.019676208496094, 40.019683837890625, 40.019691467285156, 40.01970291137695, 40.019710540771484, 40.01971435546875, 40.01972198486328, 40.01972961425781, 40.01974105834961, 40.019744873046875, 40.019752502441406, 40.0197639465332, 40.01976776123047, 40.019771575927734, 40.01978302001953, 40.01979064941406, 40.019798278808594, 40.01980972290039, 40.019813537597656, 40.01981735229492, 40.01982498168945, 40.01983642578125, 40.01984405517578, 40.01984786987305, 40.01985549926758, 40.01986312866211, 40.01987075805664, 40.019874572753906, 40.01987838745117, 40.01988983154297, 40.0198974609375, 40.019901275634766, 40.0199089050293, 40.01991653442383, 40.01992416381836, 40.019927978515625, 40.019935607910156, 40.01994323730469, 40.01995086669922, 40.01995849609375, 40.01996994018555, 40.01997756958008, 40.019981384277344, 40.01998519897461, 40.01999282836914, 40.02000045776367, 40.02000427246094, 40.02001190185547, 40.02001953125, 40.02002716064453, 40.02003479003906, 40.02003860473633, 40.020050048828125, 40.020050048828125, 40.020057678222656, 40.02006149291992, 40.02006912231445, 40.02008056640625, 40.020084381103516, 40.02009201049805, 40.02009963989258, 40.020103454589844, 40.02010726928711, 40.02011489868164, 40.02012252807617, 40.0201301574707, 40.02013397216797, 40.020137786865234, 40.020145416259766, 40.0201530456543, 40.02015686035156, 40.020164489746094, 40.02016830444336, 40.020172119140625, 40.020179748535156, 40.02018737792969, 40.02019119262695, 40.02019500732422, 40.020198822021484, 40.020206451416016, 40.02021408081055, 40.02021789550781, 40.02022171020508, 40.020225524902344, 40.020233154296875, 40.020240783691406, 40.02024459838867, 40.0202522277832, 40.020259857177734, 40.020263671875, 40.020267486572266, 40.02027130126953, 40.02027893066406, 40.020286560058594, 40.02029037475586, 40.020294189453125, 40.020301818847656, 40.020301818847656, 40.02030563354492, 40.02030944824219, 40.02031707763672, 40.02032470703125, 40.02032470703125, 40.020328521728516, 40.02033615112305, 40.02034378051758, 40.02035140991211, 40.020355224609375, 40.020362854003906, 40.02036666870117, 40.02037048339844, 40.0203742980957, 40.0203742980957, 40.02037811279297, 40.020381927490234, 40.02039337158203, 40.0203971862793, 40.02040100097656, 40.02040481567383, 40.020408630371094, 40.020416259765625, 40.020423889160156, 40.020423889160156, 40.02042770385742, 40.02043151855469, 40.02043533325195, 40.020442962646484, 40.02044677734375, 40.02045440673828, 40.02045822143555, 40.02046585083008, 40.020469665527344, 40.020477294921875, 40.02048110961914, 40.020484924316406, 40.02048873901367, 40.02049255371094, 40.0204963684082, 40.02050018310547, 40.020503997802734, 40.0205078125, 40.020511627197266, 40.0205192565918, 40.02052688598633, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.02053451538086, 40.02054214477539, 40.020545959472656, 40.02054977416992, 40.02055358886719, 40.02056121826172, 40.020565032958984, 40.02056884765625, 40.02057647705078, 40.02057647705078, 40.02058029174805, 40.02058410644531, 40.020591735839844, 40.02059555053711, 40.020599365234375, 40.02060317993164, 40.020606994628906, 40.02061080932617, 40.02061462402344, 40.02061462402344, 40.02061462402344, 40.02062225341797, 40.020626068115234, 40.0206298828125, 40.020633697509766, 40.02063751220703, 40.0206413269043, 40.0206413269043, 40.0206413269043, 40.02064895629883, 40.020652770996094, 40.02065658569336, 40.020660400390625, 40.02066421508789, 40.020668029785156, 40.02067565917969, 40.02067947387695, 40.02068328857422, 40.020687103271484, 40.02069091796875, 40.020694732666016, 40.02069854736328, 40.02070236206055, 40.02070999145508, 40.020713806152344, 40.02071762084961, 40.020721435546875, 40.02072525024414, 40.020729064941406, 40.02073287963867, 40.02073669433594, 40.0207405090332, 40.02074432373047, 40.020748138427734, 40.020748138427734, 40.020748138427734, 40.020748138427734, 40.020755767822266, 40.020755767822266, 40.02075958251953, 40.02076721191406, 40.02077102661133, 40.02077102661133, 40.020774841308594, 40.02077865600586, 40.020782470703125]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('40914322-127c-44c7-8cd8-392be8eed0ca');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjF0MGHt0nKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b69b4ea-8be8-48b8-d142-c887d24ffc29"
      },
      "source": [
        "X_test_RM_ITERATIVE = np.concatenate((S2RMX, S3RMX, S4RMX, S5RMX, S6RMX), axis=0)\n",
        "\n",
        "print(\"X_test_RM_ITERATIVE:\", X_test_RM_ITERATIVE[0:5])\n",
        "\n",
        "X_test_RM_ITERATIVE = X_test_RM_ITERATIVE.reshape((X_test_RM_ITERATIVE.shape[0], X_test_RM_ITERATIVE.shape[1], n_features))\n",
        "\n",
        "print(\"X_test_RM_ITERATIVE\", X_test_RM_ITERATIVE[0])\n",
        "\n",
        "y_test_RM_ITERATIVE = Iterative_Test(X_test_RM_ITERATIVE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02039337]\n",
            " [40.02039719]\n",
            " [40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.020424]]\n",
            "Next yhat, before iteration: [[38.54891]]\n",
            "Linesleft_: 63\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02039337]\n",
            " [40.02039719]\n",
            " [40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]]\n",
            "Next Unscaled Input Iterated:  [[40.02039719]\n",
            " [40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02041626]\n",
            " [40.02042389]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02039337]\n",
            "  [40.02039719]\n",
            "  [40.020401  ]\n",
            "  [40.02040482]\n",
            "  [40.02040863]\n",
            "  [40.02041626]\n",
            "  [40.02042389]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02039719]\n",
            "  [40.020401  ]\n",
            "  [40.02040482]\n",
            "  [40.02040863]\n",
            "  [40.02041626]\n",
            "  [40.02042389]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02039337]\n",
            " [40.02039719]\n",
            " [40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02039719]\n",
            " [40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.020424]]\n",
            "Next yhat, before iteration: [[38.54891]]\n",
            "Linesleft_: 62\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02039719]\n",
            " [40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]]\n",
            "Next Unscaled Input Iterated:  [[40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.02042389]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02039719]\n",
            "  [40.020401  ]\n",
            "  [40.02040482]\n",
            "  [40.02040863]\n",
            "  [40.02041626]\n",
            "  [40.02042389]\n",
            "  [40.02042389]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.020401  ]\n",
            "  [40.02040482]\n",
            "  [40.02040863]\n",
            "  [40.02041626]\n",
            "  [40.02042389]\n",
            "  [40.02042389]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02039719]\n",
            " [40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.020428]]\n",
            "Next yhat, before iteration: [[38.548916]]\n",
            "Linesleft_: 61\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]]\n",
            "Next Unscaled Input Iterated:  [[40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.0204277 ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0204277]\n",
            " [37.2      ]\n",
            " [37.2      ]\n",
            " [37.2      ]\n",
            " [37.2      ]\n",
            " [37.2      ]\n",
            " [37.2      ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.020401  ]\n",
            "  [40.02040482]\n",
            "  [40.02040863]\n",
            "  [40.02041626]\n",
            "  [40.02042389]\n",
            "  [40.02042389]\n",
            "  [40.0204277 ]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02040482]\n",
            "  [40.02040863]\n",
            "  [40.02041626]\n",
            "  [40.02042389]\n",
            "  [40.02042389]\n",
            "  [40.0204277 ]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.020401  ]\n",
            " [40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.02043]]\n",
            "Next yhat, before iteration: [[38.548912]]\n",
            "Linesleft_: 60\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]]\n",
            "Next Unscaled Input Iterated:  [[40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0204277 ]\n",
            " [40.02043152]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02043152]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02040482]\n",
            "  [40.02040863]\n",
            "  [40.02041626]\n",
            "  [40.02042389]\n",
            "  [40.02042389]\n",
            "  [40.0204277 ]\n",
            "  [40.02043152]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02040863]\n",
            "  [40.02041626]\n",
            "  [40.02042389]\n",
            "  [40.02042389]\n",
            "  [40.0204277 ]\n",
            "  [40.02043152]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02040482]\n",
            " [40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.020435]]\n",
            "Next yhat, before iteration: [[38.54892]]\n",
            "Linesleft_: 59\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]]\n",
            "Next Unscaled Input Iterated:  [[40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02043152]\n",
            " [40.02043533]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02043533]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02040863]\n",
            "  [40.02041626]\n",
            "  [40.02042389]\n",
            "  [40.02042389]\n",
            "  [40.0204277 ]\n",
            "  [40.02043152]\n",
            "  [40.02043533]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02041626]\n",
            "  [40.02042389]\n",
            "  [40.02042389]\n",
            "  [40.0204277 ]\n",
            "  [40.02043152]\n",
            "  [40.02043533]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02040863]\n",
            " [40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.020443]]\n",
            "Next yhat, before iteration: [[38.54892]]\n",
            "Linesleft_: 58\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02043533]\n",
            " [40.02044296]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02044296]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02041626]\n",
            "  [40.02042389]\n",
            "  [40.02042389]\n",
            "  [40.0204277 ]\n",
            "  [40.02043152]\n",
            "  [40.02043533]\n",
            "  [40.02044296]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02042389]\n",
            "  [40.02042389]\n",
            "  [40.0204277 ]\n",
            "  [40.02043152]\n",
            "  [40.02043533]\n",
            "  [40.02044296]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02041626]\n",
            " [40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.020447]]\n",
            "Next yhat, before iteration: [[38.548923]]\n",
            "Linesleft_: 57\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]]\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02044296]\n",
            " [40.02044678]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02044678]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02042389]\n",
            "  [40.02042389]\n",
            "  [40.0204277 ]\n",
            "  [40.02043152]\n",
            "  [40.02043533]\n",
            "  [40.02044296]\n",
            "  [40.02044678]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02042389]\n",
            "  [40.0204277 ]\n",
            "  [40.02043152]\n",
            "  [40.02043533]\n",
            "  [40.02044296]\n",
            "  [40.02044678]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02042389]\n",
            " [40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.020454]]\n",
            "Next yhat, before iteration: [[38.548923]]\n",
            "Linesleft_: 56\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]]\n",
            "Next Unscaled Input Iterated:  [[40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02044678]\n",
            " [40.02045441]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02045441]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02042389]\n",
            "  [40.0204277 ]\n",
            "  [40.02043152]\n",
            "  [40.02043533]\n",
            "  [40.02044296]\n",
            "  [40.02044678]\n",
            "  [40.02045441]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0204277 ]\n",
            "  [40.02043152]\n",
            "  [40.02043533]\n",
            "  [40.02044296]\n",
            "  [40.02044678]\n",
            "  [40.02045441]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02042389]\n",
            " [40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.02046]]\n",
            "Next yhat, before iteration: [[38.548923]]\n",
            "Linesleft_: 55\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]]\n",
            "Next Unscaled Input Iterated:  [[40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02045441]\n",
            " [40.02045822]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02045822]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0204277 ]\n",
            "  [40.02043152]\n",
            "  [40.02043533]\n",
            "  [40.02044296]\n",
            "  [40.02044678]\n",
            "  [40.02045441]\n",
            "  [40.02045822]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02043152]\n",
            "  [40.02043533]\n",
            "  [40.02044296]\n",
            "  [40.02044678]\n",
            "  [40.02045441]\n",
            "  [40.02045822]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0204277 ]\n",
            " [40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.020466]]\n",
            "Next yhat, before iteration: [[38.548923]]\n",
            "Linesleft_: 54\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]]\n",
            "Next Unscaled Input Iterated:  [[40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02045822]\n",
            " [40.02046585]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02046585]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.2       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02043152]\n",
            "  [40.02043533]\n",
            "  [40.02044296]\n",
            "  [40.02044678]\n",
            "  [40.02045441]\n",
            "  [40.02045822]\n",
            "  [40.02046585]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02043533]\n",
            "  [40.02044296]\n",
            "  [40.02044678]\n",
            "  [40.02045441]\n",
            "  [40.02045822]\n",
            "  [40.02046585]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02043152]\n",
            " [40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.02047]]\n",
            "Next yhat, before iteration: [[38.54893]]\n",
            "Linesleft_: 53\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]]\n",
            "Next Unscaled Input Iterated:  [[40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02046585]\n",
            " [40.02046967]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02046967]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02043533]\n",
            "  [40.02044296]\n",
            "  [40.02044678]\n",
            "  [40.02045441]\n",
            "  [40.02045822]\n",
            "  [40.02046585]\n",
            "  [40.02046967]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02044296]\n",
            "  [40.02044678]\n",
            "  [40.02045441]\n",
            "  [40.02045822]\n",
            "  [40.02046585]\n",
            "  [40.02046967]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02043533]\n",
            " [40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.020477]]\n",
            "Next yhat, before iteration: [[38.54893]]\n",
            "Linesleft_: 52\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]]\n",
            "Next Unscaled Input Iterated:  [[40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [37.2       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02046967]\n",
            " [40.02047729]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02047729]\n",
            " [37.2       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02044296]\n",
            "  [40.02044678]\n",
            "  [40.02045441]\n",
            "  [40.02045822]\n",
            "  [40.02046585]\n",
            "  [40.02046967]\n",
            "  [40.02047729]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02044678]\n",
            "  [40.02045441]\n",
            "  [40.02045822]\n",
            "  [40.02046585]\n",
            "  [40.02046967]\n",
            "  [40.02047729]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02044296]\n",
            " [40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.02048]]\n",
            "Next yhat, before iteration: [[38.54894]]\n",
            "Linesleft_: 51\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]]\n",
            "Next Unscaled Input Iterated:  [[40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [37.2       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02047729]\n",
            " [40.02048111]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048111]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02044678]\n",
            "  [40.02045441]\n",
            "  [40.02045822]\n",
            "  [40.02046585]\n",
            "  [40.02046967]\n",
            "  [40.02047729]\n",
            "  [40.02048111]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02045441]\n",
            "  [40.02045822]\n",
            "  [40.02046585]\n",
            "  [40.02046967]\n",
            "  [40.02047729]\n",
            "  [40.02048111]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02044678]\n",
            " [40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.020485]]\n",
            "Next yhat, before iteration: [[38.548943]]\n",
            "Linesleft_: 50\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]]\n",
            "Next Unscaled Input Iterated:  [[40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [37.4       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048111]\n",
            " [40.02048492]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048492]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02045441]\n",
            "  [40.02045822]\n",
            "  [40.02046585]\n",
            "  [40.02046967]\n",
            "  [40.02047729]\n",
            "  [40.02048111]\n",
            "  [40.02048492]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02045822]\n",
            "  [40.02046585]\n",
            "  [40.02046967]\n",
            "  [40.02047729]\n",
            "  [40.02048111]\n",
            "  [40.02048492]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02045441]\n",
            " [40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02049]]\n",
            "Next yhat, before iteration: [[38.66105]]\n",
            "Linesleft_: 49\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]]\n",
            "Next Unscaled Input Iterated:  [[40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [37.2       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048492]\n",
            " [40.02048874]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048874]\n",
            " [37.2       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02045822]\n",
            "  [40.02046585]\n",
            "  [40.02046967]\n",
            "  [40.02047729]\n",
            "  [40.02048111]\n",
            "  [40.02048492]\n",
            "  [40.02048874]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02046585]\n",
            "  [40.02046967]\n",
            "  [40.02047729]\n",
            "  [40.02048111]\n",
            "  [40.02048492]\n",
            "  [40.02048874]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02045822]\n",
            " [40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.020493]]\n",
            "Next yhat, before iteration: [[38.548943]]\n",
            "Linesleft_: 48\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]]\n",
            "Next Unscaled Input Iterated:  [[40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048874]\n",
            " [40.02049255]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02049255]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02046585]\n",
            "  [40.02046967]\n",
            "  [40.02047729]\n",
            "  [40.02048111]\n",
            "  [40.02048492]\n",
            "  [40.02048874]\n",
            "  [40.02049255]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02046967]\n",
            "  [40.02047729]\n",
            "  [40.02048111]\n",
            "  [40.02048492]\n",
            "  [40.02048874]\n",
            "  [40.02049255]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02046585]\n",
            " [40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020496]]\n",
            "Next yhat, before iteration: [[38.66105]]\n",
            "Linesleft_: 47\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]]\n",
            "Next Unscaled Input Iterated:  [[40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02049255]\n",
            " [40.02049637]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02049637]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02046967]\n",
            "  [40.02047729]\n",
            "  [40.02048111]\n",
            "  [40.02048492]\n",
            "  [40.02048874]\n",
            "  [40.02049255]\n",
            "  [40.02049637]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02047729]\n",
            "  [40.02048111]\n",
            "  [40.02048492]\n",
            "  [40.02048874]\n",
            "  [40.02049255]\n",
            "  [40.02049637]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02046967]\n",
            " [40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.0205]]\n",
            "Next yhat, before iteration: [[38.66105]]\n",
            "Linesleft_: 46\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]]\n",
            "Next Unscaled Input Iterated:  [[40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02049637]\n",
            " [40.02050018]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02050018]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02047729]\n",
            "  [40.02048111]\n",
            "  [40.02048492]\n",
            "  [40.02048874]\n",
            "  [40.02049255]\n",
            "  [40.02049637]\n",
            "  [40.02050018]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02048111]\n",
            "  [40.02048492]\n",
            "  [40.02048874]\n",
            "  [40.02049255]\n",
            "  [40.02049637]\n",
            "  [40.02050018]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02047729]\n",
            " [40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020504]]\n",
            "Next yhat, before iteration: [[38.661057]]\n",
            "Linesleft_: 45\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02050018]\n",
            " [40.020504  ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.020504]\n",
            " [37.4     ]\n",
            " [37.4     ]\n",
            " [37.4     ]\n",
            " [37.4     ]\n",
            " [37.4     ]\n",
            " [37.4     ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02048111]\n",
            "  [40.02048492]\n",
            "  [40.02048874]\n",
            "  [40.02049255]\n",
            "  [40.02049637]\n",
            "  [40.02050018]\n",
            "  [40.020504  ]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02048492]\n",
            "  [40.02048874]\n",
            "  [40.02049255]\n",
            "  [40.02049637]\n",
            "  [40.02050018]\n",
            "  [40.020504  ]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02048111]\n",
            " [40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020508]]\n",
            "Next yhat, before iteration: [[38.661057]]\n",
            "Linesleft_: 44\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]]\n",
            "Next Unscaled Input Iterated:  [[40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.020504  ]\n",
            " [40.02050781]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02050781]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02048492]\n",
            "  [40.02048874]\n",
            "  [40.02049255]\n",
            "  [40.02049637]\n",
            "  [40.02050018]\n",
            "  [40.020504  ]\n",
            "  [40.02050781]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02048874]\n",
            "  [40.02049255]\n",
            "  [40.02049637]\n",
            "  [40.02050018]\n",
            "  [40.020504  ]\n",
            "  [40.02050781]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02048492]\n",
            " [40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02051]]\n",
            "Next yhat, before iteration: [[38.661057]]\n",
            "Linesleft_: 43\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]]\n",
            "Next Unscaled Input Iterated:  [[40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02050781]\n",
            " [40.02051163]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051163]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02048874]\n",
            "  [40.02049255]\n",
            "  [40.02049637]\n",
            "  [40.02050018]\n",
            "  [40.020504  ]\n",
            "  [40.02050781]\n",
            "  [40.02051163]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02049255]\n",
            "  [40.02049637]\n",
            "  [40.02050018]\n",
            "  [40.020504  ]\n",
            "  [40.02050781]\n",
            "  [40.02051163]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02048874]\n",
            " [40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02052]]\n",
            "Next yhat, before iteration: [[38.661057]]\n",
            "Linesleft_: 42\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]]\n",
            "Next Unscaled Input Iterated:  [[40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051163]\n",
            " [40.02051926]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051926]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02049255]\n",
            "  [40.02049637]\n",
            "  [40.02050018]\n",
            "  [40.020504  ]\n",
            "  [40.02050781]\n",
            "  [40.02051163]\n",
            "  [40.02051926]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02049637]\n",
            "  [40.02050018]\n",
            "  [40.020504  ]\n",
            "  [40.02050781]\n",
            "  [40.02051163]\n",
            "  [40.02051926]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02049255]\n",
            " [40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020527]]\n",
            "Next yhat, before iteration: [[38.661057]]\n",
            "Linesleft_: 41\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]]\n",
            "Next Unscaled Input Iterated:  [[40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051926]\n",
            " [40.02052689]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02052689]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02049637]\n",
            "  [40.02050018]\n",
            "  [40.020504  ]\n",
            "  [40.02050781]\n",
            "  [40.02051163]\n",
            "  [40.02051926]\n",
            "  [40.02052689]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02050018]\n",
            "  [40.020504  ]\n",
            "  [40.02050781]\n",
            "  [40.02051163]\n",
            "  [40.02051926]\n",
            "  [40.02052689]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02049637]\n",
            " [40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02053]]\n",
            "Next yhat, before iteration: [[38.66106]]\n",
            "Linesleft_: 40\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]]\n",
            "Next Unscaled Input Iterated:  [[40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02052689]\n",
            " [40.0205307 ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02050018]\n",
            "  [40.020504  ]\n",
            "  [40.02050781]\n",
            "  [40.02051163]\n",
            "  [40.02051926]\n",
            "  [40.02052689]\n",
            "  [40.0205307 ]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.020504  ]\n",
            "  [40.02050781]\n",
            "  [40.02051163]\n",
            "  [40.02051926]\n",
            "  [40.02052689]\n",
            "  [40.0205307 ]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02050018]\n",
            " [40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02053]]\n",
            "Next yhat, before iteration: [[38.66106]]\n",
            "Linesleft_: 39\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]]\n",
            "Next Unscaled Input Iterated:  [[40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307]\n",
            " [40.0205307]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.020504  ]\n",
            "  [40.02050781]\n",
            "  [40.02051163]\n",
            "  [40.02051926]\n",
            "  [40.02052689]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02050781]\n",
            "  [40.02051163]\n",
            "  [40.02051926]\n",
            "  [40.02052689]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.020504  ]\n",
            " [40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02053]]\n",
            "Next yhat, before iteration: [[38.661064]]\n",
            "Linesleft_: 38\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307]\n",
            " [40.0205307]\n",
            " [40.0205307]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307]\n",
            " [40.0205307]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02050781]\n",
            "  [40.02051163]\n",
            "  [40.02051926]\n",
            "  [40.02052689]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02051163]\n",
            "  [40.02051926]\n",
            "  [40.02052689]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02050781]\n",
            " [40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02053]]\n",
            "Next yhat, before iteration: [[38.661064]]\n",
            "Linesleft_: 37\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]]\n",
            "Next Unscaled Input Iterated:  [[40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307]\n",
            " [40.0205307]\n",
            " [40.0205307]\n",
            " [40.0205307]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307]\n",
            " [40.0205307]\n",
            " [40.0205307]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307]\n",
            " [40.0205307]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.4      ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02051163]\n",
            "  [40.02051926]\n",
            "  [40.02052689]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02051926]\n",
            "  [40.02052689]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02051163]\n",
            " [40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020535]]\n",
            "Next yhat, before iteration: [[38.661064]]\n",
            "Linesleft_: 36\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]]\n",
            "Next Unscaled Input Iterated:  [[40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02051926]\n",
            "  [40.02052689]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02052689]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02051926]\n",
            " [40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020542]]\n",
            "Next yhat, before iteration: [[38.661064]]\n",
            "Linesleft_: 35\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02052689]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02052689]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020546]]\n",
            "Next yhat, before iteration: [[38.661064]]\n",
            "Linesleft_: 34\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02055]]\n",
            "Next yhat, before iteration: [[38.661068]]\n",
            "Linesleft_: 33\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020554]]\n",
            "Next yhat, before iteration: [[38.661076]]\n",
            "Linesleft_: 32\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]]\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0205307 ]\n",
            "  [40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0205307 ]\n",
            " [40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02056]]\n",
            "Next yhat, before iteration: [[38.661076]]\n",
            "Linesleft_: 31\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]]\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0205307 ]\n",
            "  [40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0205307 ]\n",
            " [40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020565]]\n",
            "Next yhat, before iteration: [[38.661076]]\n",
            "Linesleft_: 30\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]]\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02053452]\n",
            "  [40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02053452]\n",
            " [40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02057]]\n",
            "Next yhat, before iteration: [[38.661083]]\n",
            "Linesleft_: 29\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]]\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02054214]\n",
            "  [40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02054214]\n",
            " [40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020576]]\n",
            "Next yhat, before iteration: [[38.661083]]\n",
            "Linesleft_: 28\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]]\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02054596]\n",
            "  [40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02054596]\n",
            " [40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020576]]\n",
            "Next yhat, before iteration: [[38.661083]]\n",
            "Linesleft_: 27\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]]\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02054977]\n",
            "  [40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02054977]\n",
            " [40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02058]]\n",
            "Next yhat, before iteration: [[38.661087]]\n",
            "Linesleft_: 26\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]]\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02055359]\n",
            "  [40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02055359]\n",
            " [40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020584]]\n",
            "Next yhat, before iteration: [[38.661087]]\n",
            "Linesleft_: 25\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]]\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02056122]\n",
            "  [40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02056122]\n",
            " [40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02059]]\n",
            "Next yhat, before iteration: [[38.661087]]\n",
            "Linesleft_: 24\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]]\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.6       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02056503]\n",
            "  [40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02056503]\n",
            " [40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020596]]\n",
            "Next yhat, before iteration: [[38.66109]]\n",
            "Linesleft_: 23\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.6       ]\n",
            " [37.6       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02056885]\n",
            "  [40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02056885]\n",
            " [40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.0206]]\n",
            "Next yhat, before iteration: [[38.661095]]\n",
            "Linesleft_: 22\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]]\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.6       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.6       ]\n",
            " [37.6       ]\n",
            " [39.9       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02057648]\n",
            "  [40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02057648]\n",
            " [40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020603]]\n",
            "Next yhat, before iteration: [[38.661095]]\n",
            "Linesleft_: 21\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]]\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.6       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.6       ]\n",
            " [37.6       ]\n",
            " [39.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.6       ]\n",
            " [37.6       ]\n",
            " [39.9       ]\n",
            " [38.3       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02057648]\n",
            "  [40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02057648]\n",
            " [40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.020607]]\n",
            "Next yhat, before iteration: [[38.6611]]\n",
            "Linesleft_: 20\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]]\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [37.4       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [37.4       ]\n",
            " [37.6       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [37.4       ]\n",
            " [37.6       ]\n",
            " [37.6       ]\n",
            " [39.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [37.4       ]\n",
            " [37.6       ]\n",
            " [37.6       ]\n",
            " [39.9       ]\n",
            " [38.3       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [37.4       ]\n",
            " [37.6       ]\n",
            " [37.6       ]\n",
            " [39.9       ]\n",
            " [38.3       ]\n",
            " [37.9       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02058029]\n",
            "  [40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02058029]\n",
            " [40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02061]]\n",
            "Next yhat, before iteration: [[38.6611]]\n",
            "Linesleft_: 19\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]]\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [37.6       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [37.6       ]\n",
            " [37.6       ]\n",
            " [39.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [37.6       ]\n",
            " [37.6       ]\n",
            " [39.9       ]\n",
            " [38.3       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [37.6       ]\n",
            " [37.6       ]\n",
            " [39.9       ]\n",
            " [38.3       ]\n",
            " [37.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [37.6       ]\n",
            " [37.6       ]\n",
            " [39.9       ]\n",
            " [38.3       ]\n",
            " [37.9       ]\n",
            " [37.9       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02058411]\n",
            "  [40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [37.6       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02058411]\n",
            " [40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [37.6       ]]\n",
            "Current yhat_______________: [[40.020615]]\n",
            "Next yhat, before iteration: [[38.77306]]\n",
            "Linesleft_: 18\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]]\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [37.6       ]\n",
            " [39.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [37.6       ]\n",
            " [39.9       ]\n",
            " [38.3       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [37.6       ]\n",
            " [39.9       ]\n",
            " [38.3       ]\n",
            " [37.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [37.6       ]\n",
            " [39.9       ]\n",
            " [38.3       ]\n",
            " [37.9       ]\n",
            " [37.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [37.6       ]\n",
            " [39.9       ]\n",
            " [38.3       ]\n",
            " [37.9       ]\n",
            " [37.9       ]\n",
            " [37.8       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02059174]\n",
            "  [40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [37.6       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02059174]\n",
            " [40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [37.6       ]]\n",
            "Current yhat_______________: [[40.020615]]\n",
            "Next yhat, before iteration: [[38.77306]]\n",
            "Linesleft_: 17\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]]\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [39.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [39.9       ]\n",
            " [38.3       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [39.9       ]\n",
            " [38.3       ]\n",
            " [37.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [39.9       ]\n",
            " [38.3       ]\n",
            " [37.9       ]\n",
            " [37.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [39.9       ]\n",
            " [38.3       ]\n",
            " [37.9       ]\n",
            " [37.9       ]\n",
            " [37.8       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [39.9       ]\n",
            " [38.3       ]\n",
            " [37.9       ]\n",
            " [37.9       ]\n",
            " [37.8       ]\n",
            " [37.6       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02059555]\n",
            "  [40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [39.9       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02059555]\n",
            " [40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [39.9       ]]\n",
            "Current yhat_______________: [[40.020615]]\n",
            "Next yhat, before iteration: [[39.96019]]\n",
            "Linesleft_: 16\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]]\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [38.3       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [38.3       ]\n",
            " [37.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [38.3       ]\n",
            " [37.9       ]\n",
            " [37.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [38.3       ]\n",
            " [37.9       ]\n",
            " [37.9       ]\n",
            " [37.8       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [38.3       ]\n",
            " [37.9       ]\n",
            " [37.9       ]\n",
            " [37.8       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [38.3       ]\n",
            " [37.9       ]\n",
            " [37.9       ]\n",
            " [37.8       ]\n",
            " [37.6       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02059937]\n",
            "  [40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [38.3       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02059937]\n",
            " [40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [38.3       ]]\n",
            "Current yhat_______________: [[40.020622]]\n",
            "Next yhat, before iteration: [[39.158684]]\n",
            "Linesleft_: 15\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]]\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [37.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [37.9       ]\n",
            " [37.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [37.9       ]\n",
            " [37.9       ]\n",
            " [37.8       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [37.9       ]\n",
            " [37.9       ]\n",
            " [37.8       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [37.9       ]\n",
            " [37.9       ]\n",
            " [37.8       ]\n",
            " [37.6       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [37.9       ]\n",
            " [37.9       ]\n",
            " [37.8       ]\n",
            " [37.6       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02060318]\n",
            "  [40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [37.9       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02060318]\n",
            " [40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [37.9       ]]\n",
            "Current yhat_______________: [[40.020626]]\n",
            "Next yhat, before iteration: [[38.94066]]\n",
            "Linesleft_: 14\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]]\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [37.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [37.9       ]\n",
            " [37.8       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [37.9       ]\n",
            " [37.8       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [37.9       ]\n",
            " [37.8       ]\n",
            " [37.6       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [37.9       ]\n",
            " [37.8       ]\n",
            " [37.6       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [37.9       ]\n",
            " [37.8       ]\n",
            " [37.6       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.2       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02060699]\n",
            "  [40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [37.9       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02060699]\n",
            " [40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [37.9       ]]\n",
            "Current yhat_______________: [[40.02063]]\n",
            "Next yhat, before iteration: [[38.940662]]\n",
            "Linesleft_: 13\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [37.8       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [37.8       ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [37.8       ]\n",
            " [37.6       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [37.8       ]\n",
            " [37.6       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [37.8       ]\n",
            " [37.6       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [37.8       ]\n",
            " [37.6       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [36.9       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02061081]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [37.8       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02061081]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [37.8       ]]\n",
            "Current yhat_______________: [[40.020634]]\n",
            "Next yhat, before iteration: [[38.885033]]\n",
            "Linesleft_: 12\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [37.6       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [37.6       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [37.6       ]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [37.6       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [37.6       ]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337]\n",
            " [37.6      ]\n",
            " [37.4      ]\n",
            " [37.4      ]\n",
            " [37.2      ]\n",
            " [36.9      ]\n",
            " [36.9      ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [37.6       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [37.6       ]]\n",
            "Current yhat_______________: [[40.020638]]\n",
            "Next yhat, before iteration: [[38.773067]]\n",
            "Linesleft_: 11\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]]\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [37.4       ]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [37.4       ]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02061462]\n",
            "  [40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02061462]\n",
            " [40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02064]]\n",
            "Next yhat, before iteration: [[38.661114]]\n",
            "Linesleft_: 10\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]]\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [37.4       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [37.4       ]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [37.4       ]\n",
            " [37.2       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02061462]\n",
            "  [40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [37.4       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02061462]\n",
            " [40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [37.4       ]]\n",
            "Current yhat_______________: [[40.02064]]\n",
            "Next yhat, before iteration: [[38.661114]]\n",
            "Linesleft_: 9\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]]\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [37.2       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [37.2       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [37.2       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [37.2       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [37.2       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [37.2       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02062225]\n",
            "  [40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [37.2       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02062225]\n",
            " [40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [37.2       ]]\n",
            "Current yhat_______________: [[40.02064]]\n",
            "Next yhat, before iteration: [[38.54901]]\n",
            "Linesleft_: 8\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]]\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [37.        ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02062607]\n",
            "  [40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [36.9       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02062607]\n",
            " [40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [36.9       ]]\n",
            "Current yhat_______________: [[40.02065]]\n",
            "Next yhat, before iteration: [[38.38065]]\n",
            "Linesleft_: 7\n",
            "Linesleft1: 7\n",
            "Next Unscaled Input Iterated:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]]\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [37.        ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [37.        ]\n",
            " [37.        ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02062988]\n",
            "  [40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [36.9       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02062988]\n",
            " [40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [36.9       ]]\n",
            "Current yhat_______________: [[40.020653]]\n",
            "Next yhat, before iteration: [[38.38065]]\n",
            "Linesleft_: 6\n",
            "Linesleft1: 6\n",
            "Next Unscaled Input Iterated:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]]\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [37.        ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [37.        ]\n",
            " [37.        ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.0206337 ]\n",
            "  [40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [36.9       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.0206337 ]\n",
            " [40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [36.9       ]]\n",
            "Current yhat_______________: [[40.020657]]\n",
            "Next yhat, before iteration: [[38.38065]]\n",
            "Linesleft_: 5\n",
            "Linesleft1: 5\n",
            "Next Unscaled Input Iterated:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [36.9       ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [37.        ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [36.9       ]\n",
            " [36.9       ]\n",
            " [37.        ]\n",
            " [37.        ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02063751]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [36.9       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02063751]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [36.9       ]]\n",
            "Current yhat_______________: [[40.02066]]\n",
            "Next yhat, before iteration: [[38.380653]]\n",
            "Linesleft_: 4\n",
            "Linesleft1: 4\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [36.9       ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [36.9       ]\n",
            " [37.        ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [36.9       ]\n",
            " [37.        ]\n",
            " [37.        ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [36.9       ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [36.9       ]]\n",
            "Current yhat_______________: [[40.020664]]\n",
            "Next yhat, before iteration: [[38.38065]]\n",
            "Linesleft_: 3\n",
            "Linesleft1: 3\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]]\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [37.        ]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [37.        ]\n",
            " [37.        ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02064133]\n",
            "  [40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [37.        ]]]\n",
            "ITERATIVE_Test_Input[i]__:  [[40.02064133]\n",
            " [40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]]\n",
            "ITERATIVE_Test_Input[i+1]:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [37.        ]]\n",
            "Current yhat_______________: [[40.020668]]\n",
            "Next yhat, before iteration: [[38.436775]]\n",
            "Linesleft_: 2\n",
            "Linesleft1: 2\n",
            "Next Unscaled Input Iterated:  [[40.02064133]\n",
            " [40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]]\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [37.        ]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02064133]\n",
            "  [40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: [[[40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]\n",
            "  [37.        ]]]\n",
            "Current yhat_______________: [[40.020676]]\n",
            "Next yhat, before iteration: [[38.43678]]\n",
            "Linesleft_: 1\n",
            "Linesleft1: 1\n",
            "Next Unscaled Input Iterated:  [[40.02064896]\n",
            " [40.02065277]\n",
            " [40.02065659]\n",
            " [40.0206604 ]\n",
            " [40.02066422]\n",
            " [40.02066803]\n",
            " [40.02067566]]\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "ITERATIVE_Test_Input[i:i+1]__: [[[40.02064896]\n",
            "  [40.02065277]\n",
            "  [40.02065659]\n",
            "  [40.0206604 ]\n",
            "  [40.02066422]\n",
            "  [40.02066803]\n",
            "  [40.02067566]]]\n",
            "ITERATIVE_Test_Input[i+1:i+2]: []\n",
            "Current yhat_______________: [[40.02068]]\n",
            "Linesleft_: 0\n",
            "Linesleft1: 0\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "[35.260265 35.21031  35.295197 ... 40.020668 40.020676 40.02068 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "fCpnoFQctW7Q",
        "outputId": "b07810e3-6d65-454e-f435-e697c6821295"
      },
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=y_test_RM_NONITERATIVE, name=\"y_test_RM_NONITERATIVE\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_RM, name=\"y_RM\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_test_RM_ITERATIVE, name=\"y_test_RM_ITERATIVE\", line_shape='linear'))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"791aebee-d815-4b0c-ab5b-222a3e2cbef8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"791aebee-d815-4b0c-ab5b-222a3e2cbef8\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '791aebee-d815-4b0c-ab5b-222a3e2cbef8',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_test_RM_NONITERATIVE\", \"type\": \"scatter\", \"y\": [35.2602653503418, 35.33052062988281, 35.85291290283203, 35.8642692565918, 35.80202865600586, 35.61394119262695, 35.321651458740234, 35.34120178222656, 35.35487365722656, 35.25824737548828, 35.194366455078125, 35.317726135253906, 35.54853057861328, 35.557373046875, 35.60151672363281, 35.607975006103516, 35.70671844482422, 35.78215789794922, 35.80052947998047, 35.81742858886719, 35.92488479614258, 35.84754943847656, 35.88279724121094, 36.00947570800781, 35.89952087402344, 35.8535270690918, 35.99247360229492, 36.002525329589844, 35.933258056640625, 35.85602951049805, 35.7780876159668, 35.666690826416016, 35.7215690612793, 35.57883834838867, 35.41593551635742, 35.16011047363281, 34.93428039550781, 35.262882232666016, 35.01111602783203, 35.065269470214844, 35.144386291503906, 35.08710479736328, 35.193946838378906, 35.220176696777344, 35.21712112426758, 35.32944869995117, 35.38523864746094, 35.40171432495117, 35.42046356201172, 35.427738189697266, 35.43135070800781, 35.53522491455078, 35.58737564086914, 35.60204315185547, 35.61894226074219, 35.626220703125, 35.62982940673828, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.733707427978516, 35.785858154296875, 35.80052947998047, 33.75244140625, 33.885894775390625, 33.98122024536133, 34.006874084472656, 34.02737808227539, 34.038352966308594, 34.04196548461914, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.145843505859375, 34.197994232177734, 34.21266174316406, 34.22956085205078, 34.236839294433594, 34.240447998046875, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.29423904418945, 34.3203125, 34.32764434814453, 34.336097717285156, 34.3397331237793, 34.34153747558594, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.443565368652344, 34.4957160949707, 34.5103874206543, 34.52728271484375, 34.53456115722656, 34.538169860839844, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.642051696777344, 34.6942024230957, 34.7088737487793, 34.72576904296875, 34.73305130004883, 34.736656188964844, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.840538024902344, 34.89268493652344, 34.90735626220703, 34.92424774169922, 34.9315299987793, 34.935142517089844, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.03901672363281, 35.09116744995117, 35.105838775634766, 35.12273406982422, 35.13001251220703, 35.13362121582031, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.18741226196289, 35.21348571777344, 35.220821380615234, 35.229270935058594, 35.23291015625, 35.23471450805664, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.33673858642578, 35.388893127441406, 35.403568267822266, 35.42046356201172, 35.427738189697266, 35.43135070800781, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.53522491455078, 35.58737564086914, 35.60204315185547, 35.61894226074219, 35.626220703125, 35.62982940673828, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.733707427978516, 35.65276336669922, 35.68431091308594, 35.810997009277344, 35.80124282836914, 35.83854675292969, 35.838706970214844, 35.82831573486328, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.93218994140625, 35.98434066772461, 35.99901580810547, 36.01591110229492, 36.023189544677734, 36.02679443359375, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.080589294433594, 36.04011535644531, 36.05589294433594, 36.119232177734375, 36.114349365234375, 36.13300323486328, 36.133087158203125, 36.12788772583008, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.229915618896484, 36.282066345214844, 36.29673767089844, 36.313636779785156, 36.32091522216797, 36.32452392578125, 36.428401947021484, 36.480552673339844, 36.49522399902344, 36.512115478515625, 36.51939392089844, 36.52300262451172, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.62688446044922, 36.67903137207031, 36.693702697753906, 36.710601806640625, 36.71788024902344, 36.72148895263672, 36.725189208984375, 36.725189208984375, 36.82536697387695, 36.87751770019531, 36.892189025878906, 36.909088134765625, 36.91636657714844, 36.91997528076172, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.97376251220703, 36.99983596801758, 37.007171630859375, 37.01561737060547, 37.019256591796875, 37.02106475830078, 37.022918701171875, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.12309265136719, 37.17523956298828, 37.18991470336914, 37.206809997558594, 37.214088439941406, 37.21769714355469, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.321571350097656, 37.37372589111328, 37.388397216796875, 37.405296325683594, 37.412574768066406, 37.41618347167969, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.52006149291992, 37.572208404541016, 37.58687973022461, 37.60377502441406, 37.611053466796875, 37.614662170410156, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.718536376953125, 37.77069091796875, 37.785362243652344, 37.80226135253906, 37.809539794921875, 37.81314468383789, 37.81684875488281, 37.81684875488281, 37.86693572998047, 37.89301300048828, 37.90034866333008, 37.90879821777344, 37.912437438964844, 37.91423797607422, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91608810424805, 37.91608810424805, 38.016265869140625, 38.068416595458984, 38.08308792114258, 38.09998321533203, 38.107261657714844, 38.110870361328125, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.214752197265625, 38.26689910888672, 38.28157043457031, 38.29846954345703, 38.30574417114258, 38.30935287475586, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.413230895996094, 38.46538162231445, 38.48005294799805, 38.4969482421875, 38.50423049926758, 38.507835388183594, 38.511539459228516, 38.511539459228516, 38.611717224121094, 38.66386413574219, 38.67853546142578, 38.6954345703125, 38.70271301269531, 38.706321716308594, 38.71002197265625, 38.71002197265625, 38.71002197265625, 38.71002197265625, 38.71002197265625, 38.71002197265625, 38.71002197265625, 38.71002197265625, 38.71002197265625, 38.71002197265625, 38.76011276245117, 38.78618621826172, 38.79351806640625, 38.80196762084961, 38.805606842041016, 38.807411193847656, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.80926513671875, 38.80926513671875, 38.809261322021484, 38.809261322021484, 38.90943908691406, 38.96159362792969, 38.97626495361328, 38.99315643310547, 39.00043487548828, 39.00404357910156, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.00774383544922, 39.1079216003418, 39.16007614135742, 39.174747467041016, 39.191646575927734, 39.19892501831055, 39.20252990722656, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.20623016357422, 39.3064079284668, 39.35855484008789, 39.373226165771484, 39.39012908935547, 39.397403717041016, 39.4010124206543, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.40471267700195, 39.5048942565918, 39.557037353515625, 39.57170867919922, 39.58860778808594, 39.59588623046875, 39.59949493408203, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.60319519042969, 39.65328598022461, 39.679359436035156, 39.68669891357422, 39.69514465332031, 39.69878387451172, 39.70058822631836, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.70243835449219, 39.652339935302734, 39.55071258544922, 39.44648361206055, 39.47399139404297, 39.3436164855957, 39.26002883911133, 39.283447265625, 39.228118896484375, 39.24068069458008, 39.2298583984375, 39.20623016357422, 39.10602569580078, 39.0058479309082, 38.877830505371094, 38.89433670043945, 38.865596771240234, 38.85102081298828, 38.832889556884766, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.809261322021484, 38.75916290283203, 38.751155853271484, 38.74643325805664, 38.70578384399414, 38.7899055480957, 38.80047607421875, 38.804080963134766, 38.81378173828125, 38.805606842041016, 38.75730895996094, 38.71751022338867, 38.73681640625, 38.771053314208984, 38.80341339111328, 38.80533218383789, 38.80196762084961, 38.805606842041016, 38.75730895996094, 38.71751022338867, 38.73681640625, 38.62076187133789, 38.543739318847656, 38.677120208740234, 38.685752868652344, 38.712989807128906, 38.618858337402344, 38.5023307800293, 38.3779182434082, 38.398128509521484, 38.369388580322266, 38.35481262207031, 38.436859130859375, 38.46538162231445, 38.48005294799805, 38.39674377441406, 38.32072067260742, 38.26273727416992, 38.151432037353516, 38.20261001586914, 38.05988311767578, 37.96551513671875, 37.993309020996094, 37.887874603271484, 37.892433166503906, 38.18141174316406, 38.06066131591797, 38.080902099609375, 38.023014068603516, 37.93014144897461, 37.98487091064453, 37.9114990234375, 37.858787536621094, 37.86726760864258, 37.827789306640625, 37.834075927734375, 37.82865905761719, 37.81684875488281, 37.86693572998047, 37.82646560668945, 37.80859375, 37.82941818237305, 37.824134826660156, 37.727203369140625, 37.64515686035156, 37.67195129394531, 37.6402473449707, 37.652809143066406, 37.641990661621094, 37.61836242675781, 37.51816177368164, 37.43486022949219, 37.473472595214844, 37.34156036376953, 37.27082061767578, 37.298614501953125, 37.243282318115234, 37.1556396484375, 37.06151580810547, 37.076499938964844, 36.99469757080078, 36.965614318847656, 36.974098205566406, 36.93461608886719, 36.940895080566406, 36.98557662963867, 36.99983596801758, 37.007171630859375, 36.965518951416016, 36.92750930786133, 36.94861602783203, 36.93461608886719, 36.940895080566406, 36.93548583984375, 36.923675537109375, 36.82347106933594, 36.740169525146484, 36.778778076171875, 36.64686965942383, 36.576133728027344, 36.603919982910156, 36.54859161376953, 36.5611572265625, 36.550331115722656, 36.52670669555664, 36.52670669555664, 36.62688446044922, 36.67903137207031, 36.693702697753906, 36.710601806640625, 36.61767578125, 36.53798294067383, 36.4800910949707, 36.365081787109375, 36.516441345214844, 36.52606201171875, 36.52967071533203, 36.535743713378906, 36.51939392089844, 36.6231803894043, 36.54594421386719, 36.577491760253906, 36.70417022705078, 36.694419860839844, 36.83190155029297, 36.88420867919922, 36.78828430175781, 36.792869567871094, 36.923797607421875, 36.89651107788086, 36.94353485107422, 36.93036651611328, 36.81977081298828, 36.740169525146484, 36.778778076171875, 36.847251892089844, 36.911964416503906, 36.91581726074219, 36.708675384521484, 36.515602111816406, 36.26316833496094, 36.299888610839844, 36.34258270263672, 36.23249053955078, 36.22777557373047, 36.30720520019531, 36.197242736816406, 36.15125274658203, 36.039710998535156, 35.84746170043945, 35.763336181640625, 35.753211975097656, 35.60688400268555, 35.48851776123047, 35.328758239746094, 35.27153396606445, 35.201141357421875, 35.205875396728516, 35.071685791015625, 34.971038818359375, 35.004241943359375, 34.960723876953125, 34.973289489746094, 34.86225891113281, 32.746986389160156, 32.74768829345703, 32.776649475097656, 32.97950744628906, 32.911827087402344, 32.90601348876953, 33.059486389160156, 33.11360168457031, 33.136192321777344, 33.161529541015625, 33.145179748535156, 33.14878845214844, 33.25267028808594, 33.17172622680664, 33.203277587890625, 33.329952239990234, 33.32019805908203, 33.357505798339844, 33.357662200927734, 33.34727096557617, 33.350975036621094, 33.350975036621094, 33.40106201171875, 33.360595703125, 33.34272003173828, 33.413631439208984, 33.434425354003906, 33.394935607910156, 33.36298370361328, 33.374114990234375, 33.36006164550781, 33.41828918457031, 33.43894958496094, 33.43447494506836, 33.44292449951172, 33.446563720703125, 33.4483642578125, 33.450218200683594, 33.550392150878906, 33.469459533691406, 33.433712005615234, 33.4753532409668, 33.46479034423828, 33.471336364746094, 33.473846435546875, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.550392150878906, 33.602542877197266, 33.617218017578125, 33.63410949707031, 33.54118728637695, 33.528785705566406, 33.65613555908203, 33.62523651123047, 33.66856002807617, 33.65538787841797, 33.644996643066406, 33.64869689941406, 33.64869689941406, 33.64869689941406, 33.64869689941406, 33.74887466430664, 33.801025390625, 33.81569290161133, 33.83259201049805, 33.839874267578125, 33.74327087402344, 33.66367721557617, 33.70228576660156, 33.77075958251953, 33.835472106933594, 33.839324951171875, 33.83259201049805, 33.839874267578125, 33.84347915649414, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.947357177734375, 33.999507904052734, 34.014183044433594, 34.03107452392578, 34.038360595703125, 34.041961669921875, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.746910095214844, 34.59272003173828, 34.51699447631836, 34.39189910888672, 34.69456481933594, 34.59661865234375, 34.616668701171875, 34.494224548339844, 34.35061264038086, 34.320411682128906, 34.32073974609375, 34.28954315185547, 34.27784729003906, 34.406227111816406, 34.328575134277344, 34.296119689941406, 34.265533447265625, 34.33623504638672, 34.348419189453125, 34.31124496459961, 34.28980255126953, 34.34345245361328, 34.329803466796875, 34.303218841552734, 34.254981994628906, 34.26909637451172, 34.25509262084961, 34.31146240234375, 34.332122802734375, 34.32764434814453, 34.336097717285156, 34.3397331237793, 34.34153747558594, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.443565368652344, 34.4957160949707, 34.5103874206543, 34.52728271484375, 34.53456115722656, 34.538169860839844, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.642051696777344, 34.6942024230957, 34.7088737487793, 34.72576904296875, 34.73305130004883, 34.736656188964844, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.840538024902344, 34.89268493652344, 35.007530212402344, 35.07658004760742, 34.99832534790039, 34.93552780151367, 34.98511505126953, 34.95702362060547, 34.973289489746094, 34.962467193603516, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.03901672363281, 35.09116744995117, 35.105838775634766, 35.12273406982422, 35.13001251220703, 35.13362121582031, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.187408447265625, 35.146934509277344, 35.129066467285156, 35.14989471435547, 35.14461135864258, 35.147884368896484, 35.149131774902344, 35.13732147216797, 35.18741226196289, 35.21348571777344, 35.220821380615234, 35.229270935058594, 35.33308792114258, 35.38704299926758, 35.60391616821289, 35.725120544433594, 35.661529541015625, 35.5987548828125, 35.48899841308594, 35.512718200683594, 35.391178131103516, 35.29330062866211, 35.41395950317383, 35.41078186035156, 35.33781051635742, 35.260581970214844, 35.28284454345703, 35.25474548339844, 35.27101516723633, 35.26019287109375, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.18646240234375, 35.14480972290039, 35.16411590576172, 35.29853057861328, 35.238975524902344, 35.216129302978516, 35.250457763671875, 35.24748229980469, 35.25373077392578, 35.26019287109375, 35.23656463623047, 35.33673858642578, 35.255802154541016, 35.220054626464844, 35.36187744140625, 35.27037811279297, 35.3084716796875, 35.304569244384766, 35.20574951171875, 35.191070556640625, 35.19697570800781, 35.16939163208008, 35.07081604003906, 34.96562957763672, 34.992427825927734, 34.960723876953125, 34.973289489746094, 34.962467193603516, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.83863067626953, 34.755332946777344, 34.7939453125, 34.76224136352539, 34.87498474121094, 34.91631317138672, 35.007530212402344, 35.07658004760742, 34.99832534790039, 34.93552780151367, 35.08529281616211, 34.97625732421875, 34.939903259277344, 34.7872200012207, 34.63461685180664, 34.54785919189453, 34.4220085144043, 34.43873596191406, 34.38890075683594, 34.377838134765625, 34.31691360473633, 34.25163269042969, 34.27094268798828, 34.25509262084961, 34.261375427246094, 34.25596237182617, 34.14394760131836, 34.060646057128906, 34.09925079345703, 34.06755065917969, 34.18029022216797, 34.30678939819336, 34.28882598876953, 34.31734085083008, 34.42896270751953, 34.48120880126953, 34.40833282470703, 34.34377670288086, 34.38966751098633, 34.46175003051758, 34.53016662597656, 34.43381118774414, 34.34377670288086, 34.38966751098633, 34.361572265625, 34.47801971435547, 34.386253356933594, 34.311859130859375, 34.276771545410156, 34.189598083496094, 34.092708587646484, 34.13294982910156, 33.979156494140625, 33.963897705078125, 34.07672882080078, 34.02220153808594, 33.96532440185547, 33.86885070800781, 33.89706802368164, 33.969242095947266, 34.033958435058594, 34.037811279296875, 34.03107452392578, 34.038352966308594, 34.04196548461914, 34.04566192626953, 34.04566192626953, 33.94546127319336, 33.92945098876953, 34.0531005859375, 33.9219970703125, 33.88201904296875, 33.90746307373047, 33.86536407470703, 33.881629943847656, 33.77060317993164, 33.66367721557617, 33.80246353149414, 33.82291030883789, 33.74993896484375, 33.6727180480957, 33.69498062133789, 33.6668815612793, 33.68315124511719, 33.67232894897461, 33.74887466430664, 33.66793441772461, 33.6321907043457, 36.15840148925781, 36.40675735473633, 36.41774368286133, 36.37550354003906, 36.31201171875, 36.35454559326172, 36.346405029296875, 36.212364196777344, 36.0513916015625, 35.92737579345703, 36.07833480834961, 36.308712005615234, 36.473472595214844, 36.460418701171875, 36.57943344116211, 36.63719940185547, 36.666439056396484, 36.80707931518555, 36.870208740234375, 36.88848876953125, 36.909088134765625, 36.966453552246094, 36.99613571166992, 37.007171630859375, 37.01561737060547, 37.019256591796875, 37.021060943603516, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022918701171875, 37.022918701171875, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022918701171875, 37.022918701171875, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 36.97281265258789, 36.964805603027344, 37.02663040161133, 37.01118087768555, 37.03284454345703, 37.02626037597656, 37.021060943603516, 37.022911071777344, 37.423622131347656, 37.33551788330078, 37.32389450073242, 37.33027648925781, 37.25090789794922, 37.20515441894531, 37.10702896118164, 37.11095428466797, 37.068424224853516, 37.0573616027832, 37.046539306640625, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022918701171875, 37.022918701171875, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.12309265136719, 37.17523956298828, 37.18991470336914, 37.206809997558594, 37.214088439941406, 37.11749267578125, 37.03789138793945, 37.17668151855469, 37.19712829589844, 37.224361419677734, 37.230438232421875, 37.214088439941406, 37.21769714355469, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.321571350097656, 37.37372589111328, 37.388397216796875, 37.405296325683594, 37.412574768066406, 37.41618347167969, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.52006149291992, 37.43912124633789, 37.40337371826172, 37.44502258300781, 37.43445587158203, 37.44100570678711, 37.443511962890625, 37.419883728027344, 37.52006149291992, 37.572208404541016, 37.58687973022461, 37.60377502441406, 37.611053466796875, 37.614662170410156, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.61836242675781, 37.718536376953125, 37.77069091796875, 37.785362243652344, 37.80226135253906, 37.809539794921875, 37.81314468383789, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.81684875488281, 37.86693572998047, 37.89301300048828, 37.90034866333008, 37.90879821777344, 37.912437438964844, 37.91423797607422, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91608810424805, 37.91608810424805, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 37.91609191894531, 38.016265869140625, 37.935325622558594, 37.96687316894531, 38.09355163574219, 38.083797454833984, 38.1211051940918, 38.12126159667969, 38.110870361328125, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.11457061767578, 38.214752197265625, 38.26689910888672, 38.28157043457031, 38.29846954345703, 38.30574417114258, 38.30935287475586, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.313053131103516, 38.313053131103516, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.31305694580078, 38.413230895996094, 38.46538162231445, 38.48005294799805, 38.4969482421875, 38.50423049926758, 38.50783920288086, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 38.511539459228516, 33.95866775512695, 34.08808517456055, 34.26487350463867, 34.42192840576172, 34.46596908569336, 34.50053405761719, 34.51277542114258, 34.5252685546875, 34.538169860839844, 34.642051696777344, 34.6942024230957, 34.7088737487793, 34.72576904296875, 34.73305130004883, 34.736656188964844, 34.7403564453125, 34.7403564453125, 34.840538024902344, 34.89268493652344, 34.90735626220703, 34.92424774169922, 34.9315299987793, 34.93513870239258, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.03901672363281, 35.09116744995117, 35.1058349609375, 35.12273406982422, 35.13001251220703, 35.13362121582031, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.18741226196289, 35.21348571777344, 35.220821380615234, 35.229270935058594, 35.23291015625, 35.23471450805664, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.33673858642578, 35.388893127441406, 35.403564453125, 35.42046356201172, 35.427738189697266, 35.43135070800781, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.43505096435547, 35.53522491455078, 35.58737564086914, 35.60204315185547, 35.61894226074219, 35.626220703125, 35.62982940673828, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.733707427978516, 35.785858154296875, 35.80052947998047, 35.81742858886719, 35.82470703125, 35.82831573486328, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.93218994140625, 35.98434066772461, 35.99901580810547, 36.01591110229492, 36.023189544677734, 36.02679443359375, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.080589294433594, 36.10666275024414, 36.11399841308594, 36.12244415283203, 36.12608337402344, 36.12788772583008, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.229915618896484, 36.14897537231445, 36.18052291870117, 36.30720520019531, 36.297447204589844, 36.33475875854492, 36.33491516113281, 36.32452392578125, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.428401947021484, 36.480552673339844, 36.49522399902344, 36.512115478515625, 36.51939392089844, 36.52300262451172, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.62688446044922, 36.67903137207031, 36.693702697753906, 36.710601806640625, 36.71788024902344, 36.72148895263672, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.82536697387695, 36.87751770019531, 36.892189025878906, 36.909088134765625, 36.91636657714844, 36.91997528076172, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.97376251220703, 36.99983596801758, 37.007171630859375, 37.01561737060547, 37.019256591796875, 37.021060943603516, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.12309265136719, 37.17523956298828, 37.18991470336914, 37.206809997558594, 37.214088439941406, 37.21769714355469, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.321571350097656, 37.24063491821289, 37.27218246459961, 37.39886474609375, 37.38911056518555, 37.42641830444336, 37.42657470703125, 37.41618347167969, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.52006149291992, 37.572208404541016, 38.73896789550781, 38.21038818359375, 37.98578643798828, 38.06349182128906, 38.071895599365234, 37.94756317138672, 37.87065887451172, 37.5513916015625, 37.421974182128906, 37.22854232788086, 37.04148864746094, 37.059242248535156, 36.99095153808594, 36.983543395996094, 37.00920104980469, 36.99983596801758]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RM\", \"type\": \"scatter\", \"y\": [35.6, 36.0, 36.0, 35.6, 35.2, 35.1, 35.2, 35.2, 35.1, 35.1, 35.4, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.6, 35.6, 35.6, 35.4, 35.1, 34.7, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.4, 39.4, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.8, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.7, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.3, 38.3, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.8, 37.9, 38.3, 38.1, 38.1, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.7, 36.7, 36.7, 36.9, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.5, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.1, 36.1, 35.8, 35.6, 35.6, 35.6, 35.4, 35.2, 35.2, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 32.7, 32.7, 33.1, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 35.4, 34.7, 34.3, 34.3, 34.7, 34.5, 34.5, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.2, 34.2, 34.3, 34.3, 34.2, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.8, 35.8, 35.6, 35.4, 35.4, 35.4, 35.2, 35.2, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.2, 35.4, 35.2, 35.4, 35.2, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 35.1, 34.9, 34.7, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.2, 34.2, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.0, 35.8, 35.8, 36.1, 36.3, 36.5, 36.5, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.8, 37.4, 37.4, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 34.2, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 39.9, 38.3, 37.9, 37.9, 37.8, 37.6, 37.4, 37.4, 37.2, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.2]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_test_RM_ITERATIVE\", \"type\": \"scatter\", \"y\": [35.2602653503418, 35.21031188964844, 35.295196533203125, 35.2853889465332, 35.36286544799805, 35.388916015625, 35.40218734741211, 35.41742706298828, 35.43794250488281, 35.45974349975586, 35.48041534423828, 35.50265884399414, 35.5240364074707, 35.545257568359375, 35.56581115722656, 35.58632278442383, 35.60691833496094, 35.627410888671875, 35.64779281616211, 35.66813659667969, 35.68837356567383, 35.70847702026367, 35.728485107421875, 35.7484130859375, 35.768245697021484, 35.787986755371094, 35.80764389038086, 35.82720947265625, 35.8466796875, 35.866058349609375, 35.88534927368164, 35.90454864501953, 35.92365646362305, 35.94267654418945, 35.961612701416016, 35.9804573059082, 35.99921417236328, 36.017887115478516, 36.03647232055664, 36.054969787597656, 36.07338333129883, 36.09170913696289, 36.109947204589844, 36.12810134887695, 36.14617156982422, 36.16416549682617, 36.18207550048828, 36.19989776611328, 36.21763610839844, 36.23529052734375, 36.252864837646484, 36.27035903930664, 36.28777313232422, 36.30510711669922, 36.32236099243164, 36.33953094482422, 36.356624603271484, 36.373634338378906, 36.390567779541016, 36.40742111206055, 36.424198150634766, 36.440895080566406, 36.457515716552734, 36.474063873291016, 36.49053192138672, 36.50692367553711, 36.52323532104492, 36.53947830200195, 36.555641174316406, 36.57172775268555, 36.58774185180664, 36.60368347167969, 36.61955261230469, 36.63534164428711, 36.651058197021484, 36.66670227050781, 36.68227767944336, 36.69778060913086, 36.71320724487305, 36.72856140136719, 36.74384689331055, 36.75905990600586, 36.77420425415039, 36.78927993774414, 36.804283142089844, 36.819217681884766, 36.834083557128906, 36.848876953125, 36.86360168457031, 36.87826156616211, 36.892852783203125, 36.90737533569336, 36.92183303833008, 36.936222076416016, 36.95054244995117, 36.96479797363281, 36.97898483276367, 36.993106842041016, 37.007164001464844, 37.02116012573242, 37.03508758544922, 37.0489501953125, 37.06275177001953, 37.07648849487305, 37.09015655517578, 37.103763580322266, 37.1173095703125, 37.13079071044922, 37.14421081542969, 37.15756607055664, 37.17086410522461, 37.18409729003906, 37.197269439697266, 37.21037673950195, 37.22342300415039, 37.23641586303711, 37.24934387207031, 37.262210845947266, 37.275020599365234, 37.28776931762695, 37.30046081542969, 37.313087463378906, 37.325660705566406, 37.33817672729492, 37.35063552856445, 37.363033294677734, 37.37537384033203, 37.387657165527344, 37.399879455566406, 37.41204833984375, 37.424163818359375, 37.436222076416016, 37.448219299316406, 37.46016311645508, 37.47205352783203, 37.48388671875, 37.49566650390625, 37.50739288330078, 37.51906204223633, 37.53068161010742, 37.54224395751953, 37.55375289916992, 37.565208435058594, 37.57660675048828, 37.58795928955078, 37.5992546081543, 37.610496520996094, 37.62168884277344, 37.63283157348633, 37.6439208984375, 37.65496063232422, 37.66594696044922, 37.6768798828125, 37.68776321411133, 37.6985969543457, 37.709381103515625, 37.72011184692383, 37.730796813964844, 37.74142837524414, 37.75201416015625, 37.76254653930664, 37.773033142089844, 37.783470153808594, 37.793861389160156, 37.80419921875, 37.814491271972656, 37.82473373413086, 37.83493423461914, 37.84508514404297, 37.85519027709961, 37.8652458190918, 37.8752555847168, 37.88521957397461, 37.895137786865234, 37.90501022338867, 37.91483688354492, 37.924617767333984, 37.93435287475586, 37.94404220581055, 37.95368194580078, 37.963279724121094, 37.972835540771484, 37.98234558105469, 37.9918098449707, 38.0012321472168, 38.0106086730957, 38.01993942260742, 38.029232025146484, 38.03847885131836, 38.04767990112305, 38.05684280395508, 38.06596755981445, 38.075042724609375, 38.084075927734375, 38.09307098388672, 38.10202407836914, 38.11093521118164, 38.11980438232422, 38.12862777709961, 38.137413024902344, 38.14616012573242, 38.15486526489258, 38.16353225708008, 38.172157287597656, 38.18074417114258, 38.189292907714844, 38.197792053222656, 38.20625686645508, 38.214683532714844, 38.22307205200195, 38.231422424316406, 38.2397346496582, 38.248008728027344, 38.25624084472656, 38.264434814453125, 38.2725944519043, 38.28071212768555, 38.288795471191406, 38.29684066772461, 38.304840087890625, 38.31280517578125, 38.320735931396484, 38.32863235473633, 38.33649444580078, 38.34431457519531, 38.35210037231445, 38.35984802246094, 38.36756134033203, 38.37523651123047, 38.382877349853516, 38.39048385620117, 38.39805603027344, 38.40559005737305, 38.41309356689453, 38.420562744140625, 38.42799377441406, 38.43539047241211, 38.442752838134766, 38.4500846862793, 38.457374572753906, 38.464637756347656, 38.471866607666016, 38.479061126708984, 38.48622131347656, 38.49334716796875, 38.50044250488281, 38.507503509521484, 38.514530181884766, 38.52152633666992, 38.52848815917969, 38.53541564941406, 38.54231262207031, 38.5491828918457, 38.55601501464844, 38.56281661987305, 38.56958770751953, 38.576332092285156, 38.58304214477539, 38.589717864990234, 38.59636306762695, 38.60297393798828, 38.60955810546875, 38.616111755371094, 38.62263488769531, 38.629127502441406, 38.63559341430664, 38.642024993896484, 38.64842987060547, 38.65480041503906, 38.6611442565918, 38.66746139526367, 38.673744201660156, 38.679996490478516, 38.68622970581055, 38.69242858886719, 38.6985969543457, 38.704734802246094, 38.71084213256836, 38.71692657470703, 38.722984313964844, 38.72901153564453, 38.73501205444336, 38.74098587036133, 38.74692916870117, 38.75284194946289, 38.758731842041016, 38.76459503173828, 38.77042770385742, 38.7762336730957, 38.782020568847656, 38.787776947021484, 38.79350280761719, 38.799198150634766, 38.80487060546875, 38.81052017211914, 38.81614303588867, 38.821739196777344, 38.82731246948242, 38.83285140991211, 38.8383674621582, 38.8438606262207, 38.849327087402344, 38.854766845703125, 38.86018371582031, 38.86557388305664, 38.87093734741211, 38.876277923583984, 38.881591796875, 38.88688278198242, 38.892147064208984, 38.89738845825195, 38.90260696411133, 38.90780258178711, 38.91297149658203, 38.918121337890625, 38.92324447631836, 38.9283447265625, 38.93341827392578, 38.93846893310547, 38.94349670410156, 38.94850158691406, 38.95348358154297, 38.958438873291016, 38.963375091552734, 38.96828842163086, 38.973175048828125, 38.97804260253906, 38.98289108276367, 38.987709045410156, 38.99250793457031, 38.99728775024414, 39.002044677734375, 39.00677490234375, 39.0114860534668, 39.016178131103516, 39.02084732055664, 39.025489807128906, 39.030113220214844, 39.03471374511719, 39.0392951965332, 39.04384994506836, 39.04838562011719, 39.05290222167969, 39.057403564453125, 39.06188201904297, 39.06633377075195, 39.070770263671875, 39.0751838684082, 39.07957077026367, 39.08393859863281, 39.08829116821289, 39.092628479003906, 39.09693908691406, 39.10123062133789, 39.10550308227539, 39.1097526550293, 39.11398696899414, 39.118194580078125, 39.12238693237305, 39.126564025878906, 39.13071823120117, 39.13485336303711, 39.13896560668945, 39.143062591552734, 39.14714050292969, 39.15119934082031, 39.15523910522461, 39.159263610839844, 39.163265228271484, 39.16725158691406, 39.17121887207031, 39.17516326904297, 39.17909622192383, 39.18300247192383, 39.186893463134766, 39.19076919555664, 39.19462585449219, 39.198463439941406, 39.2022819519043, 39.206085205078125, 39.20987319946289, 39.21364212036133, 39.21739196777344, 39.221126556396484, 39.2248420715332, 39.228538513183594, 39.23221969604492, 39.23588562011719, 39.239532470703125, 39.2431640625, 39.24677658081055, 39.250370025634766, 39.25395202636719, 39.257511138916016, 39.261051177978516, 39.264583587646484, 39.26810073852539, 39.27159881591797, 39.275081634521484, 39.27854919433594, 39.28200149536133, 39.28543472290039, 39.28885269165039, 39.29225540161133, 39.29563903808594, 39.29901123046875, 39.302364349365234, 39.30570602416992, 39.30903244018555, 39.312339782714844, 39.31563186645508, 39.318912506103516, 39.32217025756836, 39.32541275024414, 39.32864761352539, 39.33186340332031, 39.335060119628906, 39.3382453918457, 39.34141540527344, 39.344573974609375, 39.34771728515625, 39.35084533691406, 39.35396194458008, 39.357059478759766, 39.36014175415039, 39.36321258544922, 39.36627197265625, 39.36931610107422, 39.37234115600586, 39.3753547668457, 39.37835693359375, 39.38134002685547, 39.384315490722656, 39.387271881103516, 39.39021682739258, 39.39314651489258, 39.39606475830078, 39.39896774291992, 39.40185546875, 39.404727935791016, 39.4075927734375, 39.41044235229492, 39.41328048706055, 39.416099548339844, 39.41891098022461, 39.42171096801758, 39.424495697021484, 39.42726516723633, 39.43002700805664, 39.43277359008789, 39.43550491333008, 39.43822479248047, 39.4409294128418, 39.44362258911133, 39.44630432128906, 39.448978424072266, 39.451637268066406, 39.454280853271484, 39.456912994384766, 39.45953369140625, 39.46214294433594, 39.46473693847656, 39.46731948852539, 39.46989440917969, 39.47245788574219, 39.475006103515625, 39.477542877197266, 39.480064392089844, 39.482582092285156, 39.485084533691406, 39.487571716308594, 39.49005126953125, 39.49251937866211, 39.49497604370117, 39.49741744995117, 39.49985122680664, 39.50227355957031, 39.50468444824219, 39.507083892822266, 39.50947570800781, 39.5118522644043, 39.51422119140625, 39.51657485961914, 39.518917083740234, 39.52124786376953, 39.52357482910156, 39.525882720947266, 39.5281867980957, 39.53047561645508, 39.532752990722656, 39.5350227355957, 39.53728103637695, 39.539527893066406, 39.54176712036133, 39.54399490356445, 39.54621124267578, 39.54841995239258, 39.55061340332031, 39.552799224853516, 39.55497741699219, 39.5571403503418, 39.55929183959961, 39.561439514160156, 39.563575744628906, 39.56570053100586, 39.56781768798828, 39.56992721557617, 39.572021484375, 39.5741081237793, 39.57618713378906, 39.578250885009766, 39.58030700683594, 39.58235549926758, 39.58439254760742, 39.58641815185547, 39.58843994140625, 39.590450286865234, 39.59245300292969, 39.59444046020508, 39.59642028808594, 39.598392486572266, 39.60036087036133, 39.602317810058594, 39.60426330566406, 39.606197357177734, 39.608123779296875, 39.610042572021484, 39.61195755004883, 39.613861083984375, 39.61574935913086, 39.61763381958008, 39.6195068359375, 39.621376037597656, 39.623233795166016, 39.62508010864258, 39.62691879272461, 39.628753662109375, 39.630577087402344, 39.63239288330078, 39.63420104980469, 39.6359977722168, 39.637786865234375, 39.63956832885742, 39.64134216308594, 39.64310836791992, 39.644866943359375, 39.64661407470703, 39.64834976196289, 39.65008544921875, 39.65180969238281, 39.653526306152344, 39.655235290527344, 39.65693664550781, 39.65863037109375, 39.660316467285156, 39.661991119384766, 39.663658142089844, 39.66531753540039, 39.666969299316406, 39.668617248535156, 39.670257568359375, 39.67189025878906, 39.67351531982422, 39.67512512207031, 39.67673110961914, 39.6783332824707, 39.679927825927734, 39.68151092529297, 39.68308639526367, 39.68465805053711, 39.686222076416016, 39.68777847290039, 39.6893310546875, 39.69087600708008, 39.69240951538086, 39.69393539428711, 39.69545364379883, 39.69696807861328, 39.6984748840332, 39.69997787475586, 39.70146942138672, 39.70295715332031, 39.704437255859375, 39.705909729003906, 39.707374572753906, 39.708831787109375, 39.71028518676758, 39.711727142333984, 39.713165283203125, 39.714595794677734, 39.71601867675781, 39.717437744140625, 39.71885299682617, 39.72025680541992, 39.721656799316406, 39.723052978515625, 39.72443389892578, 39.72581100463867, 39.72718811035156, 39.72855758666992, 39.72991943359375, 39.73127365112305, 39.73262023925781, 39.73395919799805, 39.735294342041016, 39.73662567138672, 39.737953186035156, 39.7392692565918, 39.740577697753906, 39.74188232421875, 39.74317932128906, 39.744468688964844, 39.745758056640625, 39.747039794921875, 39.748313903808594, 39.74958038330078, 39.7508430480957, 39.75210189819336, 39.75334930419922, 39.75458908081055, 39.755828857421875, 39.75706481933594, 39.75829315185547, 39.75951385498047, 39.7607307434082, 39.761940002441406, 39.76314163208008, 39.764339447021484, 39.76553726196289, 39.7667236328125, 39.767906188964844, 39.76908493041992, 39.77025604248047, 39.771419525146484, 39.772579193115234, 39.77373504638672, 39.77488708496094, 39.776031494140625, 39.77717590332031, 39.77831268310547, 39.779441833496094, 39.78056716918945, 39.78168869018555, 39.78280258178711, 39.78390884399414, 39.78501510620117, 39.786109924316406, 39.787200927734375, 39.78828811645508, 39.789371490478516, 39.79044723510742, 39.79152297973633, 39.7925910949707, 39.79365539550781, 39.794715881347656, 39.79576873779297, 39.79681396484375, 39.797855377197266, 39.798892974853516, 39.7999267578125, 39.80095672607422, 39.801979064941406, 39.80299758911133, 39.804012298583984, 39.80502700805664, 39.806034088134766, 39.807029724121094, 39.80802536010742, 39.80901336669922, 39.810001373291016, 39.81098175048828, 39.81195831298828, 39.81293487548828, 39.813907623291016, 39.81487274169922, 39.81583023071289, 39.81678771972656, 39.8177375793457, 39.81868362426758, 39.81962585449219, 39.820560455322266, 39.821495056152344, 39.822425842285156, 39.8233528137207, 39.82427215576172, 39.8251838684082, 39.82609176635742, 39.82699966430664, 39.82790756225586, 39.82880783081055, 39.8297004699707, 39.830596923828125, 39.83148193359375, 39.83236312866211, 39.8332405090332, 39.83411407470703, 39.834983825683594, 39.83584976196289, 39.83671188354492, 39.83757019042969, 39.83842086791992, 39.83926773071289, 39.84011459350586, 39.84095764160156, 39.841793060302734, 39.842628479003906, 39.84345626831055, 39.84428405761719, 39.84510803222656, 39.84592819213867, 39.846744537353516, 39.847557067871094, 39.84836196899414, 39.84916305541992, 39.8499641418457, 39.85075759887695, 39.85154724121094, 39.85233688354492, 39.853126525878906, 39.85390853881836, 39.85468292236328, 39.8554573059082, 39.856231689453125, 39.856998443603516, 39.857765197753906, 39.85852813720703, 39.85928726196289, 39.86003875732422, 39.86078643798828, 39.861534118652344, 39.86227798461914, 39.863014221191406, 39.86375045776367, 39.86448287963867, 39.86521530151367, 39.86594009399414, 39.86665725708008, 39.86737823486328, 39.86809539794922, 39.86880874633789, 39.8695182800293, 39.87022018432617, 39.87091827392578, 39.871620178222656, 39.872318267822266, 39.87301254272461, 39.87369918823242, 39.87438201904297, 39.875064849853516, 39.87574768066406, 39.87642288208008, 39.87709426879883, 39.877769470214844, 39.87843704223633, 39.87910079956055, 39.8797607421875, 39.88041687011719, 39.881072998046875, 39.88172912597656, 39.882381439208984, 39.88302993774414, 39.88367462158203, 39.884315490722656, 39.884952545166016, 39.885581970214844, 39.88621139526367, 39.886844635009766, 39.887474060058594, 39.888092041015625, 39.88871383666992, 39.88933181762695, 39.88994216918945, 39.89055252075195, 39.89116287231445, 39.89176940917969, 39.892372131347656, 39.892974853515625, 39.89356994628906, 39.894168853759766, 39.89476013183594, 39.895347595214844, 39.895931243896484, 39.896514892578125, 39.8970947265625, 39.897674560546875, 39.898250579833984, 39.89882278442383, 39.899391174316406, 39.89995574951172, 39.90052032470703, 39.90108108520508, 39.901641845703125, 39.902198791503906, 39.90275192260742, 39.90330505371094, 39.90385437011719, 39.90439987182617, 39.90494155883789, 39.90548324584961, 39.9060173034668, 39.906558990478516, 39.9070930480957, 39.907623291015625, 39.90815353393555, 39.9086799621582, 39.909202575683594, 39.909725189208984, 39.910240173339844, 39.9107551574707, 39.91127014160156, 39.911781311035156, 39.912288665771484, 39.91279983520508, 39.91330337524414, 39.9138069152832, 39.914310455322266, 39.91481018066406, 39.915306091308594, 39.91579818725586, 39.916290283203125, 39.91678237915039, 39.91727066040039, 39.917755126953125, 39.918235778808594, 39.9187126159668, 39.919193267822266, 39.91967010498047, 39.92014694213867, 39.92061996459961, 39.92108917236328, 39.92155456542969, 39.92201614379883, 39.92247772216797, 39.92293930053711, 39.92340087890625, 39.92385482788086, 39.9243049621582, 39.92475891113281, 39.925209045410156, 39.9256591796875, 39.92610168457031, 39.926544189453125, 39.92698287963867, 39.92742156982422, 39.927860260009766, 39.92829895019531, 39.92873001098633, 39.929161071777344, 39.929588317871094, 39.930015563964844, 39.93043899536133, 39.93086242675781, 39.93128204345703, 39.931697845458984, 39.9321174621582, 39.932533264160156, 39.932945251464844, 39.93335723876953, 39.93376541137695, 39.934173583984375, 39.934574127197266, 39.93497848510742, 39.93537902832031, 39.93578338623047, 39.936180114746094, 39.93657302856445, 39.93696594238281, 39.93736267089844, 39.9377555847168, 39.938140869140625, 39.93852615356445, 39.93891143798828, 39.939292907714844, 39.93967819213867, 39.94005584716797, 39.9404296875, 39.9408073425293, 39.94118118286133, 39.94155502319336, 39.94192886352539, 39.94230270385742, 39.94266891479492, 39.943031311035156, 39.943397521972656, 39.943756103515625, 39.94411849975586, 39.94447708129883, 39.9448356628418, 39.9451904296875, 39.9455451965332, 39.945899963378906, 39.946250915527344, 39.946598052978516, 39.94694519042969, 39.94729232788086, 39.94763946533203, 39.94798278808594, 39.94832229614258, 39.94866180419922, 39.948997497558594, 39.94933319091797, 39.94967269897461, 39.95000457763672, 39.95033645629883, 39.9506721496582, 39.95100021362305, 39.951324462890625, 39.95165252685547, 39.95197677612305, 39.95229721069336, 39.95261764526367, 39.95293426513672, 39.95325469970703, 39.95357131958008, 39.95388412475586, 39.95419692993164, 39.95450973510742, 39.9548225402832, 39.95513153076172, 39.955440521240234, 39.95574951171875, 39.956058502197266, 39.95635986328125, 39.956661224365234, 39.95696258544922, 39.9572639465332, 39.95756149291992, 39.957855224609375, 39.958152770996094, 39.95844268798828, 39.958736419677734, 39.95903015136719, 39.95932388305664, 39.95961380004883, 39.95989990234375, 39.960182189941406, 39.96046829223633, 39.960750579833984, 39.961036682128906, 39.9613151550293, 39.96159362792969, 39.96187210083008, 39.96215057373047, 39.962425231933594, 39.96269989013672, 39.962974548339844, 39.9632453918457, 39.96351623535156, 39.96378707885742, 39.964054107666016, 39.964317321777344, 39.96458435058594, 39.964847564697266, 39.965110778808594, 39.965370178222656, 39.96562957763672, 39.96588897705078, 39.96615219116211, 39.96641159057617, 39.9666633605957, 39.9669189453125, 39.96717071533203, 39.9674186706543, 39.96767044067383, 39.967918395996094, 39.96816635131836, 39.968414306640625, 39.96866226196289, 39.96890640258789, 39.969154357910156, 39.969398498535156, 39.96963882446289, 39.969879150390625, 39.970115661621094, 39.97035217285156, 39.9705924987793, 39.970829010009766, 39.971065521240234, 39.9713020324707, 39.971534729003906, 39.97176742553711, 39.97199630737305, 39.972225189208984, 39.97245788574219, 39.97268295288086, 39.97290802001953, 39.97312927246094, 39.97335433959961, 39.973575592041016, 39.97379684448242, 39.97401809692383, 39.9742431640625, 39.97446060180664, 39.97467803955078, 39.97489547729492, 39.9751091003418, 39.97532653808594, 39.97554397583008, 39.97575759887695, 39.97596740722656, 39.97617721557617, 39.97638702392578, 39.97659683227539, 39.976802825927734, 39.977012634277344, 39.97721862792969, 39.977420806884766, 39.97762680053711, 39.97783279418945, 39.97803497314453, 39.97823715209961, 39.97843933105469, 39.9786376953125, 39.97883605957031, 39.97903823852539, 39.9792366027832, 39.979427337646484, 39.9796257019043, 39.979820251464844, 39.98001480102539, 39.98020553588867, 39.98040008544922, 39.980587005615234, 39.98077392578125, 39.98096466064453, 39.98115539550781, 39.981346130371094, 39.98153305053711, 39.98171615600586, 39.98189926147461, 39.98208236694336, 39.98226547241211, 39.98244857788086, 39.98263168334961, 39.982810974121094, 39.98299026489258, 39.9831657409668, 39.98334503173828, 39.9835205078125, 39.983699798583984, 39.98387908935547, 39.98405456542969, 39.984222412109375, 39.984397888183594, 39.98456573486328, 39.98473358154297, 39.98490524291992, 39.985076904296875, 39.98524856567383, 39.985416412353516, 39.9855842590332, 39.985748291015625, 39.98591232299805, 39.98607635498047, 39.98624038696289, 39.98640823364258, 39.986572265625, 39.98673629760742, 39.98689270019531, 39.987056732177734, 39.987213134765625, 39.987369537353516, 39.98753356933594, 39.98768615722656, 39.98783874511719, 39.98799514770508, 39.9881477355957, 39.98830032348633, 39.98845672607422, 39.988609313964844, 39.98876190185547, 39.98891067504883, 39.98906707763672, 39.98921585083008, 39.98936462402344, 39.9895133972168, 39.98965835571289, 39.98980712890625, 39.98995590209961, 39.99010467529297, 39.99024963378906, 39.990394592285156, 39.99053955078125, 39.990684509277344, 39.9908332824707, 39.99097442626953, 39.99111557006836, 39.99125671386719, 39.991397857666016, 39.99153518676758, 39.991676330566406, 39.99181365966797, 39.991947174072266, 39.99208450317383, 39.992218017578125, 39.99235534667969, 39.99249267578125, 39.99262619018555, 39.99276351928711, 39.992897033691406, 39.9930305480957, 39.9931640625, 39.9932975769043, 39.99342727661133, 39.99355697631836, 39.993690490722656, 39.99381637573242, 39.99394226074219, 39.994075775146484, 39.994197845458984, 39.99432373046875, 39.99445343017578, 39.99457931518555, 39.99470520019531, 39.99482727050781, 39.99494934082031, 39.99507141113281, 39.99519348144531, 39.99531936645508, 39.99543762207031, 39.99555969238281, 39.99567794799805, 39.99579620361328, 39.99591827392578, 39.99603271484375, 39.99615478515625, 39.99626922607422, 39.99638748168945, 39.99650192260742, 39.99661636352539, 39.996734619140625, 39.996849060058594, 39.9969596862793, 39.9970703125, 39.99718475341797, 39.99729919433594, 39.99740982055664, 39.99752426147461, 39.99763488769531, 39.997745513916016, 39.99785614013672, 39.99796676635742, 39.998077392578125, 39.99818801879883, 39.99829864501953, 39.99840545654297, 39.998512268066406, 39.99861526489258, 39.998722076416016, 39.99882888793945, 39.998931884765625, 39.99903869628906, 39.999141693115234, 39.99924850463867, 39.999351501464844, 39.99945831298828, 39.99955749511719, 39.99966049194336, 39.999759674072266, 39.99986267089844, 39.999961853027344, 40.00006103515625, 40.00016403198242, 40.00025939941406, 40.00035858154297, 40.000457763671875, 40.000553131103516, 40.000648498535156, 40.0007438659668, 40.00083923339844, 40.00093460083008, 40.001033782958984, 40.00112533569336, 40.001220703125, 40.001312255859375, 40.001407623291016, 40.00149917602539, 40.001590728759766, 40.00168228149414, 40.00177764892578, 40.00186538696289, 40.001956939697266, 40.002044677734375, 40.002132415771484, 40.00222396850586, 40.00231170654297, 40.00239944458008, 40.00248718261719, 40.0025749206543, 40.002662658691406, 40.002750396728516, 40.00283432006836, 40.00292205810547, 40.00300598144531, 40.00309371948242, 40.00318145751953, 40.00326919555664, 40.00335693359375, 40.003440856933594, 40.00352096557617, 40.003604888916016, 40.00368881225586, 40.00376892089844, 40.003849029541016, 40.003936767578125, 40.0040168762207, 40.00409698486328, 40.00417709350586, 40.00425720214844, 40.004337310791016, 40.004417419433594, 40.00449752807617, 40.00457763671875, 40.00465393066406, 40.00473403930664, 40.00481033325195, 40.004886627197266, 40.004966735839844, 40.00504684448242, 40.005123138427734, 40.00519943237305, 40.00527572631836, 40.005348205566406, 40.00542068481445, 40.005496978759766, 40.00557327270508, 40.005645751953125, 40.005714416503906, 40.00579071044922, 40.005863189697266, 40.00593948364258, 40.006011962890625, 40.006080627441406, 40.00615310668945, 40.006229400634766, 40.00629425048828, 40.00636672973633, 40.006431579589844, 40.006500244140625, 40.00657272338867, 40.00664138793945, 40.006710052490234, 40.006778717041016, 40.00684356689453, 40.00691223144531, 40.00697708129883, 40.00704574584961, 40.00711441040039, 40.00718307495117, 40.00724792480469, 40.00731658935547, 40.007381439208984, 40.007450103759766, 40.007511138916016, 40.00757598876953, 40.00764083862305, 40.00770950317383, 40.00777053833008, 40.007835388183594, 40.00790023803711, 40.00796127319336, 40.008026123046875, 40.00808334350586, 40.00814437866211, 40.00820541381836, 40.008270263671875, 40.00832748413086, 40.008392333984375, 40.00844955444336, 40.00851058959961, 40.008567810058594, 40.00863265991211, 40.008689880371094, 40.008750915527344, 40.00880432128906, 40.00886154174805, 40.0089225769043, 40.00897979736328, 40.009033203125, 40.009090423583984, 40.00914764404297, 40.00920867919922, 40.0092658996582, 40.00932312011719, 40.00938034057617, 40.009437561035156, 40.00949478149414, 40.009552001953125, 40.009605407714844, 40.00966262817383, 40.00971984863281, 40.00977325439453, 40.00982666015625, 40.00988006591797, 40.00992965698242, 40.00998306274414, 40.01003646850586, 40.01008987426758, 40.0101432800293, 40.010196685791016, 40.01025390625, 40.01030349731445, 40.010353088378906, 40.010406494140625, 40.01045608520508, 40.0105094909668, 40.010562896728516, 40.01061248779297, 40.01066207885742, 40.010711669921875, 40.01076126098633, 40.01081085205078, 40.010860443115234, 40.01091384887695, 40.010963439941406, 40.01101303100586, 40.01105880737305, 40.011104583740234, 40.01115036010742, 40.011199951171875, 40.01124572753906, 40.011295318603516, 40.01134490966797, 40.01139450073242, 40.011444091796875, 40.01148986816406, 40.01153564453125, 40.01158142089844, 40.01162338256836, 40.01166915893555, 40.011714935302734, 40.01176071166992, 40.01180648803711, 40.0118522644043, 40.011898040771484, 40.011940002441406, 40.011985778808594, 40.01203155517578, 40.0120735168457, 40.012115478515625, 40.01216125488281, 40.012203216552734, 40.012245178222656, 40.012290954589844, 40.012332916259766, 40.01237487792969, 40.012413024902344, 40.012454986572266, 40.01250076293945, 40.012542724609375, 40.0125846862793, 40.01262664794922, 40.01266860961914, 40.0127067565918, 40.012752532958984, 40.012786865234375, 40.01282501220703, 40.01285934448242, 40.01290512084961, 40.012939453125, 40.01298141479492, 40.01301574707031, 40.013057708740234, 40.01309585571289, 40.01313400268555, 40.01317596435547, 40.01321029663086, 40.013248443603516, 40.013282775878906, 40.01332473754883, 40.01335906982422, 40.01340103149414, 40.0134391784668, 40.01347351074219, 40.013511657714844, 40.013545989990234, 40.013587951660156, 40.01362609863281, 40.0136604309082, 40.01369857788086, 40.01373291015625, 40.013771057128906, 40.0138053894043, 40.01383972167969, 40.013877868652344, 40.013912200927734, 40.013946533203125, 40.01398468017578, 40.01401901245117, 40.01405334472656, 40.01408767700195, 40.014122009277344, 40.014156341552734, 40.014190673828125, 40.014225006103516, 40.014259338378906, 40.01428985595703, 40.01432418823242, 40.01435852050781, 40.01438903808594, 40.01442337036133, 40.01445770263672, 40.014488220214844, 40.014522552490234, 40.01455307006836, 40.01458740234375, 40.014617919921875, 40.0146484375, 40.01468276977539, 40.014713287353516, 40.01474380493164, 40.01477813720703, 40.01480484008789, 40.01483917236328, 40.01486587524414, 40.014892578125, 40.014923095703125, 40.01495361328125, 40.014984130859375, 40.0150146484375, 40.015045166015625, 40.015071868896484, 40.01510238647461, 40.015132904052734, 40.01516342163086, 40.015193939208984, 40.01522445678711, 40.015254974365234, 40.01528549194336, 40.01531219482422, 40.015342712402344, 40.0153694152832, 40.01539993286133, 40.01542663574219, 40.01545715332031, 40.01548385620117, 40.0155143737793, 40.01554489135742, 40.01557159423828, 40.01559829711914, 40.015621185302734, 40.01565170288086, 40.01567840576172, 40.01570510864258, 40.01572799682617, 40.01575469970703, 40.015785217285156, 40.01580810546875, 40.01583480834961, 40.01586151123047, 40.01588821411133, 40.01591491699219, 40.01594161987305, 40.015968322753906, 40.015995025634766, 40.01601791381836, 40.01604461669922, 40.01606750488281, 40.01609420776367, 40.016117095947266, 40.016143798828125, 40.016170501708984, 40.01619338989258, 40.01621627807617, 40.01624298095703, 40.016265869140625, 40.01628875732422, 40.01631546020508, 40.01633834838867, 40.016361236572266, 40.01638412475586, 40.01641082763672, 40.01643371582031, 40.01646041870117, 40.0164794921875, 40.01649856567383, 40.01652145385742, 40.016544342041016, 40.016571044921875, 40.01659393310547, 40.01661682128906, 40.016639709472656, 40.016658782958984, 40.01668167114258, 40.016700744628906, 40.0167236328125, 40.01673889160156, 40.016761779785156, 40.016780853271484, 40.016807556152344, 40.01683044433594, 40.01685333251953, 40.016876220703125, 40.01689529418945, 40.01691436767578, 40.016937255859375, 40.016963958740234, 40.0169792175293, 40.016998291015625, 40.01701736450195, 40.01704406738281, 40.017066955566406, 40.01708221435547, 40.01710510253906, 40.01712417602539, 40.017147064208984, 40.01716613769531, 40.01718521118164, 40.017208099365234, 40.01722717285156, 40.01724624633789, 40.017269134521484, 40.01728820800781, 40.017311096191406, 40.017330169677734, 40.0173454284668, 40.017364501953125, 40.01738357543945, 40.01740264892578, 40.01742172241211, 40.01744079589844, 40.01746368408203, 40.01748275756836, 40.01750183105469, 40.01751708984375, 40.01753616333008, 40.017555236816406, 40.017574310302734, 40.01759338378906, 40.01761245727539, 40.01762771606445, 40.01764678955078, 40.017662048339844, 40.01768112182617, 40.0177001953125, 40.01771926879883, 40.01773452758789, 40.01775360107422, 40.01776885986328, 40.01778793334961, 40.01780319213867, 40.017818450927734, 40.01783752441406, 40.01785659790039, 40.01787185668945, 40.017887115478516, 40.017906188964844, 40.017921447753906, 40.01793670654297, 40.0179557800293, 40.01797103881836, 40.01799011230469, 40.01800537109375, 40.01802062988281, 40.01803970336914, 40.0180549621582, 40.018070220947266, 40.01808547973633, 40.01810073852539, 40.01811599731445, 40.018131256103516, 40.01814651489258, 40.01816177368164, 40.0181770324707, 40.018192291259766, 40.018211364746094, 40.018226623535156, 40.01824188232422, 40.01825714111328, 40.018272399902344, 40.01828384399414, 40.01829528808594, 40.018314361572266, 40.0183219909668, 40.01833724975586, 40.018348693847656, 40.01836395263672, 40.01837921142578, 40.01839065551758, 40.01840591430664, 40.01841735839844, 40.018436431884766, 40.01844787597656, 40.018463134765625, 40.01847457885742, 40.018489837646484, 40.01850128173828, 40.01852035522461, 40.018531799316406, 40.01854705810547, 40.01856231689453, 40.01857376098633, 40.018585205078125, 40.01860046386719, 40.01861572265625, 40.01863098144531, 40.01864242553711, 40.01865768432617, 40.0186653137207, 40.0186767578125, 40.0186882019043, 40.01870346069336, 40.018714904785156, 40.01872634887695, 40.018741607666016, 40.01875686645508, 40.018768310546875, 40.01878356933594, 40.018795013427734, 40.0188102722168, 40.01881790161133, 40.01883316040039, 40.01884460449219, 40.018856048583984, 40.01886749267578, 40.01887512207031, 40.018890380859375, 40.018898010253906, 40.01891326904297, 40.0189208984375, 40.0189323425293, 40.018943786621094, 40.01895523071289, 40.01897048950195, 40.01898193359375, 40.01899337768555, 40.019004821777344, 40.01901626586914, 40.01902770996094, 40.019039154052734, 40.01905059814453, 40.01906204223633, 40.019073486328125, 40.019081115722656, 40.01909255981445, 40.019100189208984, 40.01911163330078, 40.01912307739258, 40.019134521484375, 40.01914978027344, 40.019161224365234, 40.019168853759766, 40.01918411254883, 40.01919174194336, 40.01919937133789, 40.01921081542969, 40.01921844482422, 40.019229888916016, 40.01924133300781, 40.01925277709961, 40.019264221191406, 40.01927185058594, 40.019287109375, 40.01929473876953, 40.01930618286133, 40.01931381225586, 40.019325256347656, 40.01933670043945, 40.019344329833984, 40.019351959228516, 40.01936340332031, 40.019371032714844, 40.01938247680664, 40.01939010620117, 40.0193977355957, 40.0194091796875, 40.0194206237793, 40.01942825317383, 40.01943588256836, 40.01944351196289, 40.01945114135742, 40.019466400146484, 40.01947021484375, 40.019474029541016, 40.01948547363281, 40.019493103027344, 40.01950454711914, 40.01951217651367, 40.01952362060547, 40.019535064697266, 40.0195426940918, 40.019554138183594, 40.019561767578125, 40.01957321166992, 40.01958084106445, 40.019588470458984, 40.01959991455078, 40.01960754394531, 40.019615173339844, 40.01962661743164, 40.019630432128906, 40.01963806152344, 40.0196418762207, 40.019649505615234, 40.01966094970703, 40.01966857910156, 40.019676208496094, 40.019683837890625, 40.019691467285156, 40.01970291137695, 40.019710540771484, 40.01971435546875, 40.01972198486328, 40.01972961425781, 40.01974105834961, 40.019744873046875, 40.019752502441406, 40.0197639465332, 40.01976776123047, 40.019771575927734, 40.01978302001953, 40.01979064941406, 40.019798278808594, 40.01980972290039, 40.019813537597656, 40.01981735229492, 40.01982498168945, 40.01983642578125, 40.01984405517578, 40.01984786987305, 40.01985549926758, 40.01986312866211, 40.01987075805664, 40.019874572753906, 40.01987838745117, 40.01988983154297, 40.0198974609375, 40.019901275634766, 40.0199089050293, 40.01991653442383, 40.01992416381836, 40.019927978515625, 40.019935607910156, 40.01994323730469, 40.01995086669922, 40.01995849609375, 40.01996994018555, 40.01997756958008, 40.019981384277344, 40.01998519897461, 40.01999282836914, 40.02000045776367, 40.02000427246094, 40.02001190185547, 40.02001953125, 40.02002716064453, 40.02003479003906, 40.02003860473633, 40.020050048828125, 40.020050048828125, 40.020057678222656, 40.02006149291992, 40.02006912231445, 40.02008056640625, 40.020084381103516, 40.02009201049805, 40.02009963989258, 40.020103454589844, 40.02010726928711, 40.02011489868164, 40.02012252807617, 40.0201301574707, 40.02013397216797, 40.020137786865234, 40.020145416259766, 40.0201530456543, 40.02015686035156, 40.020164489746094, 40.02016830444336, 40.020172119140625, 40.020179748535156, 40.02018737792969, 40.02019119262695, 40.02019500732422, 40.020198822021484, 40.020206451416016, 40.02021408081055, 40.02021789550781, 40.02022171020508, 40.020225524902344, 40.020233154296875, 40.020240783691406, 40.02024459838867, 40.0202522277832, 40.020259857177734, 40.020263671875, 40.020267486572266, 40.02027130126953, 40.02027893066406, 40.020286560058594, 40.02029037475586, 40.020294189453125, 40.020301818847656, 40.020301818847656, 40.02030563354492, 40.02030944824219, 40.02031707763672, 40.02032470703125, 40.02032470703125, 40.020328521728516, 40.02033615112305, 40.02034378051758, 40.02035140991211, 40.020355224609375, 40.020362854003906, 40.02036666870117, 40.02037048339844, 40.0203742980957, 40.0203742980957, 40.02037811279297, 40.020381927490234, 40.02039337158203, 40.0203971862793, 40.02040100097656, 40.02040481567383, 40.020408630371094, 40.020416259765625, 40.020423889160156, 40.020423889160156, 40.02042770385742, 40.02043151855469, 40.02043533325195, 40.020442962646484, 40.02044677734375, 40.02045440673828, 40.02045822143555, 40.02046585083008, 40.020469665527344, 40.020477294921875, 40.02048110961914, 40.020484924316406, 40.02048873901367, 40.02049255371094, 40.0204963684082, 40.02050018310547, 40.020503997802734, 40.0205078125, 40.020511627197266, 40.0205192565918, 40.02052688598633, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.02053451538086, 40.02054214477539, 40.020545959472656, 40.02054977416992, 40.02055358886719, 40.02056121826172, 40.020565032958984, 40.02056884765625, 40.02057647705078, 40.02057647705078, 40.02058029174805, 40.02058410644531, 40.020591735839844, 40.02059555053711, 40.020599365234375, 40.02060317993164, 40.020606994628906, 40.02061080932617, 40.02061462402344, 40.02061462402344, 40.02061462402344, 40.02062225341797, 40.020626068115234, 40.0206298828125, 40.020633697509766, 40.02063751220703, 40.0206413269043, 40.0206413269043, 40.0206413269043, 40.02064895629883, 40.020652770996094, 40.02065658569336, 40.020660400390625, 40.02066421508789, 40.020668029785156, 40.02067565917969, 40.02067947387695]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('791aebee-d815-4b0c-ab5b-222a3e2cbef8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "kDS9c1j9Jkyp",
        "outputId": "a7a5e4d4-fbf7-41ce-b23a-acedb6734799"
      },
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=y_test_RT_ITERATIVE, name=\"y_test_RT_ITERATIVE\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_test_RB_ITERATIVE, name=\"y_test_RB_ITERATIVE\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_test_RM_ITERATIVE, name=\"y_test_RM_ITERATIVE\", line_shape='linear'))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"b6c821c1-d315-4188-afab-caea1c351551\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"b6c821c1-d315-4188-afab-caea1c351551\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'b6c821c1-d315-4188-afab-caea1c351551',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_test_RT_ITERATIVE\", \"type\": \"scatter\", \"y\": [34.53227233886719, 34.5089225769043, 34.58062744140625, 34.5903434753418, 34.64912414550781, 34.67234420776367, 34.6910285949707, 34.713584899902344, 34.73662567138672, 34.762962341308594, 34.78651809692383, 34.811405181884766, 34.83570861816406, 34.859825134277344, 34.883792877197266, 34.90768814086914, 34.93146896362305, 34.95506286621094, 34.97855758666992, 35.00197219848633, 35.02527618408203, 35.048465728759766, 35.071556091308594, 35.09453582763672, 35.11740493774414, 35.140167236328125, 35.16282653808594, 35.18538284301758, 35.20783233642578, 35.23017883300781, 35.252418518066406, 35.27455520629883, 35.29658889770508, 35.31852722167969, 35.340362548828125, 35.362091064453125, 35.38372039794922, 35.40524673461914, 35.426673889160156, 35.448001861572266, 35.469234466552734, 35.4903678894043, 35.51140213012695, 35.5323371887207, 35.55317687988281, 35.573917388916016, 35.59456253051758, 35.615108489990234, 35.635562896728516, 35.65592575073242, 35.67619323730469, 35.69636535644531, 35.71643829345703, 35.73642349243164, 35.756317138671875, 35.77611541748047, 35.79581832885742, 35.815433502197266, 35.8349609375, 35.85439682006836, 35.87373733520508, 35.89299011230469, 35.91215515136719, 35.93123245239258, 35.95022201538086, 35.969120025634766, 35.98793029785156, 36.00665283203125, 36.02528762817383, 36.04384231567383, 36.06230545043945, 36.080684661865234, 36.09897994995117, 36.1171875, 36.135311126708984, 36.15335464477539, 36.17131042480469, 36.18918228149414, 36.20697021484375, 36.22467803955078, 36.24230194091797, 36.25984191894531, 36.277305603027344, 36.2946891784668, 36.311988830566406, 36.32920837402344, 36.34634780883789, 36.36341094970703, 36.380393981933594, 36.39729309082031, 36.41411590576172, 36.43086242675781, 36.44752883911133, 36.4641227722168, 36.48063278198242, 36.4970703125, 36.513431549072266, 36.52971649169922, 36.545921325683594, 36.56205368041992, 36.57810974121094, 36.59409713745117, 36.61001205444336, 36.625850677490234, 36.6416130065918, 36.65730285644531, 36.67292022705078, 36.6884651184082, 36.70393753051758, 36.719337463378906, 36.73466873168945, 36.74992752075195, 36.765113830566406, 36.78022766113281, 36.79527282714844, 36.81024932861328, 36.825157165527344, 36.839996337890625, 36.85476303100586, 36.86946487426758, 36.884098052978516, 36.89866256713867, 36.91315841674805, 36.92758560180664, 36.941951751708984, 36.95624923706055, 36.97047805786133, 36.98463821411133, 36.99873352050781, 37.01276779174805, 37.026737213134766, 37.0406379699707, 37.05447769165039, 37.06825256347656, 37.08195877075195, 37.095603942871094, 37.10918426513672, 37.122703552246094, 37.13615798950195, 37.14955520629883, 37.16288757324219, 37.1761589050293, 37.18936538696289, 37.202510833740234, 37.21559524536133, 37.22861862182617, 37.241580963134766, 37.254486083984375, 37.267333984375, 37.280120849609375, 37.292842864990234, 37.30550765991211, 37.318115234375, 37.33066177368164, 37.34315490722656, 37.355587005615234, 37.36796188354492, 37.380279541015625, 37.392539978027344, 37.404747009277344, 37.416893005371094, 37.42898178100586, 37.44101333618164, 37.4529914855957, 37.46491622924805, 37.47678756713867, 37.48859786987305, 37.50035858154297, 37.51205825805664, 37.523704528808594, 37.535301208496094, 37.546844482421875, 37.55833053588867, 37.56976318359375, 37.58114242553711, 37.592472076416016, 37.60374450683594, 37.614967346191406, 37.62614059448242, 37.637264251708984, 37.64833068847656, 37.65934371948242, 37.67030715942383, 37.68122482299805, 37.69208908081055, 37.702903747558594, 37.71366500854492, 37.7243766784668, 37.73503875732422, 37.74565505981445, 37.75621795654297, 37.76673126220703, 37.77719497680664, 37.78761291503906, 37.79798126220703, 37.80830383300781, 37.818580627441406, 37.82880783081055, 37.83898162841797, 37.8491096496582, 37.859195709228516, 37.86922836303711, 37.87921905517578, 37.889163970947266, 37.89906311035156, 37.90892028808594, 37.91872787475586, 37.928489685058594, 37.93820571899414, 37.947879791259766, 37.95750427246094, 37.96708297729492, 37.97662353515625, 37.986114501953125, 37.99556350708008, 38.004966735839844, 38.01432418823242, 38.023643493652344, 38.032920837402344, 38.04214859008789, 38.051334381103516, 38.06048583984375, 38.0695915222168, 38.078651428222656, 38.087669372558594, 38.096649169921875, 38.1055793762207, 38.114471435546875, 38.12332534790039, 38.132137298583984, 38.140907287597656, 38.149635314941406, 38.1583251953125, 38.166969299316406, 38.17557907104492, 38.18415069580078, 38.192684173583984, 38.201171875, 38.20962142944336, 38.21803283691406, 38.22639846801758, 38.23473358154297, 38.24302673339844, 38.25128173828125, 38.25950241088867, 38.26768112182617, 38.27582550048828, 38.28392791748047, 38.2919921875, 38.300018310546875, 38.30801010131836, 38.31596374511719, 38.32387924194336, 38.33176040649414, 38.339603424072266, 38.347412109375, 38.35518264770508, 38.3629150390625, 38.37062072753906, 38.37828826904297, 38.38591384887695, 38.39350509643555, 38.40106201171875, 38.40858459472656, 38.416072845458984, 38.423526763916016, 38.430946350097656, 38.438331604003906, 38.4456787109375, 38.4529914855957, 38.46027374267578, 38.46752166748047, 38.4747314453125, 38.481910705566406, 38.48905944824219, 38.49617385864258, 38.50325012207031, 38.51029968261719, 38.51731491088867, 38.524295806884766, 38.531246185302734, 38.53816223144531, 38.5450439453125, 38.55189895629883, 38.55872344970703, 38.565513610839844, 38.57227325439453, 38.579002380371094, 38.58570098876953, 38.59236526489258, 38.598995208740234, 38.6056022644043, 38.6121711730957, 38.618709564208984, 38.625221252441406, 38.6317024230957, 38.638153076171875, 38.64457702636719, 38.65096664428711, 38.65732955932617, 38.66366195678711, 38.66996383666992, 38.676239013671875, 38.68248748779297, 38.68870162963867, 38.69488525390625, 38.701045989990234, 38.70718002319336, 38.713279724121094, 38.71935272216797, 38.72539520263672, 38.73141098022461, 38.73740005493164, 38.74335861206055, 38.749290466308594, 38.75519561767578, 38.761070251464844, 38.76692199707031, 38.77274703979492, 38.778541564941406, 38.7843132019043, 38.79005813598633, 38.7957763671875, 38.80146789550781, 38.807132720947266, 38.81277084350586, 38.818382263183594, 38.8239631652832, 38.82952117919922, 38.83505630493164, 38.84056091308594, 38.84604263305664, 38.85150146484375, 38.85693359375, 38.86233901977539, 38.86771774291992, 38.87307357788086, 38.8784065246582, 38.88370895385742, 38.88899230957031, 38.894248962402344, 38.899478912353516, 38.90468978881836, 38.90987777709961, 38.915035247802734, 38.920169830322266, 38.9252815246582, 38.93037414550781, 38.93544006347656, 38.94047927856445, 38.945499420166016, 38.95049285888672, 38.95545959472656, 38.96040725708008, 38.965335845947266, 38.97024154663086, 38.97512435913086, 38.97998046875, 38.98481750488281, 38.989627838134766, 38.99441146850586, 38.999183654785156, 39.00393295288086, 39.0086555480957, 39.01335525512695, 39.018035888671875, 39.0226936340332, 39.0273323059082, 39.03194808959961, 39.03654098510742, 39.041114807128906, 39.04566192626953, 39.05018997192383, 39.0546989440918, 39.05918884277344, 39.063655853271484, 39.06809997558594, 39.07252502441406, 39.07693099975586, 39.08131790161133, 39.0856819152832, 39.090023040771484, 39.0943489074707, 39.09865188598633, 39.10293960571289, 39.107200622558594, 39.11144256591797, 39.11566925048828, 39.119876861572266, 39.124061584472656, 39.128231048583984, 39.13237380981445, 39.136497497558594, 39.14060974121094, 39.14470291137695, 39.148773193359375, 39.15282440185547, 39.15685272216797, 39.16086959838867, 39.16486358642578, 39.16883850097656, 39.172794342041016, 39.17673873901367, 39.18065643310547, 39.18455505371094, 39.188438415527344, 39.19230651855469, 39.1961555480957, 39.199989318847656, 39.20380401611328, 39.20759963989258, 39.21138000488281, 39.21514129638672, 39.2188835144043, 39.22261047363281, 39.226318359375, 39.230010986328125, 39.23368453979492, 39.237342834472656, 39.24098587036133, 39.244606018066406, 39.248207092285156, 39.251800537109375, 39.25537109375, 39.25893020629883, 39.262474060058594, 39.26599884033203, 39.269508361816406, 39.27299880981445, 39.27647018432617, 39.279930114746094, 39.28337478637695, 39.286800384521484, 39.29021072387695, 39.29360580444336, 39.29698181152344, 39.30034255981445, 39.303688049316406, 39.30702209472656, 39.31033706665039, 39.313636779785156, 39.31692123413086, 39.320194244384766, 39.323448181152344, 39.326690673828125, 39.329917907714844, 39.3331298828125, 39.33632278442383, 39.33950424194336, 39.34267044067383, 39.345821380615234, 39.34895706176758, 39.35207748413086, 39.355186462402344, 39.358280181884766, 39.36135482788086, 39.364418029785156, 39.367469787597656, 39.37051010131836, 39.373531341552734, 39.37653732299805, 39.37953567504883, 39.38251495361328, 39.38547897338867, 39.388427734375, 39.3913688659668, 39.39429473876953, 39.3972053527832, 39.40010070800781, 39.40298843383789, 39.405860900878906, 39.408721923828125, 39.411563873291016, 39.414398193359375, 39.41722106933594, 39.42002487182617, 39.422813415527344, 39.425594329833984, 39.42835998535156, 39.43111038208008, 39.43385314941406, 39.436580657958984, 39.439292907714844, 39.441993713378906, 39.44468688964844, 39.447364807128906, 39.45002746582031, 39.45268249511719, 39.455326080322266, 39.45795440673828, 39.4605712890625, 39.46318054199219, 39.46577072143555, 39.46834945678711, 39.470916748046875, 39.47346878051758, 39.47601318359375, 39.47854995727539, 39.48107147216797, 39.48358154296875, 39.486083984375, 39.48856735229492, 39.49104309082031, 39.49350357055664, 39.49595260620117, 39.49839401245117, 39.500823974609375, 39.50324630737305, 39.50564956665039, 39.50803756713867, 39.51042175292969, 39.51279830932617, 39.51516342163086, 39.517513275146484, 39.51985168457031, 39.522178649902344, 39.524497985839844, 39.52680587768555, 39.52910232543945, 39.53139114379883, 39.53366470336914, 39.535926818847656, 39.53818130493164, 39.54042434692383, 39.54265594482422, 39.54487991333008, 39.547096252441406, 39.54929733276367, 39.551490783691406, 39.553672790527344, 39.555843353271484, 39.55800247192383, 39.56015396118164, 39.562294006347656, 39.56442642211914, 39.56654739379883, 39.56866455078125, 39.570762634277344, 39.572853088378906, 39.5749397277832, 39.5770149230957, 39.57907485961914, 39.58112716674805, 39.58317184448242, 39.585208892822266, 39.58723831176758, 39.589256286621094, 39.59126281738281, 39.593257904052734, 39.59524917602539, 39.59722900390625, 39.59920120239258, 39.60116195678711, 39.603111267089844, 39.60505294799805, 39.60698699951172, 39.608909606933594, 39.61082458496094, 39.61273193359375, 39.61463165283203, 39.616519927978516, 39.61840057373047, 39.62027359008789, 39.62213897705078, 39.62398910522461, 39.625831604003906, 39.62766647338867, 39.629493713378906, 39.631317138671875, 39.63312530517578, 39.634925842285156, 39.636722564697266, 39.638511657714844, 39.640289306640625, 39.642059326171875, 39.643821716308594, 39.64557647705078, 39.64731979370117, 39.64905548095703, 39.65078353881836, 39.65250778198242, 39.65422058105469, 39.65592575073242, 39.65761947631836, 39.65930938720703, 39.66099166870117, 39.66267013549805, 39.664337158203125, 39.665992736816406, 39.667640686035156, 39.66928482055664, 39.670921325683594, 39.672550201416016, 39.67416763305664, 39.675777435302734, 39.6773796081543, 39.67897415161133, 39.680564880371094, 39.68214797973633, 39.68372344970703, 39.6852912902832, 39.686851501464844, 39.68840408325195, 39.6899528503418, 39.69149398803711, 39.69302749633789, 39.69455337524414, 39.69607162475586, 39.69758224487305, 39.6990852355957, 39.70058059692383, 39.70206832885742, 39.70355224609375, 39.70502471923828, 39.70649337768555, 39.70795440673828, 39.70941162109375, 39.71086502075195, 39.71230697631836, 39.713741302490234, 39.71516799926758, 39.716590881347656, 39.71800994873047, 39.719417572021484, 39.720821380615234, 39.72221755981445, 39.72360610961914, 39.7249870300293, 39.72636413574219, 39.72773742675781, 39.729103088378906, 39.7304573059082, 39.731807708740234, 39.733154296875, 39.734493255615234, 39.73582077026367, 39.737152099609375, 39.73847579956055, 39.73978805541992, 39.741092681884766, 39.742393493652344, 39.743690490722656, 39.74497985839844, 39.74626159667969, 39.74754333496094, 39.74881362915039, 39.75007629394531, 39.751338958740234, 39.752593994140625, 39.753841400146484, 39.75508117675781, 39.756317138671875, 39.757545471191406, 39.75876998901367, 39.75999450683594, 39.761207580566406, 39.76241683959961, 39.76361846923828, 39.76482009887695, 39.76601028442383, 39.76719665527344, 39.76838302612305, 39.76955795288086, 39.770729064941406, 39.77189254760742, 39.773048400878906, 39.774200439453125, 39.77534866333008, 39.776493072509766, 39.77763366699219, 39.77876663208008, 39.77989196777344, 39.781009674072266, 39.78212356567383, 39.78322982788086, 39.78433609008789, 39.785438537597656, 39.786537170410156, 39.787628173828125, 39.78871154785156, 39.789791107177734, 39.79086685180664, 39.79193878173828, 39.79300308227539, 39.794063568115234, 39.79512023925781, 39.796173095703125, 39.79722213745117, 39.79826354980469, 39.79930114746094, 39.80033493041992, 39.801361083984375, 39.80238342285156, 39.80339813232422, 39.804412841796875, 39.805423736572266, 39.80642318725586, 39.80742263793945, 39.80842208862305, 39.809410095214844, 39.81039047241211, 39.811370849609375, 39.812339782714844, 39.81331253051758, 39.81428146362305, 39.815242767333984, 39.816200256347656, 39.81715393066406, 39.8181037902832, 39.81904983520508, 39.81998825073242, 39.8209228515625, 39.82185363769531, 39.82278060913086, 39.82370376586914, 39.824623107910156, 39.82553482055664, 39.82644271850586, 39.82734680175781, 39.8282470703125, 39.82914733886719, 39.83004379272461, 39.8309326171875, 39.831817626953125, 39.83269500732422, 39.83356857299805, 39.834434509277344, 39.83530044555664, 39.8361701965332, 39.837032318115234, 39.837886810302734, 39.83873748779297, 39.83958435058594, 39.840431213378906, 39.841270446777344, 39.84210205078125, 39.84293746948242, 39.84376907348633, 39.8445930480957, 39.84541320800781, 39.846229553222656, 39.8470458984375, 39.84785461425781, 39.84865951538086, 39.84946060180664, 39.850257873535156, 39.851051330566406, 39.85184097290039, 39.85262680053711, 39.853416442871094, 39.85420227050781, 39.854976654052734, 39.85574722290039, 39.85651779174805, 39.85728454589844, 39.85804748535156, 39.858802795410156, 39.85955810546875, 39.86031723022461, 39.86106491088867, 39.8618049621582, 39.862545013427734, 39.86328125, 39.864013671875, 39.86474609375, 39.865478515625, 39.86620330810547, 39.86692810058594, 39.86764907836914, 39.86836242675781, 39.86907196044922, 39.86977767944336, 39.8704833984375, 39.871185302734375, 39.87188720703125, 39.872581481933594, 39.87327194213867, 39.873958587646484, 39.87464141845703, 39.87532424926758, 39.87600326538086, 39.87667465209961, 39.877349853515625, 39.87801742553711, 39.87868118286133, 39.87934112548828, 39.8800048828125, 39.88066482543945, 39.881317138671875, 39.8819694519043, 39.88262176513672, 39.883262634277344, 39.8838996887207, 39.88453674316406, 39.88517379760742, 39.88581085205078, 39.886444091796875, 39.88706588745117, 39.88768768310547, 39.888309478759766, 39.8889274597168, 39.88954162597656, 39.890159606933594, 39.890769958496094, 39.89137649536133, 39.8919792175293, 39.892578125, 39.8931770324707, 39.89377212524414, 39.89436340332031, 39.894954681396484, 39.89554214477539, 39.8961296081543, 39.89670944213867, 39.89728927612305, 39.897865295410156, 39.8984375, 39.899009704589844, 39.89958190917969, 39.900150299072266, 39.90071487426758, 39.901275634765625, 39.90182876586914, 39.902381896972656, 39.90293502807617, 39.90348434448242, 39.90403366088867, 39.90458297729492, 39.90512466430664, 39.905662536621094, 39.90620040893555, 39.906734466552734, 39.907264709472656, 39.907798767089844, 39.9083251953125, 39.90884780883789, 39.90937042236328, 39.90989303588867, 39.9104118347168, 39.910926818847656, 39.911441802978516, 39.911949157714844, 39.912452697753906, 39.91295623779297, 39.9134635925293, 39.91396713256836, 39.914466857910156, 39.91496276855469, 39.91545486450195, 39.91594696044922, 39.91643524169922, 39.916927337646484, 39.91741180419922, 39.91789627075195, 39.91838073730469, 39.918861389160156, 39.919334411621094, 39.91980743408203, 39.92028045654297, 39.92074966430664, 39.92122268676758, 39.921688079833984, 39.922149658203125, 39.922607421875, 39.923065185546875, 39.92352294921875, 39.92397689819336, 39.9244270324707, 39.92487716674805, 39.925323486328125, 39.9257698059082, 39.926212310791016, 39.926658630371094, 39.927101135253906, 39.92753982543945, 39.927978515625, 39.92841339111328, 39.9288444519043, 39.92927551269531, 39.92970275878906, 39.93013000488281, 39.93055725097656, 39.93098068237305, 39.931400299072266, 39.93181610107422, 39.93223190307617, 39.932647705078125, 39.93306350708008, 39.9334716796875, 39.93387985229492, 39.93428421020508, 39.934688568115234, 39.93509292602539, 39.93549728393555, 39.93589401245117, 39.9362907409668, 39.936683654785156, 39.937076568603516, 39.937469482421875, 39.937862396240234, 39.93825149536133, 39.938636779785156, 39.939022064208984, 39.93940734863281, 39.93978500366211, 39.940162658691406, 39.9405403137207, 39.94091796875, 39.941287994384766, 39.9416618347168, 39.94203567504883, 39.942405700683594, 39.94276809692383, 39.94313430786133, 39.943504333496094, 39.94386291503906, 39.944217681884766, 39.944576263427734, 39.9449348449707, 39.94529342651367, 39.945648193359375, 39.94599914550781, 39.94635009765625, 39.94670104980469, 39.94704818725586, 39.947391510009766, 39.94773483276367, 39.94807815551758, 39.94841766357422, 39.948753356933594, 39.949092864990234, 39.94942855834961, 39.94976043701172, 39.950096130371094, 39.9504280090332, 39.95075988769531, 39.951087951660156, 39.951416015625, 39.95174026489258, 39.95206069946289, 39.95238494873047, 39.95270538330078, 39.95302963256836, 39.95335006713867, 39.95366287231445, 39.953975677490234, 39.954288482666016, 39.95460510253906, 39.95491409301758, 39.955223083496094, 39.95553207397461, 39.955841064453125, 39.956146240234375, 39.956451416015625, 39.956756591796875, 39.95705795288086, 39.95735549926758, 39.9576530456543, 39.957942962646484, 39.9582405090332, 39.958534240722656, 39.958824157714844, 39.95911407470703, 39.95940399169922, 39.959693908691406, 39.95998001098633, 39.96026611328125, 39.96055221557617, 39.96083450317383, 39.96112060546875, 39.96139907836914, 39.96167755126953, 39.96194839477539, 39.96222686767578, 39.96249771118164, 39.962772369384766, 39.963043212890625, 39.96331787109375, 39.963584899902344, 39.96384811401367, 39.96411895751953, 39.964385986328125, 39.96464920043945, 39.96491622924805, 39.965179443359375, 39.9654426574707, 39.965702056884766, 39.965965270996094, 39.96622085571289, 39.96648025512695, 39.96673583984375, 39.96699142456055, 39.96724319458008, 39.967498779296875, 39.96774673461914, 39.96799850463867, 39.96824645996094, 39.96849060058594, 39.9687385559082, 39.9689826965332, 39.96922302246094, 39.96946716308594, 39.96970748901367, 39.96994400024414, 39.970184326171875, 39.97042465209961, 39.970664978027344, 39.97090148925781, 39.97113800048828, 39.97136688232422, 39.971595764160156, 39.971832275390625, 39.97206115722656, 39.9722900390625, 39.97251892089844, 39.972747802734375, 39.97297668457031, 39.97320556640625, 39.97343063354492, 39.97365188598633, 39.973876953125, 39.97409439086914, 39.97431182861328, 39.97452926635742, 39.97475051879883, 39.97496795654297, 39.97518539428711, 39.97540283203125, 39.975616455078125, 39.975830078125, 39.976043701171875, 39.976253509521484, 39.976463317871094, 39.9766731262207, 39.97687911987305, 39.97708511352539, 39.977291107177734, 39.97749710083008, 39.97770309448242, 39.977909088134766, 39.978111267089844, 39.97831344604492, 39.978511810302734, 39.97871398925781, 39.978912353515625, 39.97910690307617, 39.979305267333984, 39.97949981689453, 39.97969055175781, 39.97988510131836, 39.980079650878906, 39.98027038574219, 39.9804573059082, 39.980648040771484, 39.9808349609375, 39.981021881103516, 39.9812126159668, 39.98139953613281, 39.98158264160156, 39.98176956176758, 39.981956481933594, 39.982139587402344, 39.982322692871094, 39.982505798339844, 39.98268508911133, 39.98286437988281, 39.98304748535156, 39.98322296142578, 39.983402252197266, 39.983577728271484, 39.9837532043457, 39.98392868041992, 39.98410415649414, 39.984275817871094, 39.98445129394531, 39.984622955322266, 39.98479461669922, 39.984962463378906, 39.985130310058594, 39.98530197143555, 39.98546600341797, 39.985633850097656, 39.98580551147461, 39.985965728759766, 39.98612976074219, 39.986297607421875, 39.9864616394043, 39.98662567138672, 39.986785888671875, 39.98694610595703, 39.98710632324219, 39.98727035522461, 39.987430572509766, 39.987586975097656, 39.98774719238281, 39.9879035949707, 39.98805618286133, 39.98820877075195, 39.988365173339844, 39.98851776123047, 39.988670349121094, 39.98882293701172, 39.98897171020508, 39.98912048339844, 39.9892692565918, 39.98942184448242, 39.98957061767578, 39.989715576171875, 39.989864349365234, 39.990013122558594, 39.99015808105469, 39.99030685424805, 39.990447998046875, 39.9905891418457, 39.9907341003418, 39.99087142944336, 39.99100875854492, 39.991153717041016, 39.991294860839844, 39.99143600463867, 39.9915771484375, 39.99171829223633, 39.99185562133789, 39.99199676513672, 39.992130279541016, 39.99226760864258, 39.992401123046875, 39.99253463745117, 39.992671966552734, 39.9928092956543, 39.992942810058594, 39.99307632446289, 39.99320983886719, 39.99333953857422, 39.99346923828125, 39.99360275268555, 39.99372863769531, 39.99385070800781, 39.993980407714844, 39.994110107421875, 39.99423599243164, 39.99436569213867, 39.9944953918457, 39.9946174621582, 39.9947395324707, 39.99486541748047, 39.994991302490234, 39.99510955810547, 39.99523162841797, 39.99535369873047, 39.99547576904297, 39.9955940246582, 39.99571228027344, 39.99583053588867, 39.995948791503906, 39.996063232421875, 39.996185302734375, 39.996299743652344, 39.99641799926758, 39.99653244018555, 39.996646881103516, 39.99676513671875, 39.99687957763672, 39.99699401855469, 39.997108459472656, 39.997222900390625, 39.99733352661133, 39.99744415283203, 39.997554779052734, 39.99766540527344, 39.99777603149414, 39.997886657714844, 39.99799728393555, 39.99810791015625, 39.99821853637695, 39.998329162597656, 39.998435974121094, 39.998538970947266, 39.9986457824707, 39.99875259399414, 39.99885559082031, 39.998958587646484, 39.99906539916992, 39.99917221069336, 39.99927520751953, 39.9993782043457, 39.99948501586914, 39.99958801269531, 39.99968719482422, 39.999786376953125, 39.9998893737793, 39.9999885559082, 40.000091552734375, 40.00019073486328, 40.00028610229492, 40.00038528442383, 40.000484466552734, 40.00058364868164, 40.00067901611328, 40.00077819824219, 40.000877380371094, 40.000972747802734, 40.001068115234375, 40.001163482666016, 40.001258850097656, 40.0013542175293, 40.00144577026367, 40.00154113769531, 40.00163269042969, 40.00172424316406, 40.00181579589844, 40.00190734863281, 40.00199508666992, 40.0020866394043, 40.002174377441406, 40.00226593017578, 40.002357482910156, 40.002445220947266, 40.002532958984375, 40.002620697021484, 40.00270462036133, 40.00279235839844, 40.00287628173828, 40.002967834472656, 40.0030517578125, 40.00313949584961, 40.00322723388672, 40.00331115722656, 40.00339126586914, 40.003475189208984, 40.00355529785156, 40.003639221191406, 40.00372314453125, 40.003807067871094, 40.00389099121094, 40.00397491455078, 40.00405502319336, 40.00413513183594, 40.004215240478516, 40.004295349121094, 40.00437545776367, 40.004451751708984, 40.00453186035156, 40.00461196899414, 40.00468826293945, 40.00476837158203, 40.004844665527344, 40.004920959472656, 40.00499725341797, 40.00507736206055, 40.005149841308594, 40.00522232055664, 40.00529479980469, 40.00537109375, 40.00544357299805, 40.00551986694336, 40.00559616088867, 40.005672454833984, 40.005741119384766, 40.00581741333008, 40.005889892578125, 40.00596618652344, 40.00603485107422, 40.006107330322266, 40.00617980957031, 40.006256103515625, 40.00632095336914, 40.00638961791992, 40.00645446777344, 40.00652313232422, 40.006591796875, 40.00666427612305, 40.006736755371094, 40.006805419921875, 40.00687026977539, 40.00693893432617, 40.00700759887695, 40.007076263427734, 40.00714111328125, 40.00720977783203, 40.00727081298828, 40.0073356628418, 40.00740051269531, 40.007469177246094, 40.00753402709961, 40.007598876953125, 40.007659912109375, 40.007728576660156, 40.00779342651367, 40.00785446166992, 40.00791931152344, 40.00798416137695, 40.00804901123047, 40.008113861083984, 40.008174896240234, 40.008235931396484, 40.008296966552734, 40.008358001708984, 40.0084228515625, 40.008480072021484, 40.008541107177734, 40.008602142333984, 40.008663177490234, 40.008724212646484, 40.008785247802734, 40.008846282958984, 40.00890350341797, 40.00895690917969, 40.00901412963867, 40.009071350097656, 40.009132385253906, 40.00918960571289, 40.009246826171875, 40.00930404663086, 40.009361267089844, 40.00941467285156, 40.00947189331055, 40.00952911376953, 40.00958251953125, 40.00963592529297, 40.00969314575195, 40.00975036621094, 40.009803771972656, 40.009857177734375, 40.00990676879883, 40.00996017456055, 40.010013580322266, 40.010066986083984, 40.0101203918457, 40.01017379760742, 40.010223388671875, 40.01027297973633, 40.01032257080078, 40.0103759765625, 40.01042938232422, 40.01047897338867, 40.01053237915039, 40.01058578491211, 40.01063537597656, 40.010684967041016, 40.010738372802734, 40.01079177856445, 40.010841369628906, 40.010887145996094, 40.01093673706055, 40.010986328125, 40.01103210449219, 40.01108169555664, 40.011131286621094, 40.01118087768555, 40.01123046875, 40.01127624511719, 40.01132583618164, 40.011375427246094, 40.011417388916016, 40.0114631652832, 40.01150894165039, 40.011558532714844, 40.011600494384766, 40.01164627075195, 40.011688232421875, 40.01173782348633, 40.01177978515625, 40.01182556152344, 40.01186752319336, 40.01191329956055, 40.011959075927734, 40.01200485229492, 40.012046813964844, 40.012088775634766, 40.01213073730469, 40.01217269897461, 40.01221466064453, 40.01225662231445, 40.01230239868164, 40.01234817504883, 40.012393951416016, 40.01243209838867, 40.01247024536133, 40.01251220703125, 40.01255416870117, 40.012596130371094, 40.012638092041016, 40.01268005371094, 40.01272201538086, 40.012760162353516, 40.01280212402344, 40.012840270996094, 40.012882232666016, 40.01292419433594, 40.01296615600586, 40.01300811767578, 40.01304626464844, 40.013084411621094, 40.013126373291016, 40.01316452026367, 40.01320266723633, 40.01323699951172, 40.013275146484375, 40.013309478759766, 40.01335144042969, 40.01338577270508, 40.013427734375, 40.013465881347656, 40.01350021362305, 40.0135383605957, 40.01357650756836, 40.013614654541016, 40.013648986816406, 40.0136833190918, 40.01372146606445, 40.01375961303711, 40.0137939453125, 40.013832092285156, 40.01386642456055, 40.01390075683594, 40.01393508911133, 40.01396942138672, 40.014007568359375, 40.0140380859375, 40.01407241821289, 40.014102935791016, 40.014137268066406, 40.0141716003418, 40.01420593261719, 40.01423645019531, 40.0142707824707, 40.014305114746094, 40.014339447021484, 40.01437759399414, 40.014408111572266, 40.014442443847656, 40.01447296142578, 40.01450729370117, 40.0145378112793, 40.01456832885742, 40.01459884643555, 40.01462936401367, 40.01466751098633, 40.01469802856445, 40.01472854614258, 40.0147590637207, 40.01478958129883, 40.01482009887695, 40.01485061645508, 40.01488494873047, 40.01491165161133, 40.01493835449219, 40.01496887207031, 40.01499938964844, 40.01503372192383, 40.01506042480469, 40.01509094238281, 40.01512145996094, 40.01515197753906, 40.01517868041992, 40.01520919799805, 40.01523971557617, 40.01526641845703, 40.01529312133789, 40.01531982421875, 40.015350341796875, 40.015380859375, 40.015411376953125, 40.015438079833984, 40.015464782714844, 40.01549530029297, 40.01552200317383, 40.01554870605469, 40.01557540893555, 40.015602111816406, 40.015628814697266, 40.015655517578125, 40.015682220458984, 40.015708923339844, 40.0157356262207, 40.01576614379883, 40.01578903198242, 40.01581573486328, 40.015838623046875, 40.015865325927734, 40.015892028808594, 40.01591873168945, 40.01594161987305, 40.015968322753906, 40.015995025634766, 40.01601791381836, 40.01604080200195, 40.01606750488281, 40.01609420776367, 40.016117095947266, 40.016143798828125, 40.016170501708984, 40.01619338989258, 40.01621627807617, 40.01624298095703, 40.016265869140625, 40.01628875732422, 40.01631546020508, 40.01633834838867, 40.016361236572266, 40.01638412475586, 40.01641082763672, 40.01643371582031, 40.01646041870117, 40.0164794921875, 40.01649856567383, 40.01652145385742, 40.016544342041016, 40.016571044921875, 40.01659393310547, 40.01661682128906, 40.016639709472656, 40.016658782958984, 40.01668167114258, 40.016700744628906, 40.0167236328125, 40.01673889160156, 40.016761779785156, 40.016780853271484, 40.016807556152344, 40.01683044433594, 40.01685333251953, 40.016876220703125, 40.01689529418945, 40.01691436767578, 40.016937255859375, 40.016963958740234, 40.0169792175293, 40.016998291015625, 40.01701736450195, 40.01704406738281, 40.017066955566406, 40.01708221435547, 40.01710510253906, 40.01712417602539, 40.017147064208984, 40.01716613769531, 40.01718521118164, 40.017208099365234, 40.01722717285156, 40.01724624633789, 40.017269134521484, 40.01728820800781, 40.017311096191406, 40.017330169677734, 40.0173454284668, 40.017364501953125, 40.01738357543945, 40.01740264892578, 40.01742172241211, 40.01744079589844, 40.01746368408203, 40.01748275756836, 40.01750183105469, 40.01751708984375, 40.01753616333008, 40.017555236816406, 40.017574310302734, 40.01759338378906, 40.01761245727539, 40.01762771606445, 40.01764678955078, 40.017662048339844, 40.01768112182617, 40.0177001953125, 40.01771926879883, 40.01773452758789, 40.01775360107422, 40.01776885986328, 40.01778793334961, 40.01780319213867, 40.017818450927734, 40.01783752441406, 40.01785659790039, 40.01787185668945, 40.017887115478516, 40.017906188964844, 40.017921447753906, 40.01793670654297, 40.0179557800293, 40.01797103881836, 40.01799011230469, 40.01800537109375, 40.01802062988281, 40.01803970336914, 40.0180549621582, 40.018070220947266, 40.01808547973633, 40.01810073852539, 40.01811599731445, 40.018131256103516, 40.01814651489258, 40.01816177368164, 40.0181770324707, 40.018192291259766, 40.018211364746094, 40.018226623535156, 40.01824188232422, 40.01825714111328, 40.018272399902344, 40.01828384399414, 40.01829528808594, 40.018314361572266, 40.0183219909668, 40.01833724975586, 40.018348693847656, 40.01836395263672, 40.01837921142578, 40.01839065551758, 40.01840591430664, 40.01841735839844, 40.018436431884766, 40.01844787597656, 40.018463134765625, 40.01847457885742, 40.018489837646484, 40.01850128173828, 40.01852035522461, 40.018531799316406, 40.01854705810547, 40.01856231689453, 40.01857376098633, 40.018585205078125, 40.01860046386719, 40.01861572265625, 40.01863098144531, 40.01864242553711, 40.01865768432617, 40.0186653137207, 40.0186767578125, 40.0186882019043, 40.01870346069336, 40.018714904785156, 40.01872634887695, 40.018741607666016, 40.01875686645508, 40.018768310546875, 40.01878356933594, 40.018795013427734, 40.0188102722168, 40.01881790161133, 40.01883316040039, 40.01884460449219, 40.018856048583984, 40.01886749267578, 40.01887512207031, 40.018890380859375, 40.018898010253906, 40.01891326904297, 40.0189208984375, 40.0189323425293, 40.018943786621094, 40.01895523071289, 40.01897048950195, 40.01898193359375, 40.01899337768555, 40.019004821777344, 40.01901626586914, 40.01902770996094, 40.019039154052734, 40.01905059814453, 40.01906204223633, 40.019073486328125, 40.019081115722656, 40.01909255981445, 40.019100189208984, 40.01911163330078, 40.01912307739258, 40.019134521484375, 40.01914978027344, 40.019161224365234, 40.019168853759766, 40.01918411254883, 40.01919174194336, 40.01919937133789, 40.01921081542969, 40.01921844482422, 40.019229888916016, 40.01924133300781, 40.01925277709961, 40.019264221191406, 40.01927185058594, 40.019287109375, 40.01929473876953, 40.01930618286133, 40.01931381225586, 40.019325256347656, 40.01933670043945, 40.019344329833984, 40.019351959228516, 40.01936340332031, 40.019371032714844, 40.01938247680664, 40.01939010620117, 40.0193977355957, 40.0194091796875, 40.0194206237793, 40.01942825317383, 40.01943588256836, 40.01944351196289, 40.01945114135742, 40.019466400146484, 40.01947021484375, 40.019474029541016, 40.01948547363281, 40.019493103027344, 40.01950454711914, 40.01951217651367, 40.01952362060547, 40.019535064697266, 40.0195426940918, 40.019554138183594, 40.019561767578125, 40.01957321166992, 40.01958084106445, 40.019588470458984, 40.01959991455078, 40.01960754394531, 40.019615173339844, 40.01962661743164, 40.019630432128906, 40.01963806152344, 40.0196418762207, 40.019649505615234, 40.01966094970703, 40.01966857910156, 40.019676208496094, 40.019683837890625, 40.019691467285156, 40.01970291137695, 40.019710540771484, 40.01971435546875, 40.01972198486328, 40.01972961425781, 40.01974105834961, 40.019744873046875, 40.019752502441406, 40.0197639465332, 40.01976776123047, 40.019771575927734, 40.01978302001953, 40.01979064941406, 40.019798278808594, 40.01980972290039, 40.019813537597656, 40.01981735229492, 40.01982498168945, 40.01983642578125, 40.01984405517578, 40.01984786987305, 40.01985549926758, 40.01986312866211, 40.01987075805664, 40.019874572753906, 40.01987838745117, 40.01988983154297, 40.0198974609375, 40.019901275634766, 40.0199089050293, 40.01991653442383, 40.01992416381836, 40.019927978515625, 40.019935607910156, 40.01994323730469, 40.01995086669922, 40.01995849609375, 40.01996994018555, 40.01997756958008, 40.019981384277344, 40.01998519897461, 40.01999282836914, 40.02000045776367, 40.02000427246094, 40.02001190185547, 40.02001953125, 40.02002716064453, 40.02003479003906, 40.02003860473633, 40.020050048828125, 40.020050048828125, 40.020057678222656, 40.02006149291992, 40.02006912231445, 40.02008056640625, 40.020084381103516, 40.02009201049805, 40.02009963989258, 40.020103454589844, 40.02010726928711, 40.02011489868164, 40.02012252807617, 40.0201301574707, 40.02013397216797, 40.020137786865234, 40.020145416259766, 40.0201530456543, 40.02015686035156, 40.020164489746094, 40.02016830444336, 40.020172119140625, 40.020179748535156, 40.02018737792969, 40.02019119262695, 40.02019500732422, 40.020198822021484, 40.020206451416016, 40.02021408081055, 40.02021789550781, 40.02022171020508, 40.020225524902344, 40.020233154296875, 40.020240783691406, 40.02024459838867, 40.0202522277832, 40.020259857177734, 40.020263671875, 40.020267486572266, 40.02027130126953, 40.02027893066406, 40.020286560058594, 40.02029037475586, 40.020294189453125, 40.020301818847656, 40.020301818847656, 40.02030563354492, 40.02030944824219, 40.02031707763672, 40.02032470703125, 40.02032470703125, 40.020328521728516, 40.02033615112305, 40.02034378051758, 40.02035140991211, 40.020355224609375, 40.020362854003906, 40.02036666870117, 40.02037048339844, 40.0203742980957, 40.0203742980957, 40.02037811279297, 40.020381927490234, 40.02039337158203, 40.0203971862793, 40.02040100097656, 40.02040481567383, 40.020408630371094, 40.020416259765625, 40.020423889160156, 40.020423889160156, 40.02042770385742, 40.02043151855469, 40.02043533325195, 40.020442962646484, 40.02044677734375, 40.02045440673828, 40.02045822143555, 40.02046585083008, 40.020469665527344, 40.020477294921875, 40.02048110961914, 40.020484924316406, 40.02048873901367, 40.02049255371094, 40.0204963684082, 40.02050018310547, 40.020503997802734, 40.0205078125, 40.020511627197266, 40.0205192565918, 40.02052688598633, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.02053451538086, 40.02054214477539, 40.020545959472656, 40.02054977416992, 40.02055358886719, 40.02056121826172]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_test_RB_ITERATIVE\", \"type\": \"scatter\", \"y\": [35.862464904785156, 35.82638168334961, 35.90367126464844, 35.89115905761719, 35.953392028808594, 35.970924377441406, 35.98359680175781, 35.998775482177734, 36.017311096191406, 36.03718948364258, 36.05527877807617, 36.074241638183594, 36.09284973144531, 36.11125183105469, 36.129207611083984, 36.1472053527832, 36.165225982666016, 36.18312072753906, 36.200927734375, 36.218692779541016, 36.236358642578125, 36.2539176940918, 36.271400451660156, 36.288814544677734, 36.3061408996582, 36.323387145996094, 36.340553283691406, 36.35763931274414, 36.3746452331543, 36.39157485961914, 36.408424377441406, 36.42519760131836, 36.441890716552734, 36.4585075378418, 36.47504806518555, 36.491512298583984, 36.50790023803711, 36.52421188354492, 36.54044723510742, 36.55660629272461, 36.572689056396484, 36.58869552612305, 36.60462951660156, 36.620487213134766, 36.63627624511719, 36.6519889831543, 36.66762924194336, 36.683197021484375, 36.698692321777344, 36.714115142822266, 36.72946548461914, 36.744747161865234, 36.75996017456055, 36.77510070800781, 36.79016876220703, 36.80516815185547, 36.82009506225586, 36.834957122802734, 36.84975051879883, 36.86447525024414, 36.879127502441406, 36.893714904785156, 36.90822982788086, 36.92267990112305, 36.93706512451172, 36.951385498046875, 36.96563720703125, 36.979820251464844, 36.99394226074219, 37.00799560546875, 37.0219841003418, 37.03590774536133, 37.049766540527344, 37.063560485839844, 37.07728958129883, 37.0909538269043, 37.10456085205078, 37.118099212646484, 37.13157653808594, 37.14499282836914, 37.158348083496094, 37.17163848876953, 37.18486404418945, 37.19803237915039, 37.211143493652344, 37.22418975830078, 37.23717498779297, 37.250099182128906, 37.262962341308594, 37.2757682800293, 37.28851318359375, 37.30120086669922, 37.31382751464844, 37.32640075683594, 37.33890914916992, 37.35136032104492, 37.36375427246094, 37.376094818115234, 37.388370513916016, 37.40059280395508, 37.41276168823242, 37.42487335205078, 37.436927795410156, 37.44892501831055, 37.46086502075195, 37.47275161743164, 37.484580993652344, 37.49635696411133, 37.508079528808594, 37.519744873046875, 37.53135681152344, 37.54291915893555, 37.55442810058594, 37.565879821777344, 37.57727813720703, 37.588626861572266, 37.599918365478516, 37.61116027832031, 37.622344970703125, 37.633480072021484, 37.64456558227539, 37.65559768676758, 37.66658020019531, 37.67750930786133, 37.68838882446289, 37.699222564697266, 37.709999084472656, 37.72072219848633, 37.73140335083008, 37.74203109741211, 37.75261306762695, 37.763145446777344, 37.77363204956055, 37.78406524658203, 37.7944450378418, 37.80478286743164, 37.81507873535156, 37.825321197509766, 37.83551788330078, 37.845664978027344, 37.85576248168945, 37.865814208984375, 37.87582015991211, 37.88578414916992, 37.89569854736328, 37.90557098388672, 37.91539001464844, 37.925167083740234, 37.93489456176758, 37.944580078125, 37.9542236328125, 37.96382141113281, 37.9733772277832, 37.982887268066406, 37.992347717285156, 38.001766204833984, 38.011138916015625, 38.02047348022461, 38.029762268066406, 38.039005279541016, 38.04821014404297, 38.057376861572266, 38.06649398803711, 38.07556915283203, 38.084598541259766, 38.09358596801758, 38.102535247802734, 38.111446380615234, 38.12031173706055, 38.12914276123047, 38.1379280090332, 38.146671295166016, 38.155372619628906, 38.164031982421875, 38.17265701293945, 38.181243896484375, 38.18978500366211, 38.19828796386719, 38.20675277709961, 38.21517562866211, 38.22356414794922, 38.23190689086914, 38.240211486816406, 38.24848556518555, 38.256717681884766, 38.26490783691406, 38.27306365966797, 38.28118133544922, 38.28925704956055, 38.29729461669922, 38.305301666259766, 38.313270568847656, 38.321205139160156, 38.329097747802734, 38.336952209472656, 38.34476852416992, 38.3525505065918, 38.36030197143555, 38.368011474609375, 38.37568283081055, 38.383323669433594, 38.39093017578125, 38.39849853515625, 38.40603256225586, 38.41353225708008, 38.42099380493164, 38.42842483520508, 38.43581771850586, 38.443180084228516, 38.45050811767578, 38.45779800415039, 38.46506118774414, 38.472286224365234, 38.47947692871094, 38.48663330078125, 38.49375915527344, 38.500850677490234, 38.507911682128906, 38.51493453979492, 38.52192687988281, 38.52888870239258, 38.53581619262695, 38.5427131652832, 38.54957580566406, 38.5564079284668, 38.563209533691406, 38.56998062133789, 38.576717376708984, 38.58342361450195, 38.5900993347168, 38.59674072265625, 38.603355407714844, 38.60993576049805, 38.61648941040039, 38.62301254272461, 38.6295051574707, 38.63595962524414, 38.64238739013672, 38.6487922668457, 38.65517044067383, 38.6615104675293, 38.667823791503906, 38.67410659790039, 38.680362701416016, 38.68658447265625, 38.692779541015625, 38.69894790649414, 38.7050895690918, 38.71120071411133, 38.717281341552734, 38.72333526611328, 38.729366302490234, 38.7353630065918, 38.7413330078125, 38.747276306152344, 38.75318908691406, 38.75907897949219, 38.76493835449219, 38.77077102661133, 38.77657699584961, 38.78235626220703, 38.788108825683594, 38.79383087158203, 38.799530029296875, 38.80520248413086, 38.810848236083984, 38.81646728515625, 38.822059631347656, 38.8276252746582, 38.83316421508789, 38.83868408203125, 38.84417724609375, 38.849639892578125, 38.85507583618164, 38.86048889160156, 38.865882873535156, 38.87125015258789, 38.876590728759766, 38.88190460205078, 38.8871955871582, 38.892459869384766, 38.897701263427734, 38.90291976928711, 38.90811538696289, 38.91328430175781, 38.91843032836914, 38.92354965209961, 38.92864990234375, 38.933719635009766, 38.93876647949219, 38.94379425048828, 38.94879913330078, 38.95378112792969, 38.958740234375, 38.96367263793945, 38.96857833862305, 38.97346496582031, 38.97833251953125, 38.983177185058594, 38.987998962402344, 38.9927978515625, 38.9975700378418, 39.002323150634766, 39.00705337524414, 39.01176452636719, 39.016456604003906, 39.021121978759766, 39.02576446533203, 39.03038787841797, 39.03498840332031, 39.03956604003906, 39.04412078857422, 39.04865646362305, 39.05317306518555, 39.05767059326172, 39.06214904785156, 39.06660461425781, 39.07103729248047, 39.07544708251953, 39.079833984375, 39.084205627441406, 39.08855438232422, 39.0928840637207, 39.09719467163086, 39.10149002075195, 39.10575866699219, 39.110008239746094, 39.11423873901367, 39.11845016479492, 39.12263870239258, 39.12681198120117, 39.13096618652344, 39.13509750366211, 39.13921356201172, 39.143310546875, 39.14738464355469, 39.15143966674805, 39.155479431152344, 39.15950393676758, 39.16350173950195, 39.167484283447266, 39.171451568603516, 39.17539978027344, 39.17932891845703, 39.1832389831543, 39.1871337890625, 39.191009521484375, 39.194862365722656, 39.198699951171875, 39.202518463134766, 39.20631790161133, 39.210105895996094, 39.213871002197266, 39.217620849609375, 39.22135543823242, 39.22507095336914, 39.22876739501953, 39.232444763183594, 39.236106872558594, 39.23975372314453, 39.243385314941406, 39.24699783325195, 39.25059509277344, 39.254173278808594, 39.25773620605469, 39.26128005981445, 39.26481246948242, 39.26832580566406, 39.271820068359375, 39.27530288696289, 39.278770446777344, 39.28221893310547, 39.28564453125, 39.289058685302734, 39.29246139526367, 39.29584503173828, 39.29920959472656, 39.30256271362305, 39.3058967590332, 39.30921936035156, 39.31252670288086, 39.315818786621094, 39.319095611572266, 39.322357177734375, 39.325599670410156, 39.32883071899414, 39.33204650878906, 39.33524703979492, 39.33843231201172, 39.34160232543945, 39.34476089477539, 39.347900390625, 39.35102844238281, 39.35414505004883, 39.357242584228516, 39.360321044921875, 39.36338806152344, 39.3664436340332, 39.369483947753906, 39.37250900268555, 39.375526428222656, 39.3785285949707, 39.38151168823242, 39.38447952270508, 39.3874397277832, 39.390384674072266, 39.39331817626953, 39.39623260498047, 39.39913558959961, 39.40202331542969, 39.40489959716797, 39.40775680541992, 39.410606384277344, 39.4134407043457, 39.416263580322266, 39.41907501220703, 39.421871185302734, 39.424652099609375, 39.427425384521484, 39.43018341064453, 39.43292999267578, 39.435665130615234, 39.43838882446289, 39.44109344482422, 39.44378662109375, 39.44646453857422, 39.44913101196289, 39.451786041259766, 39.454429626464844, 39.457061767578125, 39.45968246459961, 39.4622917175293, 39.46488571166992, 39.46746826171875, 39.47003936767578, 39.47259521484375, 39.47514724731445, 39.477684020996094, 39.48020553588867, 39.48271942138672, 39.48522186279297, 39.487709045410156, 39.49019241333008, 39.49265670776367, 39.49510955810547, 39.497554779052734, 39.4999885559082, 39.502410888671875, 39.50482177734375, 39.50721740722656, 39.509605407714844, 39.51198196411133, 39.51434326171875, 39.51669692993164, 39.51904296875, 39.52137756347656, 39.52369689941406, 39.52600860595703, 39.5283088684082, 39.53059387207031, 39.53287124633789, 39.53514099121094, 39.53740310668945, 39.539649963378906, 39.54188919067383, 39.54411697387695, 39.546329498291016, 39.54853057861328, 39.55072784423828, 39.552913665771484, 39.555091857910156, 39.557254791259766, 39.559410095214844, 39.56155776977539, 39.563690185546875, 39.56581115722656, 39.567928314208984, 39.57003402709961, 39.57212829589844, 39.574214935302734, 39.576290130615234, 39.5783576965332, 39.580413818359375, 39.58246612548828, 39.584503173828125, 39.58652877807617, 39.58855056762695, 39.5905647277832, 39.592567443847656, 39.59455490112305, 39.596534729003906, 39.598506927490234, 39.60047149658203, 39.6024284362793, 39.604373931884766, 39.6063117980957, 39.60824203491211, 39.61016082763672, 39.61206817626953, 39.61396789550781, 39.61586380004883, 39.61774826049805, 39.61962127685547, 39.62148666381836, 39.62334442138672, 39.62519454956055, 39.62703323364258, 39.62886428833008, 39.63068771362305, 39.63249969482422, 39.63430404663086, 39.63610076904297, 39.63788604736328, 39.63966751098633, 39.64144515991211, 39.643211364746094, 39.64496612548828, 39.64671325683594, 39.64845275878906, 39.650184631347656, 39.65190887451172, 39.65362548828125, 39.65533447265625, 39.65703582763672, 39.65872573852539, 39.66040802001953, 39.662086486816406, 39.663753509521484, 39.66541290283203, 39.66706848144531, 39.66871643066406, 39.670352935791016, 39.6719856262207, 39.67361068725586, 39.67522430419922, 39.67683029174805, 39.678428649902344, 39.680023193359375, 39.68160629272461, 39.68318557739258, 39.68476104736328, 39.68632507324219, 39.6878776550293, 39.68942642211914, 39.69096755981445, 39.6925048828125, 39.69403076171875, 39.695552825927734, 39.69706726074219, 39.698577880859375, 39.700077056884766, 39.70156478881836, 39.70304870605469, 39.704524993896484, 39.705997467041016, 39.707462310791016, 39.70892333984375, 39.71037673950195, 39.71181869506836, 39.713253021240234, 39.71468734741211, 39.71611022949219, 39.717525482177734, 39.718936920166016, 39.720340728759766, 39.72174072265625, 39.7231330871582, 39.72452163696289, 39.72589874267578, 39.72726821899414, 39.7286376953125, 39.72999572753906, 39.73134994506836, 39.73270034790039, 39.73404312133789, 39.73537826538086, 39.7367057800293, 39.73802947998047, 39.73934555053711, 39.74065399169922, 39.74195861816406, 39.743255615234375, 39.74454879760742, 39.74583435058594, 39.74711227416992, 39.748390197753906, 39.749656677246094, 39.750919342041016, 39.752174377441406, 39.75342559814453, 39.754669189453125, 39.75590515136719, 39.757137298583984, 39.758365631103516, 39.75959014892578, 39.760807037353516, 39.762020111083984, 39.76322555541992, 39.76442337036133, 39.76561737060547, 39.76680374145508, 39.767982482910156, 39.769161224365234, 39.77033233642578, 39.77149963378906, 39.772666931152344, 39.77382278442383, 39.77497100830078, 39.77611541748047, 39.77725601196289, 39.77838897705078, 39.779510498046875, 39.780635833740234, 39.78175735473633, 39.78287124633789, 39.78397750854492, 39.78507995605469, 39.78617858886719, 39.78726577758789, 39.788352966308594, 39.78943634033203, 39.79050827026367, 39.79158020019531, 39.79264831542969, 39.7937126159668, 39.794769287109375, 39.79582214355469, 39.796871185302734, 39.79791259765625, 39.798946380615234, 39.799983978271484, 39.80101776123047, 39.80203628540039, 39.80305480957031, 39.804073333740234, 39.805084228515625, 39.80608367919922, 39.80708694458008, 39.80808639526367, 39.80907440185547, 39.810062408447266, 39.8110466003418, 39.8120231628418, 39.812992095947266, 39.813961029052734, 39.81492233276367, 39.81588363647461, 39.816837310791016, 39.817787170410156, 39.81873321533203, 39.819671630859375, 39.82060623168945, 39.82154083251953, 39.822471618652344, 39.82339859008789, 39.82432174682617, 39.825233459472656, 39.82614517211914, 39.82705307006836, 39.82795333862305, 39.82884979248047, 39.829742431640625, 39.83063507080078, 39.831520080566406, 39.832401275634766, 39.83327865600586, 39.83415222167969, 39.83502197265625, 39.83588790893555, 39.83675003051758, 39.837608337402344, 39.838462829589844, 39.83931350708008, 39.84016036987305, 39.84100341796875, 39.84183883666992, 39.84267044067383, 39.84349822998047, 39.844322204589844, 39.84514236450195, 39.8459587097168, 39.846771240234375, 39.84758377075195, 39.848392486572266, 39.84919738769531, 39.849998474121094, 39.85079574584961, 39.85158920288086, 39.852378845214844, 39.8531608581543, 39.853939056396484, 39.85471725463867, 39.85549545288086, 39.85626220703125, 39.85702896118164, 39.85779571533203, 39.85855484008789, 39.85931396484375, 39.86006546020508, 39.860816955566406, 39.86156463623047, 39.8623046875, 39.86304473876953, 39.8637809753418, 39.86451721191406, 39.8652458190918, 39.865970611572266, 39.866695404052734, 39.86741256713867, 39.868125915527344, 39.868839263916016, 39.86954879760742, 39.87025451660156, 39.87095642089844, 39.87165832519531, 39.87235641479492, 39.873050689697266, 39.873741149902344, 39.874427795410156, 39.8751106262207, 39.87579345703125, 39.876468658447266, 39.87714385986328, 39.87781524658203, 39.87847900390625, 39.87914276123047, 39.87980270385742, 39.88045883178711, 39.8811149597168, 39.88176727294922, 39.882415771484375, 39.883056640625, 39.88370132446289, 39.88433837890625, 39.88497543334961, 39.8856086730957, 39.88623809814453, 39.886863708496094, 39.88748550415039, 39.88810729980469, 39.88872528076172, 39.889347076416016, 39.88996124267578, 39.89057159423828, 39.891178131103516, 39.89178466796875, 39.89238739013672, 39.89298629760742, 39.89358139038086, 39.8941764831543, 39.8947639465332, 39.895355224609375, 39.89594268798828, 39.89652633666992, 39.8971061706543, 39.897682189941406, 39.898258209228516, 39.898834228515625, 39.8994026184082, 39.89997100830078, 39.900535583496094, 39.90109634399414, 39.90165710449219, 39.90221405029297, 39.902767181396484, 39.903316497802734, 39.90386199951172, 39.90441131591797, 39.90495681762695, 39.905494689941406, 39.906036376953125, 39.90657424926758, 39.90711212158203, 39.90764236450195, 39.90816879272461, 39.90869903564453, 39.909217834472656, 39.90974044799805, 39.91026306152344, 39.91078186035156, 39.91129684448242, 39.911808013916016, 39.912315368652344, 39.912818908691406, 39.913326263427734, 39.9138298034668, 39.914329528808594, 39.91482925415039, 39.91532516479492, 39.91581344604492, 39.91630554199219, 39.91679382324219, 39.91727828979492, 39.917762756347656, 39.91824722290039, 39.91872787475586, 39.91920852661133, 39.919681549072266, 39.92015075683594, 39.920623779296875, 39.92109298706055, 39.92155838012695, 39.92202377319336, 39.9224853515625, 39.92294692993164, 39.92340087890625, 39.923858642578125, 39.924312591552734, 39.92476272583008, 39.92521286010742, 39.9256591796875, 39.92610168457031, 39.926544189453125, 39.92698669433594, 39.927425384521484, 39.92786407470703, 39.92829895019531, 39.92873001098633, 39.929161071777344, 39.929588317871094, 39.930015563964844, 39.93043899536133, 39.93086242675781, 39.93128204345703, 39.931697845458984, 39.9321174621582, 39.932533264160156, 39.932945251464844, 39.93335723876953, 39.93376541137695, 39.934173583984375, 39.934574127197266, 39.93497848510742, 39.93537902832031, 39.93578338623047, 39.936180114746094, 39.93657302856445, 39.93696594238281, 39.93736267089844, 39.9377555847168, 39.938140869140625, 39.93852615356445, 39.93891143798828, 39.939292907714844, 39.93967819213867, 39.94005584716797, 39.9404296875, 39.9408073425293, 39.94118118286133, 39.94155502319336, 39.94192886352539, 39.94230270385742, 39.94266891479492, 39.943031311035156, 39.943397521972656, 39.943756103515625, 39.94411849975586, 39.94447708129883, 39.9448356628418, 39.9451904296875, 39.9455451965332, 39.945899963378906, 39.946250915527344, 39.946598052978516, 39.94694519042969, 39.94729232788086, 39.94763946533203, 39.94798278808594, 39.94832229614258, 39.94866180419922, 39.948997497558594, 39.94933319091797, 39.94967269897461, 39.95000457763672, 39.95033645629883, 39.9506721496582, 39.95100021362305, 39.951324462890625, 39.95165252685547, 39.95197677612305, 39.95229721069336, 39.95261764526367, 39.95293426513672, 39.95325469970703, 39.95357131958008, 39.95388412475586, 39.95419692993164, 39.95450973510742, 39.9548225402832, 39.95513153076172, 39.955440521240234, 39.95574951171875, 39.956058502197266, 39.95635986328125, 39.956661224365234, 39.95696258544922, 39.9572639465332, 39.95756149291992, 39.957855224609375, 39.958152770996094, 39.95844268798828, 39.958736419677734, 39.95903015136719, 39.95932388305664, 39.95961380004883, 39.95989990234375, 39.960182189941406, 39.96046829223633, 39.960750579833984, 39.961036682128906, 39.9613151550293, 39.96159362792969, 39.96187210083008, 39.96215057373047, 39.962425231933594, 39.96269989013672, 39.962974548339844, 39.9632453918457, 39.96351623535156, 39.96378707885742, 39.964054107666016, 39.964317321777344, 39.96458435058594, 39.964847564697266, 39.965110778808594, 39.965370178222656, 39.96562957763672, 39.96588897705078, 39.96615219116211, 39.96641159057617, 39.9666633605957, 39.9669189453125, 39.96717071533203, 39.9674186706543, 39.96767044067383, 39.967918395996094, 39.96816635131836, 39.968414306640625, 39.96866226196289, 39.96890640258789, 39.969154357910156, 39.969398498535156, 39.96963882446289, 39.969879150390625, 39.970115661621094, 39.97035217285156, 39.9705924987793, 39.970829010009766, 39.971065521240234, 39.9713020324707, 39.971534729003906, 39.97176742553711, 39.97199630737305, 39.972225189208984, 39.97245788574219, 39.97268295288086, 39.97290802001953, 39.97312927246094, 39.97335433959961, 39.973575592041016, 39.97379684448242, 39.97401809692383, 39.9742431640625, 39.97446060180664, 39.97467803955078, 39.97489547729492, 39.9751091003418, 39.97532653808594, 39.97554397583008, 39.97575759887695, 39.97596740722656, 39.97617721557617, 39.97638702392578, 39.97659683227539, 39.976802825927734, 39.977012634277344, 39.97721862792969, 39.977420806884766, 39.97762680053711, 39.97783279418945, 39.97803497314453, 39.97823715209961, 39.97843933105469, 39.9786376953125, 39.97883605957031, 39.97903823852539, 39.9792366027832, 39.979427337646484, 39.9796257019043, 39.979820251464844, 39.98001480102539, 39.98020553588867, 39.98040008544922, 39.980587005615234, 39.98077392578125, 39.98096466064453, 39.98115539550781, 39.981346130371094, 39.98153305053711, 39.98171615600586, 39.98189926147461, 39.98208236694336, 39.98226547241211, 39.98244857788086, 39.98263168334961, 39.982810974121094, 39.98299026489258, 39.9831657409668, 39.98334503173828, 39.9835205078125, 39.983699798583984, 39.98387908935547, 39.98405456542969, 39.984222412109375, 39.984397888183594, 39.98456573486328, 39.98473358154297, 39.98490524291992, 39.985076904296875, 39.98524856567383, 39.985416412353516, 39.9855842590332, 39.985748291015625, 39.98591232299805, 39.98607635498047, 39.98624038696289, 39.98640823364258, 39.986572265625, 39.98673629760742, 39.98689270019531, 39.987056732177734, 39.987213134765625, 39.987369537353516, 39.98753356933594, 39.98768615722656, 39.98783874511719, 39.98799514770508, 39.9881477355957, 39.98830032348633, 39.98845672607422, 39.988609313964844, 39.98876190185547, 39.98891067504883, 39.98906707763672, 39.98921585083008, 39.98936462402344, 39.9895133972168, 39.98965835571289, 39.98980712890625, 39.98995590209961, 39.99010467529297, 39.99024963378906, 39.990394592285156, 39.99053955078125, 39.990684509277344, 39.9908332824707, 39.99097442626953, 39.99111557006836, 39.99125671386719, 39.991397857666016, 39.99153518676758, 39.991676330566406, 39.99181365966797, 39.991947174072266, 39.99208450317383, 39.992218017578125, 39.99235534667969, 39.99249267578125, 39.99262619018555, 39.99276351928711, 39.992897033691406, 39.9930305480957, 39.9931640625, 39.9932975769043, 39.99342727661133, 39.99355697631836, 39.993690490722656, 39.99381637573242, 39.99394226074219, 39.994075775146484, 39.994197845458984, 39.99432373046875, 39.99445343017578, 39.99457931518555, 39.99470520019531, 39.99482727050781, 39.99494934082031, 39.99507141113281, 39.99519348144531, 39.99531936645508, 39.99543762207031, 39.99555969238281, 39.99567794799805, 39.99579620361328, 39.99591827392578, 39.99603271484375, 39.99615478515625, 39.99626922607422, 39.99638748168945, 39.99650192260742, 39.99661636352539, 39.996734619140625, 39.996849060058594, 39.9969596862793, 39.9970703125, 39.99718475341797, 39.99729919433594, 39.99740982055664, 39.99752426147461, 39.99763488769531, 39.997745513916016, 39.99785614013672, 39.99796676635742, 39.998077392578125, 39.99818801879883, 39.99829864501953, 39.99840545654297, 39.998512268066406, 39.99861526489258, 39.998722076416016, 39.99882888793945, 39.998931884765625, 39.99903869628906, 39.999141693115234, 39.99924850463867, 39.999351501464844, 39.99945831298828, 39.99955749511719, 39.99966049194336, 39.999759674072266, 39.99986267089844, 39.999961853027344, 40.00006103515625, 40.00016403198242, 40.00025939941406, 40.00035858154297, 40.000457763671875, 40.000553131103516, 40.000648498535156, 40.0007438659668, 40.00083923339844, 40.00093460083008, 40.001033782958984, 40.00112533569336, 40.001220703125, 40.001312255859375, 40.001407623291016, 40.00149917602539, 40.001590728759766, 40.00168228149414, 40.00177764892578, 40.00186538696289, 40.001956939697266, 40.002044677734375, 40.002132415771484, 40.00222396850586, 40.00231170654297, 40.00239944458008, 40.00248718261719, 40.0025749206543, 40.002662658691406, 40.002750396728516, 40.00283432006836, 40.00292205810547, 40.00300598144531, 40.00309371948242, 40.00318145751953, 40.00326919555664, 40.00335693359375, 40.003440856933594, 40.00352096557617, 40.003604888916016, 40.00368881225586, 40.00376892089844, 40.003849029541016, 40.003936767578125, 40.0040168762207, 40.00409698486328, 40.00417709350586, 40.00425720214844, 40.004337310791016, 40.004417419433594, 40.00449752807617, 40.00457763671875, 40.00465393066406, 40.00473403930664, 40.00481033325195, 40.004886627197266, 40.004966735839844, 40.00504684448242, 40.005123138427734, 40.00519943237305, 40.00527572631836, 40.005348205566406, 40.00542068481445, 40.005496978759766, 40.00557327270508, 40.005645751953125, 40.005714416503906, 40.00579071044922, 40.005863189697266, 40.00593948364258, 40.006011962890625, 40.006080627441406, 40.00615310668945, 40.006229400634766, 40.00629425048828, 40.00636672973633, 40.006431579589844, 40.006500244140625, 40.00657272338867, 40.00664138793945, 40.006710052490234, 40.006778717041016, 40.00684356689453, 40.00691223144531, 40.00697708129883, 40.00704574584961, 40.00711441040039, 40.00718307495117, 40.00724792480469, 40.00731658935547, 40.007381439208984, 40.007450103759766, 40.007511138916016, 40.00757598876953, 40.00764083862305, 40.00770950317383, 40.00777053833008, 40.007835388183594, 40.00790023803711, 40.00796127319336, 40.008026123046875, 40.00808334350586, 40.00814437866211, 40.00820541381836, 40.008270263671875, 40.00832748413086, 40.008392333984375, 40.00844955444336, 40.00851058959961, 40.008567810058594, 40.00863265991211, 40.008689880371094, 40.008750915527344, 40.00880432128906, 40.00886154174805, 40.0089225769043, 40.00897979736328, 40.009033203125, 40.009090423583984, 40.00914764404297, 40.00920867919922, 40.0092658996582, 40.00932312011719, 40.00938034057617, 40.009437561035156, 40.00949478149414, 40.009552001953125, 40.009605407714844, 40.00966262817383, 40.00971984863281, 40.00977325439453, 40.00982666015625, 40.00988006591797, 40.00992965698242, 40.00998306274414, 40.01003646850586, 40.01008987426758, 40.0101432800293, 40.010196685791016, 40.01025390625, 40.01030349731445, 40.010353088378906, 40.010406494140625, 40.01045608520508, 40.0105094909668, 40.010562896728516, 40.01061248779297, 40.01066207885742, 40.010711669921875, 40.01076126098633, 40.01081085205078, 40.010860443115234, 40.01091384887695, 40.010963439941406, 40.01101303100586, 40.01105880737305, 40.011104583740234, 40.01115036010742, 40.011199951171875, 40.01124572753906, 40.011295318603516, 40.01134490966797, 40.01139450073242, 40.011444091796875, 40.01148986816406, 40.01153564453125, 40.01158142089844, 40.01162338256836, 40.01166915893555, 40.011714935302734, 40.01176071166992, 40.01180648803711, 40.0118522644043, 40.011898040771484, 40.011940002441406, 40.011985778808594, 40.01203155517578, 40.0120735168457, 40.012115478515625, 40.01216125488281, 40.012203216552734, 40.012245178222656, 40.012290954589844, 40.012332916259766, 40.01237487792969, 40.012413024902344, 40.012454986572266, 40.01250076293945, 40.012542724609375, 40.0125846862793, 40.01262664794922, 40.01266860961914, 40.0127067565918, 40.012752532958984, 40.012786865234375, 40.01282501220703, 40.01285934448242, 40.01290512084961, 40.012939453125, 40.01298141479492, 40.01301574707031, 40.013057708740234, 40.01309585571289, 40.01313400268555, 40.01317596435547, 40.01321029663086, 40.013248443603516, 40.013282775878906, 40.01332473754883, 40.01335906982422, 40.01340103149414, 40.0134391784668, 40.01347351074219, 40.013511657714844, 40.013545989990234, 40.013587951660156, 40.01362609863281, 40.0136604309082, 40.01369857788086, 40.01373291015625, 40.013771057128906, 40.0138053894043, 40.01383972167969, 40.013877868652344, 40.013912200927734, 40.013946533203125, 40.01398468017578, 40.01401901245117, 40.01405334472656, 40.01408767700195, 40.014122009277344, 40.014156341552734, 40.014190673828125, 40.014225006103516, 40.014259338378906, 40.01428985595703, 40.01432418823242, 40.01435852050781, 40.01438903808594, 40.01442337036133, 40.01445770263672, 40.014488220214844, 40.014522552490234, 40.01455307006836, 40.01458740234375, 40.014617919921875, 40.0146484375, 40.01468276977539, 40.014713287353516, 40.01474380493164, 40.01477813720703, 40.01480484008789, 40.01483917236328, 40.01486587524414, 40.014892578125, 40.014923095703125, 40.01495361328125, 40.014984130859375, 40.0150146484375, 40.015045166015625, 40.015071868896484, 40.01510238647461, 40.015132904052734, 40.01516342163086, 40.015193939208984, 40.01522445678711, 40.015254974365234, 40.01528549194336, 40.01531219482422, 40.015342712402344, 40.0153694152832, 40.01539993286133, 40.01542663574219, 40.01545715332031, 40.01548385620117, 40.0155143737793, 40.01554489135742, 40.01557159423828, 40.01559829711914, 40.015621185302734, 40.01565170288086, 40.01567840576172, 40.01570510864258, 40.01572799682617, 40.01575469970703, 40.015785217285156, 40.01580810546875, 40.01583480834961, 40.01586151123047, 40.01588821411133, 40.01591491699219, 40.01594161987305, 40.015968322753906, 40.015995025634766, 40.01601791381836, 40.01604461669922, 40.01606750488281, 40.01609420776367, 40.016117095947266, 40.016143798828125, 40.016170501708984, 40.01619338989258, 40.01621627807617, 40.01624298095703, 40.016265869140625, 40.01628875732422, 40.01631546020508, 40.01633834838867, 40.016361236572266, 40.01638412475586, 40.01641082763672, 40.01643371582031, 40.01646041870117, 40.0164794921875, 40.01649856567383, 40.01652145385742, 40.016544342041016, 40.016571044921875, 40.01659393310547, 40.01661682128906, 40.016639709472656, 40.016658782958984, 40.01668167114258, 40.016700744628906, 40.0167236328125, 40.01673889160156, 40.016761779785156, 40.016780853271484, 40.016807556152344, 40.01683044433594, 40.01685333251953, 40.016876220703125, 40.01689529418945, 40.01691436767578, 40.016937255859375, 40.016963958740234, 40.0169792175293, 40.016998291015625, 40.01701736450195, 40.01704406738281, 40.017066955566406, 40.01708221435547, 40.01710510253906, 40.01712417602539, 40.017147064208984, 40.01716613769531, 40.01718521118164, 40.017208099365234, 40.01722717285156, 40.01724624633789, 40.017269134521484, 40.01728820800781, 40.017311096191406, 40.017330169677734, 40.0173454284668, 40.017364501953125, 40.01738357543945, 40.01740264892578, 40.01742172241211, 40.01744079589844, 40.01746368408203, 40.01748275756836, 40.01750183105469, 40.01751708984375, 40.01753616333008, 40.017555236816406, 40.017574310302734, 40.01759338378906, 40.01761245727539, 40.01762771606445, 40.01764678955078, 40.017662048339844, 40.01768112182617, 40.0177001953125, 40.01771926879883, 40.01773452758789, 40.01775360107422, 40.01776885986328, 40.01778793334961, 40.01780319213867, 40.017818450927734, 40.01783752441406, 40.01785659790039, 40.01787185668945, 40.017887115478516, 40.017906188964844, 40.017921447753906, 40.01793670654297, 40.0179557800293, 40.01797103881836, 40.01799011230469, 40.01800537109375, 40.01802062988281, 40.01803970336914, 40.0180549621582, 40.018070220947266, 40.01808547973633, 40.01810073852539, 40.01811599731445, 40.018131256103516, 40.01814651489258, 40.01816177368164, 40.0181770324707, 40.018192291259766, 40.018211364746094, 40.018226623535156, 40.01824188232422, 40.01825714111328, 40.018272399902344, 40.01828384399414, 40.01829528808594, 40.018314361572266, 40.0183219909668, 40.01833724975586, 40.018348693847656, 40.01836395263672, 40.01837921142578, 40.01839065551758, 40.01840591430664, 40.01841735839844, 40.018436431884766, 40.01844787597656, 40.018463134765625, 40.01847457885742, 40.018489837646484, 40.01850128173828, 40.01852035522461, 40.018531799316406, 40.01854705810547, 40.01856231689453, 40.01857376098633, 40.018585205078125, 40.01860046386719, 40.01861572265625, 40.01863098144531, 40.01864242553711, 40.01865768432617, 40.0186653137207, 40.0186767578125, 40.0186882019043, 40.01870346069336, 40.018714904785156, 40.01872634887695, 40.018741607666016, 40.01875686645508, 40.018768310546875, 40.01878356933594, 40.018795013427734, 40.0188102722168, 40.01881790161133, 40.01883316040039, 40.01884460449219, 40.018856048583984, 40.01886749267578, 40.01887512207031, 40.018890380859375, 40.018898010253906, 40.01891326904297, 40.0189208984375, 40.0189323425293, 40.018943786621094, 40.01895523071289, 40.01897048950195, 40.01898193359375, 40.01899337768555, 40.019004821777344, 40.01901626586914, 40.01902770996094, 40.019039154052734, 40.01905059814453, 40.01906204223633, 40.019073486328125, 40.019081115722656, 40.01909255981445, 40.019100189208984, 40.01911163330078, 40.01912307739258, 40.019134521484375, 40.01914978027344, 40.019161224365234, 40.019168853759766, 40.01918411254883, 40.01919174194336, 40.01919937133789, 40.01921081542969, 40.01921844482422, 40.019229888916016, 40.01924133300781, 40.01925277709961, 40.019264221191406, 40.01927185058594, 40.019287109375, 40.01929473876953, 40.01930618286133, 40.01931381225586, 40.019325256347656, 40.01933670043945, 40.019344329833984, 40.019351959228516, 40.01936340332031, 40.019371032714844, 40.01938247680664, 40.01939010620117, 40.0193977355957, 40.0194091796875, 40.0194206237793, 40.01942825317383, 40.01943588256836, 40.01944351196289, 40.01945114135742, 40.019466400146484, 40.01947021484375, 40.019474029541016, 40.01948547363281, 40.019493103027344, 40.01950454711914, 40.01951217651367, 40.01952362060547, 40.019535064697266, 40.0195426940918, 40.019554138183594, 40.019561767578125, 40.01957321166992, 40.01958084106445, 40.019588470458984, 40.01959991455078, 40.01960754394531, 40.019615173339844, 40.01962661743164, 40.019630432128906, 40.01963806152344, 40.0196418762207, 40.019649505615234, 40.01966094970703, 40.01966857910156, 40.019676208496094, 40.019683837890625, 40.019691467285156, 40.01970291137695, 40.019710540771484, 40.01971435546875, 40.01972198486328, 40.01972961425781, 40.01974105834961, 40.019744873046875, 40.019752502441406, 40.0197639465332, 40.01976776123047, 40.019771575927734, 40.01978302001953, 40.01979064941406, 40.019798278808594, 40.01980972290039, 40.019813537597656, 40.01981735229492, 40.01982498168945, 40.01983642578125, 40.01984405517578, 40.01984786987305, 40.01985549926758, 40.01986312866211, 40.01987075805664, 40.019874572753906, 40.01987838745117, 40.01988983154297, 40.0198974609375, 40.019901275634766, 40.0199089050293, 40.01991653442383, 40.01992416381836, 40.019927978515625, 40.019935607910156, 40.01994323730469, 40.01995086669922, 40.01995849609375, 40.01996994018555, 40.01997756958008, 40.019981384277344, 40.01998519897461, 40.01999282836914, 40.02000045776367, 40.02000427246094, 40.02001190185547, 40.02001953125, 40.02002716064453, 40.02003479003906, 40.02003860473633, 40.020050048828125, 40.020050048828125, 40.020057678222656, 40.02006149291992, 40.02006912231445, 40.02008056640625, 40.020084381103516, 40.02009201049805, 40.02009963989258, 40.020103454589844, 40.02010726928711, 40.02011489868164, 40.02012252807617, 40.0201301574707, 40.02013397216797, 40.020137786865234, 40.020145416259766, 40.0201530456543, 40.02015686035156, 40.020164489746094, 40.02016830444336, 40.020172119140625, 40.020179748535156, 40.02018737792969, 40.02019119262695, 40.02019500732422, 40.020198822021484, 40.020206451416016, 40.02021408081055, 40.02021789550781, 40.02022171020508, 40.020225524902344, 40.020233154296875, 40.020240783691406, 40.02024459838867, 40.0202522277832, 40.020259857177734, 40.020263671875, 40.020267486572266, 40.02027130126953, 40.02027893066406, 40.020286560058594, 40.02029037475586, 40.020294189453125, 40.020301818847656, 40.020301818847656, 40.02030563354492, 40.02030944824219, 40.02031707763672, 40.02032470703125, 40.02032470703125, 40.020328521728516, 40.02033615112305, 40.02034378051758, 40.02035140991211, 40.020355224609375, 40.020362854003906, 40.02036666870117, 40.02037048339844, 40.0203742980957, 40.0203742980957, 40.02037811279297, 40.020381927490234, 40.02039337158203, 40.0203971862793, 40.02040100097656, 40.02040481567383, 40.020408630371094, 40.020416259765625, 40.020423889160156, 40.020423889160156, 40.02042770385742, 40.02043151855469, 40.02043533325195, 40.020442962646484, 40.02044677734375, 40.02045440673828, 40.02045822143555, 40.02046585083008, 40.020469665527344, 40.020477294921875, 40.02048110961914, 40.020484924316406, 40.02048873901367, 40.02049255371094, 40.0204963684082, 40.02050018310547, 40.020503997802734, 40.0205078125, 40.020511627197266, 40.0205192565918, 40.02052688598633, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.02053451538086, 40.02054214477539, 40.020545959472656, 40.02054977416992, 40.02055358886719, 40.02056121826172, 40.020565032958984, 40.02056884765625, 40.02057647705078, 40.02057647705078, 40.02058029174805, 40.02058410644531, 40.020591735839844, 40.02059555053711, 40.020599365234375, 40.02060317993164, 40.020606994628906, 40.02061080932617, 40.02061462402344, 40.02061462402344, 40.02061462402344, 40.02062225341797, 40.020626068115234, 40.0206298828125, 40.020633697509766, 40.02063751220703, 40.0206413269043, 40.0206413269043, 40.0206413269043, 40.02064895629883, 40.020652770996094, 40.02065658569336, 40.020660400390625, 40.02066421508789, 40.020668029785156, 40.02067565917969, 40.02067947387695, 40.02068328857422, 40.020687103271484, 40.02069091796875, 40.020694732666016, 40.02069854736328, 40.02070236206055, 40.02070999145508, 40.020713806152344, 40.02071762084961, 40.020721435546875, 40.02072525024414, 40.020729064941406, 40.02073287963867, 40.02073669433594, 40.0207405090332, 40.02074432373047, 40.020748138427734, 40.020748138427734, 40.020748138427734, 40.020748138427734, 40.020755767822266, 40.020755767822266, 40.02075958251953, 40.02076721191406, 40.02077102661133, 40.02077102661133, 40.020774841308594, 40.02077865600586, 40.020782470703125]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_test_RM_ITERATIVE\", \"type\": \"scatter\", \"y\": [35.2602653503418, 35.21031188964844, 35.295196533203125, 35.2853889465332, 35.36286544799805, 35.388916015625, 35.40218734741211, 35.41742706298828, 35.43794250488281, 35.45974349975586, 35.48041534423828, 35.50265884399414, 35.5240364074707, 35.545257568359375, 35.56581115722656, 35.58632278442383, 35.60691833496094, 35.627410888671875, 35.64779281616211, 35.66813659667969, 35.68837356567383, 35.70847702026367, 35.728485107421875, 35.7484130859375, 35.768245697021484, 35.787986755371094, 35.80764389038086, 35.82720947265625, 35.8466796875, 35.866058349609375, 35.88534927368164, 35.90454864501953, 35.92365646362305, 35.94267654418945, 35.961612701416016, 35.9804573059082, 35.99921417236328, 36.017887115478516, 36.03647232055664, 36.054969787597656, 36.07338333129883, 36.09170913696289, 36.109947204589844, 36.12810134887695, 36.14617156982422, 36.16416549682617, 36.18207550048828, 36.19989776611328, 36.21763610839844, 36.23529052734375, 36.252864837646484, 36.27035903930664, 36.28777313232422, 36.30510711669922, 36.32236099243164, 36.33953094482422, 36.356624603271484, 36.373634338378906, 36.390567779541016, 36.40742111206055, 36.424198150634766, 36.440895080566406, 36.457515716552734, 36.474063873291016, 36.49053192138672, 36.50692367553711, 36.52323532104492, 36.53947830200195, 36.555641174316406, 36.57172775268555, 36.58774185180664, 36.60368347167969, 36.61955261230469, 36.63534164428711, 36.651058197021484, 36.66670227050781, 36.68227767944336, 36.69778060913086, 36.71320724487305, 36.72856140136719, 36.74384689331055, 36.75905990600586, 36.77420425415039, 36.78927993774414, 36.804283142089844, 36.819217681884766, 36.834083557128906, 36.848876953125, 36.86360168457031, 36.87826156616211, 36.892852783203125, 36.90737533569336, 36.92183303833008, 36.936222076416016, 36.95054244995117, 36.96479797363281, 36.97898483276367, 36.993106842041016, 37.007164001464844, 37.02116012573242, 37.03508758544922, 37.0489501953125, 37.06275177001953, 37.07648849487305, 37.09015655517578, 37.103763580322266, 37.1173095703125, 37.13079071044922, 37.14421081542969, 37.15756607055664, 37.17086410522461, 37.18409729003906, 37.197269439697266, 37.21037673950195, 37.22342300415039, 37.23641586303711, 37.24934387207031, 37.262210845947266, 37.275020599365234, 37.28776931762695, 37.30046081542969, 37.313087463378906, 37.325660705566406, 37.33817672729492, 37.35063552856445, 37.363033294677734, 37.37537384033203, 37.387657165527344, 37.399879455566406, 37.41204833984375, 37.424163818359375, 37.436222076416016, 37.448219299316406, 37.46016311645508, 37.47205352783203, 37.48388671875, 37.49566650390625, 37.50739288330078, 37.51906204223633, 37.53068161010742, 37.54224395751953, 37.55375289916992, 37.565208435058594, 37.57660675048828, 37.58795928955078, 37.5992546081543, 37.610496520996094, 37.62168884277344, 37.63283157348633, 37.6439208984375, 37.65496063232422, 37.66594696044922, 37.6768798828125, 37.68776321411133, 37.6985969543457, 37.709381103515625, 37.72011184692383, 37.730796813964844, 37.74142837524414, 37.75201416015625, 37.76254653930664, 37.773033142089844, 37.783470153808594, 37.793861389160156, 37.80419921875, 37.814491271972656, 37.82473373413086, 37.83493423461914, 37.84508514404297, 37.85519027709961, 37.8652458190918, 37.8752555847168, 37.88521957397461, 37.895137786865234, 37.90501022338867, 37.91483688354492, 37.924617767333984, 37.93435287475586, 37.94404220581055, 37.95368194580078, 37.963279724121094, 37.972835540771484, 37.98234558105469, 37.9918098449707, 38.0012321472168, 38.0106086730957, 38.01993942260742, 38.029232025146484, 38.03847885131836, 38.04767990112305, 38.05684280395508, 38.06596755981445, 38.075042724609375, 38.084075927734375, 38.09307098388672, 38.10202407836914, 38.11093521118164, 38.11980438232422, 38.12862777709961, 38.137413024902344, 38.14616012573242, 38.15486526489258, 38.16353225708008, 38.172157287597656, 38.18074417114258, 38.189292907714844, 38.197792053222656, 38.20625686645508, 38.214683532714844, 38.22307205200195, 38.231422424316406, 38.2397346496582, 38.248008728027344, 38.25624084472656, 38.264434814453125, 38.2725944519043, 38.28071212768555, 38.288795471191406, 38.29684066772461, 38.304840087890625, 38.31280517578125, 38.320735931396484, 38.32863235473633, 38.33649444580078, 38.34431457519531, 38.35210037231445, 38.35984802246094, 38.36756134033203, 38.37523651123047, 38.382877349853516, 38.39048385620117, 38.39805603027344, 38.40559005737305, 38.41309356689453, 38.420562744140625, 38.42799377441406, 38.43539047241211, 38.442752838134766, 38.4500846862793, 38.457374572753906, 38.464637756347656, 38.471866607666016, 38.479061126708984, 38.48622131347656, 38.49334716796875, 38.50044250488281, 38.507503509521484, 38.514530181884766, 38.52152633666992, 38.52848815917969, 38.53541564941406, 38.54231262207031, 38.5491828918457, 38.55601501464844, 38.56281661987305, 38.56958770751953, 38.576332092285156, 38.58304214477539, 38.589717864990234, 38.59636306762695, 38.60297393798828, 38.60955810546875, 38.616111755371094, 38.62263488769531, 38.629127502441406, 38.63559341430664, 38.642024993896484, 38.64842987060547, 38.65480041503906, 38.6611442565918, 38.66746139526367, 38.673744201660156, 38.679996490478516, 38.68622970581055, 38.69242858886719, 38.6985969543457, 38.704734802246094, 38.71084213256836, 38.71692657470703, 38.722984313964844, 38.72901153564453, 38.73501205444336, 38.74098587036133, 38.74692916870117, 38.75284194946289, 38.758731842041016, 38.76459503173828, 38.77042770385742, 38.7762336730957, 38.782020568847656, 38.787776947021484, 38.79350280761719, 38.799198150634766, 38.80487060546875, 38.81052017211914, 38.81614303588867, 38.821739196777344, 38.82731246948242, 38.83285140991211, 38.8383674621582, 38.8438606262207, 38.849327087402344, 38.854766845703125, 38.86018371582031, 38.86557388305664, 38.87093734741211, 38.876277923583984, 38.881591796875, 38.88688278198242, 38.892147064208984, 38.89738845825195, 38.90260696411133, 38.90780258178711, 38.91297149658203, 38.918121337890625, 38.92324447631836, 38.9283447265625, 38.93341827392578, 38.93846893310547, 38.94349670410156, 38.94850158691406, 38.95348358154297, 38.958438873291016, 38.963375091552734, 38.96828842163086, 38.973175048828125, 38.97804260253906, 38.98289108276367, 38.987709045410156, 38.99250793457031, 38.99728775024414, 39.002044677734375, 39.00677490234375, 39.0114860534668, 39.016178131103516, 39.02084732055664, 39.025489807128906, 39.030113220214844, 39.03471374511719, 39.0392951965332, 39.04384994506836, 39.04838562011719, 39.05290222167969, 39.057403564453125, 39.06188201904297, 39.06633377075195, 39.070770263671875, 39.0751838684082, 39.07957077026367, 39.08393859863281, 39.08829116821289, 39.092628479003906, 39.09693908691406, 39.10123062133789, 39.10550308227539, 39.1097526550293, 39.11398696899414, 39.118194580078125, 39.12238693237305, 39.126564025878906, 39.13071823120117, 39.13485336303711, 39.13896560668945, 39.143062591552734, 39.14714050292969, 39.15119934082031, 39.15523910522461, 39.159263610839844, 39.163265228271484, 39.16725158691406, 39.17121887207031, 39.17516326904297, 39.17909622192383, 39.18300247192383, 39.186893463134766, 39.19076919555664, 39.19462585449219, 39.198463439941406, 39.2022819519043, 39.206085205078125, 39.20987319946289, 39.21364212036133, 39.21739196777344, 39.221126556396484, 39.2248420715332, 39.228538513183594, 39.23221969604492, 39.23588562011719, 39.239532470703125, 39.2431640625, 39.24677658081055, 39.250370025634766, 39.25395202636719, 39.257511138916016, 39.261051177978516, 39.264583587646484, 39.26810073852539, 39.27159881591797, 39.275081634521484, 39.27854919433594, 39.28200149536133, 39.28543472290039, 39.28885269165039, 39.29225540161133, 39.29563903808594, 39.29901123046875, 39.302364349365234, 39.30570602416992, 39.30903244018555, 39.312339782714844, 39.31563186645508, 39.318912506103516, 39.32217025756836, 39.32541275024414, 39.32864761352539, 39.33186340332031, 39.335060119628906, 39.3382453918457, 39.34141540527344, 39.344573974609375, 39.34771728515625, 39.35084533691406, 39.35396194458008, 39.357059478759766, 39.36014175415039, 39.36321258544922, 39.36627197265625, 39.36931610107422, 39.37234115600586, 39.3753547668457, 39.37835693359375, 39.38134002685547, 39.384315490722656, 39.387271881103516, 39.39021682739258, 39.39314651489258, 39.39606475830078, 39.39896774291992, 39.40185546875, 39.404727935791016, 39.4075927734375, 39.41044235229492, 39.41328048706055, 39.416099548339844, 39.41891098022461, 39.42171096801758, 39.424495697021484, 39.42726516723633, 39.43002700805664, 39.43277359008789, 39.43550491333008, 39.43822479248047, 39.4409294128418, 39.44362258911133, 39.44630432128906, 39.448978424072266, 39.451637268066406, 39.454280853271484, 39.456912994384766, 39.45953369140625, 39.46214294433594, 39.46473693847656, 39.46731948852539, 39.46989440917969, 39.47245788574219, 39.475006103515625, 39.477542877197266, 39.480064392089844, 39.482582092285156, 39.485084533691406, 39.487571716308594, 39.49005126953125, 39.49251937866211, 39.49497604370117, 39.49741744995117, 39.49985122680664, 39.50227355957031, 39.50468444824219, 39.507083892822266, 39.50947570800781, 39.5118522644043, 39.51422119140625, 39.51657485961914, 39.518917083740234, 39.52124786376953, 39.52357482910156, 39.525882720947266, 39.5281867980957, 39.53047561645508, 39.532752990722656, 39.5350227355957, 39.53728103637695, 39.539527893066406, 39.54176712036133, 39.54399490356445, 39.54621124267578, 39.54841995239258, 39.55061340332031, 39.552799224853516, 39.55497741699219, 39.5571403503418, 39.55929183959961, 39.561439514160156, 39.563575744628906, 39.56570053100586, 39.56781768798828, 39.56992721557617, 39.572021484375, 39.5741081237793, 39.57618713378906, 39.578250885009766, 39.58030700683594, 39.58235549926758, 39.58439254760742, 39.58641815185547, 39.58843994140625, 39.590450286865234, 39.59245300292969, 39.59444046020508, 39.59642028808594, 39.598392486572266, 39.60036087036133, 39.602317810058594, 39.60426330566406, 39.606197357177734, 39.608123779296875, 39.610042572021484, 39.61195755004883, 39.613861083984375, 39.61574935913086, 39.61763381958008, 39.6195068359375, 39.621376037597656, 39.623233795166016, 39.62508010864258, 39.62691879272461, 39.628753662109375, 39.630577087402344, 39.63239288330078, 39.63420104980469, 39.6359977722168, 39.637786865234375, 39.63956832885742, 39.64134216308594, 39.64310836791992, 39.644866943359375, 39.64661407470703, 39.64834976196289, 39.65008544921875, 39.65180969238281, 39.653526306152344, 39.655235290527344, 39.65693664550781, 39.65863037109375, 39.660316467285156, 39.661991119384766, 39.663658142089844, 39.66531753540039, 39.666969299316406, 39.668617248535156, 39.670257568359375, 39.67189025878906, 39.67351531982422, 39.67512512207031, 39.67673110961914, 39.6783332824707, 39.679927825927734, 39.68151092529297, 39.68308639526367, 39.68465805053711, 39.686222076416016, 39.68777847290039, 39.6893310546875, 39.69087600708008, 39.69240951538086, 39.69393539428711, 39.69545364379883, 39.69696807861328, 39.6984748840332, 39.69997787475586, 39.70146942138672, 39.70295715332031, 39.704437255859375, 39.705909729003906, 39.707374572753906, 39.708831787109375, 39.71028518676758, 39.711727142333984, 39.713165283203125, 39.714595794677734, 39.71601867675781, 39.717437744140625, 39.71885299682617, 39.72025680541992, 39.721656799316406, 39.723052978515625, 39.72443389892578, 39.72581100463867, 39.72718811035156, 39.72855758666992, 39.72991943359375, 39.73127365112305, 39.73262023925781, 39.73395919799805, 39.735294342041016, 39.73662567138672, 39.737953186035156, 39.7392692565918, 39.740577697753906, 39.74188232421875, 39.74317932128906, 39.744468688964844, 39.745758056640625, 39.747039794921875, 39.748313903808594, 39.74958038330078, 39.7508430480957, 39.75210189819336, 39.75334930419922, 39.75458908081055, 39.755828857421875, 39.75706481933594, 39.75829315185547, 39.75951385498047, 39.7607307434082, 39.761940002441406, 39.76314163208008, 39.764339447021484, 39.76553726196289, 39.7667236328125, 39.767906188964844, 39.76908493041992, 39.77025604248047, 39.771419525146484, 39.772579193115234, 39.77373504638672, 39.77488708496094, 39.776031494140625, 39.77717590332031, 39.77831268310547, 39.779441833496094, 39.78056716918945, 39.78168869018555, 39.78280258178711, 39.78390884399414, 39.78501510620117, 39.786109924316406, 39.787200927734375, 39.78828811645508, 39.789371490478516, 39.79044723510742, 39.79152297973633, 39.7925910949707, 39.79365539550781, 39.794715881347656, 39.79576873779297, 39.79681396484375, 39.797855377197266, 39.798892974853516, 39.7999267578125, 39.80095672607422, 39.801979064941406, 39.80299758911133, 39.804012298583984, 39.80502700805664, 39.806034088134766, 39.807029724121094, 39.80802536010742, 39.80901336669922, 39.810001373291016, 39.81098175048828, 39.81195831298828, 39.81293487548828, 39.813907623291016, 39.81487274169922, 39.81583023071289, 39.81678771972656, 39.8177375793457, 39.81868362426758, 39.81962585449219, 39.820560455322266, 39.821495056152344, 39.822425842285156, 39.8233528137207, 39.82427215576172, 39.8251838684082, 39.82609176635742, 39.82699966430664, 39.82790756225586, 39.82880783081055, 39.8297004699707, 39.830596923828125, 39.83148193359375, 39.83236312866211, 39.8332405090332, 39.83411407470703, 39.834983825683594, 39.83584976196289, 39.83671188354492, 39.83757019042969, 39.83842086791992, 39.83926773071289, 39.84011459350586, 39.84095764160156, 39.841793060302734, 39.842628479003906, 39.84345626831055, 39.84428405761719, 39.84510803222656, 39.84592819213867, 39.846744537353516, 39.847557067871094, 39.84836196899414, 39.84916305541992, 39.8499641418457, 39.85075759887695, 39.85154724121094, 39.85233688354492, 39.853126525878906, 39.85390853881836, 39.85468292236328, 39.8554573059082, 39.856231689453125, 39.856998443603516, 39.857765197753906, 39.85852813720703, 39.85928726196289, 39.86003875732422, 39.86078643798828, 39.861534118652344, 39.86227798461914, 39.863014221191406, 39.86375045776367, 39.86448287963867, 39.86521530151367, 39.86594009399414, 39.86665725708008, 39.86737823486328, 39.86809539794922, 39.86880874633789, 39.8695182800293, 39.87022018432617, 39.87091827392578, 39.871620178222656, 39.872318267822266, 39.87301254272461, 39.87369918823242, 39.87438201904297, 39.875064849853516, 39.87574768066406, 39.87642288208008, 39.87709426879883, 39.877769470214844, 39.87843704223633, 39.87910079956055, 39.8797607421875, 39.88041687011719, 39.881072998046875, 39.88172912597656, 39.882381439208984, 39.88302993774414, 39.88367462158203, 39.884315490722656, 39.884952545166016, 39.885581970214844, 39.88621139526367, 39.886844635009766, 39.887474060058594, 39.888092041015625, 39.88871383666992, 39.88933181762695, 39.88994216918945, 39.89055252075195, 39.89116287231445, 39.89176940917969, 39.892372131347656, 39.892974853515625, 39.89356994628906, 39.894168853759766, 39.89476013183594, 39.895347595214844, 39.895931243896484, 39.896514892578125, 39.8970947265625, 39.897674560546875, 39.898250579833984, 39.89882278442383, 39.899391174316406, 39.89995574951172, 39.90052032470703, 39.90108108520508, 39.901641845703125, 39.902198791503906, 39.90275192260742, 39.90330505371094, 39.90385437011719, 39.90439987182617, 39.90494155883789, 39.90548324584961, 39.9060173034668, 39.906558990478516, 39.9070930480957, 39.907623291015625, 39.90815353393555, 39.9086799621582, 39.909202575683594, 39.909725189208984, 39.910240173339844, 39.9107551574707, 39.91127014160156, 39.911781311035156, 39.912288665771484, 39.91279983520508, 39.91330337524414, 39.9138069152832, 39.914310455322266, 39.91481018066406, 39.915306091308594, 39.91579818725586, 39.916290283203125, 39.91678237915039, 39.91727066040039, 39.917755126953125, 39.918235778808594, 39.9187126159668, 39.919193267822266, 39.91967010498047, 39.92014694213867, 39.92061996459961, 39.92108917236328, 39.92155456542969, 39.92201614379883, 39.92247772216797, 39.92293930053711, 39.92340087890625, 39.92385482788086, 39.9243049621582, 39.92475891113281, 39.925209045410156, 39.9256591796875, 39.92610168457031, 39.926544189453125, 39.92698287963867, 39.92742156982422, 39.927860260009766, 39.92829895019531, 39.92873001098633, 39.929161071777344, 39.929588317871094, 39.930015563964844, 39.93043899536133, 39.93086242675781, 39.93128204345703, 39.931697845458984, 39.9321174621582, 39.932533264160156, 39.932945251464844, 39.93335723876953, 39.93376541137695, 39.934173583984375, 39.934574127197266, 39.93497848510742, 39.93537902832031, 39.93578338623047, 39.936180114746094, 39.93657302856445, 39.93696594238281, 39.93736267089844, 39.9377555847168, 39.938140869140625, 39.93852615356445, 39.93891143798828, 39.939292907714844, 39.93967819213867, 39.94005584716797, 39.9404296875, 39.9408073425293, 39.94118118286133, 39.94155502319336, 39.94192886352539, 39.94230270385742, 39.94266891479492, 39.943031311035156, 39.943397521972656, 39.943756103515625, 39.94411849975586, 39.94447708129883, 39.9448356628418, 39.9451904296875, 39.9455451965332, 39.945899963378906, 39.946250915527344, 39.946598052978516, 39.94694519042969, 39.94729232788086, 39.94763946533203, 39.94798278808594, 39.94832229614258, 39.94866180419922, 39.948997497558594, 39.94933319091797, 39.94967269897461, 39.95000457763672, 39.95033645629883, 39.9506721496582, 39.95100021362305, 39.951324462890625, 39.95165252685547, 39.95197677612305, 39.95229721069336, 39.95261764526367, 39.95293426513672, 39.95325469970703, 39.95357131958008, 39.95388412475586, 39.95419692993164, 39.95450973510742, 39.9548225402832, 39.95513153076172, 39.955440521240234, 39.95574951171875, 39.956058502197266, 39.95635986328125, 39.956661224365234, 39.95696258544922, 39.9572639465332, 39.95756149291992, 39.957855224609375, 39.958152770996094, 39.95844268798828, 39.958736419677734, 39.95903015136719, 39.95932388305664, 39.95961380004883, 39.95989990234375, 39.960182189941406, 39.96046829223633, 39.960750579833984, 39.961036682128906, 39.9613151550293, 39.96159362792969, 39.96187210083008, 39.96215057373047, 39.962425231933594, 39.96269989013672, 39.962974548339844, 39.9632453918457, 39.96351623535156, 39.96378707885742, 39.964054107666016, 39.964317321777344, 39.96458435058594, 39.964847564697266, 39.965110778808594, 39.965370178222656, 39.96562957763672, 39.96588897705078, 39.96615219116211, 39.96641159057617, 39.9666633605957, 39.9669189453125, 39.96717071533203, 39.9674186706543, 39.96767044067383, 39.967918395996094, 39.96816635131836, 39.968414306640625, 39.96866226196289, 39.96890640258789, 39.969154357910156, 39.969398498535156, 39.96963882446289, 39.969879150390625, 39.970115661621094, 39.97035217285156, 39.9705924987793, 39.970829010009766, 39.971065521240234, 39.9713020324707, 39.971534729003906, 39.97176742553711, 39.97199630737305, 39.972225189208984, 39.97245788574219, 39.97268295288086, 39.97290802001953, 39.97312927246094, 39.97335433959961, 39.973575592041016, 39.97379684448242, 39.97401809692383, 39.9742431640625, 39.97446060180664, 39.97467803955078, 39.97489547729492, 39.9751091003418, 39.97532653808594, 39.97554397583008, 39.97575759887695, 39.97596740722656, 39.97617721557617, 39.97638702392578, 39.97659683227539, 39.976802825927734, 39.977012634277344, 39.97721862792969, 39.977420806884766, 39.97762680053711, 39.97783279418945, 39.97803497314453, 39.97823715209961, 39.97843933105469, 39.9786376953125, 39.97883605957031, 39.97903823852539, 39.9792366027832, 39.979427337646484, 39.9796257019043, 39.979820251464844, 39.98001480102539, 39.98020553588867, 39.98040008544922, 39.980587005615234, 39.98077392578125, 39.98096466064453, 39.98115539550781, 39.981346130371094, 39.98153305053711, 39.98171615600586, 39.98189926147461, 39.98208236694336, 39.98226547241211, 39.98244857788086, 39.98263168334961, 39.982810974121094, 39.98299026489258, 39.9831657409668, 39.98334503173828, 39.9835205078125, 39.983699798583984, 39.98387908935547, 39.98405456542969, 39.984222412109375, 39.984397888183594, 39.98456573486328, 39.98473358154297, 39.98490524291992, 39.985076904296875, 39.98524856567383, 39.985416412353516, 39.9855842590332, 39.985748291015625, 39.98591232299805, 39.98607635498047, 39.98624038696289, 39.98640823364258, 39.986572265625, 39.98673629760742, 39.98689270019531, 39.987056732177734, 39.987213134765625, 39.987369537353516, 39.98753356933594, 39.98768615722656, 39.98783874511719, 39.98799514770508, 39.9881477355957, 39.98830032348633, 39.98845672607422, 39.988609313964844, 39.98876190185547, 39.98891067504883, 39.98906707763672, 39.98921585083008, 39.98936462402344, 39.9895133972168, 39.98965835571289, 39.98980712890625, 39.98995590209961, 39.99010467529297, 39.99024963378906, 39.990394592285156, 39.99053955078125, 39.990684509277344, 39.9908332824707, 39.99097442626953, 39.99111557006836, 39.99125671386719, 39.991397857666016, 39.99153518676758, 39.991676330566406, 39.99181365966797, 39.991947174072266, 39.99208450317383, 39.992218017578125, 39.99235534667969, 39.99249267578125, 39.99262619018555, 39.99276351928711, 39.992897033691406, 39.9930305480957, 39.9931640625, 39.9932975769043, 39.99342727661133, 39.99355697631836, 39.993690490722656, 39.99381637573242, 39.99394226074219, 39.994075775146484, 39.994197845458984, 39.99432373046875, 39.99445343017578, 39.99457931518555, 39.99470520019531, 39.99482727050781, 39.99494934082031, 39.99507141113281, 39.99519348144531, 39.99531936645508, 39.99543762207031, 39.99555969238281, 39.99567794799805, 39.99579620361328, 39.99591827392578, 39.99603271484375, 39.99615478515625, 39.99626922607422, 39.99638748168945, 39.99650192260742, 39.99661636352539, 39.996734619140625, 39.996849060058594, 39.9969596862793, 39.9970703125, 39.99718475341797, 39.99729919433594, 39.99740982055664, 39.99752426147461, 39.99763488769531, 39.997745513916016, 39.99785614013672, 39.99796676635742, 39.998077392578125, 39.99818801879883, 39.99829864501953, 39.99840545654297, 39.998512268066406, 39.99861526489258, 39.998722076416016, 39.99882888793945, 39.998931884765625, 39.99903869628906, 39.999141693115234, 39.99924850463867, 39.999351501464844, 39.99945831298828, 39.99955749511719, 39.99966049194336, 39.999759674072266, 39.99986267089844, 39.999961853027344, 40.00006103515625, 40.00016403198242, 40.00025939941406, 40.00035858154297, 40.000457763671875, 40.000553131103516, 40.000648498535156, 40.0007438659668, 40.00083923339844, 40.00093460083008, 40.001033782958984, 40.00112533569336, 40.001220703125, 40.001312255859375, 40.001407623291016, 40.00149917602539, 40.001590728759766, 40.00168228149414, 40.00177764892578, 40.00186538696289, 40.001956939697266, 40.002044677734375, 40.002132415771484, 40.00222396850586, 40.00231170654297, 40.00239944458008, 40.00248718261719, 40.0025749206543, 40.002662658691406, 40.002750396728516, 40.00283432006836, 40.00292205810547, 40.00300598144531, 40.00309371948242, 40.00318145751953, 40.00326919555664, 40.00335693359375, 40.003440856933594, 40.00352096557617, 40.003604888916016, 40.00368881225586, 40.00376892089844, 40.003849029541016, 40.003936767578125, 40.0040168762207, 40.00409698486328, 40.00417709350586, 40.00425720214844, 40.004337310791016, 40.004417419433594, 40.00449752807617, 40.00457763671875, 40.00465393066406, 40.00473403930664, 40.00481033325195, 40.004886627197266, 40.004966735839844, 40.00504684448242, 40.005123138427734, 40.00519943237305, 40.00527572631836, 40.005348205566406, 40.00542068481445, 40.005496978759766, 40.00557327270508, 40.005645751953125, 40.005714416503906, 40.00579071044922, 40.005863189697266, 40.00593948364258, 40.006011962890625, 40.006080627441406, 40.00615310668945, 40.006229400634766, 40.00629425048828, 40.00636672973633, 40.006431579589844, 40.006500244140625, 40.00657272338867, 40.00664138793945, 40.006710052490234, 40.006778717041016, 40.00684356689453, 40.00691223144531, 40.00697708129883, 40.00704574584961, 40.00711441040039, 40.00718307495117, 40.00724792480469, 40.00731658935547, 40.007381439208984, 40.007450103759766, 40.007511138916016, 40.00757598876953, 40.00764083862305, 40.00770950317383, 40.00777053833008, 40.007835388183594, 40.00790023803711, 40.00796127319336, 40.008026123046875, 40.00808334350586, 40.00814437866211, 40.00820541381836, 40.008270263671875, 40.00832748413086, 40.008392333984375, 40.00844955444336, 40.00851058959961, 40.008567810058594, 40.00863265991211, 40.008689880371094, 40.008750915527344, 40.00880432128906, 40.00886154174805, 40.0089225769043, 40.00897979736328, 40.009033203125, 40.009090423583984, 40.00914764404297, 40.00920867919922, 40.0092658996582, 40.00932312011719, 40.00938034057617, 40.009437561035156, 40.00949478149414, 40.009552001953125, 40.009605407714844, 40.00966262817383, 40.00971984863281, 40.00977325439453, 40.00982666015625, 40.00988006591797, 40.00992965698242, 40.00998306274414, 40.01003646850586, 40.01008987426758, 40.0101432800293, 40.010196685791016, 40.01025390625, 40.01030349731445, 40.010353088378906, 40.010406494140625, 40.01045608520508, 40.0105094909668, 40.010562896728516, 40.01061248779297, 40.01066207885742, 40.010711669921875, 40.01076126098633, 40.01081085205078, 40.010860443115234, 40.01091384887695, 40.010963439941406, 40.01101303100586, 40.01105880737305, 40.011104583740234, 40.01115036010742, 40.011199951171875, 40.01124572753906, 40.011295318603516, 40.01134490966797, 40.01139450073242, 40.011444091796875, 40.01148986816406, 40.01153564453125, 40.01158142089844, 40.01162338256836, 40.01166915893555, 40.011714935302734, 40.01176071166992, 40.01180648803711, 40.0118522644043, 40.011898040771484, 40.011940002441406, 40.011985778808594, 40.01203155517578, 40.0120735168457, 40.012115478515625, 40.01216125488281, 40.012203216552734, 40.012245178222656, 40.012290954589844, 40.012332916259766, 40.01237487792969, 40.012413024902344, 40.012454986572266, 40.01250076293945, 40.012542724609375, 40.0125846862793, 40.01262664794922, 40.01266860961914, 40.0127067565918, 40.012752532958984, 40.012786865234375, 40.01282501220703, 40.01285934448242, 40.01290512084961, 40.012939453125, 40.01298141479492, 40.01301574707031, 40.013057708740234, 40.01309585571289, 40.01313400268555, 40.01317596435547, 40.01321029663086, 40.013248443603516, 40.013282775878906, 40.01332473754883, 40.01335906982422, 40.01340103149414, 40.0134391784668, 40.01347351074219, 40.013511657714844, 40.013545989990234, 40.013587951660156, 40.01362609863281, 40.0136604309082, 40.01369857788086, 40.01373291015625, 40.013771057128906, 40.0138053894043, 40.01383972167969, 40.013877868652344, 40.013912200927734, 40.013946533203125, 40.01398468017578, 40.01401901245117, 40.01405334472656, 40.01408767700195, 40.014122009277344, 40.014156341552734, 40.014190673828125, 40.014225006103516, 40.014259338378906, 40.01428985595703, 40.01432418823242, 40.01435852050781, 40.01438903808594, 40.01442337036133, 40.01445770263672, 40.014488220214844, 40.014522552490234, 40.01455307006836, 40.01458740234375, 40.014617919921875, 40.0146484375, 40.01468276977539, 40.014713287353516, 40.01474380493164, 40.01477813720703, 40.01480484008789, 40.01483917236328, 40.01486587524414, 40.014892578125, 40.014923095703125, 40.01495361328125, 40.014984130859375, 40.0150146484375, 40.015045166015625, 40.015071868896484, 40.01510238647461, 40.015132904052734, 40.01516342163086, 40.015193939208984, 40.01522445678711, 40.015254974365234, 40.01528549194336, 40.01531219482422, 40.015342712402344, 40.0153694152832, 40.01539993286133, 40.01542663574219, 40.01545715332031, 40.01548385620117, 40.0155143737793, 40.01554489135742, 40.01557159423828, 40.01559829711914, 40.015621185302734, 40.01565170288086, 40.01567840576172, 40.01570510864258, 40.01572799682617, 40.01575469970703, 40.015785217285156, 40.01580810546875, 40.01583480834961, 40.01586151123047, 40.01588821411133, 40.01591491699219, 40.01594161987305, 40.015968322753906, 40.015995025634766, 40.01601791381836, 40.01604461669922, 40.01606750488281, 40.01609420776367, 40.016117095947266, 40.016143798828125, 40.016170501708984, 40.01619338989258, 40.01621627807617, 40.01624298095703, 40.016265869140625, 40.01628875732422, 40.01631546020508, 40.01633834838867, 40.016361236572266, 40.01638412475586, 40.01641082763672, 40.01643371582031, 40.01646041870117, 40.0164794921875, 40.01649856567383, 40.01652145385742, 40.016544342041016, 40.016571044921875, 40.01659393310547, 40.01661682128906, 40.016639709472656, 40.016658782958984, 40.01668167114258, 40.016700744628906, 40.0167236328125, 40.01673889160156, 40.016761779785156, 40.016780853271484, 40.016807556152344, 40.01683044433594, 40.01685333251953, 40.016876220703125, 40.01689529418945, 40.01691436767578, 40.016937255859375, 40.016963958740234, 40.0169792175293, 40.016998291015625, 40.01701736450195, 40.01704406738281, 40.017066955566406, 40.01708221435547, 40.01710510253906, 40.01712417602539, 40.017147064208984, 40.01716613769531, 40.01718521118164, 40.017208099365234, 40.01722717285156, 40.01724624633789, 40.017269134521484, 40.01728820800781, 40.017311096191406, 40.017330169677734, 40.0173454284668, 40.017364501953125, 40.01738357543945, 40.01740264892578, 40.01742172241211, 40.01744079589844, 40.01746368408203, 40.01748275756836, 40.01750183105469, 40.01751708984375, 40.01753616333008, 40.017555236816406, 40.017574310302734, 40.01759338378906, 40.01761245727539, 40.01762771606445, 40.01764678955078, 40.017662048339844, 40.01768112182617, 40.0177001953125, 40.01771926879883, 40.01773452758789, 40.01775360107422, 40.01776885986328, 40.01778793334961, 40.01780319213867, 40.017818450927734, 40.01783752441406, 40.01785659790039, 40.01787185668945, 40.017887115478516, 40.017906188964844, 40.017921447753906, 40.01793670654297, 40.0179557800293, 40.01797103881836, 40.01799011230469, 40.01800537109375, 40.01802062988281, 40.01803970336914, 40.0180549621582, 40.018070220947266, 40.01808547973633, 40.01810073852539, 40.01811599731445, 40.018131256103516, 40.01814651489258, 40.01816177368164, 40.0181770324707, 40.018192291259766, 40.018211364746094, 40.018226623535156, 40.01824188232422, 40.01825714111328, 40.018272399902344, 40.01828384399414, 40.01829528808594, 40.018314361572266, 40.0183219909668, 40.01833724975586, 40.018348693847656, 40.01836395263672, 40.01837921142578, 40.01839065551758, 40.01840591430664, 40.01841735839844, 40.018436431884766, 40.01844787597656, 40.018463134765625, 40.01847457885742, 40.018489837646484, 40.01850128173828, 40.01852035522461, 40.018531799316406, 40.01854705810547, 40.01856231689453, 40.01857376098633, 40.018585205078125, 40.01860046386719, 40.01861572265625, 40.01863098144531, 40.01864242553711, 40.01865768432617, 40.0186653137207, 40.0186767578125, 40.0186882019043, 40.01870346069336, 40.018714904785156, 40.01872634887695, 40.018741607666016, 40.01875686645508, 40.018768310546875, 40.01878356933594, 40.018795013427734, 40.0188102722168, 40.01881790161133, 40.01883316040039, 40.01884460449219, 40.018856048583984, 40.01886749267578, 40.01887512207031, 40.018890380859375, 40.018898010253906, 40.01891326904297, 40.0189208984375, 40.0189323425293, 40.018943786621094, 40.01895523071289, 40.01897048950195, 40.01898193359375, 40.01899337768555, 40.019004821777344, 40.01901626586914, 40.01902770996094, 40.019039154052734, 40.01905059814453, 40.01906204223633, 40.019073486328125, 40.019081115722656, 40.01909255981445, 40.019100189208984, 40.01911163330078, 40.01912307739258, 40.019134521484375, 40.01914978027344, 40.019161224365234, 40.019168853759766, 40.01918411254883, 40.01919174194336, 40.01919937133789, 40.01921081542969, 40.01921844482422, 40.019229888916016, 40.01924133300781, 40.01925277709961, 40.019264221191406, 40.01927185058594, 40.019287109375, 40.01929473876953, 40.01930618286133, 40.01931381225586, 40.019325256347656, 40.01933670043945, 40.019344329833984, 40.019351959228516, 40.01936340332031, 40.019371032714844, 40.01938247680664, 40.01939010620117, 40.0193977355957, 40.0194091796875, 40.0194206237793, 40.01942825317383, 40.01943588256836, 40.01944351196289, 40.01945114135742, 40.019466400146484, 40.01947021484375, 40.019474029541016, 40.01948547363281, 40.019493103027344, 40.01950454711914, 40.01951217651367, 40.01952362060547, 40.019535064697266, 40.0195426940918, 40.019554138183594, 40.019561767578125, 40.01957321166992, 40.01958084106445, 40.019588470458984, 40.01959991455078, 40.01960754394531, 40.019615173339844, 40.01962661743164, 40.019630432128906, 40.01963806152344, 40.0196418762207, 40.019649505615234, 40.01966094970703, 40.01966857910156, 40.019676208496094, 40.019683837890625, 40.019691467285156, 40.01970291137695, 40.019710540771484, 40.01971435546875, 40.01972198486328, 40.01972961425781, 40.01974105834961, 40.019744873046875, 40.019752502441406, 40.0197639465332, 40.01976776123047, 40.019771575927734, 40.01978302001953, 40.01979064941406, 40.019798278808594, 40.01980972290039, 40.019813537597656, 40.01981735229492, 40.01982498168945, 40.01983642578125, 40.01984405517578, 40.01984786987305, 40.01985549926758, 40.01986312866211, 40.01987075805664, 40.019874572753906, 40.01987838745117, 40.01988983154297, 40.0198974609375, 40.019901275634766, 40.0199089050293, 40.01991653442383, 40.01992416381836, 40.019927978515625, 40.019935607910156, 40.01994323730469, 40.01995086669922, 40.01995849609375, 40.01996994018555, 40.01997756958008, 40.019981384277344, 40.01998519897461, 40.01999282836914, 40.02000045776367, 40.02000427246094, 40.02001190185547, 40.02001953125, 40.02002716064453, 40.02003479003906, 40.02003860473633, 40.020050048828125, 40.020050048828125, 40.020057678222656, 40.02006149291992, 40.02006912231445, 40.02008056640625, 40.020084381103516, 40.02009201049805, 40.02009963989258, 40.020103454589844, 40.02010726928711, 40.02011489868164, 40.02012252807617, 40.0201301574707, 40.02013397216797, 40.020137786865234, 40.020145416259766, 40.0201530456543, 40.02015686035156, 40.020164489746094, 40.02016830444336, 40.020172119140625, 40.020179748535156, 40.02018737792969, 40.02019119262695, 40.02019500732422, 40.020198822021484, 40.020206451416016, 40.02021408081055, 40.02021789550781, 40.02022171020508, 40.020225524902344, 40.020233154296875, 40.020240783691406, 40.02024459838867, 40.0202522277832, 40.020259857177734, 40.020263671875, 40.020267486572266, 40.02027130126953, 40.02027893066406, 40.020286560058594, 40.02029037475586, 40.020294189453125, 40.020301818847656, 40.020301818847656, 40.02030563354492, 40.02030944824219, 40.02031707763672, 40.02032470703125, 40.02032470703125, 40.020328521728516, 40.02033615112305, 40.02034378051758, 40.02035140991211, 40.020355224609375, 40.020362854003906, 40.02036666870117, 40.02037048339844, 40.0203742980957, 40.0203742980957, 40.02037811279297, 40.020381927490234, 40.02039337158203, 40.0203971862793, 40.02040100097656, 40.02040481567383, 40.020408630371094, 40.020416259765625, 40.020423889160156, 40.020423889160156, 40.02042770385742, 40.02043151855469, 40.02043533325195, 40.020442962646484, 40.02044677734375, 40.02045440673828, 40.02045822143555, 40.02046585083008, 40.020469665527344, 40.020477294921875, 40.02048110961914, 40.020484924316406, 40.02048873901367, 40.02049255371094, 40.0204963684082, 40.02050018310547, 40.020503997802734, 40.0205078125, 40.020511627197266, 40.0205192565918, 40.02052688598633, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.02053451538086, 40.02054214477539, 40.020545959472656, 40.02054977416992, 40.02055358886719, 40.02056121826172, 40.020565032958984, 40.02056884765625, 40.02057647705078, 40.02057647705078, 40.02058029174805, 40.02058410644531, 40.020591735839844, 40.02059555053711, 40.020599365234375, 40.02060317993164, 40.020606994628906, 40.02061080932617, 40.02061462402344, 40.02061462402344, 40.02061462402344, 40.02062225341797, 40.020626068115234, 40.0206298828125, 40.020633697509766, 40.02063751220703, 40.0206413269043, 40.0206413269043, 40.0206413269043, 40.02064895629883, 40.020652770996094, 40.02065658569336, 40.020660400390625, 40.02066421508789, 40.020668029785156, 40.02067565917969, 40.02067947387695]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b6c821c1-d315-4188-afab-caea1c351551');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHN2mHGxJgx4",
        "outputId": "05b94dd6-60c9-494d-e741-022bc41320ee"
      },
      "source": [
        "mse = np.mean((y_RT - y_test_RT_NONITERATIVE)**2)\n",
        "print(\"MSE actual vs noniterative: \", mse)\n",
        "\n",
        "mse1 = np.mean((y_RT - y_test_RT_ITERATIVE)**2)\n",
        "print(\"MSE actual vs iterative: \", mse1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE actual vs noniterative:  0.011228547316591213\n",
            "MSE actual vs iterative:  6.74962329533998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up5MGpJznTTk"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "X_MB = np.concatenate((S2MBX, S3MBX, S4MBX, S5MBX, S6MBX), axis=0)\n",
        "y_MB = np.concatenate((S2MBY, S3MBY, S4MBY, S5MBY, S6MBY), axis=0)\n",
        "\n",
        "X_test_InputMB = X_MB.reshape((X_MB.shape[0], X_MB.shape[1], n_features))\n",
        "\n",
        "y_test_MB_NONITERATIVE = model.predict(X_test_InputMB, verbose=0)\n",
        "y_test_MB_NONITERATIVE = np.ravel(y_test_MB_NONITERATIVE) ## Convert to raveled array\n",
        "\n",
        "y_test_MB_ITERATIVE = Iterative_Test(X_test_InputMB)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "wQmrIKTQNnZn",
        "outputId": "f234be1a-acc7-4de5-987a-5090ab229b8b"
      },
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=y_test_MB_NONITERATIVE, name=\"y_test_MB_NONITERATIVE\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_MB, name=\"y_MB\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_test_MB_ITERATIVE, name=\"y_test_MB_ITERATIVE\", line_shape='linear'))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"bed11c8b-5d9a-4ff5-a241-423f282c9560\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"bed11c8b-5d9a-4ff5-a241-423f282c9560\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'bed11c8b-5d9a-4ff5-a241-423f282c9560',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_test_MB_NONITERATIVE\", \"type\": \"scatter\", \"y\": [34.50702667236328, 34.518341064453125, 34.76005554199219, 35.009857177734375, 34.81318283081055, 34.707942962646484, 34.4017333984375, 34.2098388671875, 34.23812484741211, 34.162010192871094, 34.02176284790039, 33.96488952636719, 34.232994079589844, 34.25969696044922, 34.31805419921875, 34.32403564453125, 34.40985107421875, 34.48120880126953, 34.708892822265625, 34.831939697265625, 34.868560791015625, 34.90596389770508, 34.92422103881836, 34.93143844604492, 34.938838958740234, 35.03901672363281, 34.95807647705078, 34.80238723754883, 34.566917419433594, 34.421077728271484, 34.19517517089844, 34.23524856567383, 34.14588928222656, 33.90512466430664, 33.628971099853516, 33.26486587524414, 33.781028747558594, 33.823341369628906, 33.74634552001953, 33.531158447265625, 33.78569412231445, 33.98760986328125, 34.020057678222656, 34.04057312011719, 34.11663818359375, 34.169952392578125, 34.208961486816406, 34.22956085205078, 34.28692626953125, 34.316612243652344, 34.32764434814453, 34.336097717285156, 34.439910888671875, 34.493865966796875, 34.5103874206543, 34.52728271484375, 34.53456115722656, 34.538169860839844, 34.642051696777344, 34.6942024230957, 34.7088737487793, 34.72576904296875, 34.73304748535156, 34.736656188964844, 34.7403564453125, 31.461875915527344, 31.799776077270508, 32.07008361816406, 32.12137985229492, 32.17289352416992, 32.11011505126953, 32.02946472167969, 31.92721176147461, 32.04759979248047, 32.07101058959961, 32.07110595703125, 31.969663619995117, 31.870014190673828, 32.01241683959961, 32.03656005859375, 32.06379699707031, 32.06986999511719, 32.053524017333984, 32.05712890625, 32.060829162597656, 32.060829162597656, 32.060829162597656, 32.060829162597656, 32.060829162597656, 32.060829162597656, 32.1610107421875, 32.213157653808594, 32.22782897949219, 32.244728088378906, 32.25200653076172, 32.255615234375, 32.259315490722656, 32.259315490722656, 32.259315490722656, 32.259315490722656, 32.409584045410156, 32.41162872314453, 32.418060302734375, 32.452667236328125, 32.45777893066406, 32.46385955810547, 32.46961212158203, 32.457801818847656, 32.457801818847656, 32.457801818847656, 32.457801818847656, 32.457801818847656, 32.50788879394531, 32.533966064453125, 32.541297912597656, 32.54974365234375, 32.55338668823242, 32.55519104003906, 32.55704116821289, 32.55704116821289, 32.55704116821289, 32.55704116821289, 32.55704116821289, 32.55704116821289, 32.65721893310547, 32.70936965942383, 32.724037170410156, 32.740936279296875, 32.74821472167969, 32.75182342529297, 32.755523681640625, 32.755523681640625, 32.8557014465332, 32.77476501464844, 32.80630874633789, 32.93299102783203, 32.92323303222656, 32.960540771484375, 32.96070098876953, 32.95030975341797, 32.954010009765625, 32.954010009765625, 32.954010009765625, 32.954010009765625, 32.954010009765625, 33.05418395996094, 33.1063346862793, 33.121002197265625, 33.137901306152344, 33.145179748535156, 33.14878845214844, 33.152488708496094, 33.402931213378906, 33.31405258178711, 33.29472732543945, 33.417144775390625, 33.37514877319336, 33.36743927001953, 33.398983001708984, 33.35826110839844, 33.361534118652344, 33.36278533935547, 33.350975036621094, 33.350975036621094, 33.350975036621094, 33.350975036621094, 33.350975036621094, 33.350975036621094, 33.40106201171875, 33.360595703125, 33.34272003173828, 33.36354064941406, 33.35826110839844, 33.361534118652344, 33.41287612915039, 33.42713928222656, 33.43447494506836, 33.44292449951172, 33.446563720703125, 33.4483642578125, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.550392150878906, 33.781856536865234, 33.636451721191406, 33.626930236816406, 33.766700744628906, 33.663543701171875, 33.720603942871094, 33.716697692871094, 33.632904052734375, 33.795135498046875, 33.8296012878418, 33.83681869506836, 33.856224060058594, 33.839874267578125, 33.84347915649414, 33.84718322753906, 33.84718322753906, 33.947357177734375, 33.866416931152344, 33.83067321777344, 33.872314453125, 33.961936950683594, 33.887542724609375, 33.85430145263672, 33.872314453125, 33.86175537109375, 33.86830139160156, 33.870811462402344, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.947357177734375, 33.99951171875, 34.01417922973633, 34.03107452392578, 34.038352966308594, 34.04196548461914, 34.04566192626953, 34.145843505859375, 34.064903259277344, 34.02915954589844, 34.070804595947266, 34.06024169921875, 34.06678771972656, 34.06929397583008, 34.04566192626953, 34.04566192626953, 34.145843505859375, 34.197994232177734, 34.21266174316406, 34.22956085205078, 34.236839294433594, 34.240447998046875, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.29423904418945, 34.25376510620117, 34.26953887939453, 34.33287811279297, 34.3280029296875, 34.346656799316406, 34.34673309326172, 34.34153747558594, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.443565368652344, 34.36262512207031, 34.326881408691406, 34.36852264404297, 34.35796356201172, 34.36450958251953, 34.36701965332031, 34.443565368652344, 34.4957160949707, 34.5103874206543, 34.52728271484375, 34.53456115722656, 34.538169860839844, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.742225646972656, 34.69817352294922, 34.69236373901367, 34.74565887451172, 34.74762725830078, 34.75554656982422, 34.76398468017578, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.940711975097656, 34.89665985107422, 34.87397003173828, 34.76063537597656, 34.810813903808594, 34.77743148803711, 34.78211975097656, 34.76398468017578, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.84053421020508, 34.75959014892578, 34.72385025024414, 34.76549530029297, 34.75493621826172, 34.76148223876953, 34.76398468017578, 34.7403564453125, 34.99079895019531, 35.121177673339844, 35.007545471191406, 34.924835205078125, 35.10112762451172, 34.981651306152344, 34.97400665283203, 34.99941635131836, 34.95341491699219, 34.9599609375, 34.962467193603516, 34.83863067626953, 34.82262420654297, 34.81318283081055, 34.731868743896484, 34.900123596191406, 34.921260833740234, 34.92847442626953, 34.947879791259766, 34.9315299987793, 34.93513870239258, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.03901672363281, 34.95807647705078, 34.92232894897461, 34.96397399902344, 34.95341491699219, 34.9599609375, 34.962467193603516, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.03901672363281, 34.95807647705078, 34.92232894897461, 34.96397399902344, 34.95341491699219, 34.9599609375, 34.962467193603516, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.03901672363281, 34.95807647705078, 34.92232894897461, 34.96397399902344, 34.95341491699219, 34.9599609375, 34.962467193603516, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.03901672363281, 35.09116744995117, 35.105838775634766, 35.02252960205078, 35.013797760009766, 35.007965087890625, 34.93035125732422, 34.9984245300293, 34.96741485595703, 34.9599609375, 34.962467193603516, 34.938838958740234, 35.03901672363281, 34.95807647705078, 34.922332763671875, 35.064151763916016, 35.105743408203125, 35.12696075439453, 35.1463623046875, 35.13001251220703, 35.033416748046875, 34.95381546020508, 34.992427825927734, 34.960723876953125, 35.07346725463867, 34.9817008972168, 34.92232894897461, 34.96397399902344, 34.95341491699219, 35.06013488769531, 35.11479187011719, 35.105838775634766, 35.02252960205078, 34.94650650024414, 34.98872756958008, 34.960723876953125, 34.973289489746094, 35.062644958496094, 35.09116744995117, 35.105838775634766, 35.12273406982422, 35.13001251220703, 35.033416748046875, 34.95381546020508, 34.992427825927734, 34.960723876953125, 34.973289489746094, 35.062644958496094, 34.95807647705078, 34.92232894897461, 34.96397399902344, 34.95341491699219, 34.9599609375, 34.86225891113281, 34.63539123535156, 34.39318084716797, 34.206687927246094, 34.218746185302734, 34.256309509277344, 34.359466552734375, 34.32426452636719, 34.31734085083008, 34.32878875732422, 34.42905807495117, 34.360774993896484, 34.326881408691406, 34.31842041015625, 34.266212463378906, 34.29206848144531, 34.32880783081055, 34.3375358581543, 34.33945846557617, 34.28599548339844, 34.188011169433594, 34.08558654785156, 34.11494445800781, 33.98456954956055, 33.900978088378906, 33.92439651489258, 33.969242095947266, 34.033958435058594, 34.037811279296875, 34.03107452392578, 34.13853454589844, 34.06119918823242, 34.02915954589844, 34.070804595947266, 34.06024169921875, 34.06678771972656, 34.06929397583008, 34.145843505859375, 34.197994232177734, 34.21266174316406, 34.129356384277344, 34.0533332824707, 34.19573211669922, 34.219879150390625, 34.24711608886719, 34.25318908691406, 34.28692626953125, 34.316612243652344, 34.27754592895508, 34.24434280395508, 34.167083740234375, 34.0528564453125, 33.93296813964844, 33.944068908691406, 33.90351486206055, 33.888938903808594, 33.870811462402344, 33.84718322753906, 33.84718322753906, 33.64677429199219, 33.48017120361328, 33.557395935058594, 33.493980407714844, 33.51911544799805, 33.59764862060547, 33.602542877197266, 33.71739196777344, 33.78643798828125, 33.808387756347656, 33.82889175415039, 33.839874267578125, 33.74327087402344, 33.66367721557617, 33.70228576660156, 33.77075958251953, 33.835472106933594, 33.93949890136719, 33.98492431640625, 34.006874084472656, 34.02737808227539, 33.93815231323242, 33.85845184326172, 33.90077209472656, 33.86906433105469, 33.78142547607422, 33.68730163574219, 33.70228576660156, 33.67058563232422, 33.7833251953125, 33.82465362548828, 33.815696716308594, 33.732391357421875, 33.656368255615234, 33.698585510253906, 33.67058563232422, 33.58294677734375, 33.48882293701172, 33.503807067871094, 33.672454833984375, 33.78932189941406, 33.70763397216797, 33.70178985595703, 33.839996337890625, 33.71611022949219, 33.68354034423828, 33.708984375, 33.86723327636719, 33.9878044128418, 34.10649871826172, 34.035728454589844, 34.01454162597656, 34.06340026855469, 34.06024169921875, 33.966583251953125, 33.95307540893555, 34.0531005859375, 34.122379302978516, 34.08476257324219, 34.035850524902344, 34.067100524902344, 34.06024169921875, 33.966583251953125, 33.88578414916992, 33.90077209472656, 33.86906433105469, 33.881629943847656, 33.970985412597656, 34.179412841796875, 34.251678466796875, 34.21706008911133, 34.15830612182617, 34.046791076660156, 34.10043716430664, 34.07810974121094, 34.08448791503906, 34.06929397583008, 34.145843505859375, 34.197994232177734, 34.21266174316406, 34.129356384277344, 34.03645706176758, 33.912044525146484, 33.932254791259766, 33.80331039428711, 33.70543670654297, 33.725914001464844, 33.77075958251953, 33.835472106933594, 33.839324951171875, 33.83259201049805, 33.839874267578125, 33.84347915649414, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 32.82276916503906, 32.840110778808594, 32.90266799926758, 32.922821044921875, 32.928062438964844, 32.95030975341797, 32.954010009765625, 33.05418395996094, 33.28491973876953, 33.27333068847656, 33.314178466796875, 33.32907485961914, 33.374969482421875, 33.423439025878906, 33.43447494506836, 33.44292449951172, 33.446563720703125, 33.4483642578125, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.450218200683594, 33.550392150878906, 33.469459533691406, 33.50100326538086, 33.627681732177734, 33.61792755126953, 33.655235290527344, 33.65538787841797, 33.644996643066406, 33.64869689941406, 33.64869689941406, 33.74887466430664, 33.66793441772461, 33.6321907043457, 33.67383575439453, 33.763450622558594, 33.822147369384766, 33.73912048339844, 33.649085998535156, 33.69498062133789, 33.6668815612793, 33.68315124511719, 33.67232894897461, 33.74887466430664, 33.66793441772461, 33.6321907043457, 33.67383575439453, 33.763450622558594, 33.822147369384766, 33.839324951171875, 33.732391357421875, 33.656368255615234, 33.798763275146484, 33.6898193359375, 33.73393249511719, 33.84979248046875, 33.81640625, 33.753509521484375, 33.670372009277344, 33.79875946044922, 33.82291030883789, 33.85014343261719, 33.75601577758789, 33.656368255615234, 33.79875946044922, 33.6898193359375, 33.66664123535156, 33.797637939453125, 33.68251037597656, 33.720603942871094, 33.716697692871094, 33.632904052734375, 33.6949577331543, 33.677276611328125, 33.669822692871094, 33.67232894897461, 33.64869689941406, 33.64869689941406, 33.64869689941406, 33.64869689941406, 33.64869689941406, 33.64869689941406, 33.74887466430664, 33.801025390625, 33.715492248535156, 33.71637725830078, 33.84730529785156, 33.719810485839844, 33.68354034423828, 33.708984375, 33.6668815612793, 33.7833251953125, 33.82465362548828, 33.81569290161133, 33.83259201049805, 33.839874267578125, 33.84347915649414, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.746978759765625, 33.73096466064453, 33.8546142578125, 33.82371520996094, 33.867042541503906, 33.85387420654297, 33.84347915649414, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 33.84718322753906, 34.04753875732422, 34.003482818603516, 33.99767303466797, 33.95075988769531, 33.869422912597656, 33.915958404541016, 33.89269256591797, 33.9818115234375, 34.02313995361328, 34.014183044433594, 33.930870056152344, 33.8548469543457, 33.99724578857422, 33.888301849365234, 33.8651237487793, 33.996124267578125, 33.88099670410156, 33.91908645629883, 34.048274993896484, 34.014892578125, 34.05220031738281, 34.0523567199707, 34.04196548461914, 34.04566192626953, 34.145843505859375, 34.064903259277344, 34.02915954589844, 34.070804595947266, 34.06024169921875, 34.06678771972656, 34.06929397583008, 34.04566192626953, 34.145843505859375, 34.064903259277344, 34.02915954589844, 34.070804595947266, 34.06024169921875, 34.06678771972656, 34.06929397583008, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566955566406, 34.04566955566406, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.145843505859375, 34.197994232177734, 34.21266174316406, 34.129356384277344, 34.12062454223633, 34.24787902832031, 34.22068405151367, 34.26401138305664, 34.25084686279297, 34.240447998046875, 34.24414825439453, 34.24414825439453, 34.143943786621094, 34.127933502197266, 34.251583099365234, 34.270774841308594, 34.27362823486328, 34.24258804321289, 34.253021240234375, 34.25143814086914, 34.25471115112305, 34.25596237182617, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.39441680908203, 34.328575134277344, 34.31113815307617, 34.35728454589844, 34.30420684814453, 34.268802642822266, 34.29457092285156, 34.25509262084961, 34.31146240234375, 34.26557922363281, 34.23589324951172, 34.256717681884766, 34.25143814086914, 34.25471115112305, 34.25596237182617, 34.24414825439453, 34.29423904418945, 34.55828857421875, 34.483951568603516, 34.47410583496094, 34.360015869140625, 34.505828857421875, 34.399696350097656, 34.36863708496094, 34.392154693603516, 34.35796356201172, 34.36450958251953, 34.467193603515625, 34.36262512207031, 34.326881408691406, 34.36852264404297, 34.35796356201172, 34.36450958251953, 34.36701965332031, 34.343387603759766, 34.343387603759766, 34.443565368652344, 34.4957160949707, 34.41018295288086, 34.34377670288086, 34.38966751098633, 34.361572265625, 34.377838134765625, 34.36701965332031, 34.343387603759766, 34.443565368652344, 34.36262512207031, 34.326881408691406, 34.36852264404297, 34.4581413269043, 34.51683807373047, 34.43381118774414, 34.34377670288086, 34.38966751098633, 34.361572265625, 34.377838134765625, 34.36701965332031, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.443565368652344, 34.4957160949707, 34.5103874206543, 34.52728271484375, 34.53456115722656, 34.437965393066406, 34.358367919921875, 34.396976470947266, 34.365272521972656, 34.377838134765625, 34.36701965332031, 34.29328918457031, 34.285282135009766, 34.28055953979492, 34.239906311035156, 34.27394104003906, 34.25843811035156, 34.25471115112305, 34.25596237182617, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.24414825439453, 34.29423904418945, 34.3203125, 34.32764434814453, 34.336097717285156, 34.3397331237793, 34.34153747558594, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.443565368652344, 34.36262512207031, 34.326881408691406, 34.46870422363281, 34.510292053222656, 34.53150939941406, 34.55091094970703, 34.53456115722656, 34.63835144042969, 34.6942024230957, 34.7088737487793, 34.72576904296875, 34.73305130004883, 34.736656188964844, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.840538024902344, 34.89268493652344, 34.90735626220703, 34.92424774169922, 34.9315299987793, 34.93513870239258, 34.938838958740234, 34.938838958740234, 33.974853515625, 34.00102233886719, 34.16958236694336, 34.27045440673828, 34.29615783691406, 34.32150650024414, 34.4326057434082, 34.35707473754883, 34.311859130859375, 34.31041717529297, 34.50637435913086, 34.50511169433594, 34.641719818115234, 34.682960510253906, 34.6922721862793, 34.722068786621094, 34.73305130004883, 34.736656188964844, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.840538024902344, 34.89268493652344, 35.007530212402344, 34.94348907470703, 34.91502380371094, 34.96027374267578, 34.95341491699219, 34.9599609375, 34.962467193603516, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.03901672363281, 34.95807647705078, 34.92232894897461, 34.96397399902344, 34.95341491699219, 34.9599609375, 34.962467193603516, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 35.089107513427734, 35.167335510253906, 35.13923263549805, 35.062957763671875, 34.96964645385742, 35.002567291259766, 34.97794723510742, 34.9776611328125, 34.962467193603516, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.938838958740234, 34.83863067626953, 34.755332946777344, 34.7939453125, 34.76224136352539, 34.87498474121094, 34.91631317138672, 34.90735626220703, 34.92424774169922, 34.831329345703125, 34.75163269042969, 34.7939453125, 34.76224136352539, 34.87498474121094, 34.78322219848633, 34.72385025024414, 34.76549530029297, 34.75493621826172, 34.76148223876953, 34.76398468017578, 34.7403564453125, 34.840538024902344, 34.75959396362305, 34.72385025024414, 34.76549530029297, 34.75493621826172, 34.76148223876953, 34.76398468017578, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.64015579223633, 34.55685043334961, 34.595462799072266, 34.563758850097656, 34.67649841308594, 34.584739685058594, 34.525367736816406, 34.56700897216797, 34.6566276550293, 34.71532440185547, 34.63229751586914, 34.542266845703125, 34.58815383911133, 34.56005859375, 34.57632064819336, 34.56549835205078, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.642051696777344, 34.56111145019531, 34.525367736816406, 34.56700897216797, 34.55644989013672, 34.56299591064453, 34.56549835205078, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.642051696777344, 34.56111145019531, 34.59265899658203, 34.71934127807617, 34.60938262939453, 34.630680084228516, 34.754486083984375, 34.71319580078125, 34.760215759277344, 34.747047424316406, 34.736656188964844, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 32.50297164916992, 32.533897399902344, 32.750099182128906, 32.8580436706543, 32.88918685913086, 32.92483139038086, 32.939388275146484, 33.046783447265625, 33.1063346862793, 33.121002197265625, 33.137901306152344, 33.145179748535156, 33.14878845214844, 33.152488708496094, 33.152488708496094, 33.25267028808594, 33.30481719970703, 33.31948471069336, 33.33638381958008, 33.34366226196289, 33.397361755371094, 33.42713928222656, 33.43447494506836, 33.44292449951172, 33.446563720703125, 33.4483642578125, 33.450218200683594, 33.450218200683594, 33.550392150878906, 33.602542877197266, 33.71739196777344, 33.78643798828125, 33.70818328857422, 33.645389556884766, 33.69498062133789, 33.6668815612793, 33.68315124511719, 33.77250671386719, 33.801025390625, 33.81569290161133, 33.83259201049805, 33.839874267578125, 33.84347915649414, 33.84718322753906, 33.947357177734375, 33.999507904052734, 34.014183044433594, 34.03107452392578, 34.038352966308594, 34.04196548461914, 34.04566192626953, 34.04566192626953, 34.04566192626953, 34.145843505859375, 34.197994232177734, 34.21266174316406, 34.22956085205078, 34.236839294433594, 34.240447998046875, 34.24414825439453, 34.24414825439453, 34.29423904418945, 34.3203125, 34.32764434814453, 34.336097717285156, 34.3397331237793, 34.34153747558594, 34.343387603759766, 34.343387603759766, 34.343387603759766, 34.443565368652344, 34.4957160949707, 34.5103874206543, 34.52728271484375, 34.53456115722656, 34.538169860839844, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.541873931884766, 34.642051696777344, 34.6942024230957, 34.7088737487793, 34.72576904296875, 34.73305130004883, 34.736656188964844, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.7403564453125, 34.840538024902344, 34.89268493652344, 34.90735626220703, 34.92424774169922, 34.9315299987793, 34.93513870239258, 34.938838958740234, 34.938838958740234, 35.03901672363281, 34.95807647705078, 34.9896240234375, 35.116302490234375, 35.10654830932617, 35.14385986328125, 35.14401626586914, 35.13362121582031, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.13732147216797, 35.18741226196289, 35.21348571777344, 35.220821380615234, 35.229270935058594, 35.23291015625, 35.23471450805664, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.23656463623047, 35.33673858642578, 35.388893127441406, 35.403564453125, 35.42046356201172, 35.427738189697266, 35.43135070800781, 35.43505096435547, 35.43505096435547, 35.53522491455078, 35.58737564086914, 35.60204315185547, 35.61894226074219, 35.626220703125, 35.62982940673828, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.63352966308594, 35.733707427978516, 35.785858154296875, 35.80052947998047, 35.81742858886719, 35.82470703125, 35.82831573486328, 35.83201599121094, 35.83201599121094, 35.83201599121094, 35.93218994140625, 35.984344482421875, 35.99901580810547, 36.01591110229492, 36.023189544677734, 36.02679443359375, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.03049850463867, 36.080589294433594, 36.10666275024414, 36.11399841308594, 36.1224479675293, 36.12608337402344, 36.12788772583008, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.129737854003906, 36.229915618896484, 36.282066345214844, 36.29673767089844, 36.313636779785156, 36.32091522216797, 36.32452392578125, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.32822036743164, 36.428401947021484, 36.480552673339844, 36.49522399902344, 36.512115478515625, 36.51939392089844, 36.52300262451172, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.52670669555664, 36.62688446044922, 36.67903137207031, 36.693702697753906, 36.710601806640625, 36.71788024902344, 36.72148895263672, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.725189208984375, 36.82536697387695, 36.87751770019531, 36.892189025878906, 36.909088134765625, 36.91636657714844, 36.91997528076172, 36.923675537109375, 36.923675537109375, 36.923675537109375, 36.97376251220703, 36.99983596801758, 37.007171630859375, 36.965518951416016, 36.92750930786133, 36.99870681762695, 36.94423294067383, 36.96629333496094, 37.02421569824219, 37.00752639770508, 37.026180267333984, 37.02626037597656, 37.021060943603516, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022918701171875, 37.022918701171875, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.022911071777344, 37.12309265136719, 37.17523956298828, 37.18991470336914, 37.206809997558594, 37.214088439941406, 37.21769714355469, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.221397399902344, 37.321571350097656, 37.37372589111328, 37.388397216796875, 37.405296325683594, 37.412574768066406, 37.41618347167969, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.419883728027344, 37.52006149291992, 37.572208404541016, 37.586875915527344, 37.60377502441406, 37.611053466796875, 37.614662170410156, 37.61836242675781, 37.61836242675781, 37.51816177368164, 37.314910888671875, 37.1064567565918, 37.161468505859375, 37.101131439208984, 37.100555419921875, 37.070167541503906, 37.022911071777344, 36.772403717041016, 36.56414794921875, 36.86103439331055]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_MB\", \"type\": \"scatter\", \"y\": [34.5, 34.9, 35.1, 34.7, 34.3, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.5, 34.3, 34.0, 34.0, 34.0, 34.0, 33.6, 33.1, 32.9, 34.2, 33.8, 33.4, 33.4, 33.8, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 32.0, 32.2, 32.2, 32.2, 32.0, 31.8, 31.8, 32.0, 32.0, 32.0, 31.8, 31.8, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.5, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.6, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.1, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.2, 35.2, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.3, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.2, 34.2, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.8, 33.8, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.2, 34.3, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.2, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 33.8, 33.8, 34.0, 33.8, 33.8, 34.0, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.7, 34.5, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.0, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.2, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.2, 35.2, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 32.5, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.5, 36.5, 36.9, 36.7]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_test_MB_ITERATIVE\", \"type\": \"scatter\", \"y\": [34.507022857666016, 34.52001190185547, 34.57439422607422, 34.58429718017578, 34.62986373901367, 34.663780212402344, 34.683780670166016, 34.703399658203125, 34.72844696044922, 34.75269317626953, 34.7767448425293, 34.8018798828125, 34.82645034790039, 34.850929260253906, 34.8747673034668, 34.898502349853516, 34.922271728515625, 34.9459342956543, 34.969482421875, 34.99296951293945, 35.01634216308594, 35.03956604003906, 35.062679290771484, 35.085697174072266, 35.108604431152344, 35.13140869140625, 35.15411376953125, 35.17671585083008, 35.1992073059082, 35.22159194946289, 35.24386978149414, 35.26604461669922, 35.28812026977539, 35.31009292602539, 35.331966400146484, 35.353736877441406, 35.37540817260742, 35.396976470947266, 35.41844177246094, 35.43981170654297, 35.46107482910156, 35.48224639892578, 35.503318786621094, 35.5242919921875, 35.545166015625, 35.565948486328125, 35.586631774902344, 35.60721969604492, 35.627708435058594, 35.64810562133789, 35.66840744018555, 35.68861389160156, 35.7087287902832, 35.72875213623047, 35.748680114746094, 35.76851272583008, 35.78825378417969, 35.80790328979492, 35.82746124267578, 35.846927642822266, 35.86630630493164, 35.885597229003906, 35.9047966003418, 35.92390441894531, 35.94292449951172, 35.961856842041016, 35.98070526123047, 35.99946212768555, 36.01813507080078, 36.036720275878906, 36.05521774291992, 36.07362747192383, 36.091949462890625, 36.11018753051758, 36.12834548950195, 36.146419525146484, 36.16440963745117, 36.182315826416016, 36.20013427734375, 36.217872619628906, 36.235530853271484, 36.25310516357422, 36.27059555053711, 36.28800964355469, 36.30533981323242, 36.32258605957031, 36.33975601196289, 36.35684585571289, 36.37385559082031, 36.39078903198242, 36.40764236450195, 36.424415588378906, 36.44111251831055, 36.457733154296875, 36.47427749633789, 36.490745544433594, 36.50713348388672, 36.52344512939453, 36.5396842956543, 36.55584716796875, 36.57193374633789, 36.587947845458984, 36.603885650634766, 36.619747161865234, 36.635536193847656, 36.6512565612793, 36.666900634765625, 36.682472229003906, 36.697975158691406, 36.71340560913086, 36.728763580322266, 36.744049072265625, 36.75926208496094, 36.77440643310547, 36.78947830200195, 36.804481506347656, 36.81941604614258, 36.83427810668945, 36.84907150268555, 36.863800048828125, 36.87845993041992, 36.89304733276367, 36.907569885253906, 36.92202377319336, 36.93640899658203, 36.95073318481445, 36.96499252319336, 36.97917938232422, 36.9932975769043, 37.007354736328125, 37.02134704589844, 37.035274505615234, 37.049137115478516, 37.06293487548828, 37.07666778564453, 37.09033966064453, 37.10394287109375, 37.11748504638672, 37.13096618652344, 37.144386291503906, 37.15774154663086, 37.1710319519043, 37.18426513671875, 37.19743728637695, 37.21054458618164, 37.22359085083008, 37.236576080322266, 37.2495002746582, 37.262367248535156, 37.275177001953125, 37.28792953491211, 37.300621032714844, 37.31325149536133, 37.32582473754883, 37.33833694458008, 37.350791931152344, 37.363189697265625, 37.37553024291992, 37.38780975341797, 37.40003204345703, 37.412200927734375, 37.42431640625, 37.436370849609375, 37.448368072509766, 37.46031188964844, 37.47220230102539, 37.48403549194336, 37.49581527709961, 37.50754165649414, 37.51921463012695, 37.53083038330078, 37.54239273071289, 37.55390167236328, 37.56535339355469, 37.57675552368164, 37.58810043334961, 37.59939956665039, 37.61064529418945, 37.6218376159668, 37.63297653198242, 37.644065856933594, 37.65509796142578, 37.66608428955078, 37.67701721191406, 37.687896728515625, 37.69873046875, 37.70951461791992, 37.720245361328125, 37.730926513671875, 37.74156188964844, 37.75214385986328, 37.76268005371094, 37.773162841796875, 37.78360366821289, 37.79399490356445, 37.8043327331543, 37.81462097167969, 37.824867248535156, 37.83506774902344, 37.845218658447266, 37.855323791503906, 37.865379333496094, 37.87538528442383, 37.88534927368164, 37.895267486572266, 37.90513229370117, 37.914955139160156, 37.92473602294922, 37.934471130371094, 37.944156646728516, 37.953800201416016, 37.96339797973633, 37.97294998168945, 37.98245620727539, 37.99192428588867, 38.00135040283203, 38.0107307434082, 38.02006530761719, 38.02935791015625, 38.038604736328125, 38.04780960083008, 38.05697250366211, 38.06608963012695, 38.07516860961914, 38.084205627441406, 38.09319305419922, 38.10214614868164, 38.11105728149414, 38.11992645263672, 38.128753662109375, 38.13753890991211, 38.14628601074219, 38.154991149902344, 38.16365432739258, 38.17228317260742, 38.18086624145508, 38.18940734863281, 38.197914123535156, 38.20637893676758, 38.21480941772461, 38.22319412231445, 38.231544494628906, 38.23985290527344, 38.24811935424805, 38.256351470947266, 38.26454544067383, 38.272701263427734, 38.28082275390625, 38.28890609741211, 38.29695129394531, 38.304954528808594, 38.312923431396484, 38.32085418701172, 38.3287467956543, 38.33660888671875, 38.34442901611328, 38.352210998535156, 38.359962463378906, 38.36767578125, 38.37535095214844, 38.38298797607422, 38.390594482421875, 38.39816665649414, 38.40570068359375, 38.413204193115234, 38.42066955566406, 38.428096771240234, 38.435489654541016, 38.4428596496582, 38.450191497802734, 38.45748519897461, 38.464744567871094, 38.47197341918945, 38.47916793823242, 38.486328125, 38.49345779418945, 38.50054931640625, 38.507606506347656, 38.5146369934082, 38.521629333496094, 38.52859115600586, 38.535526275634766, 38.54242706298828, 38.54928970336914, 38.55611801147461, 38.56291961669922, 38.5696907043457, 38.57643127441406, 38.5831413269043, 38.58981704711914, 38.59646224975586, 38.60307693481445, 38.60966110229492, 38.616214752197266, 38.622737884521484, 38.629234313964844, 38.63569641113281, 38.642127990722656, 38.64853286743164, 38.6549072265625, 38.661251068115234, 38.667564392089844, 38.673851013183594, 38.68010330200195, 38.68633270263672, 38.69253158569336, 38.698699951171875, 38.70484161376953, 38.71095275878906, 38.717037200927734, 38.72309494018555, 38.72911834716797, 38.7351188659668, 38.7410888671875, 38.747032165527344, 38.75294494628906, 38.75883483886719, 38.76469802856445, 38.770530700683594, 38.776336669921875, 38.7821159362793, 38.787864685058594, 38.79359436035156, 38.79930114746094, 38.80497360229492, 38.81061935424805, 38.81624221801758, 38.82183837890625, 38.82740783691406, 38.832950592041016, 38.838462829589844, 38.843955993652344, 38.84942626953125, 38.85486602783203, 38.86028289794922, 38.86567306518555, 38.87104034423828, 38.87637710571289, 38.88169479370117, 38.886985778808594, 38.892250061035156, 38.897491455078125, 38.9027099609375, 38.90790557861328, 38.9130744934082, 38.91822052001953, 38.92333984375, 38.92844009399414, 38.93351364135742, 38.93856430053711, 38.9435920715332, 38.94859313964844, 38.953575134277344, 38.958534240722656, 38.963470458984375, 38.9683837890625, 38.97327423095703, 38.97814178466797, 38.98298263549805, 38.9878044128418, 38.99260330200195, 38.997379302978516, 39.00213623046875, 39.00687026977539, 39.01158142089844, 39.01626968383789, 39.02093505859375, 39.02558135986328, 39.03020477294922, 39.03480911254883, 39.03938674926758, 39.0439453125, 39.04848098754883, 39.053001403808594, 39.0574951171875, 39.06196975708008, 39.06642532348633, 39.07086181640625, 39.07527542114258, 39.07966613769531, 39.08403778076172, 39.0883903503418, 39.09272003173828, 39.0970344543457, 39.1013298034668, 39.1056022644043, 39.10985565185547, 39.11408996582031, 39.1182975769043, 39.12248611450195, 39.12665939331055, 39.13081359863281, 39.134952545166016, 39.13907241821289, 39.143165588378906, 39.147239685058594, 39.15129852294922, 39.155338287353516, 39.15935516357422, 39.16335678100586, 39.16733932495117, 39.17130661010742, 39.17525100708008, 39.17918014526367, 39.1830940246582, 39.18698501586914, 39.19085693359375, 39.1947135925293, 39.198551177978516, 39.202369689941406, 39.206172943115234, 39.209957122802734, 39.213722229003906, 39.21747589111328, 39.22121047973633, 39.22492218017578, 39.22861862182617, 39.2322998046875, 39.2359619140625, 39.23960876464844, 39.24324035644531, 39.24685287475586, 39.250450134277344, 39.2540283203125, 39.257591247558594, 39.261138916015625, 39.264671325683594, 39.268184661865234, 39.27168273925781, 39.27516555786133, 39.278629302978516, 39.282073974609375, 39.28550720214844, 39.28892135620117, 39.292320251464844, 39.29570388793945, 39.299076080322266, 39.30242919921875, 39.30576705932617, 39.30908966064453, 39.31239700317383, 39.3156852722168, 39.318965911865234, 39.322227478027344, 39.32547378540039, 39.328704833984375, 39.3319206237793, 39.33511734008789, 39.33830261230469, 39.34147262573242, 39.344627380371094, 39.3477668762207, 39.35089874267578, 39.3540153503418, 39.357112884521484, 39.36019515991211, 39.36326599121094, 39.3663215637207, 39.369361877441406, 39.37239074707031, 39.375404357910156, 39.37840270996094, 39.381385803222656, 39.38435745239258, 39.3873176574707, 39.39026641845703, 39.39319610595703, 39.396114349365234, 39.399017333984375, 39.40190505981445, 39.40477752685547, 39.40764236450195, 39.410491943359375, 39.413326263427734, 39.4161491394043, 39.41896057128906, 39.42176055908203, 39.42454528808594, 39.42731475830078, 39.43007278442383, 39.43281936645508, 39.43555450439453, 39.43827819824219, 39.440982818603516, 39.44367218017578, 39.44635009765625, 39.44902038574219, 39.45167922973633, 39.45432662963867, 39.45696258544922, 39.45958709716797, 39.46219253540039, 39.46479034423828, 39.46737289428711, 39.46994400024414, 39.47250747680664, 39.47505187988281, 39.47758483886719, 39.48011016845703, 39.482627868652344, 39.485130310058594, 39.48761749267578, 39.49009704589844, 39.4925651550293, 39.495018005371094, 39.497459411621094, 39.4998893737793, 39.5023078918457, 39.504722595214844, 39.50712585449219, 39.50951385498047, 39.51189041137695, 39.51425552368164, 39.51660919189453, 39.51895523071289, 39.52129364013672, 39.52361297607422, 39.52592468261719, 39.52822494506836, 39.530513763427734, 39.53279495239258, 39.535064697265625, 39.537322998046875, 39.53956985473633, 39.541805267333984, 39.544036865234375, 39.5462532043457, 39.548458099365234, 39.550655364990234, 39.5528450012207, 39.55501937866211, 39.55718231201172, 39.5593376159668, 39.561485290527344, 39.56361770629883, 39.56574630737305, 39.56786346435547, 39.56996536254883, 39.572059631347656, 39.57415008544922, 39.57622528076172, 39.57829284667969, 39.580352783203125, 39.582401275634766, 39.584434509277344, 39.586463928222656, 39.58848190307617, 39.590492248535156, 39.59249496459961, 39.594486236572266, 39.596466064453125, 39.59843826293945, 39.600399017333984, 39.602352142333984, 39.60429763793945, 39.606239318847656, 39.60816955566406, 39.61008834838867, 39.611995697021484, 39.613895416259766, 39.615787506103516, 39.61766815185547, 39.619544982910156, 39.62141036987305, 39.62327194213867, 39.6251220703125, 39.62696075439453, 39.628787994384766, 39.630611419677734, 39.63242721557617, 39.63422775268555, 39.63602828979492, 39.637821197509766, 39.63960266113281, 39.64137649536133, 39.64314270019531, 39.6448974609375, 39.646644592285156, 39.64838409423828, 39.65011978149414, 39.6518440246582, 39.653560638427734, 39.65526580810547, 39.65696716308594, 39.658660888671875, 39.66034698486328, 39.66202163696289, 39.663692474365234, 39.66535568237305, 39.66700744628906, 39.66865158081055, 39.6702880859375, 39.67192077636719, 39.67354202270508, 39.67515563964844, 39.67676544189453, 39.678367614746094, 39.679962158203125, 39.681549072265625, 39.683128356933594, 39.684696197509766, 39.68626022338867, 39.68782043457031, 39.68937301635742, 39.690914154052734, 39.692447662353516, 39.693973541259766, 39.69549560546875, 39.69700622558594, 39.698509216308594, 39.70001220703125, 39.701507568359375, 39.70299530029297, 39.704471588134766, 39.70594024658203, 39.70740509033203, 39.7088623046875, 39.71031188964844, 39.71175765991211, 39.71319580078125, 39.714630126953125, 39.71605682373047, 39.71747589111328, 39.71888732910156, 39.72029113769531, 39.72168731689453, 39.723079681396484, 39.724464416503906, 39.7258415222168, 39.72721481323242, 39.72858428955078, 39.729942321777344, 39.731292724609375, 39.73263931274414, 39.73398208618164, 39.73531723022461, 39.73664855957031, 39.737972259521484, 39.739288330078125, 39.740596771240234, 39.74190139770508, 39.743202209472656, 39.7444953918457, 39.74578094482422, 39.7470588684082, 39.748329162597656, 39.74959945678711, 39.75086212158203, 39.75211715698242, 39.75336837768555, 39.75461196899414, 39.75585174560547, 39.757083892822266, 39.75830841064453, 39.75952911376953, 39.760746002197266, 39.761959075927734, 39.76316452026367, 39.764366149902344, 39.76555633544922, 39.76674270629883, 39.76792526245117, 39.769100189208984, 39.770267486572266, 39.77143478393555, 39.77259826660156, 39.77375411987305, 39.77490997314453, 39.77605438232422, 39.777191162109375, 39.778324127197266, 39.77945327758789, 39.78057861328125, 39.78169631958008, 39.78281021118164, 39.78391647338867, 39.78501892089844, 39.78611755371094, 39.78721237182617, 39.788299560546875, 39.78937911987305, 39.79045486450195, 39.791526794433594, 39.79259490966797, 39.79365921020508, 39.79471969604492, 39.795772552490234, 39.796817779541016, 39.79785919189453, 39.798892974853516, 39.799930572509766, 39.800960540771484, 39.80198669433594, 39.803001403808594, 39.80401611328125, 39.80502700805664, 39.806034088134766, 39.80703353881836, 39.80803298950195, 39.809024810791016, 39.81000900268555, 39.81098556518555, 39.81196212768555, 39.81293487548828, 39.813907623291016, 39.81487274169922, 39.81583023071289, 39.81678771972656, 39.8177375793457, 39.81868362426758, 39.81962585449219, 39.820560455322266, 39.821495056152344, 39.822425842285156, 39.8233528137207, 39.82427215576172, 39.8251838684082, 39.82609176635742, 39.82699966430664, 39.82790756225586, 39.82880783081055, 39.8297004699707, 39.830596923828125, 39.83148193359375, 39.83236312866211, 39.8332405090332, 39.83411407470703, 39.834983825683594, 39.83584976196289, 39.83671188354492, 39.83757019042969, 39.83842086791992, 39.83926773071289, 39.84011459350586, 39.84095764160156, 39.841793060302734, 39.842628479003906, 39.84345626831055, 39.84428405761719, 39.84510803222656, 39.84592819213867, 39.846744537353516, 39.847557067871094, 39.84836196899414, 39.84916305541992, 39.8499641418457, 39.85075759887695, 39.85154724121094, 39.85233688354492, 39.853126525878906, 39.85390853881836, 39.85468292236328, 39.8554573059082, 39.856231689453125, 39.856998443603516, 39.857765197753906, 39.85852813720703, 39.85928726196289, 39.86003875732422, 39.86078643798828, 39.861534118652344, 39.86227798461914, 39.863014221191406, 39.86375045776367, 39.86448287963867, 39.86521530151367, 39.86594009399414, 39.86665725708008, 39.86737823486328, 39.86809539794922, 39.86880874633789, 39.8695182800293, 39.87022018432617, 39.87091827392578, 39.871620178222656, 39.872318267822266, 39.87301254272461, 39.87369918823242, 39.87438201904297, 39.875064849853516, 39.87574768066406, 39.87642288208008, 39.87709426879883, 39.877769470214844, 39.87843704223633, 39.87910079956055, 39.8797607421875, 39.88041687011719, 39.881072998046875, 39.88172912597656, 39.882381439208984, 39.88302993774414, 39.88367462158203, 39.884315490722656, 39.884952545166016, 39.885581970214844, 39.88621139526367, 39.886844635009766, 39.887474060058594, 39.888092041015625, 39.88871383666992, 39.88933181762695, 39.88994216918945, 39.89055252075195, 39.89116287231445, 39.89176940917969, 39.892372131347656, 39.892974853515625, 39.89356994628906, 39.894168853759766, 39.89476013183594, 39.895347595214844, 39.895931243896484, 39.896514892578125, 39.8970947265625, 39.897674560546875, 39.898250579833984, 39.89882278442383, 39.899391174316406, 39.89995574951172, 39.90052032470703, 39.90108108520508, 39.901641845703125, 39.902198791503906, 39.90275192260742, 39.90330505371094, 39.90385437011719, 39.90439987182617, 39.90494155883789, 39.90548324584961, 39.9060173034668, 39.906558990478516, 39.9070930480957, 39.907623291015625, 39.90815353393555, 39.9086799621582, 39.909202575683594, 39.909725189208984, 39.910240173339844, 39.9107551574707, 39.91127014160156, 39.911781311035156, 39.912288665771484, 39.91279983520508, 39.91330337524414, 39.9138069152832, 39.914310455322266, 39.91481018066406, 39.915306091308594, 39.91579818725586, 39.916290283203125, 39.91678237915039, 39.91727066040039, 39.917755126953125, 39.918235778808594, 39.9187126159668, 39.919193267822266, 39.91967010498047, 39.92014694213867, 39.92061996459961, 39.92108917236328, 39.92155456542969, 39.92201614379883, 39.92247772216797, 39.92293930053711, 39.92340087890625, 39.92385482788086, 39.9243049621582, 39.92475891113281, 39.925209045410156, 39.9256591796875, 39.92610168457031, 39.926544189453125, 39.92698287963867, 39.92742156982422, 39.927860260009766, 39.92829895019531, 39.92873001098633, 39.929161071777344, 39.929588317871094, 39.930015563964844, 39.93043899536133, 39.93086242675781, 39.93128204345703, 39.931697845458984, 39.9321174621582, 39.932533264160156, 39.932945251464844, 39.93335723876953, 39.93376541137695, 39.934173583984375, 39.934574127197266, 39.93497848510742, 39.93537902832031, 39.93578338623047, 39.936180114746094, 39.93657302856445, 39.93696594238281, 39.93736267089844, 39.9377555847168, 39.938140869140625, 39.93852615356445, 39.93891143798828, 39.939292907714844, 39.93967819213867, 39.94005584716797, 39.9404296875, 39.9408073425293, 39.94118118286133, 39.94155502319336, 39.94192886352539, 39.94230270385742, 39.94266891479492, 39.943031311035156, 39.943397521972656, 39.943756103515625, 39.94411849975586, 39.94447708129883, 39.9448356628418, 39.9451904296875, 39.9455451965332, 39.945899963378906, 39.946250915527344, 39.946598052978516, 39.94694519042969, 39.94729232788086, 39.94763946533203, 39.94798278808594, 39.94832229614258, 39.94866180419922, 39.948997497558594, 39.94933319091797, 39.94967269897461, 39.95000457763672, 39.95033645629883, 39.9506721496582, 39.95100021362305, 39.951324462890625, 39.95165252685547, 39.95197677612305, 39.95229721069336, 39.95261764526367, 39.95293426513672, 39.95325469970703, 39.95357131958008, 39.95388412475586, 39.95419692993164, 39.95450973510742, 39.9548225402832, 39.95513153076172, 39.955440521240234, 39.95574951171875, 39.956058502197266, 39.95635986328125, 39.956661224365234, 39.95696258544922, 39.9572639465332, 39.95756149291992, 39.957855224609375, 39.958152770996094, 39.95844268798828, 39.958736419677734, 39.95903015136719, 39.95932388305664, 39.95961380004883, 39.95989990234375, 39.960182189941406, 39.96046829223633, 39.960750579833984, 39.961036682128906, 39.9613151550293, 39.96159362792969, 39.96187210083008, 39.96215057373047, 39.962425231933594, 39.96269989013672, 39.962974548339844, 39.9632453918457, 39.96351623535156, 39.96378707885742, 39.964054107666016, 39.964317321777344, 39.96458435058594, 39.964847564697266, 39.965110778808594, 39.965370178222656, 39.96562957763672, 39.96588897705078, 39.96615219116211, 39.96641159057617, 39.9666633605957, 39.9669189453125, 39.96717071533203, 39.9674186706543, 39.96767044067383, 39.967918395996094, 39.96816635131836, 39.968414306640625, 39.96866226196289, 39.96890640258789, 39.969154357910156, 39.969398498535156, 39.96963882446289, 39.969879150390625, 39.970115661621094, 39.97035217285156, 39.9705924987793, 39.970829010009766, 39.971065521240234, 39.9713020324707, 39.971534729003906, 39.97176742553711, 39.97199630737305, 39.972225189208984, 39.97245788574219, 39.97268295288086, 39.97290802001953, 39.97312927246094, 39.97335433959961, 39.973575592041016, 39.97379684448242, 39.97401809692383, 39.9742431640625, 39.97446060180664, 39.97467803955078, 39.97489547729492, 39.9751091003418, 39.97532653808594, 39.97554397583008, 39.97575759887695, 39.97596740722656, 39.97617721557617, 39.97638702392578, 39.97659683227539, 39.976802825927734, 39.977012634277344, 39.97721862792969, 39.977420806884766, 39.97762680053711, 39.97783279418945, 39.97803497314453, 39.97823715209961, 39.97843933105469, 39.9786376953125, 39.97883605957031, 39.97903823852539, 39.9792366027832, 39.979427337646484, 39.9796257019043, 39.979820251464844, 39.98001480102539, 39.98020553588867, 39.98040008544922, 39.980587005615234, 39.98077392578125, 39.98096466064453, 39.98115539550781, 39.981346130371094, 39.98153305053711, 39.98171615600586, 39.98189926147461, 39.98208236694336, 39.98226547241211, 39.98244857788086, 39.98263168334961, 39.982810974121094, 39.98299026489258, 39.9831657409668, 39.98334503173828, 39.9835205078125, 39.983699798583984, 39.98387908935547, 39.98405456542969, 39.984222412109375, 39.984397888183594, 39.98456573486328, 39.98473358154297, 39.98490524291992, 39.985076904296875, 39.98524856567383, 39.985416412353516, 39.9855842590332, 39.985748291015625, 39.98591232299805, 39.98607635498047, 39.98624038696289, 39.98640823364258, 39.986572265625, 39.98673629760742, 39.98689270019531, 39.987056732177734, 39.987213134765625, 39.987369537353516, 39.98753356933594, 39.98768615722656, 39.98783874511719, 39.98799514770508, 39.9881477355957, 39.98830032348633, 39.98845672607422, 39.988609313964844, 39.98876190185547, 39.98891067504883, 39.98906707763672, 39.98921585083008, 39.98936462402344, 39.9895133972168, 39.98965835571289, 39.98980712890625, 39.98995590209961, 39.99010467529297, 39.99024963378906, 39.990394592285156, 39.99053955078125, 39.990684509277344, 39.9908332824707, 39.99097442626953, 39.99111557006836, 39.99125671386719, 39.991397857666016, 39.99153518676758, 39.991676330566406, 39.99181365966797, 39.991947174072266, 39.99208450317383, 39.992218017578125, 39.99235534667969, 39.99249267578125, 39.99262619018555, 39.99276351928711, 39.992897033691406, 39.9930305480957, 39.9931640625, 39.9932975769043, 39.99342727661133, 39.99355697631836, 39.993690490722656, 39.99381637573242, 39.99394226074219, 39.994075775146484, 39.994197845458984, 39.99432373046875, 39.99445343017578, 39.99457931518555, 39.99470520019531, 39.99482727050781, 39.99494934082031, 39.99507141113281, 39.99519348144531, 39.99531936645508, 39.99543762207031, 39.99555969238281, 39.99567794799805, 39.99579620361328, 39.99591827392578, 39.99603271484375, 39.99615478515625, 39.99626922607422, 39.99638748168945, 39.99650192260742, 39.99661636352539, 39.996734619140625, 39.996849060058594, 39.9969596862793, 39.9970703125, 39.99718475341797, 39.99729919433594, 39.99740982055664, 39.99752426147461, 39.99763488769531, 39.997745513916016, 39.99785614013672, 39.99796676635742, 39.998077392578125, 39.99818801879883, 39.99829864501953, 39.99840545654297, 39.998512268066406, 39.99861526489258, 39.998722076416016, 39.99882888793945, 39.998931884765625, 39.99903869628906, 39.999141693115234, 39.99924850463867, 39.999351501464844, 39.99945831298828, 39.99955749511719, 39.99966049194336, 39.999759674072266, 39.99986267089844, 39.999961853027344, 40.00006103515625, 40.00016403198242, 40.00025939941406, 40.00035858154297, 40.000457763671875, 40.000553131103516, 40.000648498535156, 40.0007438659668, 40.00083923339844, 40.00093460083008, 40.001033782958984, 40.00112533569336, 40.001220703125, 40.001312255859375, 40.001407623291016, 40.00149917602539, 40.001590728759766, 40.00168228149414, 40.00177764892578, 40.00186538696289, 40.001956939697266, 40.002044677734375, 40.002132415771484, 40.00222396850586, 40.00231170654297, 40.00239944458008, 40.00248718261719, 40.0025749206543, 40.002662658691406, 40.002750396728516, 40.00283432006836, 40.00292205810547, 40.00300598144531, 40.00309371948242, 40.00318145751953, 40.00326919555664, 40.00335693359375, 40.003440856933594, 40.00352096557617, 40.003604888916016, 40.00368881225586, 40.00376892089844, 40.003849029541016, 40.003936767578125, 40.0040168762207, 40.00409698486328, 40.00417709350586, 40.00425720214844, 40.004337310791016, 40.004417419433594, 40.00449752807617, 40.00457763671875, 40.00465393066406, 40.00473403930664, 40.00481033325195, 40.004886627197266, 40.004966735839844, 40.00504684448242, 40.005123138427734, 40.00519943237305, 40.00527572631836, 40.005348205566406, 40.00542068481445, 40.005496978759766, 40.00557327270508, 40.005645751953125, 40.005714416503906, 40.00579071044922, 40.005863189697266, 40.00593948364258, 40.006011962890625, 40.006080627441406, 40.00615310668945, 40.006229400634766, 40.00629425048828, 40.00636672973633, 40.006431579589844, 40.006500244140625, 40.00657272338867, 40.00664138793945, 40.006710052490234, 40.006778717041016, 40.00684356689453, 40.00691223144531, 40.00697708129883, 40.00704574584961, 40.00711441040039, 40.00718307495117, 40.00724792480469, 40.00731658935547, 40.007381439208984, 40.007450103759766, 40.007511138916016, 40.00757598876953, 40.00764083862305, 40.00770950317383, 40.00777053833008, 40.007835388183594, 40.00790023803711, 40.00796127319336, 40.008026123046875, 40.00808334350586, 40.00814437866211, 40.00820541381836, 40.008270263671875, 40.00832748413086, 40.008392333984375, 40.00844955444336, 40.00851058959961, 40.008567810058594, 40.00863265991211, 40.008689880371094, 40.008750915527344, 40.00880432128906, 40.00886154174805, 40.0089225769043, 40.00897979736328, 40.009033203125, 40.009090423583984, 40.00914764404297, 40.00920867919922, 40.0092658996582, 40.00932312011719, 40.00938034057617, 40.009437561035156, 40.00949478149414, 40.009552001953125, 40.009605407714844, 40.00966262817383, 40.00971984863281, 40.00977325439453, 40.00982666015625, 40.00988006591797, 40.00992965698242, 40.00998306274414, 40.01003646850586, 40.01008987426758, 40.0101432800293, 40.010196685791016, 40.01025390625, 40.01030349731445, 40.010353088378906, 40.010406494140625, 40.01045608520508, 40.0105094909668, 40.010562896728516, 40.01061248779297, 40.01066207885742, 40.010711669921875, 40.01076126098633, 40.01081085205078, 40.010860443115234, 40.01091384887695, 40.010963439941406, 40.01101303100586, 40.01105880737305, 40.011104583740234, 40.01115036010742, 40.011199951171875, 40.01124572753906, 40.011295318603516, 40.01134490966797, 40.01139450073242, 40.011444091796875, 40.01148986816406, 40.01153564453125, 40.01158142089844, 40.01162338256836, 40.01166915893555, 40.011714935302734, 40.01176071166992, 40.01180648803711, 40.0118522644043, 40.011898040771484, 40.011940002441406, 40.011985778808594, 40.01203155517578, 40.0120735168457, 40.012115478515625, 40.01216125488281, 40.012203216552734, 40.012245178222656, 40.012290954589844, 40.012332916259766, 40.01237487792969, 40.012413024902344, 40.012454986572266, 40.01250076293945, 40.012542724609375, 40.0125846862793, 40.01262664794922, 40.01266860961914, 40.0127067565918, 40.012752532958984, 40.012786865234375, 40.01282501220703, 40.01285934448242, 40.01290512084961, 40.012939453125, 40.01298141479492, 40.01301574707031, 40.013057708740234, 40.01309585571289, 40.01313400268555, 40.01317596435547, 40.01321029663086, 40.013248443603516, 40.013282775878906, 40.01332473754883, 40.01335906982422, 40.01340103149414, 40.0134391784668, 40.01347351074219, 40.013511657714844, 40.013545989990234, 40.013587951660156, 40.01362609863281, 40.0136604309082, 40.01369857788086, 40.01373291015625, 40.013771057128906, 40.0138053894043, 40.01383972167969, 40.013877868652344, 40.013912200927734, 40.013946533203125, 40.01398468017578, 40.01401901245117, 40.01405334472656, 40.01408767700195, 40.014122009277344, 40.014156341552734, 40.014190673828125, 40.014225006103516, 40.014259338378906, 40.01428985595703, 40.01432418823242, 40.01435852050781, 40.01438903808594, 40.01442337036133, 40.01445770263672, 40.014488220214844, 40.014522552490234, 40.01455307006836, 40.01458740234375, 40.014617919921875, 40.0146484375, 40.01468276977539, 40.014713287353516, 40.01474380493164, 40.01477813720703, 40.01480484008789, 40.01483917236328, 40.01486587524414, 40.014892578125, 40.014923095703125, 40.01495361328125, 40.014984130859375, 40.0150146484375, 40.015045166015625, 40.015071868896484, 40.01510238647461, 40.015132904052734, 40.01516342163086, 40.015193939208984, 40.01522445678711, 40.015254974365234, 40.01528549194336, 40.01531219482422, 40.015342712402344, 40.0153694152832, 40.01539993286133, 40.01542663574219, 40.01545715332031, 40.01548385620117, 40.0155143737793, 40.01554489135742, 40.01557159423828, 40.01559829711914, 40.015621185302734, 40.01565170288086, 40.01567840576172, 40.01570510864258, 40.01572799682617, 40.01575469970703, 40.015785217285156, 40.01580810546875, 40.01583480834961, 40.01586151123047, 40.01588821411133, 40.01591491699219, 40.01594161987305, 40.015968322753906, 40.015995025634766, 40.01601791381836, 40.01604461669922, 40.01606750488281, 40.01609420776367, 40.016117095947266, 40.016143798828125, 40.016170501708984, 40.01619338989258, 40.01621627807617, 40.01624298095703, 40.016265869140625, 40.01628875732422, 40.01631546020508, 40.01633834838867, 40.016361236572266, 40.01638412475586, 40.01641082763672, 40.01643371582031, 40.01646041870117, 40.0164794921875, 40.01649856567383, 40.01652145385742, 40.016544342041016, 40.016571044921875, 40.01659393310547, 40.01661682128906, 40.016639709472656, 40.016658782958984, 40.01668167114258, 40.016700744628906, 40.0167236328125, 40.01673889160156, 40.016761779785156, 40.016780853271484, 40.016807556152344, 40.01683044433594, 40.01685333251953, 40.016876220703125, 40.01689529418945, 40.01691436767578, 40.016937255859375, 40.016963958740234, 40.0169792175293, 40.016998291015625, 40.01701736450195, 40.01704406738281, 40.017066955566406, 40.01708221435547, 40.01710510253906, 40.01712417602539, 40.017147064208984, 40.01716613769531, 40.01718521118164, 40.017208099365234, 40.01722717285156, 40.01724624633789, 40.017269134521484, 40.01728820800781, 40.017311096191406, 40.017330169677734, 40.0173454284668, 40.017364501953125, 40.01738357543945, 40.01740264892578, 40.01742172241211, 40.01744079589844, 40.01746368408203, 40.01748275756836, 40.01750183105469, 40.01751708984375, 40.01753616333008, 40.017555236816406, 40.017574310302734, 40.01759338378906, 40.01761245727539, 40.01762771606445, 40.01764678955078, 40.017662048339844, 40.01768112182617, 40.0177001953125, 40.01771926879883, 40.01773452758789, 40.01775360107422, 40.01776885986328, 40.01778793334961, 40.01780319213867, 40.017818450927734, 40.01783752441406, 40.01785659790039, 40.01787185668945, 40.017887115478516, 40.017906188964844, 40.017921447753906, 40.01793670654297, 40.0179557800293, 40.01797103881836, 40.01799011230469, 40.01800537109375, 40.01802062988281, 40.01803970336914, 40.0180549621582, 40.018070220947266, 40.01808547973633, 40.01810073852539, 40.01811599731445, 40.018131256103516, 40.01814651489258, 40.01816177368164, 40.0181770324707, 40.018192291259766, 40.018211364746094, 40.018226623535156, 40.01824188232422, 40.01825714111328, 40.018272399902344, 40.01828384399414, 40.01829528808594, 40.018314361572266, 40.0183219909668, 40.01833724975586, 40.018348693847656, 40.01836395263672, 40.01837921142578, 40.01839065551758, 40.01840591430664, 40.01841735839844, 40.018436431884766, 40.01844787597656, 40.018463134765625, 40.01847457885742, 40.018489837646484, 40.01850128173828, 40.01852035522461, 40.018531799316406, 40.01854705810547, 40.01856231689453, 40.01857376098633, 40.018585205078125, 40.01860046386719, 40.01861572265625, 40.01863098144531, 40.01864242553711, 40.01865768432617, 40.0186653137207, 40.0186767578125, 40.0186882019043, 40.01870346069336, 40.018714904785156, 40.01872634887695, 40.018741607666016, 40.01875686645508, 40.018768310546875, 40.01878356933594, 40.018795013427734, 40.0188102722168, 40.01881790161133, 40.01883316040039, 40.01884460449219, 40.018856048583984, 40.01886749267578, 40.01887512207031, 40.018890380859375, 40.018898010253906, 40.01891326904297, 40.0189208984375, 40.0189323425293, 40.018943786621094, 40.01895523071289, 40.01897048950195, 40.01898193359375, 40.01899337768555, 40.019004821777344, 40.01901626586914, 40.01902770996094, 40.019039154052734, 40.01905059814453, 40.01906204223633, 40.019073486328125, 40.019081115722656, 40.01909255981445, 40.019100189208984, 40.01911163330078, 40.01912307739258, 40.019134521484375, 40.01914978027344, 40.019161224365234, 40.019168853759766, 40.01918411254883, 40.01919174194336, 40.01919937133789, 40.01921081542969, 40.01921844482422, 40.019229888916016, 40.01924133300781, 40.01925277709961, 40.019264221191406, 40.01927185058594, 40.019287109375, 40.01929473876953, 40.01930618286133, 40.01931381225586, 40.019325256347656, 40.01933670043945, 40.019344329833984, 40.019351959228516, 40.01936340332031, 40.019371032714844, 40.01938247680664, 40.01939010620117, 40.0193977355957, 40.0194091796875, 40.0194206237793, 40.01942825317383, 40.01943588256836, 40.01944351196289, 40.01945114135742, 40.019466400146484, 40.01947021484375, 40.019474029541016, 40.01948547363281, 40.019493103027344, 40.01950454711914, 40.01951217651367, 40.01952362060547, 40.019535064697266, 40.0195426940918, 40.019554138183594, 40.019561767578125, 40.01957321166992, 40.01958084106445, 40.019588470458984, 40.01959991455078, 40.01960754394531, 40.019615173339844, 40.01962661743164, 40.019630432128906, 40.01963806152344, 40.0196418762207, 40.019649505615234, 40.01966094970703, 40.01966857910156, 40.019676208496094, 40.019683837890625, 40.019691467285156, 40.01970291137695, 40.019710540771484, 40.01971435546875, 40.01972198486328, 40.01972961425781, 40.01974105834961, 40.019744873046875, 40.019752502441406, 40.0197639465332, 40.01976776123047, 40.019771575927734, 40.01978302001953, 40.01979064941406, 40.019798278808594, 40.01980972290039, 40.019813537597656, 40.01981735229492, 40.01982498168945, 40.01983642578125, 40.01984405517578, 40.01984786987305, 40.01985549926758, 40.01986312866211, 40.01987075805664, 40.019874572753906, 40.01987838745117, 40.01988983154297, 40.0198974609375, 40.019901275634766, 40.0199089050293, 40.01991653442383, 40.01992416381836, 40.019927978515625, 40.019935607910156, 40.01994323730469, 40.01995086669922, 40.01995849609375, 40.01996994018555, 40.01997756958008, 40.019981384277344, 40.01998519897461, 40.01999282836914, 40.02000045776367, 40.02000427246094, 40.02001190185547, 40.02001953125, 40.02002716064453, 40.02003479003906, 40.02003860473633, 40.020050048828125, 40.020050048828125, 40.020057678222656, 40.02006149291992, 40.02006912231445, 40.02008056640625, 40.020084381103516, 40.02009201049805, 40.02009963989258, 40.020103454589844, 40.02010726928711, 40.02011489868164, 40.02012252807617, 40.0201301574707, 40.02013397216797, 40.020137786865234, 40.020145416259766, 40.0201530456543, 40.02015686035156, 40.020164489746094, 40.02016830444336, 40.020172119140625, 40.020179748535156, 40.02018737792969, 40.02019119262695, 40.02019500732422, 40.020198822021484, 40.020206451416016, 40.02021408081055, 40.02021789550781, 40.02022171020508, 40.020225524902344, 40.020233154296875, 40.020240783691406, 40.02024459838867, 40.0202522277832, 40.020259857177734, 40.020263671875, 40.020267486572266, 40.02027130126953, 40.02027893066406, 40.020286560058594, 40.02029037475586, 40.020294189453125, 40.020301818847656, 40.020301818847656, 40.02030563354492, 40.02030944824219, 40.02031707763672, 40.02032470703125, 40.02032470703125, 40.020328521728516, 40.02033615112305, 40.02034378051758, 40.02035140991211, 40.020355224609375, 40.020362854003906, 40.02036666870117, 40.02037048339844, 40.0203742980957, 40.0203742980957, 40.02037811279297, 40.020381927490234, 40.02039337158203, 40.0203971862793, 40.02040100097656, 40.02040481567383, 40.020408630371094, 40.020416259765625, 40.020423889160156, 40.020423889160156, 40.02042770385742, 40.02043151855469, 40.02043533325195, 40.020442962646484, 40.02044677734375, 40.02045440673828, 40.02045822143555, 40.02046585083008, 40.020469665527344, 40.020477294921875, 40.02048110961914, 40.020484924316406, 40.02048873901367, 40.02049255371094, 40.0204963684082, 40.02050018310547, 40.020503997802734, 40.0205078125, 40.020511627197266, 40.0205192565918, 40.02052688598633, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.020530700683594, 40.02053451538086, 40.02054214477539, 40.020545959472656, 40.02054977416992, 40.02055358886719, 40.02056121826172]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bed11c8b-5d9a-4ff5-a241-423f282c9560');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phlBrCCHrZv1"
      },
      "source": [
        "mse2 = np.mean((y_MB - y_test_MB_NONITERATIVE)**2)\n",
        "print(\"MSE actual vs noniterative: \", mse2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}