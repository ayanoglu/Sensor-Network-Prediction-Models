{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColdChainTransportation-Time Series Prediction For Sensor Networks - Prediction based on number of sensors ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFprHMj9VE6BoEqeZn7s3Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayanoglu/Sensor-Network-Prediction-Models/blob/main-functionsgeneralized/ColdChainTransportation_Time_Series_Prediction_For_Sensor_Networks_Prediction_based_on_number_of_sensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a41nLlQT_f4E"
      },
      "source": [
        "**IMPORT LIBRARIES AND DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "TbxX5SxSsO05",
        "outputId": "33fe1c54-dee5-4c1d-cdbd-d3292fa045a9"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from numpy import array\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.set_printoptions(linewidth=160)\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from decimal import *\n",
        "from google.colab import files\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import itertools\n",
        "from itertools import combinations\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-68b8bc9b-44d6-416e-9c00-dbf6c8c10a50\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-68b8bc9b-44d6-416e-9c00-dbf6c8c10a50\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving S2.csv to S2 (1).csv\n",
            "Saving S3.csv to S3 (1).csv\n",
            "Saving S4.csv to S4 (1).csv\n",
            "Saving S5.csv to S5 (1).csv\n",
            "Saving S6.csv to S6 (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6i5vDVy_p0z"
      },
      "source": [
        "**READ IN THE CSV FILE, LOCATE COLUMNS AND SAVE AS FEATURES**\n",
        "\n",
        "**PLOT \"Front Top\", \"Mid Top\" AND \"Rear Top\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "id": "gN_c87WNPeLu",
        "outputId": "45ceeb55-d766-4b0f-a811-ddbf3d132d8c"
      },
      "source": [
        "import io\n",
        "S2 = pd.read_csv(io.BytesIO(uploaded['S2.csv']))\n",
        "S3 = pd.read_csv(io.BytesIO(uploaded['S3.csv']))\n",
        "S4 = pd.read_csv(io.BytesIO(uploaded['S4.csv']))\n",
        "S5 = pd.read_csv(io.BytesIO(uploaded['S5.csv']))\n",
        "S6 = pd.read_csv(io.BytesIO(uploaded['S6.csv']))\n",
        "\n",
        "def csvtocolumns(dataset):\n",
        "  FT = dataset.iloc[:,1]\n",
        "  FM = dataset.iloc[:,2]\n",
        "  FB = dataset.iloc[:,3]\n",
        "  MT = dataset.iloc[:,4]\n",
        "  MM = dataset.iloc[:,5]\n",
        "  MB = dataset.iloc[:,6]\n",
        "  RT = dataset.iloc[:,7]\n",
        "  RM = dataset.iloc[:,8]\n",
        "  RB = dataset.iloc[:,9]\n",
        "  return FT, FM, FB, MT, MM, MB, RT, RM, RB\n",
        "\n",
        "S2FT, S2FM, S2FB, S2MT, S2MM, S2MB, S2RT, S2RM, S2RB = csvtocolumns(S2)\n",
        "S3FT, S3FM, S3FB, S3MT, S3MM, S3MB, S3RT, S3RM, S3RB = csvtocolumns(S3)\n",
        "S4FT, S4FM, S4FB, S4MT, S4MM, S4MB, S4RT, S4RM, S4RB = csvtocolumns(S4)\n",
        "S5FT, S5FM, S5FB, S5MT, S5MM, S5MB, S5RT, S5RM, S5RB = csvtocolumns(S5)\n",
        "S6FT, S6FM, S6FB, S6MT, S6MM, S6MB, S6RT, S6RM, S6RB = csvtocolumns(S6)\n",
        "\n",
        "#print(f'Length of S2 {len(S2FT)} , {len(S2FM)} , {len(S2FB)} , {len(S2MT)} , {len(S2MM)} , {len(S2MB)} , {len(S2RT)} , {len(S2RM)} , {len(S2RB)}')\n",
        "#print(f'Length of S2 {len(S3FT)} , {len(S3FM)} , {len(S3FB)} , {len(S3MT)} , {len(S3MM)} , {len(S3MB)} , {len(S3RT)} , {len(S3RM)} , {len(S3RB)}')\n",
        "#print(f'Length of S2 {len(S4FT)} , {len(S4FM)} , {len(S4FB)} , {len(S4MT)} , {len(S4MM)} , {len(S4MB)} , {len(S4RT)} , {len(S4RM)} , {len(S4RB)}')\n",
        "#print(f'Length of S2 {len(S5FT)} , {len(S5FM)} , {len(S5FB)} , {len(S5MT)} , {len(S5MM)} , {len(S5MB)} , {len(S5RT)} , {len(S5RM)} , {len(S5RB)}')\n",
        "#print(f'Length of S2 {len(S6FT)} , {len(S6FM)} , {len(S6FB)} , {len(S6MT)} , {len(S6MM)} , {len(S6MB)} , {len(S6RT)} , {len(S6RM)} , {len(S6RB)}')\n",
        "\n",
        "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(20, 15))\n",
        "\n",
        "ax[0,0].plot(S2FT, \"b\", label='S2 Front Top')\n",
        "ax[0,0].plot(S2MT, \"r\", label='S2 Mid Top')\n",
        "ax[0,0].plot(S2RT, \"y\", label='S2 Rear Top')\n",
        "ax[0,0].legend()\n",
        "\n",
        "ax[0,1].plot(S3FT, \"b\", label='S3 Front Top')\n",
        "ax[0,1].plot(S3MT, \"r\", label='S3 Mid Top')\n",
        "ax[0,1].plot(S3RT, \"y\", label='S3 Rear Top')\n",
        "ax[0,1].legend()\n",
        "\n",
        "ax[1,0].plot(S4FT, \"b\", label='S4 Front Top')\n",
        "ax[1,0].plot(S4MT, \"r\", label='S4 Mid Top')\n",
        "ax[1,0].plot(S4RT, \"y\", label='S4 Rear Top')\n",
        "ax[1,0].legend()\n",
        "\n",
        "ax[1,1].plot(S5FT, \"b\", label='S5 Front Top')\n",
        "ax[1,1].plot(S5MT, \"r\", label='S5 Mid Top')\n",
        "ax[1,1].plot(S5RT, \"y\", label='S5 Rear Top')\n",
        "ax[1,1].legend()\n",
        "\n",
        "ax[2,0].plot(S6FT, \"b\", label='S6 Front Top')\n",
        "ax[2,0].plot(S6MT, \"r\", label='S6 Mid Top')\n",
        "ax[2,0].plot(S6RT, \"y\", label='S6 Rear Top')\n",
        "ax[2,0].legend()\n",
        "\n",
        "ax[2,1].plot(S3FT, \"b\", label='S3 Front Top')\n",
        "ax[2,1].plot(S4FT, \"r\", label='S4 Front Top')\n",
        "ax[2,1].plot(S5FT, \"y\", label='S5 Front Top')\n",
        "ax[2,1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIcAAANOCAYAAACLMWxpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3jUVdbA8e9Nr7QQigESFCHBNFiKgEAooivKCotGRKSILLD2tbC6LyALiqCirujCooCKiLpiwYKVVRBpShESejEEIYWSniG57x83k0JmkkmdlPN5nnlm5lfvTJLJnfM791yltUYIIYQQQgghhBBCNE4uzm6AEEIIIYQQQgghhHAeCQ4JIYQQQgghhBBCNGISHBJCCCGEEEIIIYRoxCQ4JIQQQgghhBBCCNGISXBICCGEEEIIIYQQohFzc3YDLtWyZUsdEhLi7GYIIYQQogbt2LEjWWsd6Ox2iCLSBxNCCCEatrL6X3UuOBQSEsL27dud3QwhhBBC1CCl1HFnt0GUJH0wIYQQomErq/8lw8qEEEIIIYQQQgghGjEJDgkhhBBCCCGEEEI0YhIcEkIIIYQQQgghhGjE6lzNIVssFgsJCQlkZ2c7uymNkpeXF+3atcPd3d3ZTRFCCCFELZI+mHNJH0wIIURtqRfBoYSEBPz9/QkJCUEp5ezmNCpaa1JSUkhISKBjx47Obo4QQgghapH0wZxH+mBCCCFqU70YVpadnU1AQIB0SpxAKUVAQIBcMRRCCCEaIemDOY/0wYQQQtSmehEcAqRT4kTy3gshhBCNl/QDnEfeeyGEELWl3gSHhBBCCCGEEEIIIUT1k+CQg+bNm8dVV11FZGQk0dHRbNmyBYCxY8fSpUsXwsPDmTRpEhaLpdS+GzZsoGnTpkRHRxMdHc3QoUOr3J5z587xyiuvlFqekpJSeJ42bdoQFBRU+Dw3N7fK5xVCCFG/aA2ffQbnzzu7JUJUjr0+2F133UVUVBSRkZGMHj2a9PT0UvuuWLGCwMDAwr7QnXfeWeX2HDt2jLfffrvU8j179hSep0WLFnTs2LHa+n2i8cjPh/ffh7y8qh/LYjlHaur6qh9ICNEoSHDIAZs3b2bdunX8/PPP7N69m6+//pr27dsDJjgUHx/Pnj17yMrKYtmyZTaP0b9/f3bu3MnOnTv5+uuvS6y7ePFihdtkLzgUEBBQeJ6pU6fy4IMPFj738PCo8HmEEELUb2vXwvDhMHmys1siRMWV1QdbtGgRu3btYvfu3XTo0IGXX37Z5jFiY2ML+0JvvPFGiXWV6YPZCw5FREQUnmfEiBEsXLjQZr9PiLJs3gy33ALr1lX9WPv2xbJ79/Xk5iZX/WBCiAZPgkMOOHXqFC1btsTT0xOAli1bctlllwFwww03oJRCKUWvXr1ISEhw6JgrVqxgxIgRDB48mCFDhpCamsrNN99MZGQkV199Nbt37wZg9uzZTJo0iZiYGC6//HJeeuklAGbMmMHhw4eJjo7mkUceKfd833zzDd26dSMiIoJJkyaRk5MDQEhICI8++igRERH06tWLQ4cOVfj9EUIIUTdlZsKDD4Knp7kSLd9RRX1TVh+sSZMmgJnVKysry+H6PLNnz2bcuHH069ePcePGcezYMQYPHkxkZCRDhgzhxIkTAEyYMIH77ruPvn37cvnll/P+++8Dpg/2ww8/EB0dzaJFi8o93+rVq4mIiCA8PJzHHnuscLmfnx8PPvggV111FUOGDCEpKcnxN0Y0WL//bu737av6sTIz4wDIz8+q+sGEEA1evZjKvrgHHoCdO6v3mNHR8MIL9tcPGzaMOXPm0LlzZ4YOHUpsbCwDBw4ssY3FYuHNN9/kxRdftHkMaycC4JZbbiEoKKjwKliLFi2499576datGx9++CHffvstd955JzsLXmh8fDzfffcdaWlpdOnShWnTpjF//nx+/fXXwm3Kkp2dzYQJE/jmm2/o3Lkzd955J6+++ioPPPAAAE2bNmXPnj288cYbPPDAA6yrjksVQgghnO7pp+HECfjyS5g2De69F3btAkkkFZVRF/tgEydO5LPPPqNr164899xzNo+xZs0aNm7cCMD9998PwL59+9i4cSPe3t7cdNNNjB8/nvHjx/P6669z33338eGHHwImOLVx40bi4+MZMWIEo0ePZv78+Tz77LMO9ZcSExN57LHH2LFjB82bN2fYsGF8+OGH3HzzzWRkZNCjRw8WLVrEnDlzePLJJ+1mP4nGI7kgyScururHUsoVAK0rniEnhGh8JHPIAX5+fuzYsYOlS5cSGBhIbGwsK1asKLHN9OnTGTBgAP3797d5jOLDyp544gkArr32Wlq0aAHAxo0bGTduHACDBw8mJSWFCxcuADB8+HA8PT1p2bIlrVq14vTp0xVq//79++nYsSOdO3cGYPz48Xz//feF68eMGVN4v3nz5godWwghRN106BAsWABjx8K118KLL0J8PBQkoApRL5TXB1u+fDmJiYmEhYWxZs0am8coPqxs4sSJAIwYMQJvb2/ADF27/fbbARg3blxhIAng5ptvxsXFha5du1a4/wWwbds2YmJiCAwMxM3NjbFjxxb2wVxcXIiNjQXgjjvuKHFe0XhZE8ji46vjaOarnmQOCSEcUe8yh8q6ulSTXF1diYmJISYmhoiICFauXMmECRMAePLJJ0lKSmLJkiUVOqavr69D21lTqa3tqMz4+LIUT8OWKVOFEKJheOABkyG0YIF5Pnw43HgjPPkk3H47FIzMEcJhdbEPZl1/2223sWDBgsLgT3kq0wfTWleo3RUlfbDGafVqExDatAnS0mDDBrM8Pt5MKFCVXwtr5pAEh4QQjpDMIQfs37+fgwcPFj7fuXMnwcHBACxbtoz169ezevVqXFwq/3b279+fVatWAWZ2s5YtWxaOpbfF39+ftLQ0h47dpUsXjh07VlhP6M033yyRkm290rZmzRr69OlT2ZcghBCijli3Dj79FGbPLhkEeuEFsFjAgVJ1QtQJ9vpgWuvCfo3Wmo8//pjQ0NBKnaNv37688847AKxatcpuFrhVRfpgvXr14n//+x/Jycnk5eWxevXqwj5Yfn5+YR2jt99+m2uuuaZS7Rf12/z5cP/98O678MMPkFUQx0lLg5Mnq3Zsa3AoL0+CQ0KI8tW7zCFnSE9P59577+XcuXO4ubnRqVMnli5dCsDUqVMJDg4uDKqMGjWKmTNnVvgc1sLTkZGR+Pj4sHLlyjK3DwgIoF+/foSHh/PHP/6RhQsX2t3Wy8uL5cuXc8stt3Dx4kV69uzJ1KlTC9efPXuWyMhIPD09Wb16dYXbLoQQou7IzjZfNMLC4L77Sq674gp49FH45z9hyhS4pHyeEHWOvT6Y1prx48dz4cIFtNZERUXx6quvVuoc//rXv5g4cSILFy4kMDCQ5cuXl7l9ZGQkrq6uREVFMWHCBB588EG727Zt25b58+czaNAgtNYMHz6cP/3pT4DJXtq6dStz586lVatWdofFiYYrLw8OHCh6PnlyyQy9+Hho164qZ5DMISGE41RNp8hWVI8ePfT27dtLLIuLiyMsLMxJLWrYQkJC2L59Oy1btixzO/kZCCFE/fDPf8LMmWZmsiFDSq/PzISuXcHfH375BdycdJlIKbVDa93DOWcXtkgfrHb5+fmRnp5e7nbyM2i4jhwxQXuAoCBYuNAM+3V3N1meL71kJhKorG3bIsnI2EN4+Ee0bDmiehothKjXyup/lTsOSinlpZTaqpTapZTaq5R6smC5UkrNU0odUErFKaXus7N/nlJqZ8Ht46q9FCGEEELYc+wYPPUU3HKL7cAQgI8PLFoEv/4KixfXavOEEEIUU7zodGiouYEJ2jdtWvWi1FJzSAhREY5cL8wBBmut05VS7sBGpdTnQBjQHgjVWucrpVrZ2T9Lax1dTe0V1ezYsWPOboIQQohq8tBD4OICdmb0LnTzzXDddSbD6LbboHXr2mmfqDnKfAvcDpzUWt+olOoIvAMEADuAcVrrXGe2UZTkSNaQaFiOHIFevcD6o8/LK1oXFgZdupjHQUEQGAivvgqvvQYdO8Lu3SajqGKsNYcyq9x2IUTDV25wSJtxZ9b/Xu4FNw1MA27XWucXbHemphophBBCCDNzzQsv2C5SeuECrF0LTz8N7duXfRylzHCF8HCTZdSrl+3tRo2Cvn2r3m5RK+4H4gDrbBbPAIu01u8opf4N3AVUriiPEKJabNoEKSkwbRpY55254gpo1gx69jSZne+/D3/4AyQkmMkF9u+HDz+Ew4eLMoscJZlDQoiKcKjSQMHVqB1AJ2Cx1nqLUuoKIFYpNRJIAu7TWh+0sbuXUmo7cBGYr7X+0MbxpwBTADp06FC5VyKEEEI0cDt3muwgLy9wdS29PiYGyqiNW0LnzmaWnNmz4eef7W8jwaG6TynVDhgOzAMeUmZO9MHA7QWbrARmI8EhIZwqLs4MGXvxRftZQH/+s7kPCYFrroFt20xwKD6+MsEhU0FEgkNCCEc4FBzSWucB0UqpZsBapVQ44Alka617KKVGAa8Dtub+DNZan1RKXQ58q5Tao7U+fMnxlwJLwRRDrMLrEUIIIRqsdetM1s+xY9UzFOyhh8xN1HsvAI8C/gXPA4BzWuuLBc8TgCBbO8oFOiFqT3w8dOpUseFh1qFmlas/ZIJDMpW9EMIR5RakLk5rfQ74Drge09H4oGDVWiDSzj4nC+6PABuAbpVsqxBCCNGorVtnhoBJjSBhpZS6ETijtd5Rmf211ku11j201j0CAwOruXVC2JeaWlR7p6E6fx7OnYPcXPjpJ9i1q+LZP02amBpEGzeaLM+KTTRtNpbMISGEIxyZrSywIGMIpZQ3cC0QD3wIDCrYbCBwwMa+zZVSngWPWwL9gH3V0/TaNW/ePK666ioiIyOJjo5my5YtAIwdO5YuXboQHh7OpEmTsFgspfbdsGEDSimWLVtWuGznzp0opXj22WcBmDlzJl9//bXNfW+88cYSy9avX090dDTR0dH4+fnRpUsXoqOjufPOO6vzJQshhKhDTp+GrVvhppuc3RJRx/QDRiiljmEKUA8GXgSaKaWsGeLtABuVquoHe32wu+66i6ioKCIjIxk9erTNAs8rVqxAKVWij/Xhhx+ilOL9998HYPLkyezbV7p7umLFCu65554Sy5YvX17YB/Pw8CAiIoLo6GhmzJhRnS+5UQgIKMqKaahuv90U/X/+eejTxxSkjoqq+HGio+HTT00tom+/dXy//PzcgnsJDgkhyufIsLK2wMqCukMuwLta63VKqY3AKqXUg5iC1ZMBlFI9gKla68mYGc2WKKXyC/adr7Wud8GhzZs3s27dOn7++Wc8PT1JTk4mN9d82I4dO5a33noLgNtvv51ly5Yxbdq0UscIDw/n3XffZfLkyQCsXr2aqGL/HebMmeNwe6677jquu+46AGJiYnj22Wfp0aNHpV+fEEKIuu/TT839JdcLRCOntf478HcApVQM8LDWeqxS6j1gNCZgNB74yGmNrIKy+mCLFi2iSUFV34ceeoiXX37ZZpAmIiKCd955h6FDhwKl+2DFL96VZ+LEiUycOBGAkJAQvvvuO1q2bFnp19fYJSY6uwU1a/t2MyNZs2bQrh2sWGGCRBW1fDn8+KOZaXLnThgyxLH9rBMUSnBICOGIcjOHtNa7tdbdtNaRWutwrfWcguXntNbDtdYRWus+WutdBcu3FwSG0Fr/WLA+quD+tZp9OTXj1KlTtGzZEk9PTwBatmzJZZddBsANN9yAUgqlFL169SIhIcHmMYKDg8nOzub06dNorfniiy/44x//WLh+woQJhVewvvjiC0JDQ+nevTsffPCBzePZ8vzzzxMeHk54eDgvvPACYKaqDw0NZezYsYSFhTF69GgyM2U6SyGEqG/WrTNfLiJtDuIWopTHMMWpD2FqEDW4Ppg1MKS1JisrC1OHu7T+/fuzdetWLBYL6enpHDp0iOjo6ML1MTExbN++HTCZQZ07d6ZXr15s2rTJoTZqrXnkkUcIDw8nIiKCNWvWACb7e8CAAQwfPpwuXbowdepU8vPzK/dGNDAZGUWPbSTdNwipqXDmjJmd7IcfzGf3kCFmRrKKCgyEP/3J3MfFOb6fNXNIag4JIRzhUEHqOuWBB0zIvDpFR5u5ge0YNmwYc+bMoXPnzgwdOpTY2FgGDhxYYhuLxcKbb77Jiy++aPc4o0eP5r333qNbt2507969sKNTXHZ2NnfffTfffvstnTp1IjY21qGXsGPHDpYvX86WLVvQWtO7d28GDhxI8+bN2b9/P6+99hr9+vVj0qRJvPLKKzz88MMOHVcIIYTz5eTAl1/CuHGmILUQtmitN2DqO1prPfaq1hPUwT7YxIkT+eyzz+jatSvPPfeczWMopRg6dCjr16/n/PnzjBgxgqNHj5ba7tSpU8yaNYsdO3bQtGlTBg0aRLdu5ZfK/OCDD9i5cye7du0iOTmZnj17MmDAAAC2bt3Kvn37CA4O5vrrr+eDDz5g9OjR5R6zoTtZbJBjZaZorw+KF5BOTIQxY6p+zLCwihWm1joHkMwhIYRjKlSQurHy8/Njx44dLF26lMDAQGJjY1mxYkWJbaZPn86AAQPo39/WhG3Grbfeynvvvcfq1asZY+c/RHx8PB07duTKK69EKcUdd9zhUBs3btzIyJEj8fX1xc/Pj1GjRvHDDz8A0L59e/r16wfAHXfcwcaNGx06phBCiLphwwZzpV3qDYnGprw+2PLly0lMTCQsLKwwY8eW2267jXfeeYd33nnHbh9sy5YtxMTEEBgYiIeHh8MX6DZu3MiYMWNwdXWldevWDBw4kG3btgHQq1cvLr/8clxdXRkzZkyd7YOdPQvz58OcOfD00ybb5dtvYe/emjlf8eDQU0+Z886ZY6Z4z8urmXPWlk2bzGtZtKjk8rCwqh87LMwcf8OGL8jOtj1aoTipOSSEqIj6lzlUxtWlmuTq6kpMTAwxMTFERESwcuVKJkyYAMCTTz5JUlISS5YsKfMYbdq0wd3dna+++ooXX3yRH3/8sRZaTqk0a3tp10IIIeqmdevA2xsGDSp/WyFqTB3sg1nX33bbbSxYsKCwHtClevXqxZ49e/Dx8aFz58611PL60wd75RX4xz+Knl+4AC+/DN27w//+V/3nKx4cevPNkuuuuKL+1lbTGu680xSeBvNaAH77rXK1hi41aBCYrxt/ZMeOlvTrl1ROeyQ4JIRwXP0LDjnB/v37cXFx4corrwTMTGPBwcGAKWK4fv16vvnmG1xcyk/EmjNnDmfOnMHV1dXm+tDQUI4dO8bhw4e54oorWL16tUNt7N+/PxMmTGDGjBlorVm7di1vFvy3PXHiBJs3b6ZPnz68/fbbXHPNNQ4dUwghhPNpbYJDQ4eaAJEQjYm9PpjWmsOHD9OpUye01nz88ceEljM2af78+Xh5edld37t3b+6//35SUlJo0qQJ7733XonC1fb079+fJUuWMH78eFJTU/n+++9ZuHAh8fHxbN26laNHjxIcHMyaNWuYMmVKxd6Aaqa1mVbdWtkgP98ELv77X7j6apOVEhNjsojATJ++ezc0bVq97bBODnf+PPj5mce5udC6NaxeDRERZlnLluDrW73nri65uXDqVMllhw6ZwNCSJTB5ctEwYK3Bga8J5YqNhZwcU7fKYknm+HFo08b8PC9eNOdxdy/aXmoOOZ/W+Wh9ERcXD2c3RYhySXDIAenp6dx7772cO3cONzc3OnXqxNKlSwGYOnUqwcHB9Cm4HDBq1Chmzpxp91h9+/Yt81xeXl4sXbqU4cOH4+PjQ//+/UlLSyu3jd27d2fChAn06mXKC0yePJlu3bpx7NgxunTpwuLFi5k0aRJdu3a1OZuaEEKI2pWfD7NmmToUXbva327fPjh2DB5/vNaaJkSdYa8PprVm/PjxXLhwAa01UVFRvPrqq2Ueq/hEILa0bduW2bNn06dPH5o1a1aiaHVZRo4cyebNm4mKikIpxYIFC2jTpg3x8fH07NmTe+65h0OHDjFo0CBGjhzp8GuvCW+8ARMmmM+U4GB4+OGi4U8LFpgAxqhRpoCyi4v5nKrM1OuOaN4cCmqKA+DlZTKG3n7b3ABCQkywpS4mXN1yC3z8cenlLi6meHTxYFB1tv/GG3PYvds8DgmBYcNg/Xq4+244fRo++6xo2/x8a80hmYzGWU6cmM+pU6/Tu/fBOps5KISV0lo7uw0l9OjRQ1tnjLCKi4sjrDoG6jZCx44d48Ybb+TXX3+t0nHkZyCEENXryy/huuvgD3+ALVvATkIp8+fD3/8OCQkQFFS7baxJSqkdWusezm6HKCJ9sOq1YcMGnn32WdatW1el41Tnz+BPfzIBjXffNcGNfv1M9s6MGSYo5OMDWVnwwQdw5ZWQlGRm3KoJYWEmW6m433+HL74wGTDffWeGnJ0+Da1a1UwbqqJNGxPYHzeu5PKQkJodAmyxnGXTphYAPP20ZssWUzOqc2fzszp3zgSjtNb873+ugMbTM5g+fY7VXKOEXbt330Bq6uf07fs7Hh6tnd0cIcrsf0nmkBBCCOEES5eCmxvs2AGvvQb2RpusW2fqfjSkwJAQwjlatjT3J0+aAExcHNx6KxSf/8TbG8aOdU772rQxmU0Al11mgkPx8XUvOHT2rAlaPfww2ClzVWPy87MLH990k7nQcPy4ybDKzzdD3S67DLTOA0wSgMWSXLuNFIUyM+ML7yU4JOo6ma2sgQsJCaly1pAQQojqdfo0fPQR3HcfDBhgMoNSUkpvl5wMmzfX3+KsQjRmMTExVc4aqm7WUS3795usoLNn6+408tZ2xcU5tx22WKeTd8Z7Vzw4ZD3/J5+YwBAUvV/WYtRubs3Iz8+QukNOkJeXRXb2MQAyMurgL7IQl5DMISGEEKKWrVhhiodOmWKuknfrZmYKurRkyuefmw6/TGEvhKgO1iD0Dz/ASy+Zx3V11GD79maY20cfFRVZdnODm28uWauouhw6ZIb3duxY/rbWAIwz3rviM49Zg0PLlxetf+opU0/KYjH1hjw8LuPixXNYLMm4uravzaY2aunpu0lJ+QRr9lZKykd2i1K7ujYhMPDPtVKTyGJJITn5E8BEE728Lqd585gaP6+oHyQ4JIQQQtSi/Hz4z39MxlCXLmbZPfeYL2qTJ5saRFbr1plhFt27O6etQoiGJblgdNHevebm4QEO1t2udS4u0Lu3CZJ//nnR8meegUcfrf7zFUyIhyPlWOPjzXsXElL97ShP8cyhyy7LJyjIhV9+MQGzixfh229NEfFu3XKZOxc8PYPIzNyHxZKMl5cEh2rLvn23kZkZB7jg7d2J1NQvSE39wu723br9SNOmfWq8XSdOLOS3354pfK6UG9dccx5XV58aP7eo+yQ4JIQQQtSiDRvg8GF48smiZbNnm+mb77nHTCXt4gIWiynMesst1TMFshBCJCXB6NHw3HPmub+/mTWsrvr8czMM16pPH6gL1RLi4kwBaHsTCdSk4sGhvLwLxMc3IzUVmjWD7Gxo3RoyM+HoUTOszNPTFKyTukO1Jz8/l8zMAwQF3UdIyCxcXX3JzT1tc9ucnJP88ktfMjL21kpwKDNzLz4+YURGfkFKymccPDiNrKyD+PnV0LSEol6R4JAQQghRi5YuNV/G/vznomXNmplppCdMKJpqeuNGuHBB6g0JIapPcrIp7tyhg7Nb4hhPz5Jt7dq1qN5PTUlJgYCAsreJjzfDgZ2heHDo4sVz+Pk1w8/PPG/SxAwpO34coGhYGUhwqDZlZR0C8vD374m7u5lZzsvL9h+dp2cQLi5eBVlGNS8jIw5//+54eXWgadO+hcskOCRAClI7bN68eVx11VVERkYSHR3Nli1bABg7dixdunQhPDycSZMmYbFYSu27YcMGmjZtSnR0NKGhoTz88MPV3r7ly5cTHR1NdHQ0Hh4eREREEB0dzYwZM6r9XEIIISonKclMEX3nneDlVXLduHHmqvijj5qpiD/5xHwxGjrUOW0Voq6w1we76667iIqKIjIyktGjR5Oenl5q3xUrVhAYGFjYB1u0aFGNtM/aB3N1dS18/JK1qE8dkZcHqalFM5bVR2FhJjDjyNCvyiov+JSdbWYGc1Yh70uDQ5eytsvd3Zo5JMGh2madoczXt/yiVEq54u3dpXCfmpSXl0129lF8fEy7vL07Ay61cm5RP0jmkAM2b97MunXr+Pnnn/H09CQ5OZncXPOBO3bsWN566y0Abr/9dpYtW8a0adNKHaN///6sW7eOrKwsunXrxsiRI+nXr1+V2nXx4kXc3MyPcOLEiUwsmEszJCSE7777jpb1+b+/EEI0QG+8YYaL3X136XUuLrB4sak5NGuWGU4xaBCFV4SFaIzK6oMtWrSIJgWViR966CFefvllmxfFYmNjefnll0lJSaFLly6MHj2a9u2rVnslLy8P14IxRU888QRPPPEEAH5+fuzcubNKx65Or7wCP/5oHlssJqgSGOjcNlVFaCikpUFsrKn5Y+XmBjNmVD5gUzzYNGYM7NtX9Nm7Zo0J1lulp5vacc4q5F08OHT48ENERX1TopBxWBisXw9ububv5NVX2zB4sAuHDy8jN/cUHTvOq5XCx86Qm3uakycXExw8ExcXt2LLz3Dy5L8IDv4/jhx5nNzc32u0HdZgi7d3F4e29/EJJS1tKxZLCkeOzCh3Zjk3N3+uuOJZXF19S63TWnPs2JMF2Usl5eWlA/mFwSFXVy+8vDpy+vRbNrevqICAG/Hzi+DEiQVonWdzG6VcCAq6j9zcRM6cebfM43l5BdOx49wG+/taF0lwyAGnTp2iZcuWeHp6ApQIutxwww2Fj3v16kVCQkKZx/L29iY6OpqTJ08C8OWXXzJr1ixycnK44oorWL58OX5+fsyZM4dPPvmErKws+vbty5IlS1BKERMTQ3R0NBs3bmTMmDH87W9/s3surTWPPvoon3/+OUop/vGPfxAbG8uGDRuYOXMm/v7+HDp0iEGDBvHKK6/gIkUthBCixmhthpT16wdXXWV7m27dYOpU+Ne/zPb331+7bRSirimrD2YNDGmtycrKKvcLREBAAJ06dSWnr94AACAASURBVOLUqVO0b9+et956i5deeonc3Fx69+7NK6+8gqurK9OmTWPbtm1kZWUxevRoniwoEBYSEkJsbCxfffUVjz76KLfddpvdc2VnZzNt2jS2b9+Om5sbzz//PIMGDWLFihWsXbuW8+fPc/LkSe644w5mzZpV1bfJJq1NJqKHB7QwI1vo2tV8BtVXQ4ZAeDj8/HPJ5UeOQFAQzJtXueNmZBQ9/u03UxvOOqT3ySfNstati7aJioL+/St3rqoqHhw6d+47cnJO4uXVrnDZqFFw9Ci0a2cy6Xbv9sfN7RZ6997AiRNP07btFLy9Q2q72bXizJl3OH78nwQE3ESTJj2LLV/D8eNz8fQMJiHhOTw82uLiUrMFmAMDY3Fzc+zqjq9vGElJ75KU9D6nTi3D0zMYpWx/Tdc6l5yc3wgIuImAgBtKrbdYznD8+JO4u7fC1dXfxrmiaNas6Je3TZtx/P77m1y48JODr8w2i+UMaWnbCQi4kdOn38LLy/a0fzk5x3Fx8SQ9fTcZGb/i4dHW5nZ5eelYLKe57LIpeHkFV6ltwnH1Ljh08OADpKdX7xUZP79orrzyBbvrhw0bxpw5c+jcuTNDhw4lNjaWgQMHltjGYrHw5ptv8uKLL5Z5rrNnz3Lw4EEGDBhAcnIyc+fO5euvv8bX15dnnnmG559/npkzZ3LPPfcwc+ZMAMaNG8e6deu4qWAu49zcXLZv317u6/rggw/YuXMnu3btIjk5mZ49ezJgwAAAtm7dyr59+wgODub666/ngw8+YPTo0eUeUwghROV8/z0cOAAFCQZ2zZ0L775r6l5IvSFRl9TFPtjEiRP57LPP6Nq1K89ZqyzbceLECbKzs4mMjCQuLo41a9awadMm3N3dmT59OqtWreLOO+9k3rx5tGjRgry8PIYMGcLu3buJjIwETIDp50sjEzYsXrwYpRR79uwhPj6eYcOGceDAAcD0wX799Vd8fHzo2bMnw4cPp0ePHo68XRWSkGCCHgsXgo2k9nqpSxfYs6f08rCwounlK+PCBXP/9NPw97+bY914o8m2OngQHnnETBFfF1insg8NXUl8/HgyM+NKBIf69ze3pKQk9u6FVasCufnmd4iL28i0af3JzIxrsMGhjAzzS5CZGVciOGSt55OcvBaAiIjP8PevO9P0mUweTXLyxyjlRu/eB3Fxcbe5rcWSwqZNLcnMjLMZHLK+B2Fhb9GixbXlnjskZBYhIVUPUB858gQnTjxDRsZufH3D6dlzl83tfvllIBkZcWRmxtOmzUQ6d37Z5nbnzn3Pzp1mWwkO1R5JFXGAn58fO3bsYOnSpQQGBhIbG8uKFStKbDN9+nQGDBhAfzuXEX744QeioqIICgriuuuuo02bNvz000/s27ePfv36ER0dzcqVKzluKsjx3Xff0bt3byIiIvj222/Zu3dv4bFiY2Mdarc1u8jV1ZXWrVszcOBAtm3bBpgsp8svvxxXV1fGjBnDxo0bK/HOCCGEcNR//gNNm5qZgsrSogUsXw5/+5spLCpEY1ZeH2z58uUkJiYSFhbGmjVrbB5jzZo1REZG0qlTJ6ZPn46XlxfffPMNO3bsoGfPnkRHR/PNN99w5MgRAN599126d+9Ot27d2Lt3L/v27Ss8VkX6YHfccQcAoaGhBAcHFwaHrr32WgICAvD29mbUqFE11gez1s5x1vCn2lTV4FBamrnv0MFkCFmPdfiwmR6+Lr2H1swhPz8T3LBXyNhaY8jdvSVhYbBpU1iZ2zcE1td26Wu0Pk9N/QxQ+Ph0ru2mlcnHx4yHTE39DG/vK+0GhgDc3QNwdw8sDAJdyvparUPHaos5Xx5nz35V5rl9fMK4cGETeXkXyqzJZD1GQ/59rYvqXeZQWVeXapKrqysxMTHExMQQERHBypUrmTBhAgBPPvkkSUlJLFmyxO7+1ppDR48e5eqrr+bWW29Fa821117L6tWrS2ybnZ3N9OnT2b59O+3bt2f27NlkZxelkPr6lh5fWlGXpl7LWE4hhKg5KSnw/vum1pCPA5nsN91kbkLUJXWxD2Zdf9ttt7FgwYLC+ovFWWsObd++nWHDhjFixAi01owfP56nn366xLZHjx7l2WefZdu2bTRv3pwJEybU2z6YNcBRlwIbNSU01NQFsljA3f73arusmUP+/iUDTdZ7ZxWftsUaHPLyCsbNrZndYsKXBocWLw7A1bVlg/6ybX1tlwZOij/38grG1bVmh5RVlCkMrQDtUFDHxyfM7s8xMzMOFxdfPD2DqreR5Sge6Ck76BNq8/GlPDwCcXMLaNC/r3VRucEhpZQX8D3gWbD9+1rrWcr8J5sL3ALkAa9qrUtNy6CUGg/8o+DpXK31yupqfG3Zv38/Li4uXHnllQDs3LmT4ILLucuWLWP9+vV88803DtXs6dixIzNmzOCZZ57hpZde4q9//SuHDh2iU6dOZGRkcPLkSVq1agWYcfXp6em8//77lRry1b9/f5YsWcL48eNJTU3l+++/Z+HChcTHx7N161aOHj1KcHAwa9asYcqUKRU+vhBCCMe8+Sbk5IB81ApRMfb6YFprDh8+TKdOndBa8/HHHxNazjf4Hj16MG7cOF588UXGjRvHn/70Jx588EFatWpFamoqaWlpXLhwAV9fX5o2bcrp06f5/PPPiYmJqXC7+/fvz6pVqxg8eDAHDhzgxIkTdOnShZ9//pmvvvqK1NRUvL29+fDDD3n99dcr89aUafNmU7OsSRMzdX1DFxpqMnyCgqCgTrhdkZGmYHNx1syhJk3MsZYuhbZti2oRdXGsrnCtsAaHXFy88fEJLTNzyMXFF1dX78Lg1q+/htKp0xvExa2jRQtTyNvqbIom6Pvz7OwQgUdAICEhZvhkQMAIWrW6jfj4O9H6YrW8hubNryMsbEWZ28TH30WTJr257LIp7Ns3lnPnvi1ze601FksSAKmpn/Ljj0W1bCyW04WPrQGJrCy4/nqYP9/MFOqIffvgrrvg00+L6nhVB1MYOqRgJrHyI5E+PqGcOrWsxGu0sljO4usbXusX/osX3y7rNTgaHLKu//33laSkfIKnZwe6dfsBFxePMvcRVeNI5lAOMFhrna6Ucgc2KqU+B8KA9kCo1jpfKVXqX49SqgUwC+gBaGCHUupjrfXZ6nsJNS89PZ17772Xc+fO4ebmRqdOnVi6dCkAU6dOJTg4mD4FnyqjRo0qrBVkz9SpU3n22WfJyMhgxYoVjBkzhpycHADmzp1L586dufvuuwkPD6dNmzb07NmzzOPZM3LkSDZv3kxUVBRKKRYsWECbNm2Ij4+nZ8+e3HPPPYUFqUeOHFmpcwghhCib1mZIWe/eEBHh7NYIUb/Y64NZM38uXLiA1pqoqCheffXVco/32GOP0b17dx5//HHmzp3LsGHDyM/Px93dncWLF3P11VfTrVs3QkNDad++faVnlp0+fTrTpk0jIiICNzc3VqxYUVhUu1evXvz5z38mISGBO+64o0bqDX35pblfuBAaQ3L4TTeZYFhW2ZM8ERdn3pvkZCg+qW/xzKG//tU8zs8392FhJmhUV5jgkAtKuePjE0Zq6uc2t7NYknB3Ny/ymmtM3aRTp2bh4vIeBw6UnBzBYoHzZ1Zzbng2IZgSFF5efcjLy+DMmTW4uTUlN/c0bdqUzsyrqLS07SQlvUto6OsoZfvCen6+hdOn3yAn5wRt2kwiKek9/Pyi8PPrXuaxXVw8aNZsEKmp6+0s/5LWrccCpnbV99+bQI+jwaH16+Gnn2DLFvjjHx3bx1FXXLGQs2e/pW3bSeVuGxR0D0q5oHW+zfUtW95cvY1zgJubH506vUB29jFatLD/5jRrFkP79g/j5haAh8dlZR4zJGQ2SUnvkZ19nLNn15OVdRBfXzszeohqoXTxuRvL21gpH2AjMA34F3C71truvHdKqTFAjNb6LwXPlwAbtNar7e3To0cPfWmx5bi4OMIaQ05sLdmwYQPPPvss69atc3gf+RkIIUTZ9u41M8Rc6vhxuOceeO01mFR+n6/RUErt0FpX/7diUWnSB6t5K1asYPv27bz8su0irLZU5mdw222wdauZxUsU+eIL86X+++9Lzjb25ptw552m+HSnTs5rnyMOH36EkydfYcCADE6cWMCRI4/Rr99Z3N2bldhu9+4byM09Q48eJf+mtYZmzeCOO2DxYrNs2zbY+0UPQvrvKNwuKOgDvL0TOHToPvz9e5Kfn0XPnjaqgVdQYuJ/OHBgCr17H7VbGDsjI55t28Lw8AgiOvobtm4NJTR0BW3ajK/y+a1WroQJE8zsbv/9r2P7TJliLvY8/zw8+GC1NUWUIy3tF3bs6E7Xru/RqpVMoFRVZfW/HKo5pJRyBXYAnYDFWustSqkrgFil1EggCbhPa33wkl2DgN+KPU8oWHbp8acAUwA6dOjgSJOEEEKIOmPbNpMZZO96S/Pm4GAdWyGEqLL4+MZRa6iirMOr4uNLBoesmUN1KUPInvz8bFxcvICSRXubNi2Z/mKxJOPhEVhqf6XM70Z8sVJF8fFw9kQQIRQFh06eDCUy0kyFnpa2jcDAW6ql/cXbbC84ZK2jlJt7kgsXtpXYr7pYX3+87ZJNZe5TleLnouKsBcTt1dcS1ceh4JDWOg+IVko1A9YqpcIxNYiytdY9lFKjgNcB21N1lX/8pcBSMFetKnMM4ThrUUchhBBVl59vhiG0bg1r15as4WB12WVQDXVshRD13IQJE0oU064JeXmwfz8MGVKjp6mXOnQAb29Tk6n4iMHDh829v79z2lUReXlZxYJDJtp17twGvL0vx8OjNVrnc/HieSyWJHx8bBdLCg01WVTWifh+/BFa/daucP3Fi278+GMnAgKK3pD8/Oqpym1t8/nzP9idovz8+aIZ/M6cWV2wX8UKPyUmwrlz9tfvKIiDHTwIv/4KDpSOLXy/du4segxw+eVw8qSpLXgpf39o397xdovSXF198fQMJi1tK1lZR/Dy6iiTKdWQCs1WprU+p5T6DrgekwX0QcGqtcByG7ucBGKKPW8HbKhwK8255ZfASSoy9FAIIRqb1183mUNvvQVXX+3s1ghR/aQP5jyV6YMdPAjZ2dC1aw00qJ5zcYHwcFi+3NyK8/EBLy/ntKsiimcOeXl1xMXFl6NHH+f48bn06fMbp0+v4ujRx8nPz8Xd3XY18shIM6zqqmLlW2b3DQHALdudY7+HMmuWO7NmBfHll+64u1t46KEI3njDFOquCg+Plnh4tOXEiac5ceJpu9u5uvqRl5dOaupneHp2wM2tqcPnOH4crrzS1FIqi7+/KUZekXqA/v7mf37x965DBzhxwvb2SsEvv0BUlOPnEKX5+UWQkvIJKSmfEBGxjoCA4c5uUoPkyGxlgYClIDDkDVwLPAN8CAwCjgIDgQM2dl8PPKWUal7wfBjw94o20svLi5SUFAICAqRzUsu01qSkpOBVH/5bCiFELUtNhRkzzPCE2293dmuEqH7SB3OeyvbBPv7Y3F97bQ00qgF4+234+efSyy+/vH4U7y4eHHJxcaNbt/9x7twPHD78ICkpn3D69Cry8tIBaNZskM1j/OUvEBJiZniz6vf75fhNB68/DMH7gcWsWQOgSEvbSnr6Cb777o989BFMnVr11xAZub7cKcp9fa8iN/c0FktyhYsQf/CBCQwtXQpNy4gpXX21CdzYyvixxd0devQwRamtcdt33jFZwy1bFtVwsrJYTC2r996T4FBVXXnlq7RuPY4DB6Zx5sw7EhyqIeUWpFZKRQIrAVfABXhXaz2nYIjZKqADkA5M1VrvUkr1KHg8uWD/ScDjBYebp7W2lWFUyFYxRIvFQkJCAtnZ2RV+gaLqvLy8aNeuHe7u7s5uihBC1Cl//Sv8+9+mcxkZ6ezW1C9SkLrukT5Y7dPazJzVvLntIanl9cF27YJp00pmSBw+bAIdl/woRT2QmLiEvLws2rd/AID4+ElkZJQsAp2ZeRBv7yvo0aOoPpDWmp9+6kB+fi4WyxnAZN707ZuEq6uDwcW1a0115ptvNo+L0Ro6dzbDtEJCSu+qlLlQMmqU46+1qt55B557zva6I0cgKAh27675dnz3HQwebKa4X7as9PrBg80Qts6di5a5uZmi1o7OknapL7+E//s/M6zd3x/efbfk7HsNWXz8RM6ceQdf3/ASy11cvAgNXcFvvz1HRsYemjUbjIuLF8HBFc5LafCqVJBaa70b6GZj+TmgVMhOa70dmFzs+euYekSV5u7uTseOHatyCCGEEKJa/fKLCQz99a8SGBINl/TBatZ//wujR8PIkSbboaI+/tjUz7nhhqJlrVqZgJGof06efJm8vAzat38AiyWV339fjq9vOJ6eRRP2NG3aqtRU5UopOnZ8mjNnVuPi4kXTptfg6urjeGAIioru5JeeHl0pmDfPDEWzZdMmM+NbbQaHVqwwgVBbAZZWreDuu2unHf37w/332z/fE0/AokUlJ6z46ivz917Z4NDHH5u6R336mODU99/X7nvvTO3amb8NrYvS3rS+yNmzX5Kc/CGJia8Cpm6Vu3tLCQ5VUIVqDgkhhBCiqAh1QADMmePs1ggh6itrQlB5tVHsiYuD4GD49NPqa5NwDq3zyMw8gNYW8vKyCmdmuvzy+Q4NoWnT5g7atLmj8g0oIzgEcOut5mbLqFG1P4NXfDxcf70ZJuhMbm7wwgv21w8ZUro4fGRkxWZJu1Rysvm7/+QTM8NeVY5V3/j5RRER8VGJZVprNm5sSnLyxyWWWyzJ5OYm4+HRSNKqqoEDddmFEEIIUdybb5qr9c88A82aObs1Qoj6yjqUrLLBIZmyvuHIyjqK1rmAJivrQGFNnuqewt0ua2qLneBQWcLC4NChyv8eV1RGhik6XV9/98PCqhZMS06GwEAzpKxdu9oPzNU1Sil8fEI5f/57AFxcvAvXWYOswjGSOSSEEEJUwPnz8OijppDl+PHObo0Qoj6rSuZQfr4JDsXEVGuTRC24eDEdKBmESU//pfBxWtovpKfvRilPu9O9V7u8PHNfiRnywsJMcetdu0rW1lHKBDAudfEiZGZWsp0U1RKqr8Gh0FB4/31ISgJPz6Ll/v6OFUVPSiqq/RQW1rgyh+zx8QklLW0boAgIuImkpHcB83fl52fG/ivlhqurjxNbWfdJcEgIIYSogFmzTMfss8+KsvCFEKIyrF8Ei88a5agTJyAry3zRFHVTfPxEtM4jLOyNwmWnTr3G/v2Ty9jLlf37JwLg6xuFUq413MoC1l/CSmQOde1q7nv2LL1u9mzzf7O4vn3NdPBVZT1vfXPVVeZtbtWq5PK77zYzrJUnOdnMmgbm73/5chPTqw+z7dUU64x2Xl4d8ff/A0lJ76KUO4cO3cehQ/cVbOVCdPQGmjXr77yG1nESHBJCCCEctGcPvPyymQb4D39wdmuEEPWdNWOoMplD1myB+po90RicPfsNWpcMtpw79wNubi0IDn6i1PZeXh1Ryp2srAMANG1ai19irZlDlQgOdetmAhSpqSWXL14MP/xQcllmpgkM3XgjDBpUybZihlXV19/9ESPglVdMcNfqrbdKv1e2WGc4tM5OFhYG6elw8qQZYtZYtW17Ny4u3vj798DX9yp8fa9CKXcyMn4FTE2vI0ce5cKFHyU4VAYJDgkhhBAO0BruuQeaNoW5c53dGiFEQ1CV4JC1zohkDtVNFy+mk5PzW8HjC7i5NQEgMzMOP79o2rd/yJnNK60KwSGlYMKE0st37YJvvim5bP9+cz9+vJmprzHy8io9o2ByMixcaD4LrMNNbUlPh9xcExyDor//uLjGHRxyd29Bu3b3FT63FnFv0WJY4bKEhBfIyGjkBZrKIQnxQgghhAPeecdMF/vUU2aWMiGEqKrcXHNfmWFl8fHms8j6JVHULVlZ+wsfW4viaq3JzIyrvSLTFVGFmkP2hIWZjJYLF4qWSVDTNmvdpkOHyt4uKcncF88cAilK7Qgfn9DCQu/CNskcEkIIIcqRlgYPP2yGkk0uq1SEEEJUQGUzh86cgTfekOGtNSU7+zjp6btp3nwwrq6+hctzchIByMjYS15eRpnHuHBhU+HjM2fWkJOTSF7eBfLy0vDxqYORkSrUHLLHGgB67TXo2NE8/vRTU6/vyiur7TQNgvW9WrXK1BNydYUhQ8CnWP3kY8dg7Vrz2Bocat3aZDR/9ZWpZTR4cOOuPVQWH58wTp9+g6SkDwuX+fqG4+PTCYCMjH1kZpohnUopmja9Bnd3czUwLW0nfn4RJWqAZWTEkZm5H0f5+obh49OlOl5KjZHgkBBCCFGOf/4TEhPhgw9Mh00IIapDZYNDDz4I2dkSHKop+/bdzoULP9Kx49wStYH27RtDRsYeLl4869BxXF39AUVCwvMkJDxfuNzfvw7+4KowrMyebt1MoOKhS0bQRUaWnKVLmAwgb2+YN69o2YIF8MgjRc9jY2HrVvPYGmxTynwOrFtnbps2mYLfojR//+4kJi5m796Rhct8fMLo1WsfWmt27hyExXKmcF3btlPo0mUJmZkH2bGjG2Fhq2nd+rbC9bt2XUtu7kmHz+/t3YnevQ9Wz4upIRIcEkIIIcoQFweLFsGkSdC7t7NbI4RoSKzDyioaHNq922QaLFhQ/W1q7LTWZGTsASA9fXeJ5enpO8nLM2OkoqP/h6trkzKP5eHRGlDk5v5euMzV1Rdv707V3/CqqoHgUHAwHDkC586VXN6hQ7WdosHw84ODB4uGjQ0fbv7OrfLzzaQYd94JM2fCFVcUrVu7FrZsgWHDzD4SHLKtTZsJNGnSm/x884F76tRSEhOXkJeXTV7eBSyWM3ToMIPAwFgOHJha7HNgFwAZGbsAExyyWFLJzT1J+/YP06rV2HLPnZDwAklJ79bMC6tGEhwSQggh7NAa7rvPdNrmz3d2a4QQDU1lMofy8uDAAbj/fsm+qAk5OSfJy0sDKFGfJDf3VGFgyM0tgGbNBjh8TE/PNtXbyJpQA8EhgJCQaj1cgxYUZG5ghogVryN04oSZ3axfv5KBIYAmTWDoUPD1ldpDZVHKpXDKe4DMzP4kJr5KVtZBLl40U+01azYIf/9o/P27c+bM6sI6YUCJYtbWZc2axeDvH13uuX19wzh9Oou8vIwSQ1XrGilILYQQQtjx3//C11+bYWVS9FUIUd0qU5D66FGzX32dxruusxaP9vP7A5mZB9A6r8RyoG7WDKqqGqg5JCovLMwUnbfWB48v+PWzV8hbKbMuPt72elGa9e84MzO+8O/buszHJ5SLF89hsZwpXFf8M6Boe8c+iN3dTZEoiyW5ehpfQyRzSAghhLAhI8PUSYiKgqlTnd0aIWxTSnkB3wOemH7d+1rrWUqpIcBCzIXAdGCC1rqceXBEbatM5lB5XxJFxWVkxJGY+G8gn4yMfQAEBo4kPX0H+/dPwdXVp0TWgK9vA4zM1VDmkKic0FDTD5kyBby8YO9es7ysoHBoqCn4fe+9ttffdJMZeiYMUxxakZDwIlrn4OLii6dn+4J15o0+ePBezp/fCEBW1iEOHjRv7oULW1DKEy+vYIfOVTw45Og+ziDBISGEEMKGp56C336Dt98GN/lvKequHGCw1jpdKeUObFRKfQ68CvxJax2nlJoO/AOY4MR2ChusQaGKZA5Zh41I5lD1OXnyJRITl+Dm1hyApk2vITDwVhITl5CcXDSzUZMmfXFx8SYg4CZnNbXmSHCoTomJgXbtzEQYxZeVlcV8442wfr3pt1wqLQ1+/FGCQ8W5uvoQEHAj58+bmQUDA0ehCqZ68/fvgbf3lZw9+w2gaNVqDOfOfcfp00VvrtnesVlK3N3ND04yh4QQQoh65uBBePZZGDcOrrnG2a0Rwj6ttcZkBgG4F9x0wc1aLbcpkFj7rRPlqUxB6rg4aNMGmjWrmTY1RpmZ8TRpcjXdu/9YYnmfPiec1CInsAaHrOOYhFOFhZkLVBVx223mZssDD8B//mNify5SWKZQRMTHNpe7uzend+8D1XYea+ZQbm5StR2zJsivhhBCiDrl5En46KOK7ZOQAJ98Uj3n17qo0Oszz1TPMYWoSUopV6XUTuAM8JXWegswGfhMKZUAjAOkpHodZA0KZWU5/p08Pl6GlFW3zMz4hllHqCKk5lCDFhoKmZmmjyVqX32pOSTBISGEEHVGfj6MHg033wzffuv4PqNGwYgR8L//Vb0Nn3wCn38Os2dD27ZVP54QNU1rnae1jgbaAb2UUuHAg8ANWut2wHLgeVv7KqWmKKW2K6W2JyXV7SuaDZE1c0hryMkpf3utTeaQDCmrPhbLOXJzf3e4sGyDJcPKGjTrZ4bMZuYcbm7NANc6HxySYWVCCCHqjJUr4aefwMfHFFTcuRPc3cve5/XXYds2s88998DPP5e/jz1ZWSZr6Kqr7Bd0FKKu0lqfU0p9B/wRiCrIIAJYA3xhZ5+lwFKAHj16yHiSWlZ8OFlGhik8a8tPP8Ff/mKCSefOSXCoPElJ/+Xo0ZmY0ZVly883UTkJDhUEhypSAEvUG9bPjAkTioak+vqamkbt2zutWY2GUi64uwdgsdTtizCSOSSEEKJOOHcOHnsM+vY1xRT37YN//avsfVJTYcYMUxforbfg11/hlVcq34YFC+DYMXPeygaYhKhNSqlApVSzgsfewLVAHNBUKdW5YDPrMlHHFA8OJZdxQfnf/4YjRyAiAu64w2RXCvt++20RFy+m4usbXu7N3/8PtGkziWbNBjq72c5lDQ5VpACWqDcCA+Hxx01/KTwcunaF7dth1Spnt6zx8PBoTW7uKWc3o0zlZg6VMUXqCmAgcL5g0wla65029s8D9hQ8PaG1HlEdDRdCCNGwzJwJKSlmpo3oaLjhBjO0a8wY+8O7/u//4OxZePlliIyE664zx4mNNQVbK+LoUZg/3+w7aFCVX44QtaUtsFKZKVNcgHe1ZIm3PAAAIABJREFU1uuUUncD/1VK5QNngUnObKSwzTqsDEwtkC5dSm/z6acmq3LcOHjjjdprW32QnX2cM2fWUDxDSOuLXLjwIyEhswkJmem8xtU31owhCQ41SErBvHkll/XsCcuXm3XF+frClCng4QGnTpkAkjV2eKmYGAgONhfoim8zcCBcfXW1voR6z9MziJycul30yZFhZfamSAV4RGv9fjn7ZxWMgxdCCCFs2rULFi+GqVOhWzez7MUXzfCuRx4xnY5L/fKLuZr+179CVJRZ9tJL5orYY4+ZL1MV8cAD4OpqZikTor7QWu8GutlYvhZYW/stEhVR/Hu4rUKxWpsAOZiMIVHSiRPzSUz8d6nlLi5etGoV64QW1WOSOdToTJxo+lAzZpRe17EjDB9u+mJlTc5hvZj31FMll0dGmr6dKOLhEURa2i/ObkaZyh1Wpg1bU6QKIYQQVaa1qRXUvDn8859Fyzt1MoGhVavg++9L7pOfb/YJCIA5c4qWd+4Mf/ububq+aZPjbfjsM/j4Y5OJ1K5d1V6PEEI4Kje3KMsxIaH0+sRESEuDRYtg2LDabVt9kJGxjyZN+tC/f2aJ2zXXXMDHx0YalrBPgkONzvTpptZiZmbRLTHRrNu3r+i+a9eS21hv999vZk/89VeT9Whd/uCDsH+//WyjxsrTMwiL5Qz5+XX3b8yhmkN2pkgFmKeU2q2UWqSU8rSzu1fBLBg/KaVsjpCWmTKEEKLxWrUKNm40Q7patCi57vHHoUMHEwgqXiPzrbfgxx/N1SxrYUWrf/zDBHjuucexjklOjungdOliOjRCCFFbLBbzGdasme3MIevMQpGRtduu+iIzMw4fn664unqXuLm4SNG4CpPgUKPk5QXe3kW3tm2hdWsT9AHzGdS1a8ltrLfwcMjONuUAim8THm76VseOOfWl1TmenkGArtN1hxwKDtmZIvXvQCjQE2gBPGZn92CtdQ/gduAFpdQVNo6/VGvdQ2vdIzAwsDKvQwghRD104YLJDurZEybZqIji42OumO/ZA6++apadPw+PPmrGso8fX3ofX194/nkz09mSJeW34bnn4NAhMyTNw6Nqr0cIISrCYjGfO0FBtoND1i9oMjtZaRZLChZLEr6+8uZUC6k5JAqEhprPnpwcUwjf3udPaKi5z8kpelx8ufXzSxgmOESdrjtUoansi02Rer3W2lqVIUcptRx42M4+JwvujyilNmDGxR+ufJOFEEI0FE8+CadPmyFdLnYuV4wcCddea4Z83XqryTA6c8YUabW3z+jRMHgwPPEE3HKLmaXDlhMnYO5cGDVKhmwIIWpfbq6ZGbFNG/jwQ/jiC0hPh6VLzfqDB6FJk4oX2K8rUlPX89tvz1MTFSny8kzVCx+f0HK2FA6RzCFRICwMVqwwfa/8/JKBn0u3K+vxI4+YmkX2jBhhsrwbCw8PExxKTPw3x47NJihoOuDCyZMvc9ll0wgMdP40lI7MVhYIWAoCQ9YpUp9RSrXVWp9SSingZuBXG/s2BzK11jlKqZZAP2BB9b4EIYQQ9dHevabTMHmyyRyyRykztXxEBIwdCxs2wF/+An/4Q/n7REXB3/8Oy5bZ3u6hh8z9889X+mUIIUSlWSwmODRuHHz5JfznP5Caagrud+1qhniMG1d6NqH6IjHxP5w/vwk/v5oZF9eixfU0adKvRo7d6FiDQ8Wn0BON0pgxpo7QxYswdKi52GZLQADcdRccPw5DhhQtb94c7r7bHCM93fa+R47AgQONKzjk5RUCwOnTZtpJpVwAV86e/RJQ9SM4hP0pUr8tCBwpYCcwFUAp1QOYqrWeDIQBSwqmUXUB5mut99XECxFCiMbq8GGTefP/7J13eFNlG4fvk6Z7UUqBMtpCKZT1gRZFcIAoyv7cfg7c4mC4cA9EcQ9UwIE4wS2IUhcoIrhAQIbQQhezLXRAadOdnO+Pp6EtTduUJk1S3vu6ciU55z3nPGkzzvm9z/N77rzT/S4gduwQkaamX5CV33+XGfFjO1zYwuoH9Pzz4ks0a1bj2/TpI15CL70kptfex1hQFBfD4sVigh0dbd/raXVs2iQpChaL7fVXXCH9aBUKhVMoL5eysquvhi++kDKM/HzJZnz3XVdH13yKi5MICzuX/v2XujoURWNYxSGLRW71peYqWj1nnQVr1tg3tr7JN2v2Y308+SQ89hiYTGIHcCLg7d0GH5+OlJdnA2AyJaFpIscUFye5MrSjNCoONdAi1aaGqOv6euCmqsd/AP2bGaNCoVAoGuD990Usad9eMmvchcpKKenaubOuaTSA0Sg+Qu3a2be/Rx+FjRth0iSZrbKHGTNg3TopQbPFyJEw3WZR9AlAcTFccIHU6IWE2B4zeLAShxQKJ1JRIQauIKUY33wjj+sr4/AkLJZKSkpSCA8f7+pQFPZQcxanogJ86+s1pFA0H+t33M6dcFIdpaH1EhDQu0oc8qKsbHfVUi/KyvZgNpvw8nKtUtYkzyGFQqFQuB/WtqPTp8P48fVf57c0r78uRtJLlohvUHMJCoIVK5q2TXAwrF7d/GO3Sp57TnLBV61SApBC4SIqKiA0VB7X59/hqZSWpqPrFcoTyFOo2d5TiUMKJ2P9jktKOtHEoXgOH/6FiIgLycn5EuDo4+LiHQQHn+zS+FS+oEKhUHg4WVmSSZOdDU884epohAMHJNPnvPMkOUXhZqSliTikysYUCpdiNaQGKYW14ini0NatF7BqldHmbd06eRGqm5iHUFMcUr5DCicTFwdeXlJSazTWf/P2ru5W2xoICJAv+nbtLjq6rF07mUEtLnZ9ezeVOaRQKBQeTmYmDB0qZWWvviot4WteZLiCBx6AkhJpD+9uPkgKxMDJ2xtefLHxsQqFwmlYDalBTPZfekkSNmJjXRuXPei6mUOHfiQ0dAihobZFZm/vMIKDG+g4oHAfaopDRUVi8KdQOAlfX1i4UJqTNMSCBZI1ftttLROXs+nY8Rq8vAJp3/5/VFTkADphYecCUFGR79rgUOKQQqFQeDxZWWINM2uWGCxPnQo//eQ6UebPP8UH6f77xUha4WZ8+y0sWybu3p06uToaheKExmpIDeL/a+2g6AmUlu7GYimlY8friIy80dXhKJpLTc+hw4chKsp1sShOCK64ovEx27ZJ6VlrwWgMITLyegC6dJkGgNlcDIDFYnJZXFZUWZlCoVB4MBUV4iccGQkRESIQrVwJX37pmnjMZpg8GTp3hkcecU0MigYoLZUWbvHxcq9QKFxKzcwhT8PaXScgQJWNtQpqZg4dPuy6OBSKGvTuDamp8l3ZWjEY/AENs7nI1aGozCGFQqHwZA4ckHtrAsgtt8Dbb8vs8+jRYuLcksyfD//8A59+2vLHVtjBiy+K39CKFdXpCgqF4rgpKJCLFnu7Lh4+DJs3Vz83mVz3UTSZtqHrOkFB/eqsq6wsoqhoI0FBAzEaQ45ZvgGA/PwfAQgIaKUporoO69dLZ0cvLzjllNZt0mw2y+s0m5U4pHAbeveWpLYvvpCJRyuaBoMGQUCA62JzFJqm4eUVhNns+swhJQ4pFAqFB5OVJfeRkXJvNMK8eXDGGfD003JrKXJz4eGHYcQIuOyyljuuwk5275Y3xCWXwLnnujoahaJVcOutkJEBf/1l3/jbbhPxvCZhYY6PqzHKyjL5+28RhU47bTd+frVLiNLSppOV9RYdO15HfPx7R5enp99HZma1O6yvbxe8vcNbJuiW5tdf4eyzq58/+6zUS7dWzGbpbnHwIBw65OpoFAoABg6U+6uuqrtu+nR44YWWjcdZeHkFqswhhUKhUDQPaxv7mtYxp58O11wjSSLXXQc9e7ZMLA89BIWFMGeOMqF2S+6+W/4xL73k6kgUilbD+vWwd2910kVjbNokDQJnzJDnmiYJKS1NSUn60cdFRVvqiENFRZtq3ddcHhR0MrGxYmbv79/dyZG6kE1Vr/3rr6VeetOmhsd7OpWV1eKQyhxSuAn9+8OGDZKlWZOpU1vXR1Iyh5Q4pFAoFIpmcGzmkJXnnoOlS2HaNPj+e+eLNX//LR0l7rrL9Z3SFDZYvhyWLIGnnlImowqFgygthfR0sFhg167GO4xVVIh3xoUX1k5IcQXl5fuPPhbvoHFHn+u6frSlcnHxDnTdgqYZji5v3/5ywsJc/AJagqQk6dg1fjy89VbrcsW1hTVzCJQ4pHArTj7Z9rJffmn5WJyFwRCoysoUCoVC0TwyM6XDTfv2tZd37AgzZ4pYc/754O9fd9suXaTKKDTUvmN99534GVksdddt3gwdOlTPhivsID9f0q2sCl8zOdhzPwd67QdbQuDhwzA7EM7+E7b+1+59dup0C+HhYxwSn0LR2khNrf4+TEpqXBxKS5PkjPh458fWGGVlIg5pms9RIchKeXk2ZnMBgYH9MZm2Ulq6B3//GCoqDlJZeYiAADd4AS1BcrL8szRN7leulH+4oZX28zGbxQArJESJQwq3Jz4eFi6UjPXgYFdH03xU5pBCoVAomk1WlghDRhvf5pMnw8aNsHVr3XW6LmLPqlXS1bx7A5UBui6VSPfdJ+VrERF1x1g7pYWE1F2nsMGOHTBuHOzZ0+xUK13TyRidzZ6RB/DN98a72MabIdIg/7yKfU3atzucqCgU9rJoEezcKY8NBpg4UUz7f/xRSm0bEm8++aTpiSEpKdWP582DdesaHp+WJve93aC5V1nZfgwGf4KDT+Xw4ZVkZDxWax1Au3YXYTJtZdeuGfj5RVNWJnXMJ0x3sqQkmDBBHsfHS6rY9OnN77YQECDdIhcvrn7DWunWDa6/vnn7P16stZFt2ihxSOH2WL9H77uv9nnp6afLpKin4eUVSGVlQeMDnYwShxQKhcKDycysW1JmxdsbPvyw/m1XrhRv4sGDpeLozDPrjikvFwPVd9+VsR980Do6Q7iUn36CSy+Vf9DKlXImc5yYzSaSkq4lN3czkZE3EXfWPAwG1YVMceKxf7+IQSCJHrou4s2GDaLFpqTAxx/b3vbAATE71fWml+BGRUklzo8/yq0xunRxj9LbsrL9+Pp2pm3b88nIeJjdu2fVWu/tHUHHjteSlfUWBw4srLG8HUFBNmo8WhupqZCTU+2Ge8YZIgq98krz9qvrcl9eXtt4qua6M86AuLjmHed4qKysFoeUIbXCzTn1VDHzf+ut6mW6LpWgBw7YnjR1Z7y8go4K867Ew/5sCoVCoahJVlZtM+qmMGKEdNgZPx7OOUdKxq69tnp9Xh5cfLE0bHnkESlTa63Z9C3Gm2/ClCkyC52YCDExx72rsrL9bN06gaKif4iNfYkuXe5CU07gihOUpUvlfvt2mVG+4QZ4r7rJFt9+K0katkyjFy+Wi4pNm2DAgJaJ1xnoFguWolz0gAAqKg7UHVBcDLk5AJQWJOGjtSW68jKiu9bTXjLTwtAOv9VdvrcAcP0Mt1P54AO5Hz9e7nv3lvqV5qLrkh1kFYaSkqrrDHftknUffODc7CFNk9+eY3/QzWa5om7TBrKzRXGt2Tu8tWI2iyGYn59zj5OfL6Jb+/atow7KxXTuLH/SmixeLBOZixdLm3srYWEiGuk6lJS45yRnZWUgFRWuz9ZW4pBCoVB4MFlZkJBw/Nv37CkC0SWXSGezpCTxIdq5U6qe9u2TUg1bLUQVTaCyEu65B157DcaMkRqWZtTgFRZuYOvWCZjNR+jX7xvatRvX+EYKRSvmu+/k+8xaanDxxdXi0Pz5MGkS9OpV//axsfCf/zg/TmeS+/3DJHk9i3dwV8oq9jY6vsMPwHM9nB+Yp3LyyRAd7dh9ahpcdBHMni1v1poGVDEx8oP+1FNycyYzZsDjj9deZi0r69ABvvhC0txWrIBzz3VuLK7mscekI92//zrvGEVFkmZoMsn/OSPDecc6gRk1SoSf//2v9nJ/f8m0/+EH+S3Ys0c0UHchPx8++CCIYcNMZGYe/6SvI1Di0AmEruuUlqbj79+IY6JC0QTKyw9QXLzD1WHUg0Zw8Ml4eQW6OhCnUFkpqbPN/REJC5MfzKlTpcvZhg3SfczXVzpBDBnimHjdjspKWLtWToidiMVSwZFFD8Pfa+HJS+D228GyCY7T0qG4eCepqdPw9o7gpJN+JyjIw69oFQoHsHUrnHVW9fMxY6RcNixMWse3ayfXZ/WRkOD8ro7O5kj2z1hioaxiLxERlxMePrZ6ZXkZ3Hyz1GIMPAmAsP594MMwF0XrATjrx++xxyStwdbMzqefwp9/Oue4Vh5/XH7kj8UqDr34IoweLel3f//d+sWhP/6AbdukV7q9HTqayvbtIgz16iV1rhUVUlqucCiBgVKtX9PKa9s2Obfdvl3+1YWFsqwZFf0OZ88eKCkJxN+/iE2blDikaCGysuazc+etJCRsIDj4BKgXVzid8vIDrFvXm8pK961N79Tpdnr2nOfqMJzCwYOSIluf51BT8PaGN96Qicy774a+fcWo2tGTpm7FtdfWb0LiQJJmQM7VwNUAX8KWL5u9z5CQ0+jXbyk+Ph2avS+FwtMpKoK9e2sbPWuatIy3UvNxa6WY3UcfR0RcQvv2l1Sv3LQJlgM33gMX11NGpmgZ2rSBK6+0va5HD7k5k2+/te2ebvUcioqSsrZHH5WOba0dqxN9crKYMDoD699x9GgRh/LypK2swuEMHlz735iWJuJQUlL1vzopyb3EodxcKCkJwte3lI0bzYwZY6P+uYVQ4tAJQnl5LunpDwJQUPCHEocUDiE9/QHM5iL69l2M0ehG+ZlV7NnzLLm5S4mLm9sqvVgypXGMw2YYNE0aqIwdK/t0x5psh/HLLyIMTZni1KvGfNaTY7ifLkHXER470SH71DQjISGDMRh8HbI/hcLT2VGVvOoOLeJdSXFQ9URNYOAxHcWsF6fu0CpN4Vp694bPPxfzFX//6uVWz6Ga45raws/TOHRIUrDBueJQUpLMwp16qjzPyVHiUAsREyOZ8MnJ1V+D7qZ55uRAaalUOaSkmADXtf5V4tAJQkbGQ1RWHsHLK4iiog2uDkfRCigo+JPs7Pfp2vV+IiIucnU4Nikr20ty8nUUFf3TKgXRrCy5d0TmUE2cPWnpcioqpIYuJgaef772ybEDsVjKSfn7dvzpQfeT31RijkLhJFq77qHrOrm5S6ioyKt/UEU5Je0q5LEZ/D9eBfrv1eu//14MiF3RBUvhXvTuLWnHL7wghtsnSZnh0bIyK/HxMHeu1Jo3ZG64Zw8cOQL9+jk+VpNJ/I/Kyx2/b5CUQytffgllZfL43HOhe/fm77+sTIS4n36SkyvrCVtubvP3rbALLy+p5lu+XHw0QUrP5s+3Pd7XFy67zGmnhjaxZg4BGI3zWbjwEi68MIagoJaLwUqj4pCmaX7AasC3avyXuq7P0DTtfWAY1e0KrtN1fZON7a8FHql6OkvX9Q8cEbjCfo4c+ZusrAV06XIXxcXbKSzc6OqQFB6OrptJSZmCj09noqMfaXwDF9G27WhAIy8vsVWKQ9bMIUeLQ62euXOl4HzpUqf++u/b9wolJTvo3/9bJQwpFE4kKUkuAFqrsG0y/cu2bZc0PtALgnZ5YyiqwDB1St31CQnO78ikcH8GDZIPzIwZ4nG0fbssP1YcGjJEfi8vvxxSU+vf3513iulXSorjY120CG691fH7rYm3t3SJS0yUG8AFF8BXXzV/3998A9dcI4+vv17Mz0CJQy3MkCHVLe/79YN//oFbbql/vKZV/9tagtxcyM4WMfLii+/ll1/+5sMPP+P221suBiv2ZA6VASN0XS/SNM0b+E3TtO+r1t2r63q95gmaprUFZgCDAB3YoGnaN7quu69BSStD1y2kpEzGx6cDMTEz2LPnefLzV2A2l+Dl1YKSqKJVkZn5NkVFG+nT51OMRhfI2nbi49OekJDTyMtLJCbmMVeH43CysuQHrIOynbGf7Gw5IR49GiZMcNphysr2s2vXE4SHTyA8fIzTjqNQKEQcio0FHx9XR+IcTKZtAAwYsJKAgHpariUuQ7vhVrxX/gEdO8BFNjwrwsOdGKXCY+jeXa5GZ86EOXMkK8fHp9pzyMqVV8Jvv8Gbb0Jpaf3C4tatYuxSXOz4evR//5W270lJznOMDwyU15ZXlZl3220ygeQI/v1XMvbS0qBrV6kfAiUOtTCvvy4+8L6+0qQgO9v2OItFfksc9e+3l9xcSE8/j6FDc9i0aSLdu2+zaQvWEjQqDum6rgPW/g7eVTfdzv2fD6zQdT0fQNO0FcAo4JOmh6o4HrKy3qWw8G/i4xdiNIYQHJwAmDGZthAS4qS6WkWrprw8l4yMh2jT5mwiItzf1DI8fBwZGQ9TVpaFr2/rSrHJyoKICNXwokncd5+keb/6qlNbE6WlTUfXK+nRY7bTjqFQKITk5NZbUgZQXJwEGAgJGYKXVz0X6NsOQKEGvfu0csM4hUNo00YyycxmES56967rOQRw5pnSrSIlBfr3r7ufsjJIT5cytZ07YeBAx8aZnCzlbZ07O3a/trAaOA4YIBlEZWWiJjSH5GTJSoqJkedWgdYqEilaBIOhtj9nQ16dcXEt70mUkyNJZT4+7QgPH0Dnzj+TnFyJKxyADPYM0jTNS9O0TcBBROxZW7XqKU3TtmiaNlvTNFufns5AjWJO9lUtO3b/kzRNW69p2voc9WFxGBUV+aSnP0Bo6Bl06HAVQJU4BIWFyndIcXxkZDxMZeUR4uLmeITJc3j4OADy879zcSSOJzPTte0uPY7ffoOFC2H6dKf6bhw6tIqDBz8lKuoB/P0d4FmgUCjqpbJSrltbsxl1cXESfn7d6heGQK5moqOVMKSwH6uiajWdPraszNaYY0lJkXSLhsY0h6Sklld+4+PlNTmiTO7Y+L29ITRUZQ65Ma7wYc/Nra44DAzsjdFYwaFDGS0bRBV2yVG6rpuBgZqmtQG+0jStH/AgkA34APOB+4EnjicIXdfnV+2DQYMG2ZuVpGiEjIxHqaw8VKtTk69vV7y92ylxSHFcHDmynqyst+nS5U4CA/u6Ohy7CAzsj69vV/LyEomMvNHV4TiUrCzlN2Q3lZXSmaxrV3joIacdxmKpICVlCn5+MURF3e+04ygUrZWiIql2mTEDu8w409LEY96dM4fy8r4lO3vhcW9/6NBKQkOHNDwoKal1K2QKx9OrqkRx1iwxTS4oqCsO9ewpWbbPPANLlsA558DNN8u6sjJx7rXy3HPw9ddw9tkNG7rUx4oV8O67sv8RI+DZZ2H//pZ/X1u/TCZPbvgky2iE888XM+7Jk+uuX7BASu7OP7/28ogIJQ65MfHx8lb/3//sG3/LLfKWBzG8fvNN+Q079qPUELm5Us4GEBAg73d//yQKCuIIDW1C8A6gSblKuq4f1jTtF2CUrusvVi0u0zTtPWC6jU32A8NrPO8CrDqOOBVNpLDwHzIz36Rz58kEBQ04ulzTNIKCEpQ4pGgyVv8qb+/2xMQ87upw7EbTNMLDx5Od/QFmc2nDM68eRmamZD8r7OCtt2DzZul6EhjotMPs3z+P4uJt9Ou3VPm6KRTHwY8/wosvwmmnwcUXNz7eEzqV7dnzAoWFf+Pr2/W4tvfxiSAi4vL6B1gssGNH9RWKQmEPQUFw1VWwfj1s2iReRMe+hwICYOJEWLtWPpy//lotDq1dK6Jkhw7S3Wv9emkJtXLl8YlDs2dLV72SEhg+HJ6oyjkYPbpZL7PJ9Okjf4fMzOo297bYsQM++khEokmT6tb4P/643B/rb9i5M+ze7dCQFY5j3DjpV7KpTputuuzeLRqp9WPz8cfw1FNwySX2V1jquuznzDPluVUcOuWUJA4enOB+4pCmaRFARZUw5A+MBJ7TNC1S1/UsTVJSLgD+tbH5j8DTmqaFVT0/D8k4UjgRuYifgrd3ODExdZO5goNPZu/eF1rdhbLCuWRnv0dh4Tri4z/EaAxxdThNIjx8HJmZr3P48CrCw0e5OhyHYDbLOYsqK7ODnBx45BGZ8bTnavM4KSvLZteuGbRtO4rwcOeZXSsUrRlrOr+9af3Wcb3q8Wl2B4qLk2jf/gri4xc45wB79sgFtcocUjSVRYsaH/NBVaPp55+H+++Hw4fFs8iqzK5dKyWNAC+/DPfcI+bOTTVAr/nh37dP3tNvvOF4H6PG8PUVgasxYmPFb6myUrq51VSoCwsl6+npp6uv+q3Ex0umlq471ftQcXwMHiwJX/Zw0UW1f6usH4mkJPvfttnZknxmffsYjaH4+ERy663JznRAqBd7MocigQ80TfNCPIo+13U9UdO0lVXCkQZsAm4F0DRtEHCrrus36bqer2nak8DfVft6wmpOrbCTlBTYWLf1vMUC65KC2d1ndJ0vFj+/hYSG/kGvXu/g7d2mzrbBwQnoeiUm01ZCQk5xWugnBNu2SScCF2HBjMm4h+DKbk3bsGtXGDrU7uEVFYdq+Fdd3cQoXU+bNmdjMASQl5fYasShgwfle6Aly8rM5hLy879D1yvt36isTKZfzGbnBdYYf/0FCUfghTGQ87nTDnPgwMdYLKX06PGaR/hxKRTuiPVE215D0KQkEclbenbVXioq8qmoOHh0NtgpWP9o7pw+pfB8rO+v5GRJ7UtKksyirjUy4qwCZXIynH66/fsuLq7OpklLgy1bah/THendW8QhqOuKb/0CsyXYxsfDoUMycdW+vfPjVDiN3r1h2TIpbfb2bvrvF9j++g4I6I3J1MLGR1XY061sC3CSjeUj6hm/HripxvN3gXebEeOJzQUXwPbtdRYbgNOAe1nNb1Qr0oGBBXz44X3s2TOYYcOus7nLoKBqU2olDjUDi0XSaOvrh9gCpN8O+y6Ffg9Cu7+auPG338IY+1psZ2Q8SkVFfi3/Kk/Cy8uPsLBzyctLRNc9w0i7MbKy5L4lM4eSkiaSm7u46RsGOz6WJjGq6lZwDxQ491AxMY8TEOCCqR4lmAdaAAAgAElEQVSFopVQc+bV3vHunDBTXCwvKDDQiRe5DV2IKhSOwvr+SkoScSg5WVL2DDX6G9U0sG6KOLRzp2TS/Pe/4luUmFh7f+5IfLycS4O83gsvrF7XUL1rzb+REoc8mvh4SRxLS5OPQlN/v8D213dAQDwHDnyErustfs3S8v3RFPaTmirC0IwZcHntWvMZdxUw88chfHHvOg5dXy0OFRbOwGTK4cEHv2P0aAMREXV36+cXjdHYlqIi5TvULNavF2Fo9uy6ZnMtQFFlCvvyLgIspD7fhbDwb/Cy2TTwGHRdSmumTRPDP7+GSwsLCzeRmfkGnTvfXsu/ytMIDx9PXt43mEzbCArq5+pwmo1VHGqpzKH8/BXk5i4mKupBOnSYaN9Gv62BSbeI98D4cc4NsCGMRoiKdvphDAYf/P1jnX4chaK1cu+91cnS//xDnXOYwEC5Jv355+pleXlw++0tF2NN8vOXk5x8bYPZlBZLGYDzM4fCw+v+wRQKR9KtG/j4SHOH++6T7Jdjrk+IipLzymnT4MEmOImUl8v9BReIOLRgAYSFufd7uqbw8+STcj1gpbhYzj1ibZwT1MzAGjbMuTEqnIr1Xzl4sGQOHT4sz7/6yv63rskEwcG1J3sDAnpjNhdQXp6Nr2/Ldp5R4pA7Y1Wjr7lGTOKqqKyEuX/DtIAudNy3gY5Vb8yioq2sXz8XH59b2Lkzge+/l02PRdM0goOVKXWzSUyU2ZJrroG2bVv00Lquk7LpNozGUHr2fJPt2y9jb8AyYmIetW8Hr70G550HL70EDz/c8HGO+lc96aDoXUN4uGRJ5eUtaxXiUGam3LdE5pDFUk5KylT8/XsQEzMDg8EOEbKsDKb+F/x6wj2vSg2/QqFQNMCXX8r911/DTz/VrkY1mcT6ZPdumaE95xxZbjC4Uhz6gYqKQ412wvT17YKfX/cGxzQLd0+fUrQOjEZ4/fVqBVfT4IYbao/x8pIx69c3ff8dOsDVV0NGhrRvGjrUvT15Lr0U8vNFEFu9uu76gQPrmlQDdOki5Xgt3S9d4XBOOgkee6y6+Zyvr+h9y5c3bT+nnVb7rR4aOpTOnacAFofFai9KHHJnli0Tx/zutU8o/vxTvotKT0k4+gVtvYg3GkM55ZRZdOokm9sSh0B8h/bufQmLpcy+Cz1FXZYtk5TZFhaGAA4e/IyCgl/p2fNN2re/lJycS9mz52k6dJiIv39M4zsYOVKyh556SrpQREXZHHbgwCKOHPm9Xv8qT8LXtxNBQQnk5SUSHe35vvjWzKEOHZx/rH37XqWkZAf9+39r//fF7NnimfbDD0oYUigUjWK1HJk5U5r7HNvgp7ISPvlEEgwuu6y6kZErKS5OIjCwNz17znNtIElJknGhUDibG2+UW0Ncf73cjpeZM49/25YkJETSHaFuBlVDGAwi5jbFmEbhlnh52X67/ve/zdtvcPDJBAef3LydHCeGxocoXMKRI9IuclzdUozERBHvw89LkBrdwkIOHvyUgoLVdO/+DD4+4YwdKx0nrVmaxxIUdDK6XoHJ5DozZY9m3z4x2bXx/3E2lZWFpKXdQ1DQyURGir1XbOxLgIG0tLvt39HLL8v93ba3qawsIC3tXoKDB9Ox43XNC9pNCA8fx5Ejf1JenuvqUJpNZia0aycZ3s6krGw/u3bNJDx8wtHsq0bZu1dSrC+80CUllwqFwvOwWo7UZzFiNHK0c4u72JAUFycTEODiYHJz5aYyhxQKzyE+XmUOKdwSJQ65K8uXyzRZPeLQsGHgNzQBdJ3Kjb+TljadoKCEo6nN48ZJF8U1a2zvPji42pRacRxYS/5cIA7t3j2L8vJM4uLmIU0Ewc+vK9HRj5Cb+xX5+T/at6OoKCkpW7wYVqyos3rXrplUVBysMqFuHV8V7dqNB3Ty879zdSjNJiurZUrK0tKmo+uV9Ogxu/HBVu65RwzbrQKkQqFQNII9nsrWdc4Sh3TdjMm0jaKiLY3eCgs3UFq6u+XFobw86eRkvf3wgyx3F8VMoVA0Tu/esGeP1MsqFG6EKitzV5YtEyO2IUNqLU5PF4/qSZOABBF4dmc/T3mHTPr1W3JULDj3XPGDS0ysrsuviZ9fN4zGMCUOHS+JiVLu18InYyZTMvv2zaZjx+sJDT2t1rquXe8mO/s9UlKmcsopW+0r/5k+Hd5/H6ZOlZPMqjQUk2kb+/a9RmTkzYSEDHLCK3ENQUEn4eMTSV5eIh071lNz6SFkZjrfjPrQoVUcPPgp0dEz8Pe30y/j55/hiy+k5iMmxqnxKRQK0DTND1gN+CLndV/quj5DkxYns4BLATPwhq7rr7ku0obZtElS9OMaaPY3cKD8/Pbs6ZwY9u17lbS0e5q0TWBgf+cEYwuzWUwu9u6tu66f53vpKRQnDH36yP3mzeKtpFC4CUocckfMZvjuO2kzbqz9L7J2dhw3DujQAdMp7dkXsYqOHW8gJGTw0XEBAdKIatkymbw/1s9N0zSCgk5W4tDxUFwsTpmTJrWoUZ6u66SmTsNgCKB792frrDcYfOnR4zW2bh3N3r2ziY5+oPGd+vrCq6/C2LHwyitw3321/Ku6d3/aCa/EdWiagfDwsRw8+DkWSzkGg5NrspxIVhb0d+I1icVSQUrKFPz8YoiKut++jcrLRWjs3r26Dl+hUDibMmCErutFmqZ5A79pmvY90BvoCsTrum7RNM2teyZ/8w2cdVbDDTTvukuqVQMCnBPDwYOfEhjYn5iYx+0abzD4ExY20jnB2OLPP0UYeuihoxOEgLTDrsc7UKFQuCFnny3XeMuWKXFI4VYoccgdWbdO6sfrKSmLj5fOiLqukzrZglepZlMsGDdONKYdO2ynaQcHJ7Bv3ysef5Hc4qxcCaWlLV5Slpv7FYcOraBHj9fw8bF9jh8ePop27S5g9+4n6dDhavz8ujS+4zFjxPnziSfgqqvI8f6Nw4dXERf3Bt7e4Q5+Fa4nPHw8WVkLKCj4jbCwEa4O57iwWCA727mZQ/v3z6O4eBv9+i3Fy8vfvo3mzJEa+mXLGr7CUygUDkPXdR0oqnrqXXXTgduAK3Vdt1SNO+iaCBtG12H0aPnqaKzrWGAg9O1r/74PH/6V1NR7AJ1u3WZRWroLs7mIkJDTSE29C0mossahYzJtplu3Z4iIuOi4XovD2bABbrsNKirkeU6OZPjef7+Y4SoUCs8kLAyGD5ee55MmwVVXQUmJrAsJgSVLILz1nYMr3J/WYSTS2khMlNzqY4xcCwth1apqTSInZzGHonPp9rYFn7K6F2Jjx1bvzhbBwQnoerkypW4qiYkQFCRTnC2E2VxMaupdBAb+h06dbmtwbGzsbMDStNT42bOhspLKB+4gNVXMrjt1url5QbspYWHnoGm+5OUtc3Uox01OjiQYOstzqKwsm127ZtC27SjCwyc0vgFIndvjj8sXjwu8uBSKExlN07w0TdsEHARW6Lq+FogFLtc0bb2mad9rmmazYEvTtElVY9bn5OS0ZNiAfHX8+COEhsIVVzh23zk5izGZ/qW4eAcHD35GZuab7N8/t2r5Vnx9o47e/PyiiYi4jI4dr3VsEM3hm2+kJXhUlNwSEuDpp5UwpFC0Bs4+W2bwlyyRrMCOHUU0Wr0afv/d1dEpTlBU5pCDsZQVk/3LfZh7dwODV531bdueT2BgIz41y5bBGWfIF0QNli+XyaPx48FsNpGWdjeB5m5EfpMhNatnnFFrfFQUDBggWsb06XUPU21KvdFl7fI8Dl2XP+j554OvL3/9tQ5//z+c3s2+sHAtZWV76N17EQZDwx9bf/8YoqIeZNeuGWRk9MRotGPmwRuYO5wj+xZTXg59+35x1L8KoKhIzM1HjWrRSjqn4OUVSFjYCHJzlxEb+zKam72giopDHDz4MRZLRb1j9u+Hiy8WSx9b1hO12LlD0oyaQH5wMpYgEz1+Oxlt5av2bfTdd1JW9qqd4xUKhcPQdd0MDNQ0rQ3wlaZp/RAPolJd1wdpmnYR8C5wpo1t5wPzAQYNGqS3YNhAdcMeZ0yUFxcnERT0H7y8Qigu3kZx8Q50vYzCwvUEBvanf/+vHXtAR5OUJGW6X7t5nAqFoulYfUuXLpVa2W+/lUyANm3EoX+CnZNzCoUDUeKQg8l5ZyI7+yyBDNvr9+xpz6mn7sDbu43tAbt3w9at8OKLdVYlJsr3xdChsHv305SV7aVPzFIMlgsk9fgYcQhkAv/ZZ+HQoTpaE35+3fHyCqWoaANwUxNf6QnKpk1yZT5uHDt3ZlBQMIzS0lIOHXL+oTt1uo02beqc19uka9d7yclZwu7ds+w/QA+5df6lLaGn1zahvu02WLQIPvsMLrusCUG7Ke3aXcTOnTeTl5dY1cHMfUhKmkh+/reNjpsyRe7T0hoZ6AV0bnoc3RZAwEdN9Jx69lmpeVUoFC5B1/XDmqb9AowC9gFLqlZ9BbznssAawJ4uZceLyZREWNgIvLxCyMycd3T5kSO/0779VY4/oKNJTlZdyBSK1or1s/3bb2I0bzBICmVkpGpzr3AZShxyJOnp5B1cincknHqrP6xdB52rPV+Ki7fzzz9nsGvX48TFvWJ7H/W0SLdYZNXo0VBensLevS/SocNEQmP/K18iG2wbS48bB089JZ1Oj03X1jSN4GBlSt0kEhMldWbMGFZ/cwudO3txzTXJTJ7cgbvvdt5hNU3DaAy1e7yXlz8JCesxm4saH1zzON99h/GJqyBsHtx5JyAZQ4sWic3BPfeIRVFQUJN263Z07Hgt+/a9TGrqHYSFjcTLyz38cXJzE8nP/5Zu3Z6hU6db6x23aFF1g7muXesZZDFLynJurvhk+dnpG0TV+21uCMxtQvBeXhAc3IQNFAqFI9A0LQKoqBKG/IGRwHPAUuBsZLpqGLDTdVHWT1KSVEk110OtsrIASaASzGYT5eX7CQjojdFYtwyr0SxuV1NaKpOFx1gMKBSKVkJsrJhSV1bWVsfj4+Hff6GgQMQis1ke+/qK8ZpC4USUOORALPfcQf6NFtoFj8H7yEq47wn4/POj60NDh9Kp063s3z+XyMgbCQqy0WooMRF69KjTp/Xvv8VnZPx4a8cqX7p3f15WJiTUKw6deipERMhubdXyiyn1HCyWCgwG7+N+7ScMiYkweDDLN22kR4+l7NnzLKec0ouZM+Xv607NQgwGIwZDPRlq9THhChi1EGbMgP/9j8p2HZkyRQSId9+FkSNFbHzmGefE3FIYDN7Exc1l8+Zz2Lv3eWJiHnN1SJjNpaSm3kFAQG+6dr2nwc/jvn1gMkHnzuBd37A33oA/t8Knn0IXJ0zJKxQKdyES+ECTWmAD8Lmu64mapv0GfKRp2l2IYbVbpgjv3CnXQs2p8M3MXMDOnbZ98gID++DlVVccCgjoc/wHbAmszttNceBWKBSeg7c3xMWJQl7zc963L8ydK+UiH3wAixeL/5i3N2zbJtsoFE5CiUOO4rvvOJKRSGUQhPe5ER46DR57TFqen3vu0WHdus3i4MHPSUmZzMCBv9b2OzGZZIb/ttvqnCUtWyYT86ed9g27d/9AbOzL+Pp2lJUJCeL3YTLVUZQNBvGH/fprEaaNx/zHg4IS0PUyTKZtBAcPdOifpNWRnQ3r1lE+83EOHZqKrvfkssvuYtgw+P57yar54gtXB9lMNA1eew369YP77uPNUz9kyxb48kt5G197Lbz0Elx3HfTq5epgm0dY2AgiIi5jz55n6NDhGvz9Y1waz969z1Nams6AAT83KtRmZYk3h69vPQNyc+HhhyVzqDXUASoUinrRdX0LcJKN5YeBsS0fUdPIz29+1lBBwW8YjW2JiZlRa7nBEEDbtqPRNC969XoXb+9wDAZfSkv3Eh4+pnkHdSZHjkB6uvwWX3qpq6NRKBTO4oMPJAPg8surlz34oJxkP/KIGFOvWSPfBf/+Kx2tlTikcCKqW5kjKC2FadPIGxOGpvkQFjYS7r1XTASnThWT1iq8vdvSvfszFBSs4eDBT2rv56efoKys3hb2w4aVcODAnQQE9KVz5ynVKxMSpO5s0yab4Y0bJ55Df/5Zd53VlFp8hxQN8t13ACxrm0eHDqmEh8/Bz8+H6Gh46CERUH76ycUxOoK4OFG6Fi4k8YHfOPdcuKiqq+9zz4G/P0ybJt7cnk5s7IuAgbS0u1waR0nJLvbseYaIiMsICxvR6PisrEYuph56SC4u5szxfAdxhULRqjlypPnNt4qLkwkKGkiXLtNq3Tp1ugmDwQdN8yIy8nratZtA27bnVy2vT113A3bskPsnnlBlJApFa+aUU+D222u78XfqJMaS/fvDr7/KRdzEiZIlYDVpUyichBKHHMFLL0FaGrkjA2nTZjhGYzD4+UnXnuRkycSoQWTkDQQHDyItbTqVlUeqVyQmyhnSmbVNh/fulWZkN9zwHKWlu4iLm1s7s+Dkqk5j9ZSWjRwpmYjLbHTu9vePxcsrRPkO2UNiIkX9Iwns8Q6pqRcycuR5R1dNny6lw8dogZ7Lww+TH9iF50xTmDO78qi+0KGDnKsuXy7NFTwdP7+uREc/Sm7uUvLyfnBZHCJOGarEqsbJzGxAHPr7b1iwQBQ8VY6gUCjcnMLC5tmV6bpOcXEyAQGtqHzWegGozKgVihOX+PhqoXjAAEk6UEbVCiejxKHmsmcPPPUUxTecR4m2j/DwGlk/48bJbeZMuZqrQtO8iIubR3l5Frt3PykLLZbqFuk+PrUOkZgIkZHpdO78LO3b/4+wsOG1Y+jUSa7a6xGHQkJg+HDZz7FomoGgoJMoLNx4HC/+BKKsDJYvZ+3EADTNwvDhs2utrqkFtoZO3n9tDeQW08sMYDPxv75Va93kyZLdetddUFzsogAdSNeud+HvH0dq6jQslrIWP35e3g/k5i4lOvpR/Pzqc5euTVaWfOzrYLHIbFP79vD44w6NU6FQKJyBvZlDuq6Tl/cDBw9+WeuWnf0BZnMBAQGtQEjZulXSkBMTxQdAdX9UKE5cappU9+4tt8WLJWtAoXASShxqLlUtqvLuPA2gtjgE8MorUFEhZWY1CAk5lY4db2TfvlcwmbbDxo3iaVNPSdm9996FwWCke/cX6sagaVJatrF+gWfcOBGbbbW9Dg5OwGTajMVS2ciLPYFZtYr8Xia8TkkjK+shevSIrjNk7FgYP14ya/bvd0GMDsJsFn3hj8hLqBx+jtQ85+QcXW80wrx5sHu3dC73dAwGX+Li5lBSksLevbMb38CBWCxlpKZOw98/jq5d7Stts1gaKCt77z2pR3/hhebXaSgUCoWTqayEkhL7MocKClazdetotm+/tNZtx47rgeoyeY9F12HECPEY+vxzyRSot+OAQqFo9QwaJPdBQdClS/Xzm9yyt4CildCoOKRpmp+maes0Tdusado2TdNmHrP+NU3TbPbL1jQtRtO0Ek3TNlXd3nRU4G7BihWi4D78MHnm3wkI6Iu/f7faY2Jj4b774OOPpW60Bt27P4OXVxApKVPRE5eJyDN6dK0xJhMcOfIdJ530DTExj+Hn18V2LAkJsH17vakcY6ssKb/9tu664OAELJZSiou32/WyT0Qqln3NzmkaOQe7cdll99Y7rh4t0KNYsECS0F58ScP4+hwoKoIHHqg15qyz4Mor4fnnbQuOnkbbtufTrt0F7N79JKWlLTcjs3fvy5SUpBAXN8du/4u8PLmgqpM5lJ8v/6czzoCrr3Z8sAqFQuFgCgvl3h4tu6hoKwADB/7KoEFba90GD04nNHSIEyNtAQ4cqG4msHUr/PyzqyNSKBSuZNgwOcnOyJAOQw89BKefLt8PCoWTsCdzqAwYoev6AGAgMErTtNMANE0bBIQ1sn2arusDq263Ni9cN6K8XAxmYmOpvOMmCgp+rZs1ZOWBByA6WtIxKquzc3x8IujWbRaHD68kZ88iGDJE+s7XYOXKUm69dRq63osuXe6sPx6rKfXmzTZXx8ZKNqIt3yHrbJvyHaoHXSfD9Cml0TpBwa8REOBX79Du3eH+++GTT+pogR5BXp789gwbBv/7H/KmufNO6WO/dm2tsS+8IJOadzbwtvQkYmNnAxbS0qa3yPFKS/eye/cs2rW7gLZtz7d7O2uFap3MocceE4Fo7lxlQq1QKDyCpohDxcVJeHmFEhp6JkFB/Wrd6kzMeSJWn6Fhw6R2OzTUtfEoFArX0707tGsnj728pEQhKwsKClwbl6LV0mgre13XdcCaGeRdddM1TfMCXgCuBC50WoTuyiuviEnYt9+SX7waXa8kPHwcJpNcNNdO4Amg78DZXPv1RWzpewX5oTVOYjQLTIsgdXQ6m7t3Zs8799U6TGnpTvr2TaNPnx8xGGp7EdUioSqdesMGEZlsMH48zJ5dt77f3z8OL69gCgs3EBl5fdP+Di7kyBF4/3249dY6Nk31cjjjEGvu+JLf467D4lU7XTsi4jvatl1VZ5vAkhw6X3qII//+hwlT6hEAa3D//dKZ8uab4YIL7IvLXdiwQX5vajW5evRR+OgjuP76WmWPnYCfE2BVIqRd3ERrBE2D665zK7NNf/8YoqIeZNeuGRzaPIqwAfZ/Fg4c+ISion+adLyCgt8BS5UoZSeLFhH89RaeA4Z8BayrWl5RAW+8IR0vBgxoUhwKhULhKo5U9eSwp6ysuDiJgIB4tNYqfluNZt3od1GhULgZVh+i5GQYPNi1sShaJY2KQwBVQtAGoAcwT9f1tZqm3QF8o+t6ViM/1N00TfsHOAI8ouv6Ghv7nwRMAoiKimriS3AB+/eLscz48TBmDHlJ12I0tiUk5DTmzBH/aX//Y7bRL8DgdS0X7/y8zu6KnrKw8wkD+qC/6cz6OuvT029j+PDz6iyvRefOYkJbjyk1wH//K2VAixfLdb4VTTMQHJxAYeHaerd1Rz78EO64Q/wK7r/fvm3+PWca4zMW8YexgLne1RkiXbtuZ968/6LrGmZz7Y+FL+UY86H3yfZVRQYEwFtvSWXP3Ll2vxy3QNPk/du/f42FISEiPFx/fZ0XdArQXwO+At0P7D5lLy+XN+K//4qbt5vQtdPdZP85g5ScqQzqf3XtroD1cOjQKpKSrkTTpF2yvWiaF7GxL+LvH2PfBitWwMSJRBl9mYIBvyXU/oP37y/fSwqFQuEhVGcOmUhNnYHFYmpg7AYiIi5qocicxJ498OKLIugfy9q14i3SuXPLx6VQKDwDq3h83XXwzz9udQ6taAYWi5zDHzhQe/nMmXJ934Jokhhk52BNawN8BcwAngaG67peqWlaka7rQTbG+wJBuq7naZqWACwF+uq6fuTYsVYGDRqkr19fVyBxK664Ar76CrZvR+8WzR9/dKRt21HExy+kXz+ZAfvrLxfENXq0CFdbtthcrevS2bpNG/jjj9rrMjIeZffuZzjjjMMYjXX+lW7J5ZeLZ2NgoAjoXeqxY7Kyee4aBkw9i1LfEPy8LZL51akTuq6zefO5FBVt5NRTd+LjU6O0b/VqSfF+9FF14V0PP/0EI0fKn+fRR+3caMUKOO88mDVL/BXchd27yb0ihn+fhtjQB+h60jMNDrdYKli//iQsFhOnnLIdL69jVWEHUV4O//kPVFby7NX/8uBMP0pK1DmBwrPRNG2DruuDXB2HopqWPgf78UcYNQrWrPmKysqLMBrD6xXZNc1Ajx5zaN/+khaLz+HMmiU/lPWd7I8bB++807IxKRQKz6GyErp2lSZG331Xx6tW4aFs2SKZ/6Gh4FvDg/Svv6Cb48umGzr/alK3Ml3XDwO/AGcjWUSpmqbtAgI0TUu1Mb5M1/W8qscbgDSgZ9PCdzNWrYJPP5VUle7dOXJkLRUVuYSHj+PPP8UTetIkF8VmNaUuKbG5WtOk1OnPP+t6mYWGngmYOXLkT+fH6QB0HdasEe9dsxmmN2ITU1laid+9U9jv1RV99W+1XKNzcr7k8OGVdOv2VG1hqLJSfKKio+sYMiuqOfdcuOQSePpp2LXLzo1GjoSLL4annpKZVHchI4PwP6HtX7Ar72XKyrIaHL5//zyKi7fRo8crzhOGAF59VcTM115jX64fYWFKGFIoFJ6PNXPIx0dKqk47bRenn37A5m3o0CzPFoZASseio2V22NZNCUMKhaIhjMbqizirT5nC87GWFf/6a+3fBCcIQ41hT7eyiKqMITRN8wdGAht0Xe+o63qMrusxQLGu6z3q2dar6nF3IA5Id+QLaFEqKkQsiIk5Khbk5SWiaUbCws5n/nzJGrr8chfFl5AgSkk9mUMA11wj/jxvv117eUjIEMBAQUGdqj+3JD1d/NiuvFL+FZ99Br/8Uv/4Pya+Qa/SLey9azb+p/Y/2kGuctX3pKXdTVDQQDp1uqX2RvPmyRfw7NlSK6aol5dflkYKd9/dxI2giRs5mfR0NKDHil5Y9HLSUu+pd2hZWTa7ds2gbdvRhIdPcF5M+/dLWumECTBmDJmZ9bSxVygUCg/D6jmkaUn4+nb1mMzl4yYpSXkKKRSK5tGuHYSHVwsKCs8nOVmyOHq6PofGnsyhSOAXTdO2AH8DK3RdT6xvsKZpEzRNs9bfnAVs0TRtE/AlcKuu6/nNDdplzJsH27aJGXWVqVBeXiKhoWdSVNSGzz6Dq66SMieXUNOUuh7CwyVhY+HC2glGRmMwQUEneYw4tKYqzDPPFJ2nWzfR7WyV8edsO8iALx9lQ9uRDH6uyq+gqoPcnuXXUFa2j7i4ebVT2Q8ckO5P55/vea7SLqBrV3jkEam2/PFHOzeKipKSssWLpczMHUhPBy8vAqY+S9QncDDnEw4fXl3P0PuxWErp0eNV5xqk3nuvZLHNFuPqrCwbbewVCoXCA7FmDlksYjbdqrFUlbPHt/LXqVAonE/v3ipzqDWRnCzJJ3VMi1see7qVbQFOamRMUI3H3wDfVD1eDCxuZozuQXY2zJghxfETJEugtHQ3JtNWYkkQjdgAACAASURBVGNf4qOPoLTUhSVlIFfo7do1KA6BxPjJJ/DllzBxYvXyNm3OJDPzTSyW8oY7o7kBa9ZAWBj06SMZK6+8Iobbc+bUTUTZceEDnEoxbRe9hmaouogPCKD41XvZGzCFDvmnEBo6tPZG998v6tlrr6m24HZy993w3nswbZokr9Usma2X6dOl5dzUqbKRvW3nnEV6uqT8jxtH1D2dyP5vPikpU0hI2IjBUP11WVDwOwcOfEhU1EMEBMQ5L55Vq+TDOmOGtDNFWtkPG+a8QyoUCkVLsGsX3HknXH/9Y5SUbKBt26muDsk+Vq6U2ajKyqZtZ7FIK1slDikUiuYSHy/nz47KNDEYpN32+PGO2Z/CNoWFoiXk5NRevncvjBjhmpiOwa5uZQpELCgtrSUW5OVJAlXbtuOYP18Sd05qUEZzMpomQTQiDg0bBnFxMH9+bXEoNPRM9u17hcLCDYSGDnFysM3D6jdkqMp9q2ocx+OPi1+4tezm3wV/cUbKe6w69T6Gj64+IdN1nZSYZRiyjMTemQRDs6FjR1n5xx/Si/6BB9wivc9T8PWVj8fo0SLW2dVBztdX/HTGjpX7Kh8ol5GeLiKM0YjXxJvp8fJMts3cSmbm63TpMg0Ai6WSnTsn4+vblejoh5wXS0WFiGYxMUf/mLouOrXKHFIoFJ7OwoXg41PClVe+jMEQSGTkja4OyT5ef12+iEeNavq2Z511dIJRoVAojptbbpFJbIvFMfv76SeZYVfikHNZtkyuMydMqF1qNGgQ3Ogev4FKHLKH33+XvukPPiiqShV5eYn4+8fx77892bpV2pe7nIQE6VdfWlqvY63VmPq++8S/uk8fWR4aegYABQVr3Focys6GlBR5DVY0TbSFvn3ldS1cCOZyM153TCbL0IlBSx+ptY/c3K85dOhHerR/CJ/sF0QIev998WyaMkVan7lTFy0PYdQoyeB68kkpsWysgxwgqt6ECeKrc+WVrm3jm55eXUZ4ww20e2ImYXndyPB6lPbtL8fHpwNZWW9hMm2mT58v8PJyYg3pvHnw77+wdOnRNNP8fGlcpjyHFAqFp/PNN2W89NLVGI0m+vX7kaCgAa4NqKoLbaN8/z1ce62IRAqFQuEKBg2CRYsct7/775frxwUL4KabHLffE5lDh+TvWV5evezrryUZ4auvqjMc3AwlDjVGZSVMniwlWzXEgsrKIg4dWknnzlN45hkR/664woVxWklIkJi3bIFTT6132LXXystZsKDaF9jHpz3+/r2qfIfua5l4j4PffpP7M8+svbxHD0k8eeopKZ3T33ybs4o38sfUTxgaGXx0nNlcQmrqnQQG9qPTwJlwjwWefVY22rQJ/vlHHK6DWrkxppN45RUphZ4+XRr72cXs2aJSTp8uZVSuoLBQ0jyryreIikIbPYa4Z9bz90slpKc/QPfuz5OR8Qht2pxDRMTFzovFRhkrSEkZKHFIoVB4NgUF4OPzI/36LcFg8KdNm+GuDai8HC67zL5SMW9vuPpq58ekUCgULcVVV4k4dPPN0lkpOLjxbRQNs2iRZCwcy4MPuq0wBE1sZX9C8tZbsHmzKCg10r8OH/4ZXS/H338cn34qwpBbfI5OPlnuGykta98eLrxQqqdKS6uXt2lzJgUFv6PrDkpTdAJr1kgihfWl1uShh8Tn+IGb8+j3ycP802Y4Q16p3T5uz55nKSvbTVzcXPGReeQRSXG55RZ5PGIEXHppC72a1kdMjHzvNdZBrhbdu0v21qefis+OK8jIqI7FyqRJBGw4SJfS8WRnv8+2bRdjNhcRFzfHuSbUNspYQcyoQZWVKRQKzyY5GaKipNPOkCH7Xe9zmJoqwtD774tQ1NCtpASGDm10lwqFQuEx/Oc/8MUX8njHDtfG0lpISoLQUCgrq/0b8vTTro6sQVTmkK7DO+/Ar7/aXr9sGZxzjrT4OrqJzsGDn+LlFcKyZWdQXOxiI+qaREdD27aNikMgMX/+OSxZItU8IL5DWVkLMJm2ERTU38nBHh9r1sBpp4FPWpKIdjXUrQBgdQxkr95JCAUEvTun2oQaKCrayp49z9G+/RW0aVPl6hsYCC+9JEq50Sg1t8qEulnce6+cY0+eLKW1bdrYsdH994taed11ddPCoLoe0tY6R5CeLvc1xaGxYyEykug3TRy4tzMFBWvo2nU6gYHNbEVcViY/DtZj1qSiQpS1Y8pYQWUOKRSK1kFSEkRHJ2EwROLtHebqcKpbQvfrJ5lBCoVCcaLRr5/cJyVJ2ZqieSQlSSmFq5vtNJETWxwqL4fbbxdxqHNn2+2V4uKkrrxKLLBYytm581YOHvyUzp3vYto0bwYOdKPPkKaJ4WFiolxkNnCSc/bZch08f35tcQjEd8gdxaEjRySR670rlsOQy8QjqH37WmOigIAQ+O3sFxh+Yb+jy/PyfmD79svx9g4jNvbF2ju+9FJJc4mPrzZhUhw3/v7w5puirQwdKhprbKwdG737rng+/fFH3fUHDsh087p1TonZZuaQ0Qg33IDxmWeIf/YD9pd/RnT0Y807Tk4OXHSR1Ed262ZbiDz/fJueV9bMISUOKRQKTyYpCWJikgkObqbQ7iisLaFVJzGFQnGiEhsr573W70NF80hOli49HsaJKw7l5Uk20K+/ykXYE080Wv9XXp7Ltm0XU1Cwmujox8jNncGmTeIb61aJJjfeKCa2y5bJRWg9GAySiPHgg7BzpzTm8vOLwcdHMiQ6d769BYO2jz/+gNssc7n6kzuhbx95jdHRtcZoQAQwvOq5ruvs3z+3ymeoP/37L8PX95i6HE2DN95ogVdw4nDeedL84KKLYPBgyVA766xGNjr7bNi2zfa6116DO+4QTyhntAVMT5f0z7BjZrFvvBGefpq2n6bRdsay5h1j2zbpBJGVJSV0l1/e+DY1yMyUEAMCmheGQqFQuJLkZJ3hw5MIDHSxd8/hw5Kt+vffUpMe6MQmAwqFQuHOeHuLgeu778Kff8qySy+F2247vv1Nmybn1m+/fWLNai5eLOJAdrZkDnkYJ6bnUHKyXK3+9ZeYRc2a1agwZDJtZ+PGwRw5spbevT+mW7eZLFhgwN9fPLzcilGjxENn/vxGh153nYjEb78tzzVNo02bMzl8eDW6rjs3zqZSWUmbhyczl6lYzhstXeSOEYaOxWKpICXldlJTpxEePp6TTvoNP7+uLRSwYtgwWLsW2rWDc8+F995rxs6uvlo68FnfrI7G2sb+WKW3WzdRuhYskEy14+X772HIEPGr+PXXJgtDIJrSifT7qlAoWieZmdn4+x8hIMDFJ85r1kj3mMhIuZBRKBSKE5mpU6VqprJSJjRfffX49nP4sNh0fPstrFjh2BjdnfnzYeNGsaUZP97V0TSZE08cWrFCDGsKC6WMyA5lJz//RzZuHILZbGLgwFV06HAFhYXw8cdyfRca2gJxNwWjUbIdli+HXbsaHNqxozRDev99sUEBKS0rL8+ktDTD6aHazeHDMGYMp218nYUdp2NMXNqoA3hFxSG2bBlNZuabdO16H/36LcFoVB3IWpoePWQCYtgwuOEG8SM6Lo2lbVuZwfjoIzCZHB7nUXHIFjffDPv2wQ8/NH2/ui4/ruPGScruunUNdhJsiKwsZUatUCg8m7Iy0HXx+AkIcHEZl7V84qef4J57XBuLQqFQuJrbb4fVq+V2002QliY2JU2lZmnaiVamlpQk5/w//eSRpconljj0xhtS+xcVJRdoQ4Y0usmWLXPZsmUMuh6Dv/86MjJOY9MmEUOLitzIiPpYbrhB7t95p9GhkyZBbq5MnkFt3yG3IC0NhgxB/+UXbjUuYNOVL4CXV4ObFBensHHjaRQUrKZXr/eIjX0OTTux3u7uRFgYfPed/Oa8+KJ0ytu4ETZtsv+2ZQtYbrxZjKc+/9yxAVos4jlUnzg0YQJ06ND0rKWKCknHvfNO2ceaNdD1+DPXMjNV5pBCofBsUlOhSxe5WGi2uX9zSUqSWTK7uiYoFArFCUTv3pJBlJra9G2tJv9GY/XjE4GiIti71yPLyaycOJ5Dy5bJlem4cZLyY0ff+aVL19GmzVR+/308Tz31ESUltbfp31+SkNySqCgRwt59F2bMkA9nPYwcKe3HX3wRLrkEAgP7YjSGcfjwGjp2vLblYq6Piy6CgwfZ+vJPvDVtGF810qzKbDaxefO5mM0mBgz4mTZtnNTdStEkvL2lBLd3b7EOWnYc9j1XXnEGH8XHS8rm9dc7LrisLJnO7tbN9npvbzne88/D9u32m5Y//DC89ZZ0Ynv66UbLVxti3z7Yv79Z2pJCoVC4nKQkaxv7YHx8XJwKmZzskTO7CoVC4XSs343WrltNITlZunSdf/6JJQ7t3Cn3ShzyAMaOFcOTiRMbzToBEf6WLPmHG26Afv3m8vHHdcWkk092MyPqY5k0CS64QFI2Jkyod5jBADNnwrXXSnnZDTcYCA093T0yh1JTJWXk1Vf5tkhaz59xRsOb7N79DGVlexg4cLUShtyQKVNg+PCmT0QsXw5vvKEx49ZJ9Hzzbti6VRRaR2Crjf2x3H23tGCbNk3KUxv78CclwezZksX37LPNDvHee6tN5BUKhcJTyc9/n4sumktg4KlozjqJmjxZOs3aw/GarSoUCkVrplcvub/4YvGISE6Wa2iLBfr2bbxcrF8/OU9ftkzOmf39xe/3P/9xfuyNMXSoeF5ERoqgExQEjzwCTz0lotbKlXD66U3b54gRYlkDShzyCAwGcV+2k1mzIChoJ+DHhAld3FsEqo+xY+VNP39+g+IQiGY2f74kOFx4oZSW5eUlUl5+AB+fDi0UsA0SE+V+7FjWTJXPWrt29Q8vLk5h794X6NDhaiUMuTH9+smtKYwaBT/+CNf9PJHffR5Ae/tt6WDmCOwRhyIi5IthyhT48kvxP6oPXRdTv6AghwhDv/wizc1mzGg4RIVCoXB3NO0nAHr2nO28gyxfDgMGyARZQxgM0uxAoVAoFLUJDhafzyVLpAPXrl3inblvnwhDF1wg37P1MWKEjPfzk6yL55+XZkKuFocKC0UYiouDlBQx3h48WH43evSQ2evVq5smDpWWSrOZc86RihcPzkg9ccShJrBjB7z8Mrzzzk4CA+M816vGaJSshWeekfrHBupRNA3mzoWEBHjsMZg1y+o79BsRERe3VMR1SUyE3r0xx8Ty++/wv//VP1TXdVJT78Bg8KV79+dbLkZFi+DnJ77O48e3I/mki+m9cCE895zMRDSX9HT5EDTS/Y5bbhHfobvvhjFj6m97vHgx/PyzfKgiIpoVWkWF6EwxMSLeKhQKhSfj55dEevp5DB8+1DkHKC2V7/RHHoHHH3fOMRQKheJE4Mor5QR08WLJiI+Nrc4YuvNO6TbTGI8+KpOmr7/uHiVm1vhvuklOrJOTpVFMcrIkkixZ0nQT7ZQUyai66aaGL1Y9AA9VPZyHrkvVSEAAxMbuJCCgl6tDah433igv6t13Gx06cKBkV7/+OqSnJ2Aw+HP4sAtLy44cERV2/Hi2bpWnZzaQDJSXt4z8/O+JiXkcX1/l2tsaGTdOEuLuTpokHey+/NIxO87IEPHUx6fhcUajGCft2yepp7YwmUQ8GjBAxKRmMneuTGq88opjdDCFQqFwFWazhYiIZHTdiSn3qalyku7BM7cKhULhNli/S62CiVXgaUrplKbJftyhc5k1hjFjxFM0KUk6vhQWSoy9ezddxLKObwW/Oypz6BiWLpWssldfraCiIh1//0tcHVLz6NYNzjsPFiyQWbRG/JaefBI++wymTvVhzpzBrvUdWr5cXPLHjWNNVRj1iUNmcwmpqXcSENCHzp2ntlyMihbn1Vehb59hZAXHETl/vtRENpeG2tgfy+mnwzXXiIP7dddBz5611z/9tGTqffJJg0bw9pCd/X/27jw8yurs4/j3ZE8IEEjYSmQREEGEUHEXFRdARbQWi0sVUF5aKyi4FbUi0qpYqFrFtQpqK+JOEau2VajghgRR2ZewE5YAAbNv5/3jTMieTNYJeX6f65prMs829xwy4cw959zHTSUbOrTKmaEiIo3etm07iYjIKH+VssxMV48C3Gof/mbDN2xwf3MLffGFuz+Gaz6IiDQarVtD27buC/v+/WHxYrcMcXVHxvfq5Wp2fvpp+fuNcdO7Ckfl5+a6L2NDQ10MUVFue06OmxaWl1f1cxrjigQfPOj6+StWuBhCQlxNpR494Msv3YrEhTGuXQuvvurqDp16atWLWOXlubrGUPYzwbHIWtuobqeccooNlPR0azt1svbkk609cmSDXbQIm5z8SsDiqTPvvGMtWLtwoV+Hv/SSO3z+/AfsokVBNjf3cD0HWIEbb7S2VStrc3Pt1Vdbe9xxFR+6ZctUu2gR9uDBzxouPgmYBx6w9i7+7H5RV6+u/QU7dLD2ppv8Pz452doWLawdOtTagoKi7Rs2WBsWZu0NN9Q+JusuExbmLivS1ADLbSPod+jWcH2wTz752C5ahF20aHHZndOmub/pYO0f/+jfBXNyrI2OLjqv8BYR4Tp1IiJSe0OGlPwbO2hQ9a/xl7+U/Vtd+jZ5ctHxTz9tbXi4+yxYfPtTT1V9neK3du1cZ3rJkqJt/fq5a11/fdG24GBr9+619uWXi7b97ndVv645c9yxXbtWv00CpLL+l0YOFTN9Omzf7hKjOTluKbrIyCaQARw+3GVE//Y3NyenCmPGuOLUzzxzDvfdV8CRI1/RuvWQBgi0mPx8t8rapZdig0NYssTVNStPZuYWtm+fTps2I2nValDDxikBMXkyDJwzitxd9xP84ksEPfl4zS+WkeGWsq9Opef27d0Sf5MmwYIFcMUVRXNSw8Nd0b1aWroU/v53uO8+98WGiMixbt++dcTHwwknlDOqZ+VKV9vCWvj+e/8uuHmzK3Q6ZQpcdFHR9g4dir5lFhGR2pk719U4KFSTkZnjx7tRofn55e8fN879P1AoMRGys91txYqi7StXQmwsvP++f8/5ww/u59dfd/evvQYXX+x+fvbZohIQbdq4EVI33gi9e7tVL/35v6gw5iWNYJXvOlBlcsgYEwF8DoT7jn/HWvtgsf1PATdZa6MrOP9e4GYgH7jNWvtJXQRe1zZvdp/nrr8ezj0XduxYD0BUVBNIDoWGuozPjBmwaxd07Fjp4UFBrqzKeeedibVBpKYuafjk0LJlkJICw4axebObXlPRlLJNmyYBwXTrNrNBQ5TAiYqCPzzVlveu+gXD//YqkdMfcRWra2LrVndf3WXAbr3VTdecONFN3fz3v+Hjj101+/btaxaLT16e+//suONcckhEpCnIylrLTz+1on37cqYjrFvnarUVFPhf76GwdsSwYW74v4iI1L3WrSsv/OqPsDC3hHxF+vd308UKFa9PVPz/hHXr4KST/Ivn5z8vSg69/76bZjZiRNG05RYtyl4nJMQlsQYMcLVNraXSZcvXrXPPU8Xn62OFPwWps4ELrLX9gARgqDHmDABjzACgVUUnGmN6A9cAJwFDgWeNMZUXvQmQiRNdDqXwC/+MjA2EhLQmNDQ2sIHVlbFjXaa2cE5kFQYMgF//ujkbNvQnOTkAmdCFC119pCFD+Pxzt6m8vwEHDnzEgQP/pEuXB4iIiG/YGCWgrrwSvjvl/4jMOMjhOe/V/EKFy9h37Vq980JDXRZ161ZXGGjiRPef1fjxNY/F54UX3JcVjz9e8YJoIiLHmpCQtRw82IugoFId7bw8t9pLr17utnGjf/UkmlARUBERT+vVC7ZtcyP6rS2ZENqxw40SLdzu78il4sft3ev6+v7Ws+vVy9UqSkmp/Li1a5vU/0FVjhzyzUtL8z0M9d2sL8kzA7gO+EUFp18BzLPWZgNbjDGbgNOAryo4PiAWLnS3mTPhZz9z2zIzNzSNUUOFunWDCy90Ix3uu88ND6rCI4/AI48MpFu350hJ+RemsqxpNbRseTYhIS0qP+iDD+Ccc6BVK5YscQnr0n8HCgqy2bTpNiIjTyA+flKdxCbHDmNgzN8vYHPv48mZ9jda3nJdzS5UmByqYuRQVpbLA5X4+3/eeXDttW5UHsCiRS5pVAv797va8RdeCL/8Za0uJSLSqMTErCMl5fKyO6ZOdcVHTzzRdf5zctyqj1UVAv3oI/dtbVXHiYhI41b49/+OOyA6Gg4fLrn/zjvdSJ9Dh2qWHCrvsT/n/v73bqpyeax1NWma0AIIftUc8iWCEoHuwDPW2m+MMbcDC6y1yZUkDToCXxd7vNO3rfT1xwHjADp16uR/9HVkxgxX0+O224q2ZWRsoFWriyo+6Vj0m9/Ar37lVlG6/voqD4+NhYSEiwgJeZJVq6quVeSvZs36ccopywkKquDXb9s2+PFHmDmT1FSXuDvvvLL5rB07/kJm5ib69v2EoKAqliCXJqlnryA+Om8cl/xvMj++8CUn/6aS4aoVSUpyw3OqWHXhb3+D2293i+mcdlqxHTNmuPpYw4bB+edX//nLeZ7Dh+HppysfxSoicixJTf2JmJh9HDlSqohaejo8/LD7uXDKQcuW8Nxz/l141Ki6C1JERALjjDPcaICXX3aPmzeHQYPcKNJvvoHZs4u2+zvF7fTToXt3N7L/gw9cGQh/nXKKq9n7979XflyzZu6DahPhV3LIWpsPJBhjYoD3jTHnAlcD59dFENbaF4EXAQYMGGDr4pr+P7ebvnHddUVf+OflpZGTs6tpjRwCNwzh1FPh7rtdkWo/vmm79tpLufLKlRw5ksVbb9W+vuORI9+yadMEdu9+nvj4CqbffPihux82jAcfhAMH4IEHSh6SlbWdbdv+RFzcVbRuXY03ujQ5A+fdSnLHpwmZNJ78Md8SHFbNmauFy9hXkYlZs8b9vRg/3iWIjiYrO3Z0RctiYmr2AkpZtsythNmEvoQQEWHnzl0AREeXmgK+y23n1VeLqu+npjZgZCIiEnDHHec+9NWltm3dNOWaiItzRW89plqrlVlrU40xi4BBuFFEm3yjhqKMMZustd1LnbILOK7Y43jftkZj1y73Lf1JJxVty8zcBDSRlcqKCwqCWbNcZnbatKKpMJUICTHcf38/zjoLnngCHnusdiE0b34aBw78ky1b/kDbtr8iLKxt2YM++AC6d+eHrBOYNQt++1tXo6y4zZvvBKB791qsUiVNQnT7aH4Y/xfOeuoaPh/1Iue+cUv1LrBli5t2WYWkJLcQ2bffui8vxo4ttjO27mqTJSa6ovgiIk3J3r27CA6G1q1LDSAvTA41kWKeIiIix6oqC88YY9r4RgxhjIkELgYSrbXtrbVdrLVdgIxyEkMAC4BrjDHhxpiuQA9gWd2FX3uFq/L16VO0LTPTLWPf5EYOgZsPc/PN8OSTfq8GcuaZbrGzJ54oWTi+JowxdO/+NAUF6SQl3Vv2gLQ0+Owz7LDLGT/B0KoV/OlPJQ85ePC/7N//Dp063UdEROfaBSRNwplP/IrvYgZx8pv3c2B9FYXjirO2aORQFZKS3IC7c86ByZNdjbq6tm8f7NzpFj0QEWlKUlNdEqhdOyWHREREGiN/VivrACwyxvwAfAv8x1q7sKKDjTHDjTHTAKy1q4G3gDXAx8CtvilqjcaqVe6++MihjAy3jH1kZHn5ribgkUdcoa8JE9yHYz9Mn+6mlN12m9+nVKhZsxOJj5/Enj2zOXz465I7P/0UcnL4NHIYS5bAo4+66aeFCgpy2LRpAhER3TjuuLtqF4g0GSbI0PyVp4m2P7H6ymqs/b5vn1sVoYrkUH6+K0bdrZsbfHfoUNmpjnVhxQp3f8opdX9tEWmajDERxphlxpjvjTGrjTEPldr/lDEmraLzG0pamksCxccrOSQiItIYVZkcstb+YK3tb63ta63tY62dVs4x0cV+XmCtnVLs8cPW2m7W2p7W2o/qLvS6sXq1qzUVF1e0LTNzA+HhxxEcXMsCO41Vmzau+OOnn8K77/p1Stu28Mc/wn/+A++/X/sQOnd+gLCwn7Fx43hK5AsXLsS2aMFNs8/h1FPdIKfidu58ioyMdfTo8VeCgyNqH4g0Gd2vOIkvTrmNc9a9xJpXv/XvJD9XKtu509XDO/546NcPbr0Vnn8evvuulkGXkpjo7ktPoxQRqUQ2cIG1th+QAAw1xpwBYIwZALQKZHCFcnN3kZYWQ/PmpfpWu3a5GohacUxERCSg/Bk51KStWlVyShm4lcqionoGJqCG8pvfQEKCWy4wPd2vU265Bfr2hUmT3GCL2ggJaU63bjNJS0skOfklt7GgABYu5Pv2Q9m5L4xZs0quUJadvZtt2x4iNnYYsbF1t3qaNB0/n/8g+4PaUXDreAryCqo+wc/kUOnDpk1zZYbGj3e/tnUlMdHVY23Zsu6uKSJNm3UKRwaF+m7Wt9LsDOCegAVXwi6OHClndNCuXRAfX3a7iIiINChPJ4cKCtzIoeJTyqy1ZGZuaHrFqEsLDnbzY3bscNPM/BAS4k7Zvt1N96qttm2voWXL80hKuo/c3ANuTs2ePTy5aRg331xquXBg8+a7KSjIpXv3J2v/5NIktYhvwaZxM+iTvowvxs6p+oTCrE+XLpUetmWLuy9MDsXEuOLsX34J//hHzeMtLTFRU8pEpPqMMcHGmJXAPtz0/2+A8cACa21yFeeOM8YsN8Ys379/f73FGBKyn+zsdmV3JCdDhw719rwiIiLiH08nh7ZtcyNgio8cys1NIS8vtWkWoy7t7LPhxhth5ky/l/kbOBCuvx7+/GfYtKl2T2+MoUePWeTlHSYp6X7sBwspwLAk+pIy+arU1P+xb99cOnW6h8jIqleWEu8665nr+b7FOfR6bTKHNldRNTopCX72M4iofIpiUpLLpx5XbO3FUaPg9NPhnnvcioe1lZLiEq9KDolIdVlr8621CbhVYU8zxpwLXA087ce5L1prB1hrB7Rp06behkXtFAAAIABJREFUYjQmBwgvu2P/fjd3XURERALK08mh8opRF65U1uRHDhV67DH3wbgalaZnzHBLek+cWPunj47uQ3z8BJKTX2TXkjf5ijO589E4ivdPCwry2LhxPOHhnenUaXLtn1SaNBNkiPjbLFrZg/zwiymVH1yNlco6d3aj5woFBcEzz7ia1lOn1i5mKKo3pJXKRKSmrLWpwCJgENAd2GSM2QpEGWNq+ZVO7QQF5QBhZXekpJQs/CgiIiIBEVL1IU1X4TL2JVcqa8LL2JenfXt46CFXSGjBArjiiipP6dDBfRh+4c71PD4pipiTj6vynMoYM5VO8a+zZ+Q6ktr9kiuumE1ysUHwP/20gvT0VZx00vtNt0i41Kmev+rH4odvZeAPz7DujZs58doKKjwnJcEFF1R5vYpySKecAuPGwdNPw003wckn1zxmJYdEpCaMMW2AXGttqjEmErgYeMxa277YMWnW2oAuwRoUlIsxoSU35uZCaqqSQyIiIo2Ap5NDq1a5aSLFi79mZKzHmFDCwzsHLrCGduut8NJLbijQ4MEQGVnlKROu2c/Ye84g5clW9GYN2dRm5bCWPD/oLHre/0+O6/kuGzeWXUEtLu5K4uKqTlyJFOq/YBoHu84jeOJ4uGYpGFPygOxsVwjVz5FDv/hF+fsefhjeeAMefxzm+FHmqCIrVkC3bq6ekYhINXQAXvUVoA4C3rLWLgxwTGUEBeWUTQ4d9E39VXJIREQk4DydHCpdjBrwFaPuRlCQh5omNNRVmh40yE0z82OOTOiD9xFij9CCVPbc8WeO3F7F9J1KhCRtoMPgf5Hx+kiCn/1zuceEh8djSn+4F6lEy84xPJ3wGBO+uwn+/ndXX6u4bdvcVMoqkkM//eRKYnTtWv7+2FgYNgw+/NAVuQ+q4WTdxMSyRdhFRKpirf0BqGB45NFjohsonAq5kUOlppUVFsCux1pHIiIi4h8PZUBKys+HtWvhootKbs/I2EBkZBNfxr48558P11wD06e7D9GVfWBetgxefhlzxx2wYwcxzz5KzIQbq1zxqVzWwrjbIDKSZo/+FSLKWclEpIZyrxvF19+9wKl33UPwFVeUHCbo5zL2pVcqK8+wYTB3rntrnHFG9eM8cAC2boVbbqn+uSIix4Lg4FyCgkqNHEpJcfcaOSQiIhJwni1IvXmzm1VSchn7fDIzN3mn3lBpM2e6iruTJlV8TEGBm4bWvj1MmQJ/+YsbKlHZOZX55z/hk09g2jRop8SQ1K2B5wVxK88QlFJO1Wg/k0P+HDZkiFvNbGENJ3KsWOHutVKZiDRFubkQEpJDUFCpkUNKDomIiDQank0OFa5UVnwZ+6ysHVib7Z2Vykrr2NElfBYsgH/9q/xjXn4Zli93S5a1aAHx8fDAAzB/Pnz8cfWeLyPD1Tnq08clnETqWP/+sL7ZKXzR21c1+scfi3YmJbmV+tq3r/gC+Jccat0azj675smhwmLU/SudGCIicmzKzISQkFyCgzVySEREpLHydHLIGOjVq2hb4TL2nh05BC5Z07OnW9o+K6vkvoMH4d57YeBAuO66ou2TJsEJJ7hzsrP9f67HHnN1X2bNKrlGuEgdCQmBM8+E++zDbkrZhAluKiMULUFWRS2rpCR3aqtWlT/X5ZfD99/D9u3VjzMx0dU0at26+ueKiDR2LjmUUzY5VFhzSMkhERGRgPNscmj1avdhrFmzom2Fy9h7duQQQFiYG2GxebObMlbcH/7glpydNavkB+rwcHjqKdi40S3Z5I/Nm11y6Npr4bzz6i5+kVIGDoSla2PJ+MMj8L//wbx5bkdSUsVVpovxM4fEsGHu/sMPqx/jihWaUiYiTVfhyKGQkFLTypKTXVY8LKz8E0VERKTBeDY5tGpVySllAJmZ6wkObk5YmMdr31x8Mfzyl26N7m3b3LYVK+D55930r759y54zZIhb6/tPf4IdO6p+jkmT3CppM2fWbewipQwc6AYLLeo21mVg7rrLLUFWmPWpwpYtfh1Gz55uKfrqTi07dMiFouSQiDRVGRmWkJA8QkJKjRzatctNaRcREZGA82RyKCcHNmwou4x9RsYGoqJ6asl0KBoBdOedrgj1+PFuqdmHHqr8nIICd05lPvwQPvjA1Tf62c/qLmaRcpx+ustDLvky2I16273bTZ/86acqsz4FBf4nh4xxo4c+/RTS0/2PT8WoRaSpy8zMBSg7ckjJIRERkUbDk8mhDRsgL6+8kUMbvD2lrLhOneD+++Hdd+Hmm+Grr9w0sJiYis/p0gXuuw/eftt9Qi5PVhbcfjuceKK7F6lnUVEu8bJkCW6d+Ztugtmz3c4qsj7Jya6Mlj/JIXB1h7KzK/71L09hMeqf/9z/c0REjiVFySGNHBIREWmsPFkFePVqd1985FB+fhZZWdto3350QGJqlO68E+bMgVdecR+qb7yx6nPuvtsdf8stMGpU2f0//ujqDf3nP6oxIA1m4EB48klX9yLy0Ufhvfdc/aw6WMa+9PM0b+6mlg0f7t85iYnQuTPExvp3vIjIsSYrK4eQEAgNLZYcys2FvXuVHBIREWkkPJkcWrUKgoNdjZBCWVmbAauRQ8VFRLg6Q2PHwrPPQpAfA80iIuC55+Cqq1wB6/KMGwcXXVS3sYpUYuBAmDEDli2D885r64qtT5vmigRVojA55EfdasDlO4cMcckha6suYg0uOaQpZSLSlGVl5RIdDWHFvxTas8f9oVRySEREpFHw5LSy1auhRw+XxyhUuFKZp5exL89FF7miK/37+3/O4MFw+LAr7lTe7YUX6i9ekXKcfba7X7LEt+Gmm2DrVoiMrPS8pCSX4Onc2f/nuvxyNx3tu++qPjY11Q2kU3JIRJqyrKwcoNTIoV273L2SQyIiIo2CJ5NDq1aVLUadmVm4jH2PAETUyNWkQHdwsKsCXN5NpIG1bu1qjB1NDvkpKQmOO656MyAvucS9ZT74oOpjCxNISg6JSFOWne1qDpUYObR3r7vv0CEAEYmIiEhpVSaHjDERxphlxpjvjTGrjTEP+ba/7Nv2gzHmHWNMdDnndjHGZBpjVvpuz9fHi6iOzEzYtKlsMeqMjPWEhbUnJKRFYAITkXo1cCB8+aUrRu8vP1e7L6FNG1eiy58l7QuLUSs5JCJNWXa2GzkUHl7sC6KUFHcfFxeAiERERKQ0f0YOZQMXWGv7AQnAUGPMGcAka20/a21fYDswvoLzN1trE3y339ZN2DW3bp2b4l7eMvaRkT3LP0lEjnkDB0JaGnz/vf/n1CQ5BG5J++XL3fSyyiQmuoUB9dlIRJqywpFD4eHFRg4pOSQiItKoVJkcsk6a72Go72attUcAjDEGiARsvUVZh1atcvflLWOvekMiTdfAge7e36llGRmuXmpNkkOXX+7uP/yw8uMSE7WEvYg0fTk5hcmhYiOH9u93dd+iogIUlYiIiBTnV80hY0ywMWYlsA/4j7X2G9/2OcAe4ETg6QpO72qM+c4Y8z9jzMAKrj/OGLPcGLN8//791X8V1bB6tasf0r170bbc3EPk5u7XSmUiTVh8PHTp4n9yaMsWd1+T5FCfPm5EUGVTyw4fho0bNaVMRJq+nJxyClKnpGjUkIiISCPiV3LIWptvrU0A4oHTjDF9fNvHAD8D1gIjyzk1Gehkre0P3AHMNcaUKepjrX3RWjvAWjugTZs2NXwp/lm1yi1hX7x/kpm5EdBKZSJN3cCBLjlk/RjnWLiMfU2SQ8a4qWX/+Q9kZZV/jIpRi4hXnH22GzkUFFRqWlk99/lERETEf9VarcxamwosAoYW25YPzAN+Wc7x2dbaA76fE4HNQEAzMKtXl1eMunClMiWHRJqygQPdTIYNG6o+tjYjh8AlhzIyYPHi8vevWOHulRwSkaauVy83cigoSCOHREREGit/VitrY4yJ8f0cCVwMrDfGdPdtM8BwYF0F5wb7fj4e6AEk1V341ZOWBlu3VrSMfRCRkTX8FCgix4Tq1B1KSoJmzWr+2WXQIFdKo6Il7RMT3VS3tm1rdn0RkWOFtW7kkDHFRg7t36/kkIiISCMS4scxHYBXfUmeIOAt4ENgiW+KmAG+B24BMMYMBwZYa6cA5wLTjDG5QAHwW2vtwbp/Gf5Zs8bdl7eMfURE15LDnUWkyenZ081iWLIExo6t/NjClcqMqdlzRUTAxRfD22+Xv/+//4Uzz6zZtUVEjiUFBW7kkDEaOSQiItJYVZkcstb+APQvZ9fZFRy/AFjg+/ld4N3aBFiXClcqK7uM/Tqiok5s+IBEpEEZA+ec4//IoeKF62vi5pvh66/hrbfK7gsKgl+WmYwrItL0FI4cOjqtLD0djhzR0EkREZFGxJ+RQ03GqlVu1dSuXYu2FRTkkZGxjtath1Z8oog0GQMHwvvvw65d0LFj+cdY65JDgwfX7rkuvxz27KndNUREjnVlppWtX+/ue/YMUEQiIiJSWrUKUh/rVq+G3r0hOLhoW2bmJqzNoVmzPhWfKCJNhj91h/buhczMmhejFhGRImWmla3zlans1StAEYmIiEhpnkoOrVpVdkpZerqba9as2UnlnCEiTU1CAkRHV54cqs0y9iIiUlLRtDLfyKG1a93c2trO3RUREZE645nk0KFDsHt32WLULjlkiIrSt1ciXhAS4gpBKzkkItIwyh051K0bhIcHMCoREREpzjPJodWr3X3ZYtSriYzsRnBwZMMHJSIBce65biThhg3l7y9MDnXp0mAhiYg0WUU1h3zJobVr4UQtBCIiItKYeCY51KMHzJ4Np51Wcnt6+irVGxLxmLFj3dSy2293xadLS0pyxaojIho+NhGRpqbEtLK8PJeZV70hERGRRsUzyaF27WDMGIiLK9pWUJBNRsZGJYdEPKZ9e3joIfj4Y1iwoOz+pKSSqxqKiEjNlZhWtmUL5OYqOSQiItLIeCY5VJ6MjPVAPlFRKkYt4jXjx7tpphMnupXJituyRfWGRETqSomRQ2vXuo2aViYiItKoeDo5lJ7uChFp5JCI94SGwqxZsHUrPPZY0fasLNi1S8khEZG6Ym0OYDAmGNascRuVHBIREWlUPJ4cWoUxIURFnRDoUEQkAM4/H665BqZPLypCvW2bq0Ok5JCISN1o1qwf7drd6B588gn07g0xMYENSkRERErweHJoNZGRJ7hhziLiSTNmuOXtJ01yj7WMvYhI3WrbdgS9er0CKSnw+edw1VWBDklERERK8XhyaBXNmqnekIiXxcfDlCmuMPW//qXkkIhIvVm6FAoK4NJLAx2JiIiIlOLZ5FB+fgZZWUmqNyQiTJwIPXu6pe3XrnVL2LdvH+ioRESamMJi1H3U9xIREWlsQgIdQKBkZKwFrJJDIkJYGDz1FAwZAtu3Q/fuYEygoxIRaWLWrYOOHaF580BHIiIiIqV4duRQevoqAE0rExEABg92ZTBycjSlTESkXqxdC716BToKERERKYeHk0OrMSaciIhugQ5FRBqJJ56AqCi3kI6IiNQha93IIS1hLyIi0ih5dlpZevoqoqJOJCjIs00gIqV06gRr1kBsbKAjERFpYnbvhp9+0sghERGRRsrTI4dUb0hESuvcGaKjAx2FiEgTs26du9fIIRERkUapyuSQMSbCGLPMGPO9MWa1MeYh3/aXfdt+MMa8Y4wp9+OUMeZeY8wmY8x6Y8yQun4BNZGXd4Ts7O2qNyQiIiLHtEr6aa/7+l6rjDGzjTGhAQ20cKUyjRwSERFplPwZOZQNXGCt7QckAEONMWcAk6y1/ay1fYHtwPjSJxpjegPXACcBQ4FnjTHBdRZ9DaWnrwbQyCERERE51lXUT3sdOBE4GYgExgYuRFxyqEULaN8+oGGIiIhI+apMDlknzfcw1Hez1tojAMYYg+t02HJOvwKYZ63NttZuATYBp9VJ5LWg5JCIiIg0BZX00/7l22eBZUB8wIIE2LDBTSkzJqBhiIiISPn8qjlkjAk2xqwE9gH/sdZ+49s+B9iD+2bq6XJO7QjsKPZ4p29b6euPM8YsN8Ys379/fzVfQvWlp68iKCiKiIjO9f5cIiIiIvWpon6ab18ocAPwcQXnNkwf7OBBaNOm/q4vIiIiteJXcsham2+tTcB963SaMaaPb/sY4GfAWmBkTYOw1r5orR1grR3QpgE6DhkZq2nW7CSM8Ww9bhEREWkiKuqn+TwLfG6tXVLBuQ3TB0tLU7V/ERGRRqxa2RFrbSqwCFc/qHBbPjAP+GU5p+wCjiv2ON63LaDS01epGLWIiIg0KaX7acaYB4E2wB2BjAuA9HQlh0RERBoxf1Yra2OMifH9HAlcDKw3xnT3bTPAcGBdOacvAK4xxoQbY7oCPXDz3gMmN/cAOTl7VG9IREREjnkV9NPWGWPGAkOAa621BYGMEXAjh5o1C3QUIiIiUoEQP47pALzqW2UsCHgL+BBYYoxpARjge+AWAGPMcGCAtXaKtXa1MeYtYA2QB9zqG2kUMIXFqKOiNHJIREREjnll+mnW2oXGmDxgG/CV+x6P96y10wISobUaOSQiItLIVZkcstb+APQvZ9fZFRy/ADdiqPDxw8DDNQ2wrmmlMhEREWkqKuqnWWv9+QKwYeTkQF6eRg6JiIg0Yp6ryJyevorg4JaEh5dZNE1ERERE6lpamrvXyCEREZFGy5PJIbdSmQl0KCIiIiJNX3q6u1dySEREpNHyVHLIWkt6+mpNKRMRERFpKIUjhzStTEREpNHyVHIoJ2cveXkHtIy9iIiISEPRyCEREZFGz1PJoYwMFaMWERERaVCqOSQiItLoeSo5lJ6+CkAjh0REREQaSuHIIU0rExERabQ8lhxaTWhoHKGhbQMdioiIiIg3aOSQiIhIo+ex5NAqmjXro5XKRERERBqKClKLiIg0ep5JDhWuVBYVpSllIiIiIg1GBalFREQaPc8kh7Kzd5Kff0TFqEVEREQakkYOiYiINHqeSQ5lZKwFVIxaREREpEGlp0NoKISFBToSERERqYBnkkOtWw/mrLP20aLFaYEORURERMQ7pkyBXbsCHYWIiIhUIiTQATSksLA2gQ5BRERExFsiItxNREREGi3PjBwSEREREREREZGylBwSEREREREREfEwJYdERERERERERDxMySEREREREREREQ9TckhERERERERExMOqTA4ZYyKMMcuMMd8bY1YbYx7ybX/dGLPeGLPKGDPbGBNawfn5xpiVvtuCun4BIiIiIiIiIiJSc/4sZZ8NXGCtTfMlgJYaYz4CXgd+7TtmLjAWeK6c8zOttQl1Eq2IiIiIiIiIiNSpKpND1loLpPkehvpu1lr7r8JjjDHLgPh6iVBEREREREREROqNPyOHMMYEA4lAd+AZa+03xfaFAjcAt1dweoQxZjmQB0y31s4v5/rjgHG+h2nGmPX+v4RqiwNS6vH6jZ3XXz+oDUBtAGoDUBuA2gAC1wadA/CcUonExMQUY8y2erq83mv+U1v5T21VPWov/6mt/Ke28l9jaKsK+1/GDQzyjzEmBngfmGCtXeXb9jcg3Vo7sYJzOlprdxljjgc+Ay601m6uTvR1yRiz3Fo7IFDPH2hef/2gNgC1AagNQG0AagNQG0jD0O+Z/9RW/lNbVY/ay39qK/+prfzX2NuqWquVWWtTgUXAUABjzINAG+COSs7Z5btPAhYD/WsYq4iIiIiIiIiI1DF/Vitr4xsxhDEmErgYWGeMGQsMAa611hZUcG4rY0y47+c44GxgTV0FLyIiIiIiIiIiteNPzaEOwKu+ukNBwFvW2oXGmDxgG/CVMQbgPWvtNGPMAOC31tqxQC/gBWNMge/c6dbaQCeHXgzw8wea118/qA1AbQBqA1AbgNoA1AbSMPR75j+1lf/UVtWj9vKf2sp/aiv/Neq2qlbNIRERERERERERaVqqVXNIRERERERERESaFiWHREREREREREQ8zDPJIWPMUGPMemPMJmPM5EDH0xCMMbONMfuMMauKbWttjPmPMWaj775VIGOsb8aY44wxi4wxa4wxq40xt/u2e6YdjDERxphlxpjvfW3wkG97V2PMN773xJvGmLBAx1rfjDHBxpjvjDELfY891QbGmK3GmB+NMSuNMct92zzzXgAwxsQYY94xxqwzxqw1xpzppTYwxvT0/fsX3o4YYyZ6qQ2k4XmxD1aZ6vTPjPOUr+1+MMb8PHCRN7zq9uO83F7V7e8ZY8J9jzf59ncJZPyB4G+/0OttVZ3+o5ffg1C9fmZjbCtPJIeMK6b9DHAJ0Bu41hjTO7BRNYhXgKGltk0GPrXW9gA+9T1uyvKAO621vYEzgFt9//Zeaods4AJrbT8gARhqjDkDeAx4wlrbHTgE3BzAGBvK7cDaYo+92AaDrLUJ1toBvsdeei8A/BX42Fp7ItAP9/vgmTaw1q73/fsnAKcAGcD7eKgNpGF5uA9WmVfwv392CdDDdxsHPNdAMTYW1e3Hebm9qtvfuxk45Nv+hO84r/G3X6i28r//6OX3IFSvn9no2soTySHgNGCTtTbJWpsDzAOuCHBM9c5a+zlwsNTmK4BXfT+/ClzZoEE1MGttsrV2he/nn3Bv0I54qB2sk+Z7GOq7WeAC4B3f9ibdBgDGmHjgMuAl32ODx9qgAp55LxhjWgLnAi8DWGtzrLWpeKgNSrkQ2Gyt3YZ320Dqnyf7YJWpZv/sCuA13//lXwMxxpgODRNp4NWgH+fZ9qpBf694G74DXOjrG3lCNfuFnm6rCug9WEoN+pmNrq28khzqCOwo9ninb5sXtbPWJvt+3gO0C2QwDck3BLQ/8A0eawffsNmVwD7gP8BmINVam+c7xAvviSeBe4AC3+NYvNcGFvi3MSbRGDPOt81L74WuwH5gjm8Y+UvGmGZ4qw2KuwZ4w/ezV9tA6p/6YP6p6D2o9vPxsx/n6faqZn/vaFv59h/G9Y28ojr9Qq+3VXX6j15+D1a3n9no2sorySEph7XW4t7sTZ4xJhp4F5horT1SfJ8X2sFam++bRhKP+xb3xACH1KCMMcOAfdbaxEDHEmDnWGt/jhvGeqsx5tziOz3wXggBfg48Z63tD6RTavqUB9oAAF8dheHA26X3eaUNRBorvQfL8no/zl9e7+/5S/3CavN6/9Ffx3w/0yvJoV3AccUex/u2edHewuFqvvt9AY6n3hljQnEditette/5NnuuHQB8QxsXAWfihi6G+HY19ffE2cBwY8xW3JSGC3Bzgr3UBlhrd/nu9+HqzJyGt94LO4Gd1tpvfI/fwf0n7qU2KHQJsMJau9f32IttIA1DfTD/VPQe9Hz7VbMf5/n2Ar/7e0fbyre/JXCggUMNlOr2C73cVtXtP3r5PVjdfmajayuvJIe+BXr4KtCH4YbSLwhwTIGyABjl+3kU8M8AxlLvfPOBXwbWWmsfL7bLM+1gjGljjInx/RwJXIybs78IGOE7rEm3gbX2XmttvLW2C+79/5m19no81AbGmGbGmOaFPwODgVV46L1grd0D7DDG9PRtuhBYg4faoJhrKZpSBt5sA2kY6oP5p6L34ALgRt+qNmcAh4tNT2jyatCP82x71aC/V7wNR+D6Ro12RENdqkG/0LNtVYP+o2ffgzXoZza6tjIe+b3GGHMpbm5pMDDbWvtwgEOqd8aYN4DzgThgL/AgMB94C+gEbAN+Za0tXRSxyTDGnAMsAX6kaE7xfbj56p5oB2NMX1zxs2BcQvgta+00Y8zxuG9LWgPfAb+21mYHLtKGYYw5H7jLWjvMS23ge63v+x6GAHOttQ8bY2LxyHsBwBiTgCs+GQYkAWPwvS/wThs0A7YDx1trD/u2eer3QBqWF/tglalO/8yXHJmFW90sAxhjrV0eiLgDobr9OC+3V3X7e8aYCODvuDpOB4FrrLVJgYk+cPzpF3q5rarbf/TyexCq189sjG3lmeSQiIiIiIiIiIiU5ZVpZSIiIiIiIiIiUg4lh0REREREREREPEzJIRERERERERERD1NySERERERERETEw5QcEhERERERERHxMCWHREREREREREQ8TMkhEREREREREREPU3JIRERERERERMTDlBwSEREREREREfEwJYdERERERERERDxMySEREREREREREQ9TckhERERERERExMOUHBIRERERERER8TAlh0REREREREREPEzJIRERERERERERD1NySERERERERETEw5QcEhERERERERHxMCWHREREREREREQ8TMkhEREREREREREPU3JIRERERERERMTDlBwSEREREREREfEwJYdERERERERERDxMySEREREREREREQ9TckhERERERERExMOUHBIRERERERER8TAlh0REREREREREPEzJIRERERERERERD1NySERERERERETEw5QcEhERERERERHxMCWHREREREREREQ8TMkhERERkWOcMSbYGPOdMWah73FXY8w3xphNxpg3jTFhgY5RREREGi9jrQ10DCXExcXZLl26BDoMERERqUeJiYkp1to2gY6jqTDG3AEMAFpYa4cZY94C3rPWzjPGPA98b619rrJrqA8mIiLStFXW/wpp6GCq0qVLF5YvXx7oMERERKQeGWO2BTqGpsIYEw9cBjwM3GGMMcAFwHW+Q14FpgKVJofUBxMREWnaKut/aVqZiIiIyLHtSeAeoMD3OBZItdbm+R7vBDqWd6IxZpwxZrkxZvn+/fvrP1IRERFplJQcEhERETlGGWOGAfustYk1Od9a+6K1doC1dkCbNprlJyIi4lWNblqZiIiIiPjtbGC4MeZSIAJoAfwViDHGhPhGD8UDuwIYo4iIiDRyx0RyKDc3l507d5KVlRXoUDwpIiKC+Ph4QkNDAx2KiIiIFGOtvRe4F8AYcz5wl7X2emPM28AIYB4wCvhnTa6vPlhgqQ8mIiIN5ZhIDu3cuZPmzZvTpUsXXI1FaSjWWg4cOMDOnTvp2rVroMNX4WmBAAAgAElEQVQRERER//wemGeM+RPwHfByTS6iPljgqA8mIiIN6ZioOZSVlUVsbKw6JQFgjCE2NlbfGIqIiDRy1trF1tphvp+TrLWnWWu7W2uvttZm1+Sa6oMFjvpgIiLSkI6J5BCgTkkAqe1FRES8S/2AwFHbi4hIQzlmkkMiIiIiIiIiIlL36rTmkDEmGFgO7LLWDjPGdMUVQowFEoEbrLU5dfmcDeXhhx9m7ty5BAcHExQUxAsvvMDpp59+dP9tt93G7NmzSUtLK3PuK6+8wt13303Hjh0B6Nu3L6+99lqt4tm6dStffvkl1113XYntP/74IzfccAMA27dvp2XLlrRs2ZK4uDj++9//1uo5RcQb8vJg0iTYt6/svrAwePhh6NTJPd6+He6/H3KK/WUfMQKuvrphYhWRpq+iPtjo0aP53//+R8uWLQHX30pISChx7uLFi7niiiuO1uypi/5Qamoqc+fO5Xe/+12J7QcOHODCCy8EYM+ePQQHB9OmTRsAli1bRlhYWK2eV6ShWVvA5s13kZ3tncUOY2LOpWPHWwHIy0tj69YpdO58P6GhsXX2HDt2PEGLFqfTsuVZZfbt2vUcqamLS2wLC2tHt26Ps3//26SkzAfAmBA6dvwd+/e/R2zsMHbvfgGwJc4LCoqkQ4eb2L37BdzClfjODeK44+6mefOfk5b2I9u3P4q1+QC0bTuSNm2uAiAraye7dj1F69ZDSE9fTXBwC0JD44iLG0Zu7iG2bfsjcXFXkZq6iM6d/+DXSEdr89myZQrt248hKqp7dZqtXhw6tJj09FXEx48vs+/IkeXs3PkXrC0IQGTQvftfCQ9v36DPWdcFqW8H1uKWUQV4DHjCWjvPGPM8cDPwXB0/Z7376quvWLhwIStWrCA8PJyUlBRyin0SWr58OYcOHar0GiNHjmTWrFnl7svLyyMkpHr/FFu3bmXu3LllkkMnn3wyK1euBGD06NEMGzaMESNGVOvaIuJt33wDs2ZB584QGVly37p10Ls33Huve/yPf7jbiSe6x8nJ8OOPSg6JSN2oqg82Y8aMKvs5AwcOZOHCheXuq0kfLDU1lWeffbZMcig2NvZoH2zq1KlER0dz1113VevaIo3JkSPL2LnzCcLDOxEcHBXocOpdbu4BDhxYQPv2NxEcHElKynvs3PkEERGdiY+/vU6eIydnL5s330Hr1kPp2/ejEvvy87PYvPlOgoOjjyaj8vMzyc7eRlzcVWzefDcFBRmEhbUjM3MTBw4sJD//CMnJL2FtDhERXY5ey1pLZuZ6UlLep6Agg8jIokRMZuYWIJjevf/Bzp1/Zf/+d4iM7EZ2djJpad8dTQ7t3v08O3bMYPfuF8jPTyMoKJKIiM7ExQ1j37657Nz5BMnJL5Off4S2bX9FVFTPKl//kSNfs337I+Tnp9Gjx19r36C1tHXrFA4f/pJ27X5NaGhMiX07dswgJeWfREYGZkGAGpYKrJU6Sw4ZY+KBy4CHgTuMSx1eABRmL14FpnIMJoeSk5OJi4sjPDwccN86FcrPz+fuu+9m7ty5vP/++35fc+rUqWzevJmkpCQ6derEo48+yk033URKSgpt2rRhzpw5dOrUidGjR9OiRQuWL1/Onj17+POf/8yIESOYPHkya9euJSEhgVGjRjFp0qRKn++NN97gkUcewVrLZZddxmOPPQZAdHQ0//d//8e///1v2rdvz7x5845+yyUi3rR0qbv/9lso/efgpJOK9hcee9JJsGqVezx9ukscHTgAsXX3JZuIeFRlfbCaeuWVV3jvvfdIS0sjPz+f999/n5tuuomkpCSioqJ48cUX6du3L1OnTmX79u0kJSWxfft2Jk6cyG233cbkyZPZvHkzCQkJXHzxxcyYMaPS5/v000+56667yMvL49RTT+W5554jPDycLl268Ktf/YqPPvqIyMhI5s6dS/fugf8mXaTQwYMfAUEMGLCiTkfONFYHDnzMjz9eQmrq/4iNHep7/XDgwEd1lhw6ePATAFJTF5Ofn0lwcNG3cIcPf05BQSYnnfQ2sbGXAZCXd4Qvvohlx44Z5OTsomfPl+jQ4WZ++OESDh78GID8/CN06PAbevZ8vsRzffPNiWRmric2djgnn/zPo9vXrr2Bgwc/xtp8Dh78mNjY4fTp8w47dz7Fpk23k5m5mcjIbkdff37+EQAKCtLJyFhDVtZ2Dhwoue/AgY/8Sg4VnueuHdjkUG5uKocPfwnkc+jQf2nbtuiLhoKCPA4d+jft2l3HiSfODlyQDawuRw49CdwDNPc9jgVSbdEYtp1Ax9o+ycSJ4PtSps4kJMCTT1a8f/DgwUybNo0TTjiBiy66iJEjR3LeeecBMGvWLIYPH06HDh0qfY4333yTpb5PVLff7v64rFmzhqVLlxIZGcnll1/OqFGjGDVqFLNnz+a2225j/nw3bDA5OZmlS5eybt06hg8fzogRI5g+fTozZ86s8Juw4nbv3s3vf/97EhMTadWqFYMHD2b+/PlceeWVpKenM2DAAJ544gmmTZvGQw89VOEIJxHxhi++gJ49yyaGAM4+G95+GwoKwFr48ksYObLkfnDbL7+8YeIVkYbR2PpgAPfffz/Tpk3jwgsvZPr06UeTSMUtWbLk6HSzq6++mo4dO7JixQp++OEHWrduzYQJE+jfvz/z58/ns88+48Ybbzw6AmjdunUsWrSIn376iZ49e3LLLbcwffp0Vq1adfSYymRlZTF69Gg+/fRTTjjhBG688Uaee+45Jk6cCEDLli358ccfee2115g4caJf/TqR8uTmHmDHjscpKKi70QYpKe/SosVpnkgMAcTEnEdQUATbtv2RQ4f+60tkGFJTF7NpU92MAkxN/RQwFBRksX79WMLCij5DHjnyNcaEExMz6Oi2kJAWtGhxNgcP/guA1q2H+u4v9SWHDGCJjb2kzHPFxl7Czp3rad265L7WrS9h795/sHbtr8nJ2XX0XHfc7WzceDtRUT1IS1tx9PruHsCyceN4UlM/K7EvOfkFsrN3Vvn69+9/BzBkZm5k48YJGFP2b3ZDyc7eAeQDhh07/syRI18f3ZeXd5C8vNQybdfU1UlyyBgzDNhnrU00xpxfg/PHAeMAOhUWsmhEoqOjSUxMZMmSJSxatIiRI0cyffp0Bg8ezNtvv83ixYurvEbpaWVTp05l+PDhRPrmbHz11Ve89957ANxwww3cc889R4+98sorCQoKonfv3uzdu7fa8X/77becf/75R0cEXX/99Xz++edHrzvS98nu17/+NVdddVW1ry8iTUdBgUsO/eIX5e8/5xz4299gzRp37OHDbluhU091dYmWLlVySERqr6I+2OjRo3n00Udp3749OTk5jBs3jscee4wpU6aUuUbpaWWvvPIKF198Ma1btwZg6dKlvPvuuwBccMEFHDhwgCNH3Lfhl112GeHh4YSHh9O2bdtq98PWr19P165dOeGEEwAYNWoUzzzzzNHk0LXXXnv0vqpR4CKV2b//HbZvf4SgoCiKPsjXjjGGTp3uq5NrHQuCgyNp1+5G9u59nbS07wkKCqVr1z+yY8fj7N79fNUX8FN8/CQOHvyIlJR/ltnXrt2vy0zh69BhDGlpK4iJOY/wcDfWIi7uF+za9QwdOtzM3r2vExNzYbnXOnjwY+LiriixvXXroUREdCEl5QPCw48jNnYYAFFRPYiJuYDU1MWkpi4mNLQt8fG3k5Iyn5YtzyUvL5XMzI0cOvQZQUGRdO48hb17/0Fc3JXs3PmkX21kTBBdujzI7t0vkpw8x+82qy/NmvUlJuZckpPnkJ6+psS+iIjjad16cIAiC4y6Gjl0NjDcGHMpEIGrOfRXIMYYE+IbPRQPlFvNzFr7IvAiwIABA2x5xxSq7Nul+hQcHMz555/P+eefz8knn8yrr75KmzZt2LRp09EhwBkZGXTv3p1Nmzb5dc1mzZr5dVzxb8GsrbR5ak1Lpoo0rIICePFFuO46aNGi6uMrkp0NM2ZAOTXxqyUtDQ4eLBoBVFrh9vvuK7sNICICTjkF3nkHCv+cBAXB2LFw/PG1i01EAqsx9cFGjx59dNR2eHg4Y8aMYebMmX5fsyZ9sODgYPLy8io5uvqK97vUB5PayMjYSFBQBAMH/oQxWpC6pnr2fIGePV8osa1z5/vr4Zn+4veR7duPon37USW2RUTEc/rp6wDo1Onucs9r3vwUTjttbZntoaGtOeOMLeWek5DwaZltnTtXnCDs3HkyAMcf/6cKjylPly4PVuv4+tajx9OBDqFRqJPkkLX2XuBeAN/IobustdcbY94GRuBWLBsFlE2PHgPWr19PUFAQPXr0AGDlypV07tyZyy67jD179hw9Ljo62u/EUGlnnXUW8+bN44YbbuD1119n4MCBlR7fvHlzfvrpJ7+ufdppp3HbbbeRkpJCq1ateOONN5gwYQIABQUFvPPOO1xzzTXMnTuXc4oPARCRevfll3DLLZCV5aZs1NS//gUPPOBG7dT280XbtnDxxeXvO/54OP10+Pe/3eMzzoCuper0jRwJkycXfZDMzobUVHj22drFJSLeU1EfDNy0+w4dOmCtZf78+fTp06dGzzFw4EBef/11HnjgARYvXkxcXBwtKsnWV6cP1rNnT7Zu3Xr0y8S///3vJabFvfnmm0yePJk333yTM888s0bxiwBkZm4iMrK7EkMiUmN1vVpZab8H5hlj/gR8B7xcz89XL9LS0pgwYQKpqamEhITQvXt3XnzxxTp9jqeffpoxY8YwY8aMowWpK9O3b1+Cg4Pp168fo0ePrnQococOHZg+fTqDBg06WpD6iivc8MJmzZqxbNky/vSnP9G2bVvefPPNOn1dIlK5wuLOX3xRu+TQ0qUQHu6meZVTcqPOGANff135Mbff7m6Fhgxxr09EpLoq64Ndf/317N+/H2stCQkJPP98zaZ9TJ06lZtuuom+ffsSFRXFq6++WunxsbGxnH322fTp04dLLrmk0oLUERERzJkzh6uvvvpoQerf/va3R/cfOnSIvn37Eh4ezhtvvFGj+EUAMjM3EhV1QqDDEJFjmKnvaUrVNWDAALt8+fIS29auXUuvXr0CFFHTFh0dTZof81D0byBSP4YNgw8/hPbtYffumo/6Of10N2poyZK6ja8uTJsGU6e66WoxMVUeLh5hjEm01g4IdBxSRH2whtWlSxeWL19e5Qps+jeQqlhbwOefRxEfP4Fu3SpfPU9EvK2y/ld9jxwSEZEKFBZ/jo6GPXtgy5aa1eXJyIAVK+CuullIo86dc45b2eyrr+ASby36ICIiUi15eYc5dGgRbhUof885hLXZREb2qL/ARKTJU3LI4/wZNSQiNbN2LXz+ecX7DxxwtXjuvBP+8hd47DH4+c+r/zzbtkFeXslVwxqT00+H4GA39U3JIRERZ+vWrYEOQRqhbdseZseOmo3+adasbx1HIyJeouSQiEg9yM+HK66AjRsrPy40FMaPh7fecquW1VR0NJx1Vs3Pr0/NmsFpp8G778If/+hWLxMREZGy0tPXEBV1Ir17z6vWeUFBzYiK6l5PUYmIFyg5JCJSD+bPd4mh2bNh6NCKj2vWzC1hv369G0VUU82buwRRYzVhAlx3HSxYAFdeGehoREREGqfMzI00a3Yy0dH9Ah2KiHiMkkMiIrX09NPw3HMltyUnQ/fucOONbkpVVSIj3a2puvpquP9+GDPGtdeHH0JERKCjEhERaTwKCvLIytpCmzZXBToUEfEgDe4XEamlZ591RaH79Cm6XXyxSxj5kxjygpAQeP55V1Pps89g2bJARyQiItK4ZGfvwNpcIiM1PUxEGp6SQ356+OGHOemkk+jbty8JCQl88803JfbfdtttRFcwp+OVV17BGMN///vfo9vmz5+PMYZ33nkHgLFjx7JmzZpyzx0/fnyJbXPmzCEhIYGEhATCwsI4+eSTSUhIYPLkybV9mSJSTSkpsG4d/Pa3rm5Q8dtFFwU6usZl8GB4+2338xdfBDYWETl2VNQHGz16NF27dj3aJ1q5cmWZcxcvXowxhpdeeunotpUrV2KMYebMmQBMmTKlRB+t+LnDhg0rse2TTz45+nzR0dH07NmThIQEbrzxxrp8yVIPrHWrf+XmppKTs69R3n76KRFAq46JSEBoWpkfvvrqKxYuXMiKFSsIDw8nJSWFnJyco/uXL1/OoUOHKr3GySefzLx587jI92nxjTfeoF+/ornExTstVRkzZgxjxowBoEuXLixatIi4uLjqvCQRqSNffunuG+tKYY1N69bQu7dbuUxEpCpV9cFmzJjBiBEjKr1Gnz59eOuttxg7dixQtg82bdo0v+MZMmQIQ4YMAeD8889n5syZDBgwoDovSQIgN/cgX3/dlZ/9bBw7dswMdDhViow8IdAhiIgHKTnkh+TkZOLi4ggPDwcokYjJz8/n7rvvZu7cubz//vsVXmPgwIEsWbKE3NxcsrOz2bRpEwkJCUf3F+9gzJkzh0cffZSYmBj69et39HkrY63lnnvu4aOPPsIYwx/+8AdGjhzJ4sWLmTJlCs2bN2fTpk0MGjSIZ599liAtFyRSJ5YuhbAw0GcD/519thtBVFCglctEpHKV9cH81blzZ44cOcLevXtp27YtH3/8MZdeeunR/aNHj2bYsGGMGDGCjz/+mIkTJxIVFcU51cj6P/7448yePRtwo8EnTpzI1q1bGTp0KKeccgorVqzgpJNO4rXXXiMqKqrar0FqJz19Nfn5R9i161kAunf/K8Y0zo9B4eEdCQ9vH+gwRMSDGudfxcpMnAjlDBuulYQEePLJCncPHjyYadOmccIJJ3DRRRcxcuRIzjvvPABmzZrF8OHD6dChQ6VPYYzhoosu4pNPPuHw4cMMHz6cLVu2lDkuOTmZBx98kMTERFq2bMmgQYPo379/lS/hvffeY+XKlXz//fekpKRw6qmncu655wKwbNky1qxZQ+fOnRk6dCjvvfdeld+yiXjViy9C27YVr6i1YgVMneqWqgdYvtwlhv6fvTsPb7LMGj/+vZMu6d5Cy1Z2yiIUWSxriyAIA+KIuCGu6CijDDo68/rqjPNzRQVlFn11FNzQcUMcdWYYcUO2VlRAEREKLYsFLFBom25JmzbP74+7aSnd26Rpk/O5rlxpnvUk0PTJybnPLc2Vmy4lBV58EWbNgsce82xibflynYyaMMFz5xDCb7SzazCA+++/n0ceeYRp06axdOnSer9Qu+KKK1izZg2jRo1i9OjRdW5nt9u59dZb+eKLL0hISGDevHlNego7duzg1Vdf5euvv8YwDMaNG8fkyZOJiYlh3759vPzyyyQnJ3PzzTfz97//nf/5n/9p0nGF+9hsGQA4nSUEBnahZ887vRyREEK0P/KdbROEh4ezY8cOVq5cSVxcHPPmzWPVqlX8/PPPrFmzhjvuuKNJx7n66qt55513eOedd5g/f36d23z99ddMmTKFuLg4goKCmnxhkpqayvz58zGbzXTt2pXJkyezbds2AMaOHUv//v0xm83Mnz+fVBnPIUS9liyBRx+tf/2LL8Inn8DJk/rWuzcsWtR28fmCWbNgyhTYtEkn4zzp4YfhzTc9ew4hhOfUdw0G8MQTT5Cens62bdvIzc1l2bJl9R7nqquuYs2aNbz99tv1XoOlp6fTr18/Bg4ciFKK6667rkkxpqamMnfuXMLCwggPD+eyyy5jy5YtAPTq1Yvk5GQArrvuOrkG8xKbLbPqZ2n2LIQQdet4lUMNfLvkSWazmSlTpjBlyhSGDx/Oa6+9RlxcHJmZmSQk6D8yJSUlJCQkkJmZWecxxo4dyw8//EBoaCiDBrXdWGKlVIOPhRBaeTkcO6ZvhYUQEVF7m7Q0ndj45JM2D89nxMXBhg1w0UWe7z3kcEBpqWfPIYQ3KaUswGYgGH1d955hGA8qpaYCy4EgYAfwK8Mwylt1snZ0DbZgwYKqqu3g4GBuuummqgbTdenWrRuBgYF89tlnPP3003zpahjnYXIN1j64KodAmj0LIUR9pHKoCfbt20dGRvUflZ07d9KnTx9mz57N8ePHOXz4MIcPHyY0NLTexJDL0qVLefzxx+tdP27cODZt2sTp06dxOByscU3t04hJkyaxevVqKioqyMnJYfPmzYwdOxbQw8oOHTqE0+lk9erVzRpDL4Q/yc7WfXCcTvjqq9rr8/Jg925pPu0uKSmwdy+cPu25czgcYLd77vhCtAOlwFTDMEYAI4GZSqmJwGvA1YZhJAI/ATd6McYWq+8aDPRQfNB9Fz/88EMSExMbPNYjjzzCsmXLMJvNda4fMmQIhw8f5sCBA4BuXN0UkyZN4sMPP6SkpITi4mI++OADJk2aBEBWVhZbt24F4K233pJrMC85s3IoNFSSQ0IIUZeOVznkBUVFRdxxxx3k5+cTEBBAQkICK1s4FmLWrFkNru/evTsPPfQQEyZMIDo6ukbT6obMnTuXrVu3MmLECJRSPPnkk3Tr1o309HTGjBnD4sWLqxpSz507t0WxC+HrsrKqf05Lg+nTa67fuhUMQ/ewEa3n+oz05Zfwy1+6//iuRJ8kh4QvM/T83EWVDwMrbxVAmWEY+yuXfwb8AXi57SNsnYauwa699lpycnIwDIORI0fywgsvNHisiRMnNrjeYrGwcuVKZs+eTWhoKJMmTaKwsLDRGEePHs2CBQuqvpS75ZZbGDVqFIcPH2bw4ME899xz3HzzzQwdOpTbb7+9ic9cNOTUqbWcPPkOoaEDOX78H41ub7cfJiQkAZstU4aVCSFEPZS+pmg/kpKSjO3bt9dYtnfvXs455xwvRdSxbdy4keXLl7N27dpWHUf+DYQ/ePttuOYaiIzUQ5/OzuV++y18/TVYrRAW5p0YfYnNBlFR8LvfwdKl7j9+aaluFH7xxfCf/7j/+KJ1lFI7DMOQef7cQCllRg8dSwCeA+4DDgOXG4axXSn1NLq6aHgd+y4EFgL07t37vJ9++qnGevn733KHDx/m4osvZvfu3a06jvwb1LZ37wJOnHiNwMAumM1hREY2nPhTykR8/G84depf9O79BwIC6hg3LoQQfqCh6y+pHBJCiEquyqHf/x6efhreeqv2NvPmSWLIXUJC4LzzPNd3yOHQ91I5JHydYRgVwEilVDTwATAMuBr4q1IqGPgUXU1U174rgZWgv6Brm4iFaB3XMDGH4yTdu/+B/v3rb9lwpsjIcZ4MSwghOjRJDvk4VwNHIUTjsrIgJgYeeEDfhOelpMAzz+gEjsXi3mNLckj4G8Mw8pVSG4CZhmEsByYBKKVmAG03E4YAoG/fvq2uGhJ1k9nHhBDC/aQhtRBCVDpyRE9NL9pOcjKUlcGOHe4/tiSHhD9QSsVVVgyhlAoBpgPpSqkulcuCgXuBhhvyCNFBlJcX4HCcqHosySEhhHAPSQ4JIUSlrCxJDrU1V3NvTwwtk+SQ8BPdgQ1KqV3ANuAzwzDWAvcopfYCu4D/GIbxhTeDFMJdbLYDNR7L1PRCCOEeMqxMCC9yOmHVKsjPd/+xx42DpCR45RXd+BdgzhwYMMD95+oIrFZ4/fXqhEFdDhyQaerbWlwcDBqkk0P33uveY0tySPgDwzB2AaPqWH4PcE/bRyRE8xmGk0OHHqC09Eij25aWHgUgMDCOiopigoK6eTo8IYTwC5IcEsKLtmyBX/3KM8eOj4cnn4RFi6qXbd0Ka9Z45nzt3csv60bTjUmSuZPaXEoKfPihTpaa3FjPKskhIYToGIqKviMr6zGCgrphMjXegC4yciJdu15DScl+lFJtEKEQQvg+SQ410WOPPcZbb72F2WzGZDKxYsUKxo2rnvHgzjvv5JVXXqGoqKjWvqtWreKee+4hPj4eu93Or3/9a+6++263x7em8lP/Dz/8wPDherbam2++mTvvvNOt5xLu4xpK89NPEB3tvuO+9JJOhLzxBkRE6OMvWgQbN4JhgD9eR6WmQv/+8N139W9jMkF4eNvFJLSUFF3hlp4OQ4e677iSHBLCN9R3DbZgwQI2bdpEVFQUoK+3Ro4cWWPfjRs3MmfOHPr164fdbufiiy9m+fLlbo3v1Vdf5emnnwZgz549DB48GLPZzMyZM1m6dKlbz+WrcnM/BiApaSdBQV29HI0QQvgnSQ41wdatW1m7di3ffvstwcHBnDp1irKysqr127dvJy8vr8FjzJs3j2effZbTp08zePBgrrjiCnr16tWquCoqKjCbzQDcf//93H///QCEh4ezc+fOVh1btI3UVP1h2N19bqZN0/fr1sH06XoGrsmT4Z134OBB/xtaZhj6tZ41CyIjvR2NOJur71BamiSHhBA1NXYN9tRTT3HFFVc0eIxJkyaxdu1abDYbo0aNYu7cuSS73nhaqLy8nIAAfRl90003cdNNNwF6hrINGzYQGxvbquP7ktzcz8jP39TgNjk57xIefp4khoQQwoukIXUTZGdnExsbS3BwMACxsbH06NED0Amae+65hyeffLJJx+rcuTMJCQlkZ2cD8MYbbzB27FhGjhzJr3/9ayoqKgC4/fbbSUpKYtiwYTz44INV+/ft25d7772X0aNHV1UK1cdut3PTTTcxfPhwRo0axYYNGwD9zdqcOXOYMmUKAwcO5OGHH27eCyLcoqJCD/PyRI+bxMTqJIjr+K77tDT3n6+9y8iAnBzpJ9ReDRyoew+5uym1JIeE6PgaugZrrpCQEEaOHMmxY8cA+PTTT5kwYQKjR4/myiuvrKr+fuSRRxgzZgyJiYksXLgQwzAAmDJlCnfddRdJSUlVlUL1MQyDe+65h8TERIYPH87q1asBXcl0/vnnM3v2bAYPHsxtt92G0+ls0fPpKDIyfkNW1mNkZS2t92a3H6Jbtxu8HaoQQvi1Dlc5lJFxF0VF7q2KCQ8fycCBf6t3/YwZM3jkkUcYNGgQF154IfPmzWPy5MkAPPvss1xyyV3XlNQAACAASURBVCV07969SefKysrCbrdz7rnnsnfvXlavXk1aWhqBgYEsWrSIN998kxtuuIHHHnuMTp06UVFRwbRp09i1axfnnnsuoBNM3377baPneu6551BK8cMPP5Cens6MGTPYv38/AN988w27d+8mNDSUMWPGMHv2bJKk2YrH7dwJhw7pn7OzdZPkVn55WSezGSZMgE8+qT7+0KF66Nq77+qhZu4SGwuTJlU/Pn1a3wYNani/776Dc84BS+OtBRq0aRPk5ja8zZYt+t4Tr7VoPaX0v42nkkPl5foW0OH+4gnRvrS3azDQldOPPPII06ZNY+nSpVVJpLrk5eWRkZHB+eefz6lTp1iyZAmff/45YWFhLFu2jL/85S888MADLF68mAceeACA66+/nrVr1/LLX/4SgLKyMrZv397o83r//ffZuXMn33//PadOnWLMmDGcf/75gL4G27NnD3369GHmzJm8//77jVY/dVROpwOb7SC9e99P//5LvB2OEEKIBsilchOEh4ezY8cOtmzZwoYNG5g3bx5Lly5lxowZrFmzho0bNzZ6jNWrV7N582bS09N59tlnsVgsrF+/nh07djBmzBgAbDYbXbp0AeDdd99l5cqVlJeXk52dzZ49e6qSQ/PmzWtS3Kmpqdxxxx0ADBkyhD59+lQlh6ZPn07nzp0BuOyyy0hNTZXkkIdlZ8P48VBaWr3MbNbDvTzhF7/QlUmu1lgmE0ydCu+/D//9r3vPtXWrfm4A112nH//0E1S2gahl92447zy4+274859bft7Nm2HKlKZt26MHDBnS8nMJz5o8WTel/uEHqGyZ1mpnzkxXWirJISE6ovquwRYsWMATTzxBt27dKCsrY+HChSxbtqwqqXOmLVu2MGLECDIyMrjrrrvo1q0ba9euZc+ePVXDy8rKypgwYQIAGzZs4Mknn6SkpITc3FyGDRtWlRxqzjXY/PnzMZvNdO3alcmTJ7Nt2zYiIyMZO3Ys/fv3B2D+/Pmkpqb6bHLIbv8JqCAkJMHboQghhGhEh7tUbujbJU8ym81MmTKFKVOmMHz4cF577TXi4uLIzMwkIUH/wSspKSEhIYHMzMxa+7t6Dm3fvp0ZM2ZwySWXYBgGN954I0888USNbQ8dOsTy5cvZtm0bMTExLFiwAPsZ4yLCwsJa/XzOntlBZnrwvL/9TX9Y/fxzXW0DuheQu/sNudx5J9x4Y80Gy//4B9Rx3dxi5eW6p9GyZfDBB7oa6GPdU5IXXqh/avInn9R9gFasgPvvh06dWnb+pUv1cKSPP9aJtob06OHembCEe11/PfzpT/DUU/D66+455pnJIbsd3PDWKYRfa0/XYAsWLKiq2g4ODuamm26qt9G0q+fQoUOHGD9+PFdddRWGYTB9+nTefvvtGtva7XYWLVrE9u3b6dWrFw899JBcg7WCzZYBQGjoQC9HIoQQojEdLjnkDfv27cNkMjFwoP7DtnPnTvr06cPs2bM5fvx41Xbh4eF1JobOlJSUxPXXX8/TTz/N9ddfz5w5c7j77rvp0qULubm5FBYWUlBQQFhYGFFRUZw4cYJ169YxpanlEWeYNGkSb775JlOnTmX//v1kZWUxePBgvv32Wz777DNyc3MJCQnhww8/5JVXXmn28UVNhw7BbbfVrAw607ZtcOWV1c2iPc1srp10CQ2FESPce57Fi+HRR3XlR1aWHrI2fDg8/rhuiF2XtDSYOVMndc4/vzpZ1hyGoSuHliyB0aNb9xyE93XuDLfeCs8+C489Bq3s1w/UTg4JITqe+q7BQPcj6t69O4Zh8OGHH5KYmNjgsfr168d9993HsmXLeOaZZ/jNb35T9SVfcXExx44dq6rgjo2NpaioiPfee69FVT2TJk1ixYoV3HjjjeTm5rJ582aeeuop0tPT+eabbzh06BB9+vRh9erVLFy4sNnH7yhsNn1dLJVDQgjR/klyqAmKioq44447yM/PJyAggISEBFauXNni47kaSv/xj39kyZIlzJgxA6fTSWBgIM899xzjx49n1KhRDBkyhF69erV4Ro1FixZx++23M3z4cAICAli1alXVWPyxY8dy+eWXc/ToUa677joZUuYGa9bAp5/qZEddXwImJ8NDD7V5WB7329/qXkoFBdCnD/zxjzBqFNx3n64sqsv06fDSS/D3v7e8z4xScMkl8JvftDx20b4sXKgr7D75BG65pfXHk+SQEB1fQ9dg1157LTk5ORiGwciRI3nhhRcaPd5tt93G8uXLKS4uZtWqVcyfP5/Sym91lixZwqBBg7j11ltJTEykW7duVUP/m2vu3Lls3bqVESNGoJTiySefpFu3bqSnpzNmzBgWL15MZmYmF1xwAXPnzm3ROdqTkpKMqkTQmfLzN2A2hxMY2MULUQkhhGgO5ZqBob1ISkoyzm70t3fvXs455xwvReR7Vq1axfbt23n22WebvI/8GzRuzhzYuxcq2zoJIZrJMKBLF7j4Ynj11dYf7/334fLL9c+7d8OwYa0/pnAfpdQOwzDkm4l2RK7BPG/jxo0sX76ctWvXNnmfjvBv8OWX8ZSV/VznusjIiYwe7YdTpQohRDvU0PWXVA4J4QaGoYdKXXKJtyMRouNSClJS3DdrmVQOCSGE5zkceZSV/Ux8/G/p2nV+rfUypEwIIToGSQ75oQULFrBgwQJvh+FT9u3T07enpHg7EiE6tuRkPWvZiRPQtWvrjiXJISFEe+NqrO1LbLYDAERHTyEycpyXoxFCCNFSHWbunvY2/M2fyGvfsLVrYdEi/bMkh4RoHdfvUJobRiBIckgI95DrAO/pCK+9zEgmhBC+oUMkhywWC6dPn+4QfyB9jWEYnD59GovF4u1Q2q0//hG+/hpmzICBcl0kRKuMHg0Wi3uGlp2ZHKpvFkEhRMPkGsx7Oso1mKsRtcXS38uRCCGEaI0OMaysZ8+eHD16lJycHG+H4pcsFgs9e/b0dhjtUn6+bnT70EPwwAPejkaIji8oCMaOdU/l0Jmz5UnlkBAtI9dg3tUersEMw0lFRVG960tK9hIc3AuzOaQNoxJCCOFuHSI5FBgYSL9+/bwdhhC1bN2qm1HLcDIh3CclBZ58EoqLISys5ceRYWVCtJ5cg4k9e+aRk/Neg9tER09ro2iEEEJ4SodIDgnRXqWmgtkM46T/ohBuk5ysq36++QYuuKDlx5HkkBBCtJ7VmkZk5ATi4q6od5uYmOltGJEQQghPkOSQEE3w0UewfXvt5f/8J4wa1brqBiFETRMm6Gntn3pKJ4mmt/AzhySHhBCidcrLiygryyY+fjG9ev3O2+EIIYTwIEkOCdGI8nK4+mooLKx7/eOPt208Qvi6mBiYNg3WrdNJ2RMndLKouSQ5JIQQrWO362nqQ0ISvByJEEIIT+sQs5UJ4U0//KATQ//4B1RU1L794Q/ejlAI3/Ppp7BiBeTkQEZGy44hySEhhGidkhL9BhwSItOxCiGEr5PkkBCNcM2aNGkSmEy1b0II91OqutF7S2cuczj07GdKSXJICCFawjVNvVQOCSGE75NhZUI0IjUVevaE3r29HYkQ/mXIEOjUSf8O3nRT8/d3OCAwUDeNl+SQEELULyPjt+Tlra+1vKwsm8DArgQERHghKiGEEG1JkkNucOoUfPyxntK8KZKToX9/z8YkWq+iAj78EDZtgsmTW9bzRAjRciYTTJwIn38OH3wAl17avN9DV3JIKoeEL1NKWYDNQDD6uu49wzAeVEpNA55CV4kXAQsMw8j0XqSivTIMg+zsl7BYehMaOqzGutDQIcTETPVSZEIIIdqS25JD/nxx8uCD8Pe/N337yZNh40aPhSPc5L//hSsqZ21t6WxJQojWmTED1q6Fyy6Dzz6DCy9s+r4OBwQE6ASRJIeEDysFphqGUaSUCgRSlVLrgOeBOYZh7FVKLQL+BCzwYpyinSory8bpLCE+/g7i4xd5OxwhhBBe4s7KIb+9ONm8GS64AF58sfFtly3TjY1LSyE42POxiZbbvFn/G+3ZA/36eTsaIfzT4sU6OTtsGGzZ0vzkUGAgWCySHBK+yzAMA/3lG0Bg5c2ovEVWLo8Cfm776ERHYLNJ02khhBBuTA7568VJXh7s3g2PPgoDBjS+/cyZOon07bcwYYLn4xMtl5oKY8bIEEAhvEkp3XtoxAj9O9kcruRQcLAkh4RvU0qZgR1AAvCcYRhfK6VuAT5SStmAAmB8PfsuBBYC9Jbmen5Jmk4LIYQAN89WppQyK6V2AieBzwzD+BpwXZwcBa4Hltax30Kl1Hal1PacnBx3huRxW7fqe9esOo1JTtb3LZ19R7SNkhLYsaPp/65CCM9KSYGvvqo5PX1jpHJI+AvDMCoMwxgJ9ATGKqUSgbuBiwzD6Am8Cvylnn1XGoaRZBhGUlxcXNsFLdqNkpIMlAokOLiXt0MRQgjhRW5tSG0YRgUwUikVDXxw1sXJ10qpe9AXJ7ectd9KYCVAUlJSE9s6t15ZmU4AhITAyJEtO0Zqqu5pMXZs07bv2hUSEuCjj+quHOrbF+LjWxaLaJ7CQti1q+51u3dDebkkh4RoL1JS4P/+D95+G+bOhYgmTJwjySHhbwzDyFdKbQBmASMqv6QDWA187L3IhLcdP/4PrNa6yy/z8zdgsfTHZJJ5aoQQwp955K9AR7k4efBBWFpZx7R2Lcye3bz9DUM3LT7vPAgNbfp+rv5EdSUeunaFQ4d0wkp41nXXwb//Xf/6wEAZ+idEe5GSomcvu/FGPYvg++83vo8rORQWppPBQvgipVQc4Ki89goBpgPLgCil1CDDMPZXLtvrzTiF9zidpezffztKKczm8Dq36dbtpjaOSgghRHvjztnKOtTFSX4+PPccXHQR/PgjPPFE85NDn32mK09eeaV5+y1fDlddpZNLZzp4EG67DV59FRbJZBEetXu3TgwtWqSnx65L9+7QqVPbxiWEqFuPHvDdd3pmyBUrYO9eOOechvdxJYcGDGhaMkmIDqo78Fpl3yET8K5hGGuVUrcC/1RKOYE84GZvBim8Jz9/C05nMYmJ/yE29mJvhyOEEKKdcmflUIe6OFmxQn+TvGSJHhp25526D5CrJ1BDHA747W/hk0/0ELBrr23euSMj655xxzBg1Sp4+GHYsKFpx7rwQvj1r5t3fn/x+OMwa5ZuGv788zXX7d2rqwkefVQSQEJ0FOeeq39nX38dnnqq8cS8Kzk0eDCcOqVvsbFtE6sQbcUwjF3AqDqWfwB80PYRibZUUrKPkyfXNLhNfv5GlAoiJuaCNopKCCFER+TO2co61MXJhx/C+PEwahQMGgT33KOXNSU59OWXOtnQv7+emj4oyD0xKaUrmO68U0+f3piTJ+Hzz+HWW/VwC1HtyBG4/37Yvx+ys3Xir0+fmttIYkiIjicuDq68Ev71L3A6G37vcyWHhgzRj/ftk+SQEMK3HD78CCdPvtXodl26XI3ZHNYGEQkhhOio/LLznM2mG1H/7nf6cVgYJCU1fQYx13bbtrk/uTBlSv1Nks/2+uu6/8aePZCY6N44OjrXv9Hmzbpa4IYb9HAUIUTHN3myfv/bt6/hoWUOh07eu5JD6elN+wJACCE6Cpstg+joaZx7bsMtPXVhvxBCCFE/v6w32bZNf2g4syF0Sgps364TR41JTYVhw7xfdeKKP7XuySf8mus1OXRIDx+UWceE8B1Nfe9zVQ716QPBwTqZJIQQvsIwDGy2DEJDB2EyBTR4U0p5O1whhBDtnF8mh1wfKCZOrF6WnKw/SGzf3vC+TqceVtYevn3u1w+6dWt6xZM/SUvTw09c2sO/lxDCPQYO1L/fjb33uZJDZrPeJz29beITQoi2UF6eS3l5PiEhA70dihBCCB/gl8PK0tJg6NCalT+uRNGjj+pm0/XNXPbjj2C1to9KFKV0HJ98Avfe2/z9g4Lgrrugc2f3x+YNu3fDG2/oBN6uXXDfffCXv+geI717ezs6IYS7uN77Pv5Yv/cFBsIdd0DXrjW3cyWHQDelbuqQXSGE6AhstkwAQkISvByJEEIIX+B3ySGnUyeHrrqq5vLOnfXMVp9+Cj/8AD//rD+AnM1VddQekkMAV1wB69bBM880f1+7XSfI7r7b/XF5w8MPwz//qYePREbCZZfBiRPQpUvd/5ZCiI7riiv0+/Uzz+j3MosF/vSnmtucmRxKTIQPPoCCAv3+IITouEpK9lNaesyj5zCZLERGjkMpE+XlBRQW7vDo+VoiP19PbSuVQ0IIIdzB75JDDVX+fPSRnoVs0SLdq6Z//9rbpKVB9+7Qt6/HQ22SefP0rSX699fPxxeSQ4ahn8v8+fDmm9XLX3rJezEJITznmmv0DXTip64hZmcmh5KT9ZcDX30FM2a0XZxCCPdyOkvZvn0kTmcTmkS2UmLiv4iNvYQDB35Pdnb7vKAwmUIJCenn7TCEEEL4AL9LDrk+QNRX+XNmo9O6kkOpqXobX6hESUnR37wbRsd/PocO6Snr20tFlxCi7aSkwDvvQEWF7i/kcmZyaPx4Pe19Wpokh4ToyGy2QzidNvr0eYCYmKkeOYfTWcauXTMoLt5NbOwlFBfvJiJiDAMGPOWR87VGUFA8JlOwt8MQQgjhA/wuOZSaqps496vnS5ZhwyAqSm93ww011x09Cj/95BuVNqC/Sf/HP+DAAUjo4MPVXcP9pPG0EP4nORlWrNCVoeeeW738zORQRASMGCGzOwrR0dlsGQB07nwRkZHjPHaeoKDuVT19SkoyiIu7gujoyR47nxBCCOFtfpkcaqjyx2TSHzQ+/RReeEF/sLjqKv3BorGqo47G9TyefBJGj/bMOcxm3RskJgby8+H772HyZPjXv3Slj7u8955O6g0b5r5jCiE6Btd7WVpa/ckh13Yvv6zf25WCOXP0lwVCiI6jrZowh4QMxGbLwOHIo7z8tDR9FkII4fP8KjlUUKArf26/veHtZs3S/Ydc2+Xnw+9/rxNLYWH622dfcM450KcPvPiiZ8/z88/w4IM6CbV0qf4Ad+ml7j/PVVfVHFIihPAPfftCjx76PfrM9/ezk0OzZsH//V/1Njt36j5zQoiOw2bLJCAghsBAz061GhKSQG7uR1XJqNBQafoshBDCt/lVcshq1fdnTmFfl8WLdZPnigr9TfOWLdXJofHjIcBHXjWTCdLTdfLLU37xC/36AWzerPsbLVtW/XigG6+14uLcdywhRMehlK74PHvIWF3JodOnoawMrruu+r1JCNExGIZBScmeNqniCQkZSFnZ8apZyqRySAghhK/zkTRH0xQW6vuIiMa3dSUaJk2C//xHJ5Z27YL/9/88F583WCyeHVZx/vnw6qtQXAzbtull//oXREfrD3Mmk+fOLYTwHykpsGYNHDkCvXrpZeXlNZNDUP3lwJQp+v08L08PexVCeMfp0x+RkbGYMWN+wGwOa3DbvXuvJT9/I126XOPxuEJDBwGQkXE7YMJiqWOWEiGEEMKH+NVHc1dyKDKy6fukpOhvml9/XU+DLA2PmyclRSeGXnlFf1sfFKSXS2JICOFOZ/YdAl2leHblUF3bf/ml52MTQtQvP38DdvshSkrSm7DtRpQKpG/fBzweV6dOsxgw4K/06/cYQ4euxmwO8fg5hRBCCG/yq4/nzakccnElg/78Z53MGD/e/XH5sjNfP9BDOcB3mnoLIdqHc8/VPeFcQ8sqKvR9fcmhsWP1EOF33oGMjLaJUQhRm6unj+u+PhUVxZSVZdO370OEhg72eFxmcwi9et1Fnz5/pEuXKzx+PiGEEMLb/Co5VFCg75uTHBo8WDc6/ekn/WGiOfsK6NlT9xX66SdITIRrr9XLp03zblxCCN8SEAATJlRXDjkc+r6+5FBoqN7+jTd00t+VTBJCtK2SEp2dbSw5VD1LmTSGFkIIITzBr5JDLakcUkr3yvnyS917SDTf5s369Vu/HqZO1YmiMWO8HZUQwtckJ+vecAUFjSeHAD74AJYsgdxc2L27bWIUQlQzDCd2+wGgOklUn7aawl4IIYTwV36ZHGpOzyHQlUMTJkBsrPtj8gfduunXr0sX/bh3b+/GI4TwTSkpujfcV181LTnUuXN1NePZM50JITyvtPQYTqcdaLxyyJU8kuSQEEII4Rl+lRxqybAyIYQQHcO4cWA260RPU5JDAH36QHy8JIeEaGsORz7ffjsWgODgPhQWbmP79iS2b0/ixIm3am1vs2USGNiVgAC5iBNCCCE8we+msg8IgOBgb0cihBDC3SIiYMQI+OILuPBCvSygkb9ySunhaFu2wP79tddHRUHXru6PVQh/V1T0HWVlxwkJSWDgwOc4duxZDMNJQcGXnDjxBl271pyu3mbLkKohIYQQwoP8LjkUEaE/DAghhPA9558Pf/sbTJ6sHzelUvT88+Hdd/UEBGcLCoI9e2DAAPfGKYS/Kys7DkBi4r8ICxtKp04zAPjxx6soKvqu1vY2W2bVNkIIIYRwP79LDjW335AQQoiO44EH9PAyp1NXic6e3fg+N9+sq4PKymoudzhg4UJYvhyef94z8Qrhr1zJoaCg7jWWh4QM5NSpD3A6HZhMelyonsb+Z6kcEkIIITzIr5JDBQXSb0gIgPLyQn7++Xl69ry76uJbCF8QEwNXX928fUJC4Ior6l6XlgavvqorTl1Vp+PHw/XXty5OIdxFKWUBNgPB6Ou69wzDeFAptQVwXfV0Ab4xDONSL4VZS1nZcZQKIiAgusbykJAEDKMcu/0nQkN1MshmO1C5TqaxF0IIITzFr5JDrmFlQvi706f/zcGD9xIWlkjnzhd5Oxwh2q1774XPPoM1a/TjkhJYtUonoBprdi1EGykFphqGUaSUCgRSlVLrDMOY5NpAKfVP4F9ei7AOZWXHCQrqhjprrL8rAWSzZZ6RHJKZyoQQQghP86vZyiQ5JIRmt2cBYLWmeTkSIdq3AQPg0CHIydG3V17RCaLvv/d2ZEJohlZU+TCw8ma41iulIoGpwIdeCK9eZWXZtYaUQXUCyGbLwOE4zYED/8vRo8/UWCeEEEII9/O75JD0HBICSktdySGZv1uI5khO1vep8qsj2hGllFkptRM4CXxmGMbXZ6y+FFhvGEZBPfsuVEptV0ptz8nJaYtwgerKobMFBXUlMDCWwsJvOH58FUeOPEVx8S6io6cSECAXcUIIIYSn+FVySHoOCaHZ7UcAKCz8BqezrJGthRAuPXtCnz66F5EQ7YVhGBWGYYwEegJjlVKJZ6yeD7zdwL4rDcNIMgwjKS4uztOhVqkvOaSUIiZmBrm5n3D69H8JDR1KSkoeI0eub7PYhBBCCH8kPYeEaCes1q/cXskTEtKPuLjLay0vLc3CZLLgdNopLPyWqKjxTT5mbu7nhIUNIzi4Ow5HPidOvAaY6d79V5jNIW6MvnmKinYBBuHhI9xyPIcjn4KCNDp3bsJ0V8KvpKTA+vVgGNVNqoVoDwzDyFdKbQBmAruVUrHAWGCudyOryel04HCcIji49rAygE6dZnHy5Fvk52+gV6//aePohBBCCP/kN8khw4CiIkkOifYrPf16bLZMtx93woRsgoNrfjtrt2fRufPF5OS8h9Wa2uTkUHl5Abt2zaRHj1sZNOh5srNf5ODB/wUgICCSbt1ucHv8TZWevgDDKGfMmF1uOV529kscPHgP48YdICSkv1uOKXxDcjK8+SYcPKh7EgnhTUqpOMBRmRgKAaYDyypXXwGsNQzD7rUA61BWlg0YBAX1qHN9p04zMZsjqagoIi6unqkEhRBCCOFWfpMcKikBp1N6Don2qbT0ODZbJv36PUZ8/J1uOWZh4Ta+/34qBQVfEhd3WdXy8nIrFRVWIiLGUlj4HQUFaUDTvpktKNgKVFRVOFmtqVgsAygvz8VqTfVacqi83EpR0feAgcORR2BgTKuPabcfAnTTbkkOiTOlpOj7tDRJDol2oTvwmlLKjG4X8K5hGGsr110NLPVaZPVobPaxoKBYkpNzMAwnZrOlLUMTQggh/JbfJIcKKtswSuWQaI90gobKhpvhbjlmVNRETCYLVmtqjeSQq9+QxdKbqKgUcnM/wjCMWtMJ18U1u1lx8W4cjlys1jRiYy+hrOykV5tbFxR8BTgrf95K584XtfqYZzbt7tbt+lYfT/iOYcMgKko3pb7Be8VyQgBgGMYuYFQ966a0bTRN46qSbWj2MZMpqK3CEUIIIQR+1JC6sFDfS3JItEdWaxomk4WIiNFuO6bJFExExJha09WXlurkUHBwb6KiknE4cqq+xW08zlRMJv0t7vHjqygvP01UVApRUcmUlOzF4TjttvibQyemTCgV4LYklSuJJjO6ibOZTDBxojSlFqKlSkoyMJksBAfHezsUIYQQQlTym8ohSQ6J9sThyCU/fxNgAJCb+wkREePc/k1pVFQKR448xcmTa9AjDiAvT8/4YrH0JiAgGoCff36eqKhJjRzNoKDgK7p2va5yeuG/VJ2jrOwEAEeP/h/h4efWe4TQ0KGEhQ1p5bOqzWpNIzx8FEqZayXD6lJc/CPBwX0arNIqLc1CqQBKSvbgcOQSGNipyfHk56ficJxs8vZNERExFoulJwAFBd8QETEGpRQOx2kcjjxCQ+v+Bt4wDAoLvyEiYmyTqsOcznLy8j7FMCro1GkGJlOwW5+Hr0hJgXXr4O23IbjyJRo2DAYP9m5cQnQENlsGFssAlPKb7yiFEEKIds/vkkPSc0i0BwcP/pHs7BU1lvXt+4jbzxMTcyFZWU+wZ89VNZYHBEQTFNSNoKAeBAXFc/To3zh69G9NOmbnzr+kpCQDq3UTwcG9CAkZSHBwb8zmSH766eEG9w0O7sn48VlNSlI0ldPpoKDgK7p3vxWlzPz88/M4nWX1JtrKy4vYvv08evW6m/79n6hnm0LKy/Po1GkWubnrsFq/JDb24ibFY7MdYufOxhJtzRcdPZWRI9dj1Lr0+QAAIABJREFUtX7Jd98lM2zY+8TFzSUz8y7y8tYzYcKxOl/XvLzP2LXrF5x77md06nRho+c5dep99uyZB8DAgX8nPv52tz8XX3DhhXD//XDNNdXL+vaFQ4e8FpIQHYbNlklo6CBvhyGEEELU6cgR6NoVvvtO9y32hpEjIaSNJ4L2m+TQmDGwaxf06+ftSIQAq3UT0dEXkJDgSsiYCA11f0VNdPQFjB27H6fTVmN5UFDXqkqipKSdlJX93KTjmUzBhIQMIjr6Auz2QwQF9UAphdlsYezYfQ1Wy+TkvM9PPz2M3X7IrQ2ei4p24nTaiIpKQSkzR4/+lcLCb+udga2g4CsMo5T8/I31HtM19C4u7nLy8j6noCCtyckhq3UzAImJ/8Ji6dus51Kfo0f/ysmT7+B0llXFnZ+/kdjYS8nL+4Kysmxstv2EhtYuW8nP31C1fVOSQ/n5mzCbwzGbI8nP3yTJoXqMHQuZmVBcrB+vWQNLlkBWFvTu7d3YhGjPDMPAbj9Ip04zvR2KEEIIUcvmzTB5MsybB6tXey+OjAxIqL81n0f4TXIoLAyGD/d2FEJAWdkpSkrS6dr1xgaHYLmDUorQ0IENbhMUFEtQUGyzjhsQEFEr9uDgbgQHd2twv59+etjts3+5egJFRSVXJbys1tQGkkN62Flh4Q4qKmyYzbVT8q7kUGjoECIizmtW3yGrNY2AgGg6d77YbUMmOnWazfHjqygq+q5q2JzVmobd/lNVYs9qTaszOeSK3fW8mxJ/ZOR4AgNjyc/f0uRm5f7ozJnKKip0cigtTZJDQjTE6SzB6bQTFNTV26EIIYQQtXz3nb5//33o2RNeesk7cfTo0fbn9JvkkBDtRUHBl4Du1eNPwsKGYTZHuX32L6s1FYulH8HB+h00JCShMiHyP/VuD2YMw0Fh4Taio8+vtY3drmcqCw7uTWRkMseOPYvTWdqk/jtWayqRkRPd2ksjKioZgPz8zZX/f8wUFe0kL++Tyi3MWK2pdO9+c439nM5SCgq2AWYKCr7G6XRgMgXWe57ycivFxbuIi3uQwMBYTp58h9LSLCyWPm57Lr5q+HDd0y41FebP93Y0QrRfDkcuAAEBTe/jJoQQQrSVTD2hJg4HJCbCL37h3XjakiSHhGgjNttBysutnD79H5QKIiIiydshtSmlzERFTSQ/fxOFhd+57bhWaxqdOs2oehwZmUxu7n/rOYduqt2ly5WcPPkOOTnvYzZXd6lXykxo6NDKaexNBAV1JyoqhaNH/8zJk2sICxvWYCwVFcWUlOyla1f3Jb8AgoO7Y7EMIDv7ZcrL8+nS5RpOnnyLI0f+itkcQVTU+XW+rsXFP2IYpXTpMp+TJ98mJ+efxMbOqVUtVVaWQ2npUQoLtwEGUVEpBAR0BuDkydXExEwHICAgipCQ/pSVnaC0tHooYkBANCEhrRuzaxgVFBfvwTDKUcpMWNiwqkqwjiAgAMaPh40bYf9+GCTtVISoU3m5Tg41p8m/EEII0VYyzpjEua2HdXmbJIeEaAMlJZl8880gXLOTRUVNwmy2eDcoL4iOnkxu7jp27Bjt9uNW/zyFEydea/AcsbFzKS7ew7FjT3Ps2NM11vXvvwyb7QDBwT0xmQIqq3bMpKc3PeFzZjzuEh09mePHXwGgd+//JSdnDTbbPjp1uoiYmKnk5v63nudspnfv+zh5cjV7986nW7ebGTLk5aq1hmHw7bdjsdsPA1QmLsdhNocQEBDDwYP3AvdWbT9mzB527pyMw5FzxjlMjBuX0arhgseOPU9m5h1VjxMSnqFnzzsa2KP9mTwZ/vQnPWPZ1q06WSSEqEkqh4QQQrRnrsohgIENd+fwOZIcEqIN6KbABoMGvUhQUBzh4ed5OySviI+/k7CwRAyj3G3HVCqImJjqRstdu15LUFA3DKO0zu1NJgsxMRcSETGG4uJdNdYdOHAPeXnrsdn2ExExBoCgoDhGj97a5KbdZnMkkZETWvhs6jdgwFPExs4hMDCO8PARjB79FaWlR4iIGEdAQDShoYPrfF2DgnoQHn4uo0dv5eDBe8nP/6LGerv9EHb7YeLj7yQmZirBwb0JCAgHYNSoLdhs+i+kw5HLvn03c+zYMzgcOfTqdS9RURMoKzvB/v2/Jj9/U6uSQ/n56wkO7s3Agc+QkfFb8vLWd7jk0N13wznnwOWXwxdfSHJIiLpI5ZAQQoj2yuGAw4erH0vlkBDC7azWVAID4+je/Vd+3dzXbA6hc+fZHj2HyRRI586Nz4ITEtKv1lCo3NyPyc5+FcMoJT7+t1XLIyPHuD3O5goM7ERs7CVVjyMiRhMRUV0p1NjrGhk5ls6d53DgwN3Y7UexWHoC1Q2ru3e/hfDwml37w8KGVQ2lMwyDgwf/QHb2qwDEx9+OxdIHw3By8OB9lT2PbmrRczMMo3J44EXExs7h1KkPOX16bYdrhh0aCpddBkOH6t5DQojaHI7TAAQGdvZyJEII0fH997/62sPh8HYkvsHQgzwYORJ27vS/NgGSHBKiDVitaZWzaXWcD7r+KCoqhZ9/fqHqZ1/jek4FBWlYLPMAnRwym6Ma7aeklCIqKoVTp/5JcHBPgoN7Vy43ERWVXDWLWkvYbBk4HDlVjbcjI5M5fnwVNtv+Omdga++Sk+Hdd8HpBJP7+pIL4RNkWJkQQrjPpk06ofGnP3k7Et9hscD11+vXViqHhBCt5nQ6qKgoBsDhyMFuP0B8/O1ejko0JjJSJydMplDCw0d4ORr3Cw8fgckUSl7eBmJi9NQLVmsqUVFNm10tKiqZU6f+SWRkzURnZGQyp0+vxWY72OgHPrM5HJOp+k+PYVSQl/d55fFTatzn5X1OYGDd010HBES024bVKSnw4ovw9dd6mBnoqqKgIO/GJUR7UF6ei8lkqdUYXwghRPNlZsKAAfDII96OxPdcd523I2h7bksOKaUswGYguPK47xmG8aDSnyCWAFcCFcDzhmE8467zCtHeOJ2lfPPNUOz2gzWW+2Iliq+xWPoQHNyTkJDBDU753lGZTIFERk4gO3sF2dkrqpZ37dq0v35RUZMAiI6eVGO56/HXXw9o9BihoeeQlLQLkymgshn2BAoLtxEQ0LmqSig0dDCBgXFkZCwmI2NxnceJjJzIqFGp7bIaL6XyV33ixOplcXF6FrPoaO/EJER74XDkStWQEEK4SWam/zVNFp7jzsqhUmCqYRhFSqlAIFUptQ44B+gFDDEMw6mU6uLGcwrR7pw48QZ2+0F69bqXoKBugO4XExEx1suRicYopUhM/LDG9Pa+ZuDAZ8nN/bjqsckUSJcu1zZp34iI8xg27AM6darZ0ykyciJDhqzC4chrcP/S0iyOHv0rOTnv0bXr1eTmrqOwcBs9etxGly7zq6qXlFIMG/ZPCgt31HmckpI9ZGe/SF7eejp1urDObbypf389rOzYMf24sBAeeABeeAHuu8+7sQnhbeXludKMWggh3MDp1Mmh6dO9HYnwFW5LDhmGYQBFlQ8DK28GcDtwjWEYzsrtTrrrnEJ4Q0VFCceOPYvTaatz/fHjrxEePpr+/Z9ol1UNomEREb49k1xY2BDCwoa0aF+lFHFxl9a5vFu3Gxvd3zCc5Oau4/DhB7HZ9pGT80+Cg3uRkPA0JlPNMVfR0ZNqVSi5OJ2lnD69loMH76GgoHY8zRUWNpy4uMtafZwzXXllzcdpafDXv0LpGZPonX8+XHCBW08rRLsnlUNCCOEe2dlgs/lfXxzhOW7tOaR0A4gdQALwnGEYXyulBgDzlFJzgRzgTsMwMs7abyGwEKB3797uDEkIt8vJWcPBg/c2sIWZxMS/SWJIiLMoZaJv34fYs+daDh9+CFAMGrSiVmKoMSZTMH36/ImMjMUUFe10Q1xBpKRYMZstrT5Wff7f/4OpU+Ghh6qX9e0Lhw557JRCtEvl5acJCZExEEII3/Tkk/Dvf7fNuYoqyzJkWJlwF7cmhwzDqABGKqWigQ+UUonoHkR2wzCSlFKXAa8Ak87abyWwEiApKclwZ0xCuJvVmkpAQAzJyTlA3U18JTEkRN26dJlHXNxVVY9b+rsSH7+IHj1a3+T99Ol/s3v3pRQWbic62nN9wZKTwW6vfvy3v8HvfqeHnsXHe+y0QrQ75eX5BARI8y0hhG969lk93GtIy4q0m8Vi0dPYjxvn+XMJ/+CR2coMw8hXSm0AZgJHgfcrV30AvOqJcwrRVqzWVCIjJ7bbmZKEaO/clTx1x3FcM9RZrakeTQ4BnBmuq2l1WhpcdVXd2wvhi8rLrQQERHk7DCGEcDubDY4cgYcf1r0GhehoGp+7uImUUnGVFUMopUKA6UA68CHg6qowGdjvrnMK0dbKyk5RUpIuM48J4SOCgmIJDR2C1ZrapucdOVJPb5/atqcVwqsMw0lFRSFmc6S3QxFCCLc7WDlRsQzzEh2VOyuHugOvVfYdMgHvGoaxVimVCryplLob3bD6FjeeUwiPy839nLKynwEoLv4RkGnphfAlUVEp5OS8x/Hjr7vtmEoF0LnzLwkIqHvmu8BAGD8ePv4YXj/rtGPHtk05uhBtraKiEEAqh4QQPikzU99Lg2jRUblztrJdwKg6lucDs911HiHakt2exa5dNeeHDAiIJiIiyUsRCSHcLSZmBtnZL5Ge3viMa83Rr9/j9Onzh3rXz5ihp7a/8azTjhwJ333n1lCEaBfKy60AUjkkhPBJGZVTLklySHRUHuk5JISvsFq3ADB8+EeEhg4GICCgk0dnNRJCtK0uXa4kMvIIhlHmtmP+8MPsyveP+pND//u/MG+eblzp8txzesp7qxWipLhC+Jjy8gJAKoeE6Kj274dXX635d0tUW78eOneGmBhvRyJEy0hySIgGWK1pmM0RdOo0QxpQC+HDLJaebj1eVNT5nDy5GsOoqPe9Qyk9nf2ZLroI/vIX2LoVZs50a0jCRymlLMBm9OywAcB7hmE8qHTH9iXAlUAF8LxhGM94L1KoqNCVQ5IcEqJjevRReOMNPUuWqNsVV3g7AiFaTpJDQjRAz0w2QRJDQohmiYpKJjt7JcXFPxIefm6T9xs3Dsxm3ahakkOiiUqBqYZhFCmlAoFUpdQ64BygFzDEMAynUqqLV6OkunJIhpUJ0fE4nfDJJ3DNNfDmm96ORgjhCZIcEn6rvLyQoqKdBAV1ITR0MCUl+ykrO1G13um0U1y8m7i4K70YpRCiI3I1rT9x4q2qPitNddllunR/8+aay/v2hch6PlMbBpw4AT16WIiISEIXjQh/YBiGgZ7wAyCw8mYAtwPXGIbhrNzupHcirOb6XZDKISGap6JCV+wUFnovhpwcfZs1y3sxCCE8S5JDwm9lZNzBiROvoVQAY8bsZtu2c+vsORIdPaXtgxNCdGgWSz+Cg3tz5MgyjhxZ1qx9Fy3S92f3dHBNkduQ48chMfE/xMZe3Kxzio6tcqbYHUAC8JxhGF8rpQYA85RSc4Ec4E7DMDLq2HchsBCgd+/eHo2zeliZVA4J0RypqbBggbejgPBwqWoVwpdJckj4rfz8L7BY+mK3HyYr60kMo4yEhP8jLOycqm3M5nAiIsZ6MUohREeklGLUqM3YbJnN3tduh/T0msmhjz+GLVvg3/+GwMDa+zz2GGzc6OSpp35Jfv4GSQ75GcMwKoCRSqlo4AOlVCK6B5HdMIwkpdRlwCvApDr2XQmsBEhKSjI8GWf1sDKpHBKiOfbt0/c7d0J8vPfiCA3VNyGEb5LkkPBLdnsWpaVH6N//KQ4d+hMnTvwDpQLp3v1XmM0h3g5PCOEDLJY+WCx9WrRv9+41HxcUwPLlcOAATJhQe/s1ayArC06dGkNkZFqLzik6PsMw8pVSG4CZwFHg/cpVHwCvei2wSnpYmQmzOczboQjRoWRmQnAwDB8OJpO3oxFC+Cp5exF+yWrVH55iYqYRGTkGw3AQEXGeJIaEEO1ScrK+T02tve7IEZ0YAti5M4Wioh1UVJS0XXDCq5RScZUVQyilQoDpQDrwIXBB5WaTgf3eibBaRYWVgIBI6YklRDNlZED//pIYEkJ4llQOCb9RVnYCp9MBQF7e55jN4YSFDScqKgWrNbWqgawQQrQ3XbrAwIHwxRcwf37Ndf/9r76/9FL44osUpk9fyunTHxMZ2fohsQEB0QQEhLf6OMKjugOvVfYdMgHvGoaxVimVCryplLob3bD6Fm8GCXpYmQwpE6L5MjP13wAhhPAkSQ4Jv3DixFvs3XttjWUxMTMwmQKIijofWFp5L4QQ7dOkSfDKK9CrV+11ERGweDFceulEKipM7NlzuVvOOXDgc8THL3LLsYRnGIaxCxhVx/J8YHbbR1S/8nKrNKP2kMJC3Y/MYvF2JL6jpETPEhYRAceOwcmTuiFz//7w4496XVswDD2keMaMtjmfEMJ/SXJI+IXc3I8JDIylX78nqpbFxEwFoFOnmQwf/hGdOv3CW+EJIUSjliyBiRP1B4WzDR2qexG9/HIMf/nLpyQmHuK3v239OaOiJrb+IEJUqqgowGyW5JAnREbqpMWBA96OxHf8+tdw9Kiuzhw0SCeLAK65Bt56q+3jGTq07c8phPAvkhwSfsFqTSMqahI9etSuqldK0bnzLC9EJYQQTde9O/zqVw1vc9VVsH79NFasgMcfB7O5bWIToimcTjtmswxT9JSDB70dgW/Zvl1XDGVm6sTQLbfASy/Be+9B166wYkXbxRIUBFOntt35hBD+SZJDwueVlv6M3X6Q+PjfeDsUIYTwuJQUWLlSD3s491xvRyNENafTRmBgrLfDEKJRFRU62VZWBl9+qZfdeiusWqWXDR0Kc+Z4NUQhhHA76XkvfJ5rZrKoqGQvRyKEEJ7X0MxmQniT02nHZJKmOKL9O3pUJ4EA1q3T90OGQL9++ueEBO/EJYQQniTJIeGTKipsfPPNOWzaFMSePVdjMoUQHl6rX6cQQvicfv30ELTFi/VQhLpu06bV3btICE+S5JBnyO+y+2VkVP+8bp2eMTIysjopJMkhIYQvkmFlwicdP76KkpJ0evS4nYCAaCIizsNkCvJ2WEII4XFK6VnNNm+ue/3hw/D22/DZZzL7jWhbkhzyDLu9+ueyMp0AFq2TmVn9s8NRnQwaOFAni2RaeSGEL5LkkOgQTp9eR0zMVEym4DrXFxbupKDgy6rHR448RWTkeAYOfA6lVFuFKYQQ7cLMmfpWl9JS2LQJ7r+/5gcggKQkGDvW8/EJ/9QmyaE5c6p/AWbOhEWL4J13YMsWCPDNy97i4uqfc3OhWzfvxdKY/fv10NczY26PHA4ICdFVmAcPVieDXPdSOSSE8EW++VdS+JSSkgx++OEiBg58tt6m0unp11NcvLvGsoEDn5XEkBBCnCU4GO67D+68U8/Gc6aePSErS1cfCeFuHk8O2e3wn//ocVYREToT8dBDkJ+v/2P37++5c3uRa4p1aP/JoS+/hFOn4Lbb9D9RezZqFERF6WT6DTfoZdddpyuzEhO9G5sQQniCJIdEu2e367lZ8/O31JkccjhyKS7eTe/e99Oz550AKBVEYGB0m8YphBAdxR13wDXX6Bl5XFatgnvv1Z+h+/TxWmjCRxmGUZkcCvHcSQ4e1ImhjIzqpjH5+fo+M9Nnk0NnVw61Z5mZYDbDM89AYKC3o2maiy6q/jk6GhYu9F4sQgjhSdKQWrR7dnsWAFZrKkYdXRetVj2crFOn6QQFdSEoqIskhoQQohGdO+smq66bq/+QzHImPMEw9NRPHq0cciWEDh6EffvqXueDzq4cas8yMnTT/I6SGBJCCH8iySHR7pWW6uRQWdmxqp/PZLWmolQgERFj2jo0IYTwGcOH62EeaWnejkT4IqdTd032aHLI1USrrAw2bKh7nQ/qSMmhzEzp1yOEEO2VDCsT7Z7dfgSdx3Ty888vEhWVUmN9Xt4nRESch9kc6pX4hBDCF5jNMGECrF8PH3+slw0dCr17ezcu4RvaJDl0ZnXQyZP1r/MxHWVYmWvE38SJ3o5ECCFEXSQ5JNq90tIsIiLGYLcfICvrsTq36d37j20clRBC+J5p03TfoVmz9OPnntOTPQnRWm5NDjkcunHWPffonkJ//KNuoPX99xAfD8eO6e1cP8fHw+bNcOGFrT93Q+bNg1tvhcJCuOsuuPpq+POfobxcr7/2WrjpJref9szKodOn3XfcL76AJ57QSR13qKjQL41MAy+EEO2TJIdEu2e3ZxEZOYZhw96jtPRorfVKmQgPH+GFyIQQwrfcfTdccEF1o+q+fb0ajvAhFRU2wE3JoR9+gBUrYMAAPfXV+vUwfjwMHgwLFuhp648ehd//Xk9j/8tf6g7Idnvrz12fffsgJ0cnh7ZsgVdegc8/13FMmADp6ZCX5/Hk0NHal0kt9vrrugfZeee575hTp8LMme47nhBCCPeR5JBo1wzDSWnpEYKDL8di6YnF0tPbIQkhhM8KDIQx0r5NeIBbK4dcQ8QyMnRyaODAmp3Ub7ml+mfXVFNXXtn68zbkd7/TCSvDqO5vlJWlE1ipqbrS6fXX9Xql3Hpq17CywYPdO3ouIwPGjYONG913TCGEEO2XNKQW7VpZ2UkMo+z/s3ff8VFVaR/Af2cmk0YKEEKRFiAgKCUqYAGkKK6IfVFELIhl7b3gqivurt19dVEXX14V0AVEERVZcC00g0iT0ARMAiEgBAikEEif8/7xzM3MJJOQMpObzP19P5/5zMy9M/eeO/Xe5z7nOQgPZ9ELIiKi5sqvwSEj+JKWJpem0E8pMVFSeA4e9I7QGG1LTATy8yW7yM+MzKEBA/xbd7upvLRERNQ4mDnUUEVF0Bm7UdjyBOytOiEsrAMAoLy8CEop2GxhJjeweTNGJwsLY3CIyO+OH5fhqcgasrPrXpCkbVugVavAtIcsJSDBod9+kwrMga4lVBtGFCU11TtCYwzNZcxPS5PvlR8ZmUP9+wOffgrk5QGxsQ1bZn6+1PTmyGJERNbB4FBD3XgjMlp+gb23AIAN556bhoiIbti+/RrYbOHo2/cLs1vYrBUW7gYAZg4R+dv69VIHY/t26YtAwa2gAOjWTa7rghWpyU8C0q3MKDzdFNJbjCiKkc1k8MwcMub7ebiukyeBkBAZXdBYRUPrBBmb0BReWiIiahwMDjXU9u3IeUgh9KhGSZwTubnLERraATk5y6CUA05nGWw2vsz1lZ//M2y2CERG9jG7KUTBJSVFqg5v3crgkBX8/LMEhv7yF6B379o/b+DAwLWJLMUdHIpo+MLS0iQ1Ji9P7jeF9JbOnaVo18cfA3v2uNtntC0hAbDZgFmzgPR07+f26we0awdERtYrqnPyJNCihXtVr7/e8J/1Xbvkuim8tERE1DgYtWgIrVGetRfHTwc6fQYcHBeJvLzViIw8HVqXQOsSnDixGdHRfhzmwWLy8pIRE3MubLZQs5tCFFwyM72vKbglJ8uB6WOPATExZreGLMhvmUP79wOHDgH33QfMnAmEhgJJSX5oYQOFhMhQf99+K0GiKVOA995zB3tCQ4Hhw4Hly+VS+bmtWgEdOwKbNtV51SdOSFypZ0+gSxdg/nw/bA8k3sXMISIi62BB6obIzsbxhGJou0bs/taI3d8SeXnJyMtzj5iRl7faxAY2b2VlBSgoSEFMzBCzm0IUfBgcspbVqyU7gYEhMonfgkNLl8r1PfdIVCQnB4iPb2Dr/OS//5XRyEpKJDiUkSEZQYZly2S+52XlSqCsTApVp6RIQes6MjKHwsOBvXurrqK+l8xMIMIPiV5ERNQ8MHOoITIzkddPbsbGXYiTPy/H0YQDyM7+EhERveB0FiEvLxmdOj1objubEKezFICu1WPz81cDKEds7NCAtonIkvbt876mwCotlcwdu732j9e1+608pbIyYM0aYNIk/yyPqB78Ghzq3NldYMePfv1VavR37uw9PTNTEoICQZWdj5sjYhBamA8A2DHhr8ju6p1xnnPamSgLjUR8xnoAQHlIGPacMw7loRK52bFDMoewcSPQtSvQpo10I/38c0klGjlS6svFxgKdOlVtRFoaoBTQo0dgNpKIiJoFBocaIjMTeX2BSFt3OAZdhNj3vgRukDo57dtPhtNZhNzc5dBaQylldmtNl5X1EXbunITaBoeEDbGx5weoRUQW5itz6KOPgFdfBTZvlm4O5B+zZgG33San4Ldu9X0AprUUCL/uOul+8mAATioMZaA9GCmlwgGsAhAG2a9boLV+Xik1C8BwAK7CPJiktU4xp5V+DA79+CNwxRUSzPCza6+VmNPChd7Tn34amDvX76tzccCGa9ED6TgNB9Bn5XtVHnEMrXAQHXAmfq2Y9sGHwL9xc8X9q65wAiNGAJMnA//8JzB7NnD//TKzsBC46irg7LNlOLPKJk2S4PWqVX7eNiIiak64998Aet9e5J8JxMcOAW65BTHFReg57WmUjT4Pbc99DseOLcHhw3NRVJSBiIhuZjfXdNnZXyE0tB06dnyg1s+JjDwdISENHI+ViLw5ne6MIc/g0Pffy6nzLVvkIIL848svgZYtgdxcST+4556qj8nIANaulQO0iAgZWeyOO/zXhshI4Oqr/bc8akqKAYzSWhcopRwAkpVSrr5XeEJrvcDEtlVwOgsBNDA4lJMDZGcHJGuopEQGQbP5KLiwa5ck33z0kd9XK8rfB5xOqLJSHMw95jUrcv6HaP2P59FK5aLglvtw/O4n0H5YIt69ZxdeftL9uHbF+4HEAmDnTnejDTt2ALt3u9KLfNixo/ZZjUREFLQYHGqAEzkpKOsPxLa9CIiJgXrscXT88ivgozLgnoSK7lB5ecmWDw5prZGXtxqtW49G165/Nrs5RNZ2+LAcCbVvD2RlyVnliAj3QUVyMoND/qK11Pu56ioJDK1e7Ts4tNpVn27DBilme/vtwJ/5W0mnprXWAApcdx2ui5/6JPqPO3MorP7uKIq0AAAgAElEQVQLCeD46hkZEjdPT5eBHI1YidYSNLrlFt89svzD7ro4AFQK4Bw+B/gHoLRG1NAkRJ3XFUhIQMyhVMR4tmeZ67VJTZVr47UC5LdHa5mmtXfW1bFjcgFkdLVYnpAjIrIqFqRugDxsBQDEthzmnjhkiPT5PnkSLVqcCbs91qtAtVUVFqajtPQQ6wcRNQVGtpDRzWj/fjlgMIJDq1lI329++00yHYYOlUtyNf8HxvTSUqkuyy5gVAdKKbtSKgXAYQDfaa3Xuma9qJTaopR6UynlMyqjlLpLKbVBKbXhyJEjAWuj01kEpcIa1s3eCHgEYHx1Y9ElJfKTaMjOBvLzTRzS3TMQZtxOTPQO/gDuoNDeve40qFGjZNqSJXJdWAgcOOD9PM/lVF4mERFZCoNDDZAXsxehBaEID/fICho6VHbu16+HUnbExp7PEcuAigAZRx4jagKM4NAQ1/dx3z7JIDp+XGoNJSf7rxiy1RlBn6FD5fXeu9f7yNPzcYMGue8P4W8l1Z7WulxrnQSgE4DBSqm+AJ4G0BvAIACtATxVzXNnaK0Haq0Hxgdw1C+ns6jh9YaMAEj37g1vUDWLBrxjJMZ004Z0T0hw93UzIlQ9e0rDPH+njUY7nTIvIwM491zp0upZS6i6oJKveUREZCnsVlZPWjuR1zkHsdldvM+CXXCBXF91FXDHHYi9ohOO6W/wU3IHwGZHjx6vol27idUu99ix/yIj42/o338pQkKia2xDRsbfUVSUgd693/fHJtVbeXkhNm8ehaKi6ofELi/PR0hIS7Ro4f86AWRhBQXAH/4AvPwycOGFZremeXjjDeDvf5fbRgDCGKkGAK68UqqxpqYCvXo1bF2ZmfL+5OfX/Li77waee65h62pqtJbi0v/9LxAXB5x+ugy7DQBJSUBYpSSOAwfkfSkokLP7HTs2fpup2dNa5yqllgO4VGv9hmtysVJqJoDHTWxa7YND+fkynHvPnhIUMbpDlZUBmzbJUGKu8dW1BvbsAYqLG96+jRtldU6nJE+edppMN+K7pmUOhYZKgOjgQXejEhMlmL9mDdCqlUzbvNm9AZ9+Kq9Xz55yWb/ePS85GWjb1r38tWvl919r4Oefgf79ZXpMjHQ99gxCJSZKZmNMjEwrKJDbhuxs4MgRoEULGSXNUFgo66/8u1df7P5GRBQQDA7VU/b++Shu40Sbo+d6z2jdGnjrLeCzz4A330T7n3uieCCgB3dAbo8C7N79DOLjr4fN5qiyTK01du+egoKCFBw8OAOdOz9W7fpLSg5h796/Q+tidOx4D6Kjz6n2sYGWlTUL+fk/o23bCbDbW1T7uNjY4VCKyWrkRzt3Aj/9BAwfzkyX2vrsM9mpnjIFOOcc2dl//333MOdPPgksWgS88w4wbVrD1vX661LA45Zbqh9ZKCUFeOkl4E9/8j5gae5WrJBhpMeMAW66Sbb/rLOAZ5+VLK3KQkOBW28FBg6U7FOiWlJKxQModQWGIgCMBvCqUqqD1vqgkjNYVwPYZmY7ax0cuvBCCXS88Qbw2GPAhx96F2cfPbri5pIlwOWX+6+NgwbJ38rzz8vFEBYm8RnTnHmmBIGM39E+feS6cobhyJHAypXAX/8q93v3lseuXw8MHiyjJT77rFw8de8ugaO33pILIOu6+WbvKtw33ignD+bOleLgjz4K/P67BIOKi2UkRuNkwLp17mzIyy+XYNHMmQ1/LZKTZVS2HTtMTOciIgpODA7Vg9YamXteQvgBID7+j1Uf8NBDwLhxQLduCFu9C722xgAf70X2zLuwLeYVZGRMRVTUWfLYtFTX6SiFoqLdKChIgd0eg337/gdhYV2rbcPRo4uhdQns9ijs3v00OnS4CzabA61aXQK7PcLnc4qLf4fW5QgP71JlXmnpMeTmLkdISBxatRpR7XoLCrYiPLwbnM5C5OauBADs2/c6oqPPRZ8+cxpWS6CxZWfL2Sdfw0pT83D4sPv2unWy89sUOZ1SaLi69i1b5i4IOmgQ0LXSdz89Xc6Yn3460K+fnOkdPFi6KIWHy9ndU9m0SQ4Sdu0CJk6U4BAAPP64ZO6UlMgO/uDBMv/99yUTsr5D2peVAR98IIGR92vIbty5U0Yeeuop4M473dmXgbRunQTGahqd59gx+Y3o2VOKuR4/Xrd1/POfQLt2ciAV7jogttmAv/2t5ucFruItBa8OAGYrpeyQcgGfaq0XK6WWuQJHCkAKgLvNbKTWpT5PjFWRni7Xe/bI9aZNQHQ08H//J/fPO6/ioZs2yfWcOf4ZbOuccyS24dnTCpDBA0NDG778envvPfmNNowaBXz9tTsb0TB0qLxuv/8uGT3nnSeBn8suk8BzTo779fXUt6+cYNm+Xe5nZsqJgk8/lf+jV1+VgNPChUBRkQSbjh2Tfai0NGDAABkNLT9fTgZ89JEE/gcNkuWuWwccOuSf12L9eqkYvmkTg0NERH7G4FA9FBXtwXHnNvT4ArC9cKbvB3XsCEyeLEMYz5kDjB6NuKtfQYsv45GZ+ZL3Y3913wwL64xevd7D1q1j8euv19XYjvj48YiM7Im9e/+OnJzvAAA9eryJzp0f9vn4X3+dCKfzBM45Z32VeXv2PIsDB6YDAAYN2u6z+1dZWR42bhyIzp2fQFFRBg4fnlMxLzHxreYVGAKAZ56Rg/LKe4HUfHhmYHzwQdMNDn32GXDDDd5nUg1btgAXXeS+P2RI1aLFf/yjnElv3Rr46itg2DDZSX/mGTlq+c9/al7/gQNyYPDww7Iz37u3e96tt0rAYvNmWa5SclAwZw4wYULDtjskBHjiiZof07u3BNNnzQJmz5aDmg4dGrbemmzeLHU4PvpIzopX57HHgMWLgQULgEsvrd+6Xn3VHRgiChCt9RYAZ/mYPsqE5lRL61IodYrgUGmpdFUC3AHztDQJjI8fX+XhaWmyu3Xjjf5ta5MbrNHoTmaw26tPmarcJbVdO+/XbuDA6tfTt69c5+bK/0BRkXSDHT8eWLoU+NW1w5qW5v3+DBjgrld0113AJ5+47x8+LO9perqcKLE1MIPcWC7rIxER+R2DQ/VQVJQBAIjarWrOOnn7beCVV6QY4J49UNdfj7PeKEfRkmUy//9mANPeBm6aCDwlZ/HDwk6Dw9Ea556bjvLykzW2IyIiETabA23bToDWTmzbdgXy8lb5DA6VlxchP38NtC5DWVkeQkK8+2rn5q5EZGRvnDy5E7m5q3wGh/Lzf4bWJcjNXY6ior2Ii7sc3bq9DJstHJGRZnXGb4CsLDnTVVZW/+wIMpcRHBo5svpRoJqCFSvkeuXKqsEho1DosmWyQz1zpntoeUDO9G7eLEGUnTuBd9+V6fPnS1r9vn2n/gwnJ8tOuZHS7xkcCg+X5R886M5YOuMM+W7k5TVos9GyZe0yYT76CLj+eqnRs3q1BIsCZaVkPGLFipqDQytWSObQdAma4+efJbOqtux2OaAlIgCA1mVQ6hT/tTk57ttHj8p1amq1gf/UVBNrAQWzli2BNm3kN9CzCLYhNdUdHDJOsBnBmt69JVvJuG/MLyqSExUNzY6svFwiIvIbvx0RK6XCAawCEOZa7gKt9fMe86cBmKy1jvLXOs1iFF4OD+tSc3E9h0P+YAE56LroIoS8/jqibN2ByEjg21QgA8DSncDf+no9NSKi9iNxGIGc2NgLcezYUmitq2TxHD++AVpLSnJ+/s9o3foPFfNKS4/i5Mlf0a3bi9i/fxry8pLRsWPV7HNjxLH8/J8BONGq1RRERfWt8rhm4/hxOWA+cMC7cCI1H1lZ8h27+GLJojl2TLJrmhojcJWcLN24Ks/r1EkCXAUFwIwZkjZvFNj+6Se5fvJJyUb89FO5/9lncl1QIHUkzqqSOFB1/caBV+WgRVycXDx17iyXxhAeLkX8IyKkrYEMDnm+F9XZv19G+gHkde7bV7KNiKjenM5aZA4ZAQfjdkmJfBerSQ1KS5Ma+hQAPXu6u9YC3lG4bdvkpATgHaxp2VL+gxMTqwaNjMc0NDjka7lEROQX/qwOXAxglNZ6AIAkAJcqpc4DAKXUQACt/LguUxUXS3AorG0dR94aOlT+TNetk/7Sa9bI2eWUFHcadQPExg5FaekRFBZW/cM0AjuADXl5qyvN+6ni+bGxQ5Gfvxq+yPPsAJwVj2/WjBoixrDe1PxkZUnK/FDXZ9EIpDQlOTmyI223S1aMZ+FsrYEff3S336i34xm4SE6WrKDx46W7ldMpyzKuAVluTVavdj82MrJp1rVxOCQAc6ptaQit3a/Fb7/JqDq+GG0wXuehzfy3jqgJqFXmkBEcatNGbmdkyHfQR22Z/HzpscTMoQAxXlhfmUNGYAjwDtb07Cldk3v2lPtae2f4NDSgU1IitfY810tERH7jt8whrbUGYEQ4HK6LdhVIfB3AjQCu8df6zFRUuBehxwBbz2rqDVXn/PPleuFCSZfOy5PCr3PmSHeS6s789+kjB3SnEBsro1YcPjwfcXFjvObl5HyLiIjTYbe3QE7Od2jT5qqKednZX0EpB6KjByE2dgiysz/HsWPfw+Fwx/O0diI/fy3ath2Pw4fnwm6PRlRUv7ptf1PD4FDDFBfLkLatTIz7ZmVJMeZBgyS48OWXEkBp2TLwhcaNdR85UvNnaO1aub7hBvmuf/21uybEkSOSuWaMOBMXJ9/3b7+VIeAB4IcfpABGZKQ8bsEC97IuuECKjy5d6v59qaykRALQEybIc04/veE1HwJlyBDpirtmTWCqvx4+LK+38bs7d67vwM/XX8vrPXq01HiqPCIQEdVZrWoOGV3JEhOlG60rmDD2oUSsqdRjvrxcrlmTOECMF7Zy5tAZZ7hrD51xhpzAaN1a9mmN2kaJidI9unVr2U/o3l3qyT34oAw+UF9Op1yMNtQmU3jsWOm6fMklwL33AtdUOhR55BHZj3n4YanHdPKkdD+uaR9i0SIZ1e277+QkwjXXyOWWW6o+9s475X/90Ufd0z77TEbhW7Kk+pE8iYhM4NdCK65A0EYAiQDe1VqvVUo9BGCRazhVf67ONMW5vyHsELzrdtRGq1ZS2O/tt+UCuEeDuPPO6p93003Axx+fcvGRkb3hcLRFRsZzyMh4rsr8Dh3ugt3eAvv3v4mNG70LEsbEDIHdHoGWLUcAALZsGV3l+YCMznbixFaEhXWBvN3NmDHc6r595rajuZo6FZg3T4ITZn23s7IkcBIRIVknH3wgF6XkrGKgAkQ//ijdvtatk+4OpzobGh4u3cnmzJHuU5UNH+6+PWKE1LnxLBpqFHUeNUqCy08+KUGikSOly+q//y07mTWZNEnaPWBAbbbQHCNGAC++GPgRyx59VAKJD/su3g9AAkOjR0ugaNiwwLaHyAK0Ljv1aGVG5lBiotT52rULAJByoidu8jHWWosWcsxPAXDHHUB8vLt7cUyM1K0791z5z3E45P9s1izJEFJKBjgApH7cnj1yEgmQQRdycoBffml4uyIiJAjz4YdSwLwmq1dLgP/AAeD774GEhKrBoU8/le0cNcqdjbRmTc37D0uWAMuXS8CrdWv5P3E4qgaHtHaf/PUMDi1eDHzzjQRD27Sp9aYTEQWaX4NDWutyAElKqZYAvlBKXQjgOgAjanqeUuouAHcBQJdmUPuluCAdLQ4BGFePGhRffCH1QQCgbVugf3/JLNi/3/fj335bCtUaf7w1UMqGpKSVKCz0lWqrEBs7FErZ0KrVRdDa6TU3KkqylqKjz0JS0iqUleVWWYLNFo5WrUYhOnowbLYaai01F8wcapht2yS9+7ffzCu8a2TvABKo2rRJskPuuOPUZ/4a4jsZHRAffiiBofvvr/kIpUsXCcr89JPUcPDUqpUMT2946SUZdtjofmazuesP3XmnZLr06yfZQF26yNnZ66+vub0tWkggadUqGRK6qbroIsmUqjw8sz+1bi0BxTVr3HWFfBk4UHbahw93F+omonqToexPUdTdCA4Z2Srr1+NESAxa9WyDadMC2z6qpEMH4O5KEblJk+T6xRfd03xlvcfHA6+/XnW68Xx/+Mc/Tv2YadOAhx5ydzmv3BXtxAkJHOXlyb6M4VRd1oz5qanuen2+nnPokJSNqDzP8/kMDhFRExKQIZq01rlKqeUARkKyiNJcWUORSqk0rXVipcfPADADAAYOHKgrL68p0VqjSB1B6/wwSWutq4QEuXg666zqu5Tt3y8HohkZMmT1KbRo0RstWtSc0RQXN7bG+S1b1nyWPDy8CdYrqSun013nicGh+jFet+Rkc4JDJ09KgM8IDnXqJBetJW09OVkKOAeCURPIGP3r9tslK/BUquv65ally+qHKA4JcQeSjMzFyEjgiitOvVyg6Qc5lJKzt42hXz/voFx1+jbjovtETYjUHKpF5pDN5t7fWbsWGSGJSOwZHJnn1MiMIOPSpXJdOcvXuH/ihLu+X/v2pw4OGc9LS3MHNI0aS54nco3lZGXJ/opxcsYzOFSb/QIiokbit8ITSql4V8YQlFIRAEYD2Ki1bq+1TtBaJwA4WTkw1NyUlR2DM6QMYVHdG6duh1HrIpBFWq3IMzOBwaH68QwOmeHQIbk2gkMGpeR7E6h2lZa66wgVF8vOXm2CDEREFiY1h05xTvLoUcmmNLIpdu/GryU9WVeI6seok2QEh37/XU4sGTyDRUuXSlCyT5+au4oXFbnLEaSluR9bUCCZy548l5OeLte5ue4MYo64RkRNjD8zhzoAmO2qO2QD8KnWerEfl98kFB3aAgAI71jDsNH+dOaZQGysdC277DLfj2nRAggLgm5ejcnoUhYWVrXmUC268PnVyZOysxEIgfps5OfLDg7gv8BlXp67wmhtGCnglYNDgHS9WrRIHuPvlO3Nm+U9u/xyqRtw/vnukcCIiMinWmcOtW7tVWh4lzORI5JR/SQkyP9zVpZ7Wnq6+4SOZ4ZQVpZk8SckSDf16vYFd+92d/tOTfUuip2aKiOoet73vJ2U5B0Q4ohrRNTE+HO0si0AaoyYaK2j/LU+sxRv+QGIBML6jGicFdrtkgUxc6a7C0tlbdtK7Zfw8MZpUzAwilH36SO1W3JzpTvPRRfJn3dt+rL7w5490j2opCQwy/f12Zg+HXj5ZdlBcpxiR706RkDtnHOAjRsli8dzh6iuPvhA6gTVx2mnVZ1mFBAOZHe3KVMkOMRixUREp+R01pw5tGgR0OmHY+gc0xrxHgfcqeiJC5g5RPXhcEiwJz1d9hUOHHAX2gaA7dulZlBOjpQbSEyUx+fmyihnvnoIGFk/HToAK1bIyJrGsh96SKYbNm+W+wcPyiAes2e7s55PO01GJjW6kTscUr9w+nTvE4bjxwM33yy3Dx6UuoSvvy4lJw4elEz4AQOq75L9xhsyouuAAcCf/ywjgsbEuOc7nTL9lluqlst47z0pSD7WRzkKI4B2441ywuyJJ4AXXpATckuWyL7nPffIYBzp6cCrrwK9evluI5EV/fqrDDj10ksSiNYaeO45Keiflyff87ffrlqOJsACUnMomBXtWQucCYQnNeLwGG+95R7WurLffgPefVcO0Dnccu0ZmUPDh0twaN06GZno55/lD62xgkPffy+Bob/+VTLE/Mn4bGzY4D1c91dfSXAnJUV2GOrDCA5NmCCfvdWrgWuvrX9bFy+WHaW6DnFbuZiz4dxzZejanJz6t6kmXbrI923JEn7viIhq4VRD2b/7LvBm9n5sK+mFkV274vDgy7FzXR52th/pNXgjUZ3cdZeMSPbAA3J96JA7kyguToIiubmy//fHP8pJtc8/r9pFzNOll0pQZNo0OaD7059k9LG9e72zlNq1kwEjdu2SQJEx74orZNS0d991T9u4UfbLMjIkkBMSIkGVAwfcwaEvvwTeeUeW+T//A+zYIW2//HLfwaGyMuDpp4Fx42Qfbfp0GTzj6qvdj8nMlMCN3e5daByQA9WkJN/BoZdfdgeHkpOBf/0LGDxYRqx76y0ZIGT8ePf+9Hnn1X0fjyiYffyxBGvvuUeOKw4dku9gYaHcX7xYTp43MgaH6qj42A6oUgVHdCOOqtazJ6rtcH/kiPy5rF7Ng9S6MIJDo0dLVDY5WUYnOnlS/ow9R8EKpNWr5SzLs8/6vyub52fDCA6Vl8soTca66xscMuoNXX21tD05uf7BIa2lLZddBjz4YP2WUZlS7p2pQBozJvDrICIKAtKtrPrdzt2p5eiBdHxbOBYjHKH4ZOLXeGgdkJXinehAVCdPPikXQAIXtVHb7vKe+xl33lm3dgHAbbe5b3fqJIEhh0MCRXa77BPNmuXu4mZ0SUtNlYuRhVRd7aK9eyVAZDzeeK4nz2V6Mmoj+Vq20+me7nRWXXZqqjx3wwb3c9iFjsib53evSxfv+8XF8sdnZDk2okaoqBxEiopQVH4Q4SWxUI1Zk6Ym8fHSdcasosDNlREcOu00OSuyerV3YerGKgCenCyBm0B8nnx9NrZtc3epa8hnJjNTdlwSEuRMUUNer9RUCWR5ZjcREVFQkaHsfWcOlZQAZRn7EYYS/FraE1lZsp8cHS2JHERBzyis1b27u45hYqLsrxpZTEaAZcsWKa5tMEZKq8xzVDXjuZWDPdVNN+5nZsqBqqeDByW7obBQMps811Nc7N6f/uYbuW7VisW3iSqr/N3zvJ+WJt9/E+INDA7VxcaNKG7jRFhYZ7Nb4m3IEOCnnyR6T7VjBEiioyUo8fPP7pEkgMYJDmVlyToDmfFV+bNhBISGDpVt9LUzURuZmXKWy6iJ9csv3iPA1YXxWjPzjYgoaNWUOZSRAXTXsoOchsSKY1mT9o2JGp/RQ8Czp4Bx2zP4AkitIk8nTnh3ZzMYB5t5ee5RVmvKHPLcJ/TMDNqzx/dyjcd5Zgzt2ePe5/zmG/kCX3IJM4eIPGntOygEyLHhrl3V9xoKMHYrq4vkZBR3A1q1OdPslngbOhT48EOJzNdlLyo0VOrPnH9+4NpmFq2Biy8G7r5bCntVZmQORUdLUGLaNODrr2VaQzKxcnOlq9aRI6d+bFmZXAcyKFL5s1FYKNlSN90kr03LlvXb8y4oAC64wL2Ol1+Wbnj1GbWrsFBG+whk8WgiIjJVTTWH0tKAnpAd5FT0rDhxevbZjdlCIhMZmUOeQ/MZt9PSpGaPcRJzx46qz09L8y6GbUwzGM+pLkOooEAylIzBRTwfl5Ymg6f4Wq7xZTVuewaBduyQ7jL9+gHz50sQq0WLqm0nsppDh9wn1SsHf0tK5IzJxImmNI3BoTpw/rQKxYOAsNgmVm1/3DgpPlxYWLfnTZ8uxe2CMTiUlwcsWybBn9oEhwBg4UIZ9v2aa6RCfH3+xFaulC/3TTdJocNTiY+X4smB4uuzMWoUcOGFcnbHc0SMurrqKrm++GLgL3+R17y+hg3zPSoIEREFBV9D2Z88Cfzv/wKrVgFDkAYdHo5DpR0xd67sG19/vTltJWp0RiDIM1sgIUFOus2ZI/tyJSWy3+q5DwvI/XffrXpi8/vvqz5+3z736EiA1AUyHvO3vwEdO8r0r792T3//fWDrVu/lOhyyjHnzgN275bH5+VKY2ljX8eOyXca2/eUvUmfTcMkl0j1u8ODGqfMZKCdOyMn2CROqnnAtLQU++QQYMUJeq759pcYmIBlWM2YEbvAUsyglxci7dZNjg2++kR/z9euBH34ArrwSOPNMOQZZsECKmlc+BigvB+bOleWEhnrP01qCjWPHur8Dnj75pGq2W1Ozf79cR0fL6/Lyy9KLxfP7alLmELTWTepyzjnn6CapvFwXnh6rly+HPnDgfbNb4x/nn6/1kCFmtyIwduzQGtC6TRutnc6q8595RmubzT0vIUEe36OH1kuWyO1ly+q+3iee0Do0VOvCwoa1n4goyAHYoJvAfgcvjbMPtmJFmE5Le9Jr2nvvyd8toPWayJHamZSkhw+X+zab1v/5T8CaQ9S0ZGZq3bat1ps3e08fNsz9JQkN1frZZ+V2v35a33ST1vfeq3XXru7HVL7cfbfWHTrI7Wef1dpur/qYp57SOjq66vQJE7Q+4wzfyx05UuuLLnJ/WZ99VmuHQ+736aP1xIlye+pUrdPTtY6IqLqMzp3l+t57TXnJ/eYf/5DtWLGi6ry5c2Vely7u9/DkSZm3bFn171tzv0yYINs4darc/+UX+VwYnx2t3X8Avn7ov/xS5n3wQdV5a9fKvBdfrDpv717zt722l+horZ980nva449rHRUl39Pt2+v3eayFmva/mDlUWzt3oihUMiPCwhpxpLJAGjoU+Oc/JXIbHm52a/zLGMEhO1vOtlTuspSfL1XgjQj/0KFymrJLF8mkUkrOwIwcWbf1JidLt7Jgez2JiIgawFfm0NKlQNeuwM71xxHWMRnqkkew7GU52W6zSXICkSV07uzed/W0YoV8IQDJIgoJkSHmQ0Lc2RbTprlLFVQWGgq8845kqTgckr3jWaNUKXnM3/8u2RqVn6u1e/2ejC+n55fVWLaRVfTBB5KRD8h+t+fy33hDRrsF5IdA6+ZbYGzpUvf18OG+5xlFuktK5D0dM0bmORxS4DsqqtGaG3B33AEsWSLvt7H9773n7tqYnCzZMZ6vm5FNZfCcN3ly9fP+/Gff8zZvbvrlKozv89/+Jp9/QL4vr7wi90PMCdMwOFRbq1ej2NUNt8kVpK6vIUOk+9SGDcE3UpRnYb7k5Ko/EMePe6ciDhkC/PvfEhxq2VLSPutalLqwUF7LRx6pf7uJiIiCjJyoLPcqSF1SIj0MJo8/gfAnHpCDzDFjYLO5jyeJLM/XF6JyNxu7veaaj57zq4u4hoT4PhhVquYvpOe8ysv2nFd5+Zdf7g4O7dkjAaxWrapfT1PldEq/WEDKU/Tt6z3fCFYAEhB77jkJlBw9CnzxhZRVqE0ZiubkssvkmOrll4F162TajBlyPX06cM890sEOxH4AACAASURBVLXxhx9k2qJFVUtsLF4s1999J8vy9Nlncr1mDTBrlvfn6uOP5YxDv37NJ9jo6/tsIssHhwoLd2PHjpvgdJYgMfFNtGw5zPcDV65EUfcWAE4gPDxIgkNGQeEbb/TvD9PQocDbb/tveZ7eeksynaZMqflxRnAoMlKiyu+8I/fDw+WHo3JwyAiOdenivv/++8BZZ9W+bcXFsnMbbIE2IiKiBtBasho8h7L/5RepgTtZfwDMni0HhsZ+CREFt/79gV695CB+0SLgwQfNblHDXHedBC1uvrnqvAkTJHA0bpwEND7/XLYZAB56qHHb2RguuUSOv557Tu6PGye1hXr3liygv/xFsmOAml+3668HPv3U9zzjebfdVnXeww83n8BQE2T54NDevS/h+PFfEBISjT17/oyzzvqx6oMOHwY+/xyF73SBw5EHuz1IKu3Hx8sXd/Nm/y0zK0sCMZMmAeec47/lGv73f6Xg88SJkoJbUzscDonSf/ONe/p33wEvvlg1OHTGGcDTTwM33CD377pLllE5xfZUBg2Sgs9EREQEANBauqV4Zg4Zgxp1OeHqarB7d9UzqEQUnJQCNm2S7/yxY9LtrLmKiJCRgF991XfXvM6d5bgjJkaKixuBEbtdio4Hm7g46UaXkyMD+7RrJ6U74uPl9fjtNyn7ER4uBdBfe61qt0iHQ07Yv/GGnHz3ZLPJ6/bPf7pH/DIoFZyvaSOydHCouPh3HDr0ETp0uBORkacjLe0h5K18F7Gqn/cD580DiotxsncLREZ28L2w5uqvf/Xv8vLy5Mv8/PPAk09Wnd++vZwpyMiQH8tTpc5pLemm3btLVk5amvyAPPcc8NhjcsbBl6ws+TG65x65GB58UNI5O3b0Hi7UZpMUR0NSkkT5iYiIqEHcwSF35lBamvz1xh5Jk9GKWrY0q3lEZIbISLlu21YuzV23btXPi4mR67Aw7+OPYBUX590rpXt39+2WLb1/72sK5tSUCNAhyI7JmwhLB4eysmZD61J07vwYQh1tkZHyCA78dD9iX/bx4HHjcFIvR3zkuEZvZ7MSGwvcd5/0M/3Pf6rODwuTTKV+/aSA3t1317y8L78E/vhHGUIzJEQCQ23aSAr67NnArl0SbKosK8v3sJiPPSbBoYwMdv8iIqJmTykVDmAVgDDIft0CrfXzHvOnAZistTat4qnRraxy5lCXLoAtPVXq/hEREZGpLB0cys1dhRYt+iIiojuwezdabnQib2gs8EPVjJGSpG4o29IdkZG9TWhpM/P888Af/lA1tXLbNulb+9prkgX0ww+nDg59/71kDy1f7o4ef/qpdPW74QZg5crqg0OdOlWd3rWrBKcOHgTOPrt+20dERNR0FAMYpbUuUJKak6yUWqq1/lkpNRCA6VVenU7fmUN9uhcDyzOlKzoRERGZyrLBIa3LkZ//E9q2vVEmJCcjdiuQPTwPxeefjrCwjl6PP5mbDACIjGziw+I1BWFhVYdyBCRt/NFHpSA0IKOInWroyuRk97VRHPrssyU984EHZPqdd1Z9XlYWMHCg72X26SMXIiKiZk7LUGAFrrsO10UrpewAXgdwI4BrTGoeAM/MIYfrvmQOXX7pbrljhW4WRERETZzN7AaY5cSJbSgvP47YWFfXotWrEbtHCk3n5VUdwvzkyZ0AwMyhhoiKklo+paUSEMrKknpC1cnNle5kSkkQaOdO6V8aGyvThg71Pdx8eTlw5IjvbmVERERBRillV0qlADgM4Dut9VoA9wNYpLU+aG7rqhakPnZM/uL7tdgtD2BwiIiIyHSWDA6Vlxfh6NElAIDY4p7Avn3AqlWIOm0YbLZIHDv2LYqK9nldCgo2wmYLR3h4F5Nb38wZdX6uvlquFy+W19/X5T//kTOKV18N/P67dDHr3dt7WenpQEqK9/O2bpUAEYNDRERkAVrrcq11EoBOAAYrpS4EcB2At0/1XKXUXUqpDUqpDUeOHAlQ+7yHsjdGKkuIcq2vXbuArJeIiIhqz5LdyjZvHoX8/DUIK2uF8ITzKqbbbr0VsbFlyMr6AFlZH1R5XlRUEiRLm+rtwgtl6MG775Z6QQ89JJfqOBzA448DX3wB7N8PXHWVe96wYXJtdDerrGNH39OJiIiCkNY6Vym1HMBIAIkA0pR03Y5USqVprauk6GitZwCYAQADBw7UgWmXd+ZQWppMPy3smNxo3ToQqyUiIqI6sGRw6OTJXWjZ8iL0WNgGKuo/wFtvyUhY116LXiHXITd3hc/nRUef27gNDUZXXw3897/A6NHA0qWS5VOTxETgggtk1LKjR4GxY93zBg4EPv8cyMmp+ryICOCyy/zbdiIioiZGKRUPoNQVGIoAMBrAq1rr9h6PKfAVGGoslWsOpaVJ7/A4dQyw293DPBMREZFpLBcccjrLUFaWg9jYoYjekgJ06wbcfnvF/AhEIyKih4ktDHI2G3DJJXJ78GC51IZnxpBBKeDaa/3XNiIiouanA4DZrgLUNgCfaq0Xm9wmL5Uzh4xh7EPyjkrWUE0DUxAREVGjsFxwqKwsB4CGwxEHZGa6h0cnIiIiama01lsAVNO/uuIxUY3UnGrWXzVzqGdPSGVqdikjIiJqEixXkLq09CgAwOFoI8WLu7DANBEREVGgOJ1VM4cSE8HgEBERURNiweBQNgDA4YwCsrMZHCIiIiIKIHe3MgeOHZNSgcwcIiIialqsGxw6KinODA4RERERBY67W1lIxTD2iYmQgSYYHCIiImoSrBscOnhSJjA4RERERBQwRuaQzeaoGMa+InMoLs68hhEREVEF6waH9uXJBBakJiIiIgoYz4LUxjD23TqVAsePM3OIiIioibBkcMhmi4R97yHZO+nY0ewmEREREQUtz6HsjWHswwtzZCaDQ0RERE2CJYNDDkcbYO9eoEMHwOEwu0lEREREQaty5lDFSGUAg0NERERNhHWDQ7/8AvTta3ZziIiIiIKa51D2qamuekP798vM9u3NaxgRERFVsFxwqKzsKByIAbZtA4YONbs5REREREHNyBzKz5eh7BMTAe/K1ERERGQ2ywWHSkuz4cjVgNYMDhEREREFmFFzKCMjBIArHpSaCoSHA6edZmLLiIiIyGDN4NCBE4DdDgwebHZziIiIiIKakTmUkSF1HisyhxITAZvldkWJiIiaJEv9IzudZSgry4Uj7TBw9tlAixZmN4mIiIgoqBmZQ+npIVAK6N4d7uAQERERNQmWCg6VleUCAEK2ZwJjx5rcGiIiIqLg5w4OOdC5MxAe6gTS01lviIiIqAmxVHCovPw4AMBe6gDuu8/k1hAREREFP6Nb2aZNIejTB0BWFlBcDHTrZm7DiIiIqIK1gkMnjwAAQs4bBbRpY3JriIiIiIKfMZT91q0OjB4N4OBBmcFi1ERERE2GpYJDZTm/AwDsXXqb3BIiIiIiazAyh8rLQzBmDCRzCADatzevUUREROTFUsGh8uOHAQD2SGYNERERETUGrUvhdNrRrp1ydysDgA4dTG0XERERuVkrOHTC1a2sRbzJLSEiIiKyhk6dHsTcuZvQti2gFNzdytq1M7VdRERE5BZidgMaU3nhUcAB2KMYHCIiIiJqDKGh7bB3bztERbkmZGUBrVoBYWGmtouIiIjcLJU5VFZ0FABgj2EfdyIiIqLGUlAA7+AQu5QRERE1KZYKDpUX5wIA7C05OgYRERFRYzl+3CM4dPAgi1ETERE1MdYKDpXmwVYE2FrGmd0UIiIiIssoKACio113srIYHCIiImpi/BYcUkqFK6XWKaU2K6W2K6VecE2fo5TapZTappT6UCnl8Nc666qsvAD2QnicuiIiIiKiQKvoVqY1M4eIiIiaIH9mDhUDGKW1HgAgCcClSqnzAMwB0BtAPwARAO7w4zrrpFwXwF5kcw2VQURERNS81XBy7gPXtC1KqQVKKVPPjFUEh7KygMJCoHt3M5tDRERElfgtOKRFgeuuw3XRWuslrnkawDoAnfy1zroq1ycRUmI3a/VERERE/lbdyblHtNYDtNb9AWQCuN+sBpaWAsXFruBQWppMTEw0qzlERETkg19rDiml7EqpFACHAXyntV7rMc8B4GYA3/h43l1KqQ1KqQ1HjhzxZ5O8lNkKYS8NDdjyiYiIiBpTDSfn8gFAKaUgmdvapCbixAm5jooCkJoqd3r2NKs5RERE5INfg0Na63KtdRIkO2iwUqqvx+x/AViltf7Rx/NmaK0Haq0HxsfH+7NJXsptxbCXMzhEREREwaO6k3NKqZkAsiDd+9+u5rkBP0FX4ApdVWQOhYQAXboEZF1ERERUPwEZrUxrnQtgOYBLAUAp9TyAeACPBmJ9tVXuKEWIM8LMJhARERH5VXUn57TWtwE4DcAOAOOreW7AT9AdPy7XFZlD3btLgIiIiIiaDH+OVhavlGrpuh0BYDSAnUqpOwD8AcAErbXTX+urj7LQMtjB4BAREREFn8on51zTygF8AuCPZrWroAC4EzNw5SPdga+/Zr0hIiKiJsifp206AJitlLJDgk6faq0XK6XKAOwFsEa6vWOh1vqvflxvrZWHlcNu4zD2REREFByUUvEASrXWuR4n515TSiVqrdNcNYeuBLDTrDYWFADjsACOkhPA9dcDkyaZ1RQiIiKqht+CQ1rrLQDO8jG9SeQNa2cZnOFAiD3a7KYQERER+UuVk3MA/gPgR6VUDAAFYDOAe8xqYEEB0BepOD74IsR99JFZzSAiIqIaNInATWMoKzgMALCHxJjcEiIiIiL/qO7kHIAhjd2W6pzMKUYXZCKn+81mN4WIiIiqEZCC1E1Ree4BAIA9tKXJLSEiIiKyDrU3A3Y4YTudw9cTERE1VdYJDuUfAgDYw1uZ3BIiIiIi6wjdmwoAcPRhIWoiIqKmyjrBoeMSHAqJaGNyS4iIiIisI+L3NLnuz8whIiKipsoywSGtnQg/HAJHbGezm0JERERkGWGhTux2nI6QdnFmN4WIiIiqYZmC1LEX3IHzcIfZzSAiIiKylBFfPQrgUbObQURERDWwTOYQERERERERERFVxeAQEREREREREZGFMThERERERERERGRhDA4REREREREREVkYg0NERERERERERBbG4BARERERERERkYUxOEREREREREREZGEMDhERERERERERWRiDQ0REREREREREFqa01ma3wYtS6giAvQFcRRsA2QFcflNipW0FuL3BzErbCnB7g52Vtrembe2qtY5vzMZQzQK8D2alz71Z+BoHHl/jwOLrG3h8jQOvqb/G1e5/NbngUKAppTZorQea3Y7GYKVtBbi9wcxK2wpwe4OdlbbXSttKNeNnIfD4GgceX+PA4usbeHyNA685v8bsVkZEREREREREZGEMDhERERERERERWZgVg0MzzG5AI7LStgLc3mBmpW0FuL3Bzkrba6VtpZrxsxB4fI0Dj69xYPH1DTy+xoHXbF9jy9UcIiIiIiIiIiIiNytmDhERERERERERkQuDQ0REREREREREFmaZ4JBS6lKl1C6lVJpSaorZ7QkEpVSGUmqrUipFKbXBNa21Uuo7pVSq67qV2e2sL6XUh0qpw0qpbR7TfG6fEtNc7/cWpdTZ5rW87qrZ1qlKqd9d72+KUuoyj3lPu7Z1l1LqD+a0uv6UUp2VUsuVUr8qpbYrpR5yTQ+697eGbQ3K91cpFa6UWqeU2uza3hdc07sppda6tmu+UirUNT3MdT/NNT/BzPbXVQ3bO0sptcfj/U1yTW+2n2WDUsqulNqklFrsuh+U7y3VnxX2wRqDlfaDzGClfRGzWG2fwCz8Xw4sVYdj7ub2O2GJ4JBSyg7gXQBjAJwBYIJS6gxzWxUwI7XWSVrrga77UwD8oLXuCeAH1/3mahaASytNq277xgDo6brcBWB6I7XRX2ah6rYCwJuu9zdJa70EAFyf5RsAnOl6zr9cn/nmpAzAY1rrMwCcB+A+13YF4/tb3bYCwfn+FgMYpbUeACAJwKVKqfMAvArZ3kQAOQBudz3+dgA5rulvuh7XnFS3vQDwhMf7m+Ka1pw/y4aHAOzwuB+s7y3Vg8X2wQJtFqyzH2QGK+2LmMVq+wRm4f9y4NX2mLtZ/U5YIjgEYDCANK31bq11CYBPAFxlcpsay1UAZrtuzwZwtYltaRCt9SoAxypNrm77rgLwkRY/A2iplOrQOC1tuGq2tTpXAfhEa12std4DIA3ymW82tNYHtda/uG4fh/yhdUQQvr81bGt1mvX763qPClx3Ha6LBjAKwALX9MrvrfGeLwBwkVJKNVJzG6yG7a1Os/0sA4BSqhOAsQDed91XCNL3lurNyvtgfmWl/SAzWGlfxCxW2ycwA/+XTRMUvxNWCQ51BLDP4/5+1Hww1lxpAN8qpTYqpe5yTWuntT7oup0FoJ05TQuY6rYvWN/z+10piR8qdxfBoNpWV0rrWQDWIsjf30rbCgTp++tKb04BcBjAdwDSAeRqrctcD/Hcportdc3PAxDXuC1umMrbq7U23t8XXe/vm0qpMNe05v7+vgXgSQBO1/04BPF7S/XS3D/jTV1Q/0+axUr7Io3NavsEJuD/cuDV5Zi7Wf1OWCU4ZBVDtdZnQ9LX7lNKXeg5U2utUfMZ7GYt2LcPkobYA5KGexDAP8xtjv8ppaIAfA7gYa11vue8YHt/fWxr0L6/WutyrXUSgE6QLILeJjcpoCpvr1KqL4CnIds9CEBrAE+Z2ES/UEpdDuCw1nqj2W0houD7nzSLlfZFzGC1fYLGxP/lRhO0x9xWCQ79DqCzx/1OrmlBRWv9u+v6MIAvID+4h4zUNdf1YfNaGBDVbV/Qveda60OuP1QngP+Du2tRUGyrUsoB2Rmbo7Ve6JoclO+vr20N9vcXALTWuQCWAzgfklYb4prluU0V2+uaHwvgaCM31S88tvdSV3cFrbUuBjATwfH+DgFwpVIqA9JVaBSAf8IC7y3VSXP+jDcHQfk/aRYr7YuYzWr7BI2E/8uNoI7H3M3qd8IqwaH1AHq6KrWHQoq7LjK5TX6llGqhlIo2bgO4BMA2yHbe6nrYrQC+MqeFAVPd9i0CcIurQvx5API8Uv2apUr9U6+BvL+AbOsNrhEHukEKnq1r7PY1hKt/8wcAdmit/8djVtC9v9Vta7C+v0qpeKVUS9ftCACjIXUclgMY53pY5ffWeM/HAVjmOgPTLFSzvTs9dhgUpB+65/vbLD/LWuuntdadtNYJkP/VZVrriQjS95bqLej3wUwWdP+TZrHSvohZrLZP0Nj4vxx49Tjmbl6/E1prS1wAXAbgN0i/1mfMbk8Atq87gM2uy3ZjGyH9Rn8AkArgewCtzW5rA7ZxHqS7TSmkv+bt1W0fAAUZHSUdwFYAA81uvx+29WPXtmyB/NB08Hj8M65t3QVgjNntr8f2DoWkX24BkOK6XBaM728N2xqU7y+A/gA2ubZrG4C/uKZ3hwS50gB8BiDMNT3cdT/NNb+72dvgp+1d5np/twH4N4Ao1/Rm+1mutN0jACwO5veWlwZ9PoJ6H6wRX0fL7AeZ9PpaZl/ExNfYUvsEJr/W/F8OzOtap2Pu5vY7oVyNJiIiIiIiIiIiC7JKtzIiIiIiIiIiIvKBwSEiIiIiIiIiIgtjcIiIiIiIiIiIyMIYHCIiIiIiIiIisjAGh4iIiIiIiIiILIzBISIiIiIiIiIiC2NwiIiIiIiIiIjIwhgcIiIiIiIiIiKyMAaHiIiIiIiIiIgsjMEhIiIiIiIiIiILY3CIiIiIiIiIiMjCGBwiIiIiIiIiIrIwBoeIiIiIiIiIiCyMwSEiIiIiIiIiIgtjcIiIiIiIiIiIyMIYHCIiIiIiIiIisjAGh4iIiIiIiIiILIzBISIiIiIiIiIiC2NwiIiIiIiIiIjIwhgcIiIiIiIiIiKyMAaHiIiIiIiIiIgsjMEhIiIiIiIiIiILY3CIiIiIiIiIiMjCGBwiIiIiIiIiIrIwBoeIiIiIiIiIiCyMwSEiIiIiIiIiIgtjcIiIiIiIiIiIyMIYHCIiIiIiIiIisjAGh4iIiIiIiIiILIzBISIiIiIiIiIiC2NwiIiIiIiIiIjIwkLMbkBlbdq00QkJCWY3g4iIiAJo48aN2VrreLPbQW7cByMiIgpuNe1/NbngUEJCAjZs2GB2M4iIiCiAlFJ7zW4DeeM+GBERUXCraf+L3cqIiIiIiIiIiCyMwSEiIiIiIiIiIgtjcIiIiIiIiIiIyMKaXM0hX0pLS7F//34UFRWZ3RRLCg8PR6dOneBwOMxuChERERERETUDPI43T32O4ZtFcGj//v2Ijo5GQkIClFJmN8dStNY4evQo9u/fj27dupndHCIiIiIiImoGeBxvjvoewzeLbmVFRUWIi4vjB8oESinExcUx2ktERERERES1xuN4c9T3GL5ZBIcA8ANlIr72REREREREVFc8ljRHfV73ZhMcIiIiIiIiIiIi//NrcEgpZVdKbVJKLXbdn6OU2qWU2qaU+lAp1WwrGr/44os488wz0b9/fyQlJWHt2rUApD/fM888g169eqFPnz6YNm1aleeuWLECsbGxSEpKQlJSEi6++OIGtyc3Nxf/+te/qkw/evRoxXrat2+Pjh07VtwvKSlp8HqJiKiJyMkBLrwQ2LTJ7JYQEZGfHDkCDB8OZGaa3RKi4FDdcfztt9+OAQMGoH///hg3bhwKCgqqPHfWrFmIj4+vOJ6+5ZZbGtyejIwMzJ07t8r0rVu3VqyndevW6Natm99iB7Xl74LUDwHYASDGdX8OgJtct+cCuAPAdD+vM+DWrFmDxYsX45dffkFYWBiys7MrAi2zZs3Cvn37sHPnTthsNhw+fNjnMoYNG4bFixf7nFdWVoaQkLq9FUZw6N577/WaHhcXh5SUFADA1KlTERUVhccff7xOyyYiombgiy+AH38EZswApje7v1YiIvIhJQVYtQpYsQLww3EokaXVdBz/5ptvIiZGwhaPPvoo3nnnHUyZMqXKMsaPH4933nnH5/LrcxxvBIduvPFGr+n9+vWrOI6fNGkSLr/8cowbN65Oy24ovwWHlFKdAIwF8CKARwFAa73EY/46AJ0aup6HH5YfTX9KSgLeeqv6+QcPHkSbNm0QFhYGAGjTpk3FvOnTp2Pu3Lmw2SQJq23btrVa56xZs7Bw4UIUFBSgvLwcX3zxBSZPnozdu3cjMjISM2bMQP/+/TF16lRkZmZi9+7dyMzMxMMPP4wHH3wQU6ZMQXp6OpKSkjB69Gi8/vrrNa7vhx9+wOOPP46ysjIMGjQI06dPR1hYGBISEnD99ddj6dKliIiIwNy5c5GYmFirbSAiIhMtXCjXX3wBvPMOYLeb2x4iImqwY8fkevduc9tB5G9N7TjeCAxprVFYWFjrGj1Tp05Feno6du/ejS5duuDll1/G5MmTkZ2djfj4eMycORNdunTBpEmTEBMTgw0bNiArKwuvvfYaxo0bhylTpmDHjh1ISkrCrbfeikceeaTG9c2bNw8vvfQStNYYO3YsXn31VQBAVFQU7rzzTnz77bdo3749PvnkE8THx9dqG6rjz25lbwF4EoCz8gxXd7KbAXzj64lKqbuUUhuUUhuOHDnixyb5xyWXXIJ9+/ahV69euPfee7Fy5cqKeenp6Zg/fz4GDhyIMWPGIDU11ecyfvzxx4o0sRdffBEA8Msvv2DBggVYuXIlnn/+eZx11lnYsmULXnrpJa+UtZ07d+K///0v1q1bhxdeeAGlpaV45ZVX0KNHD6SkpJwyMFRUVIRJkyZh/vz52Lp1K8rKyjDd4yxzbGwstm7divvvvx8PP/xwQ14qIiJqDPn5wHffAT16AIcOAWvWmN0iIiLyg5wcuWZwiKjhajqOB4DbbrsN7du3x86dO/HAAw/4XMb8+fMrjuNnzpwJAPj111/x/fffY968eXjggQdw6623YsuWLZg4cSIefPDBiucePHgQycnJWLx4cUVW0iuvvIJhw4YhJSXllIGhAwcO4KmnnsKyZcuQkpKC9evX48svvwQAnDhxAgMHDsT27dsxfPhwvPDCC/V+nQx+yRxSSl0O4LDWeqNSaoSPh/wLwCqt9Y++nq+1ngFgBgAMHDhQ17SumiKDgRIVFYWNGzfixx9/xPLlyzF+/Hi88sormDRpEoqLixEeHo4NGzZg4cKFmDx5Mn78sepmVu5WNmvWLIwePRqtW7cGACQnJ+Pzzz8HAIwaNQpHjx5Ffn4+AGDs2LEICwtDWFgY2rZti0OHDtWp/bt27UK3bt3Qq1cvAMCtt96Kd999tyIQNGHChIrrU31AiYiogdavB7Zta9gytmwBSkqAt98Grr4aeOMNoPLJCaWAMWOAdu0ati4iImo0RnAoPd3cdhD5W1M7jgeAmTNnory8HA888ADmz5+P2267rcoyKncrmzp1Kq688kpEREQAkK5rC13Z3DfffDOefPLJisdeffXVsNlsOOOMM+p8DA8A69evx4gRIyoygiZOnIhVq1ZVLHf8+PEAgJtuugnXXnttnZdfmb+6lQ0BcKVS6jIA4QBilFL/1lrfpJR6HkA8gD/5aV2msNvtGDFiBEaMGIF+/fph9uzZmDRpEjp16lTxRlxzzTU+P1DVadGiRa0eZ6TBGe0oKyurW+NPwTOFjkMNEhEFUEkJcMklQG5uw5fVtSvwhz8AV1wBfP458NVXVR9z773Au+82fF1ERBQQR44ATieQlgYcPw5s3y7TmTlE5B/VHcd7zr/hhhvw2muv1fpYvj7H8VrXmAPTYP44jvdLtzKt9dNa605a6wQANwBY5goM3QHgDwAmaK2rdDdrLnbt2uXVXSwlJQVdu3YFINHA5cuXAwBWrlxZkZ1TV8OGDcOcOXMAyOhmbdq0qegH6Ut0dDSOHz9eq2WffvrpyMjIQFpaGgDg448/xvDhwyvmz58/v+L6/PPPr1f7iYioFpYvefuqYwAAIABJREFUl8DQRx8BGRkNu2zbBthswLx5vucPGtTwDCUiIgqo224D+vUDhg6VZE/X4QCysoCTJ81tG1FzV91xvNa64thYa41Fixahd+/e9VrHBRdcgE8++QQAMGfOHAwbNqzGx9flOH7w4MFYuXIlsrOzUV5ejnnz5lUcxzudTixYsAAAMHfuXAwdOrRe7ffk79HKKnsPwF4Aa1yRrIVa678GeJ1+V1BQgAceeAC5ubkICQlBYmIiZsyYAQCYMmUKJk6ciDfffBNRUVF4//3367WOqVOnYvLkyejfvz8iIyMxe/bsGh8fFxeHIUOGoG/fvhgzZkyNdYfCw8Mxc+ZMXHfddRUFqe++++6K+Tk5Oejfvz/CwsIwb968erWfiIhq4fPPgeho4LrrgPBw/yzT4ZAsosr69wcWLfLPOoiIKCC2bpXsIQA4/3zvEnK7dwN9+5rTLqJgUN1xvNYat956K/Lz86G1xoABA7xq8tbF22+/jdtuuw2vv/56RUHqmvTv3x92ux0DBgzApEmTaizr0qFDB7zyyisYOXJkRUHqq666CoBkL61btw5///vf0bZt24qEj4ZQgU5vqquBAwfqDRs2eE3bsWMH+vTpY1KLgltCQgI2bNjgVbndF74HREQNVF4OdOgAXHSRZPsE2j/+ATz+OJCdDcTFBX59daSU2qi1Hmh2O8jN1z4YEQVOSYmcJ9BaEkFfeAF47jn3/K++Aq680rz2ETUUjyEDJyoqCgUFBTU+xtfrX9P+V6Azh4iIiILD4cPAxRfXv15QebmcHr7mGv+2qzrGzsCOHdJfgYiImpS9eyUwBABdugBGr5aICKCwkEWpiahxMThkcRkZGWY3gYioeViwQPL/b7wR8CgwWCexsY13GpjBISKiJs0z+NO9u1wAySKKiWFRaiKq3qmyhuqDwSEiIqLaWLhQTusa1UKbuq5d5fTzjh1mt4SIiCCJp/fdBxjHdJmZ7nk9esgFAE6cAM46S8rUZWYCfyz9BDc75qFiLCKbTboNDxki/dC2bJHpY8cCd93VWJtDREGGwSEiIqJTOXoUWLECeOops1tSezYbcPrpDA5ZhFLKDmADgN+11pcrpboB+ARAHICN+H/27js8qjp7/Pj7ppBC6AmEDklo0gICFkRQAQuowKKIBQJrXwt+dZVdXGVZVEB+FlZkxYJYKCqBVWwgAhJpArKIFCEh0muC1JB2f3+c3MwkzCQzybQk5/U888zMnTv3fmYymZl75nzOgXtM08z25xiVqup++AHmzJHfGayeBEOHQt26cPvtklz64IOy7NdfYdYs2LwZ/r73VcwaOzDiC1KLtm2DmBi49FKYOBEaNYKsLLmTBoeUUmWkwSGllFJVW26uVP0sqZbQpk1SM2jIEN+NyxPatYPVq/09CuUbjwPbgZoF1ycDr5qmOc8wjP8AfwbK1opFKeUR1jSylBTnfQKshknXXQePPQZffw3xN6Vy9NrhxC76j9x4xRWysT175PqUKRIwmjJFPtNC9BBPKeU+fedQSilVtb34Ijz/fOnrtWkDXbt6fzye1L69dEb74w/5SVpVSoZhNAEGAC8A/2cYhgFcC9xZsMpsYDwaHFLKr9LSpJZQ3bqu3ych5g+iOcHG8DhirYVxcRL4t4oSxcVJ5lBuLuzbBy1benroSqkqIMjfA6goXnjhBdq3b0+nTp1ITExk3bp1AJimybhx42jdujXt2rVj2rRpF913xYoVGIbBO++8U7hs8+bNGIbB1KlTAXjuuef47rvvHN534MCBRZZ9++23JCYmkpiYSFRUFG3atCExMZERI0Z48iErpVTFkp0Nu3e7d1q5El54AW67TQo7lHT6+WcwjNLHEUi6d5fzDRvkoKH44z982L/jU57yGvA0kF9wvR5w0jTN3ILr+4HGju5oGMb9hmFsMAxjw7Fjx7w/UqWqsNRUqSvkzkdJ8zwJAO3Oj7ctjI+Xz6UdO2zXrYJF2uJMqSKcHcdbHnvsMaKiohze9/333ycmJqbw2NsTx9vp6enMmTPnouW//PJL4X7q1q1Ly5YtSUxMpG/fvuXep6s0c8gFa9asYfHixWzatImwsDCOHz9OdrZM23///ffZt28fO3bsICgoiKNHjzrcRocOHfjkk0+49957AZg7dy6dO3cuvH3ChAkuj+f666/n+uuvB6BPnz5MnTqVbt26lfXhKaVU5TBqlBRzcFfduvDGG1C/vufH5G9WcGjdOum29p//XLzOzp3QurVvx6U8xjCMgcBR0zQ3GobRx937m6Y5E5gJ0K1bN9PDw1PKqfyCUGZQZfmpOi9PHoxd5Md6jIYBR4/Crl3QuTMSrHdyzFBcta2bANiQEUe/jIKso7g42fiSJVC9utQfslqd/fwzXHKJLAsN9eADVKriKek4HmDDhg1kZmaWuI1hw4bxxhtvOLwtNzeXEDencVrBoTvvvLPI8o4dO7J582YAkpKSGDhwIEOHDnVr2+VV8YJDY8ZIZTZPSkyE115zevOhQ4eIjo4mrKB1cXR0dOFtM2bMYM6cOQQVfLLVd3Jw0bx5c06dOsWRI0eoX78+33zzDTfddFPh7fYvgG+++YYxY8YQGRnJVW60H37llVd47733ALj33nsZM2YM6enp3HDDDVx66aVs2rSJ9u3b88EHHxAZGenydpVSKuCdOweLFsHAgTBsmHv3veKKyhkYAqhTRwI/P/4Ia9dKEYukJLlt714YN05+edbgUEXWE7jFMIybgHCk5tDrQG3DMEIKsoeaAAf8OEalLhIdDR06SJHmCi8vD5o1g+eegwceKFx8991yU+/e0qUMpNg0d98N8+e7vnmCeGtZPFPrSay/h/WevWSJtDUzDGjcWDpUPv20nK6/Hr75xoMPUqlyCrDj+Ly8PP76178yZ84cFi5c6PIux48fT2pqKmlpaTRr1oyXXnqJ0aNHc/z4cWJiYpg1axbNmjUjKSmJmjVrsmHDBg4fPsyUKVMYOnQoY8eOZfv27SQmJjJy5EieeOKJEvc3d+5cXnzxRUzTZMCAAUyePBmAqKgo7rvvPpYsWUJsbCzz5s0jJibG5cfhSMULDvlB//79mTBhAq1bt6Zv374MGzaM3r17A5Camsr8+fNZuHAhMTExTJs2jVatWjncztChQ/n000/p0qULXbt2LXyR2svKyuK+++7j+++/JyEhgWEuHuRs3LiRWbNmsW7dOkzT5LLLLqN3797UqVOHnTt38u6779KzZ09Gjx7Nm2++yVNPPVX2J0QppQLNkiUSIBozRgIgyuayy+Cjj8A05ehk8GBZfuiQBIf27/fv+FS5mKb5N+BvAAWZQ0+ZpnmXYRifAkORjmUjgf/6bZBKOZCZCatW+XsUHrJ/Pxw8KJEbu+DQ2rWS4BMVJbH6KVPg1luBKzfKe/Po0S5t/mBQc57/oyZPPSWJQT3uu1zqyZ06JdsBCA6W6tU7d8K8eTKdWKkqrqTj+DfeeINbbrmFhg0blriN+fPnk5KSAsDjjz8OwLZt20hJSSEiIoKbb76ZkSNHMnLkSN577z0ee+wxFi1aBEhwKiUlhR07dnDLLbcwdOhQJk2axNSpU1m8eHGp4z948CDPPPMMGzdupE6dOvTv359FixYxaNAgzp49S7du3Xj11VeZMGEC//znP51mOLmq4gWHSogMektUVBQbN25k1apVLF++nGHDhjFp0iSSkpK4cOEC4eHhbNiwgeTkZEaPHs0qJ590t99+O8OGDWPHjh0MHz6c1Q46yOzYsYOWLVsWBpjuvvtuZs6cWeoYU1JSGDx4MNWrVwdgyJAhrFq1iltuuYWmTZvSs2fPwu1NmzZNg0NKqcolOVly7a++2t8jCTyXXQYffii/KBdMSQYkWyokBA5oQkkl9QwwzzCMicDPwLt+Ho9ShfLy/D0CD7Pq/NjV+8nJkQRN04Tt26V55L33IlPK0tOl1p2LbeebAk/kSzw/NRWZvnbHHRev2Lu3nE6ehOXLtRmBCiwBdBzfv39/Pv30U1asWFHqNopPKxs/fjy33HILERERgExdS05OBuCee+7h6aefLlx30KBBBAUFcckll3DkyBG3x//TTz/Rp0+fwoygu+66ix9++KFwu1Yiyd13380QD3TUrXjBIT8JDg6mT58+9OnTh44dOzJ79mySkpJo0qRJ4R9i8ODBjBo1yuk2YmNjCQ0NZenSpbz++usOg0PeYBSrelf8ulJKBaSzZ+Gpp+SX0dJ8/rl80db6Chfr0UPOb7wR7KcUBwdDw4YaHKpETNNcAawouJwG9PDneJRy5sQJ2+XMTMmqqdCsrmHWORIYsoJga9bAPfcU3LB/vwSI4uNxR1CQNCGz24Vz1rb37JFpN0pVYY6O42NiYti9ezcJCQkAnDt3joSEBHbv3u3SNq2EjNLYzxQyTe+W9fPEMX5lKQHnVTt37mTXrl2F1zdv3kzz5s0BiQYuX74cgJUrV9K6lLoNEyZMYPLkyQQHBzu8vW3btqSnp5Na8MvD3LlzXRpjr169WLRoEefOnePs2bMsXLiQXr16AbB3717WrFkDwJw5c9yqY6SUUn6TnCwFlNesgfXrSz41beryL7BVTmIiDBgAjz128W1Nmui0MqWUz9n/gO5SsCPQWRlDBw5IS3kuflxWvejCdQsXuC4uzsXny9q2di5TVZyz4/gBAwZw+PBh0tPTSU9PJzIy0uXAUHFXXnkl8+bNA+Djjz8uPAZ3pkaNGpw+fdqlbffo0YOVK1dy/Phx8vLymDt3buG0uPz8fD777DPAc8f4mjnkgjNnzvDoo49y8uRJQkJCSEhIKJzqNXbsWO666y5effVVoqKiirSrd+TKK68s8fbw8HBmzpzJgAEDiIyMpFevXi69eLp27UpSUhI9Cn4hvvfee+nSpQvp6em0adOG6dOnM3r0aC655BIeeughFx+5Ukr50YIFErzYvbsStbPxg9BQcDavvXFj2LrVt+NRSgWcjAyYPh2ys+Ut4+GHYdMmSS7s2NHz+7MPDk2ZYquJX7u2lI5z8htqYHj//YuDLl9+KeemyYE7/8quY7XJOAr2vYhv/Rn4B/DLL7KgjMGhr76CZctKKa9nbbtSRN6UKruSjuM95d///jejRo3i5ZdfLixIXZJOnToRHBxM586dSUpKKrEgdcOGDZk0aRLXXHNNYUHqW2+9FZDspfXr1zNx4kTq16/PfDeK3DtjeDu9yV3dunUzNxQroLZ9+3batWvnpxFVbOnp6QwcOJCt5fzyr38DpZRPnTkjbXjvuw+mTfP3aCqvMWPg3XfBxV+wPMkwjI2maXbz+Y6VU46+g6mqYfJkGDtW4vD5+dJ067XXoFs3CUR42scfS8MusMX+TVNO33xTtDxaQNm/XzJVDaNIy3pAojVr15J3+qzDuwYFQeE9EhJg2za3o2CffSYzqJs0kWlrJc4iadRI6g+5OAtBKW/QY0jviYqK4syZMyWu4+j5L+n7l2YOKaWU8p5jx6SghLu+/15S8z1QXE+VoEkTCcSdOgU1a/p7NEopP1m6VNrK//ILdO0KL74oZXFSUqQRpH25Mk+wMocyMmz1hs6dk8tLlwZwcGjpUjnfvBk6dbro5r17oXlzeOUVKKU7dZkMHQozZsBDD8Fvv0GbNiWsfO210skzP1+zb5VSLtHgUCXXokWLcmcNKaVUmZw4AS1ayDf+soiJAa2R5l2NG8v5gQMaHFKqCvj9d5g6VYIXoaEyG+qNN+CHH+CRR2Sdfv2kXTrINLN+/Tz/9rBrF1SrJtPILJGR0LOnzNr69VdZ1rUrvPCCZ/fttrfflhp4IG3HGjSAjh155x2Z/Wzv2DE579fPe8Oxtn3HHRAbCzffLNMAP/1UmpPde6/dih9/DLfcIk0bNECkVKVSWtZQWWhwSCmllHd8/rkEhv7f/5NvsO7q0EFarSvvadJEzvfvlz7LSqlK7YknYOFCqVF/ww0wcyasWgWXXQZJSbLOiBGwerUkxuzbJ1k+GRmeHUe9enDTTRdPi3r8cXjpJdnfwYOS+DJ+vJ8bUb76Khw9Kh3AGjSA4cPBMHjtNTh8uGjTseBguOsuaN/ee8OJj4fRo6Vc3MaNEmh7+GEJ+B09ahccuvlm+Qz98kv5QxY001FKKWf0W7dSSinvSE6WL6NPPFFKYQTlN/aZQ0qpSs8Kshw/LudpaVIq57//ta3Tvr0EjPzh1lvlBJJBNGqUZDsVdJv2vfx8eZIefRRefrlwsWnK4ocekt8/fO3dd+V83Dgp6J2bKzWyMzLkckgIULeuFHDq21cGq8EhpVQpNL9QKaWU550+LT/5DhmigaFA1qiRnGs7e6WqBKu+z4EDtgBHGZpm+URANNw6dAguXLjoSTp8GM6f9/9zFxcnwaBt22RKW16e1D0qZKU1aUt7pZQLNHNIKaWUey5cgCeflJpCzhw7JsUqtKB0YAsPh+ho+PBDW5EPe6NGQf/+vh+XUsorcnPlPC1NpoudO1d0WlQgscbl1+CQtfNiT5KTxT5n7d++o1yRgF+TJpJGpC3tlVIu0MwhF73wwgu0b9+eTp06kZiYyLp16wAwTZNx48bRunVr2rVrxzQHLZdXrFhBrVq1SExMpG3btjz11FMeH9+sWbNITEwkMTGRatWq0bFjRxITExk7dqzH96WUquK++QamT4e1a2HTJsenffvgxhvhiiv8PVpVmjvvlHNHf0dr7olSqlKwmkdu2wZr1shlf2e/ONOwIYSFSWOw9HQ57d8vGU8edeECmCa5ubbgGdnZssOffpLrxZ4kKxHH38+dtf8vv7QtW71aHkdODhIYatFC5gmW54nLzi7PMJXyK2fH8UlJSbRs2bLwGHrz5s0X3df+OD4xMZG+ffuWezwnT57kzTffvGj5iRMnCvcTGxtL48aNC69n++h/UDOHXLBmzRoWL17Mpk2bCAsL4/jx44V/oPfff599+/axY8cOgoKCOHr0qMNt9OrVi8WLF3P+/Hm6dOnC4MGD6dmzZ7nGlZubS0hBsdZRo0YxatQoQDqULV++nOjo6HJtXymlHEpOlhYzv/3m5yqhyiNef93fI1BK+YhVWDolRU4ArVv7bzwlCQqSsb31lpwss2bZimeXW06OZFA+8wzNPphEWBjs2YNkvVoRl2rVLqrXk5YmM6b9XcancWPp8rZsmTxf+fnw/PPSuaxpU/jqK6Tf/ZdfwsSJ8I9/uL+THTukYcG8eTBsmMcfg1LeVNJxPMDLL7/M0KFDS9yGdRzviP3xuKus4NDDDz9cZHm9evUKA1Tjx48nKirKK0klJalwwaFdu8Zw5szFUb3yiIpKpFWr15zefujQIaKjowkLCwMoEnSZMWMGc+bMIaigPWT9+vVL3FdERASJiYkcKCj+uWTJEp5//nkuXLhAfHw8s2bNIioqigkTJvDFF19w/vx5rrzySt566y0Mw6BPnz4kJiaSkpLC8OHDefLJJ53uyzRNnn76ab7++msMw+DZZ59l2LBhrFixgueee44aNWqwe/durrnmGt58883Cx6CUUk7l5EgXsltu0cCQUkpVMJmZcOmltrb1MTF+LPbsgjlzYMMG2/VHHoGff/ZgcMiKlk2ezCEm2Zb//DP06QMjR8rcrWKfd2lpEnwpODTwm+BgKe+3a5eM548/4E9/kk5mhw4VrPTvf0twyEFWhEus+yUna3BIlUugHceX1fvvv09ycjJnzpwhLy+PhQsXMnr0aNLS0oiMjGTmzJl06tSJ8ePHs3fvXtLS0ti7dy9jxozhscceY+zYsaSmppKYmEi/fv142a7YvSPLli3jqaeeIjc3l+7duzNjxgzCwsJo0aIFt99+O19//TURERHMmTOHhHK+oVe44JA/9O/fnwkTJtC6dWv69u3LsGHD6N27NwCpqanMnz+fhQsXEhMTw7Rp02jVqpXTbWVmZrJr1y6uvvpqjh8/zsSJE/nuu++oXr06kydP5pVXXuG5557jkUce4bnnngPgnnvuYfHixdx8880AZGdns8H+k9KJ5ORkNm/ezP/+9z+OHz9O9+7dufrqqwFYv34927Zto3nz5txwww0kJyeXGjVVSlUSaWllTxH/6Sc4eVK+fSqllKpQMjMl5uGx4IqXdeggJ8trr3m4trI1z85O3pnzBB88KK3InDxRqan+n1Jm6dlTTiAzxyIjpZbUiRMSLKrVsiUMGFD2J8768Tg/3zMDVsqHSjqOBxg3bhwTJkzguuuuY9KkSYVBJHurVq0iMTERgNtuu43GjRuzadMmtmzZQt26dXn00Ufp0qULixYt4vvvv2fEiBGFGUA7duxg+fLlnD59mjZt2vDQQw8xadIktm7d6nAaW3FZWVkkJSWxbNkyWrduzYgRI5gxYwZjxowBoFatWvzyyy988MEHjBkzxmmGk6sqXHCopMigt0RFRbFx40ZWrVrF8uXLGTZsGJMmTSIpKYkLFy4QHh7Ohg0bSE5OZvTo0axy0P9z1apVdO7cmV27djFmzBhiY2NZvHgx27ZtK5xelp2dzRUF9TmWL1/OlClTOHfuHBkZGbRv374wODTMxai9lV0UHBxMgwYN6N27Nz/99BM1a9akR48exBV8qg0fPpyUlBQNDilVFfzznzB+fPm2ERUF/fp5ZDhKKaV8JzNTOpxXVHFxMsvJYxwEh46s3UMja2dOpKVJvCXQGIYMe+tWuZ6WBl26INlPP/wg0SN3O4hawSGPF3tSVU2gHce/9NJLxMbGkp2dzf3338/kyZMLkzPsFZ9W9v7779OvXz/qFryZpqSksGDBAgCuvfZaTpw4walTpwAYMGAAYWFhhIWFUb9+fY4cOeLW+Hfu3EnLli1pXTD/d+TIkUyfPr0wODR8+PDC8yeeeMLNZ+diFS445C/BwcH06dOHPn360LFjR2bPnk1SUhJNmjRhSEE3nsGDBxfW/SnOelHt2bOHyy+/nNtvvx3TNOnXrx9z584tsm5WVhYPP/wwGzZsoGnTpowfP56srKzC26tXr17ux2MU+2Aofl0pVQlt2SI1BwYNKl9qeOvWEBHhuXEppZTyupwcOH3a1s6+IoqPlzo6+fm2mEW5WNPKgBByyCWU4+tSJTjkpBXZ2bPSyj5QMoeKsw8OpaYWBIfi4uSPf/y4zCV0h3WMoJlDqoJydhzfsGFDAMLCwhg1ahRTp051eZuuHo/bZyIFBweTW1j13jPsj+E9cTyvwSEX7Ny5k6CgoMLpYps3b6Z5QQW6QYMGsXz5clq2bMnKlSsLo3rOtGzZkrFjxzJ58mSmTZvGX/7yF3bv3k1CQgJnz57lwIEDhXWLoqOjOXPmDJ999lmZsnp69erFW2+9xciRI8nIyOCHH37g5ZdfZseOHaxfv549e/bQvHlz5s+fz/333+/29pVSfnT33bB0qXv3OXNGjgreeQfq1fPOuJRSSgWMBQtg/Xq5bP3OWJGDQ3Fx0lxszJiiv1GEhMgssCZN3NygXebQa4zhDFGcnrTZtjOk2POSJRffJZCDQ5b//Edmg7dLjScJJJXI3eCQTitTFVhJx/GHDh2iYcOGmKbJokWL6GA/h9UNvXr14uOPP+Yf//gHK1asIDo6mpo1azpdv0aNGpw+fdqlbbdp04b09PTCeMGHH35YZFrc/PnzGTt2LPPnzy+cgVQeGhxywZkzZ3j00Uc5efIkISEhJCQkMHPmTADGjh3LXXfdxauvvkpUVBTvvPNOqdt78MEHmTp1KmfPnuX9999n+PDhXLhwAYCJEyfSunVr7rvvPjp06EBsbCzdu3cv07gHDx7MmjVr6Ny5M4ZhMGXKFGJjY9mxYwfdu3fnkUceKSxIPXjw4DLtQynlBwcOwMcfQ+/e0kHEVYYhxTU1MKSUUlXC/fdL3RmrnnKtWgWZJBXUlVdKcOvtt4suz8qSpmLPP+/mBu2CQ6N5Ty6cgT9ad6NWQeHap56SxNtq1Wx3i46GHj3K8AB8oF8/+PFHKVb9449yapkVJ8Gh1FS47DL3NqjTylQFVtJx/F133cWxY8cwTZPExET+85//lGkf48ePZ/To0XTq1InIyEhmz55d4vr16tWjZ8+edOjQgRtvvLHEgtTh4eHMmjWL2267rbAg9YMPPlh4e2ZmJp06dSIsLOyi2UhlYZgB9o/erVs3s3ix5e3bt9POnQMgVaIVK1YwdepUtwpW6d9AqQAyfbq0bNm2zb3gkFIBxDCMjaZpdvP3OJSNo+9gquKy6gu9/LIEOCqzZs3gmmuglGOyi02YAM8/z9H92TRoEsrYsTBpEsyYAQ8+KPGQWrVg1Ch4/XWvDN0nevc4z8qfIuFf/4Jnn3Xvzp9/DrfeCgMHwhdfeGeAqtLSY0jvadGiBRs2bCixA5uj57+k71/au1wppSqahQuhbVsNDCmllHIqLU3OA3X6kyfFxZWxGVdGBkRFcTZbUqtat5b29Na2jh+XUj0V/TlsnBDB4eBGtheFO3RamVJVhk4rq4KsglxKqQBy5gzMnSsVQ0uSlwcrVsAzz/hkWEqpwGYYRjjwAxCGfK/7zDTN5w3DuBaYClQDNgJ/Nk3Ts5UwVUCz4gBO6ipXKvHx8PXXZbhjQXrVmTNyNSoKWra0PXeV5TmMj4fdeXHU353qfmaABoeUCkjp6eke32aFCQ6Zpqkdtfwk0KYeKlUpTZ8OY8e6tm5wMNxxh3fHo5SqKC4A15qmecYwjFAgxTCMb4HZwHWmaf5mGMYEYCTwrj8HqnzLCmy0bOnfcfhCXBwcOgTnzkFkpBt3zMyEOnU4e1auRkVJIMXKHLLOK3rmUFwc7Caey3ctcz84pN3KVDnpcbx/lOUYvkJMKwsPD+fEiRMapPAD0zQ5ceIE4eHh/h6KUpVbcjJceikcOVL6KSMDOnb094iVUgHAFAV5D4QWnPKAbNM0fytYvhT4kz/Gp/xj1Sr5vaFOHSihaY44fRoSEmRFd0/XXQdt2sjlm27yyWNzxMrsqV/f8TAMaO3BAAAgAElEQVTr1Mjll+DOnDJqcjbE7oYvvyySOVS9umzrf/+Tm0ePluUtWvjlYXlMfDykEUfQ4QPE1MiiQQN5jPb69rU9LSNH2t2Qlyfnlew47Nw56N5d/ldc9csv0KmTTDdUrtHjeP8o6zF8hcgcatKkCfv37+fYsWP+HkqVFB4eThO3e4MqpVy2b5/0Gn7pJflmq5RSbjAMIxiZOpYATAfWAyGGYXQzTXMDMBRo6uS+9wP3AzRr1sw3A1Zet3KlnL/2mgsr//qrpMj86U9S2dlV69bB99/L5caN4dtvITu7aFsvH7npJvj73+H8ece31zv5Ox1nbWFtvZtYfaIND98NhcdMt9zC2T/kYlQU/OUv0t3NSpRp29bNbKQAdMUVcHxwHEELTR4ZmM74eW1ZvRo6d5bbz52DZcvg6qslVvjVV3Z3toJDlSxzaPt22LABvvsOevVy7T7Ll0uAaNMm6N/fu+OrLPQ43n/KcgxfIYJDoaGhtKwKObFKqapp0SI5HzLEv+NQSlVIpmnmAYmGYdQGFgLtgTuAVw3DCAOWINlEju47E5gJ0q3MNyNW3paaCo0awYgRLq4M0snKnUYHs2bB6tVy+bbbJBK1d69kIflYzZrwwgslrLAkFWZBzhPP8OSzV3P1aOhm16vnzMdyXr26FKWeOtWrw/W50FAY8td4WAj/uDOVSYvaFingbU1BfOgh+RM+8wycOlWQdVZJM4eKTx105z5lqetdVelxfMXi0eBQwS9XG4ADpmkONAyjJTAPqIf8onWPaZrZntynUkr53b/+BdOmlf3+p09D+/byjVQppcrINM2ThmEsB24wTXMq0AvAMIz+gL7BVCFpaW7UybGOdN2dO2W/g759JTiUluaX4FCpCh5jdI+4wqv2wSH7mkOVVsHfKyg9rUjRbSja2S4kxLYsMZFKmzlUvOi4t+6jVEXi6cyhx4HtgDW7eTLwqmma8wzD+A/wZ2CGh/eplFL+k5cHb7wBDRu6npfsyJ+0HIhSyn2GYcQAOQWBoQigHzDZMIz6pmkeLcgcegYoKa9CVTKpqRKvcUlamkwLi4hwbydWoZ+YmIIoAoF71JyWBmFhNL2sUeFVe/Y1hyqt+vXlAaamEhfnODgUHy9ZRtYyDQ6V/z5KVSQeCw4ZhtEEGIB8+fg/Q0qSXwvcWbDKbGA8GhxSSlUmq1fD0aPw73/D7bf7ezRKqaqnITC7IHs7CPjENM3FhmG8bBjGwIJlM0zT/N6vo1Q+k5UFBw64kDm0ZAkcOwY//VS2dlyNGkl9obg4+YEkLEz6ydeo4fo2QkJg4EAJWqxdK0Vwigep1q2TJgyuFP7ZsQM2brx4eUoKtGxJVM0g6teX+jpN7apwrV0r55U6OGQYWFGhuDhYsQI+LphOt2SJTCGrW9eWObRwodRwarE6n57AyYx8avtr7GWwdKl8PXNm3To5P3IEZs+2Pe6SWEGhn3+2PXcAl18uyy5cuPg+tWrBgAG2pm9KBTJPZg69BjwNWJ8I9YCTpmnmFlzfDzR2dEcthqiUqrCSk+UL8Y03+nskSqkqyDTNLUAXB8v/CvzV9yNS/rZtm5yXOFP511/h+utt1x95xP0dBQXJ3KwuXeRy587w+edycscLL8Cdd8KVV8LEiVJZ2rJ3r1RTnjABnn229G0NGgQ7dzq+bdgwQLJhliyRQsT2YmNtWTOVVuvWsHEjiYNkKt3dd9tu6tVLAhi1askMw48+ktPd5NET+HVrPg3dma7oR9u3u1YwOiEBdu+GpCTXt23dx/65i4hwXgwd4IcfypdcrpSveCQ4VPDL1FHTNDcahtHH3ftrMUSlVED47DM4dMi9+3zyiXzBdueXUqWUUspLrKBH794lrPTtt3K+ejVER0NZC8YuWwbBwXL5++/h4EH37j90qERqYmKk4PG33xYNDi1dalteWnDo998lMPTcc0WP3C3NmwPSA2L//otvjolxb+gVUu/esGABo3qncU1aHLm5tpsa2/2E/7//SUYNQI3kPBgLBiZLl8IDD/h2yGWxZImcr1oFDRo4X69lS3nJOsr4cSQ0VDLO9uyx1ed++WV4+22ZtbdqVdEMoXPnoGtXGY8Gh1RF4KnMoZ7ALYZh3ASEIzWHXgdqG4YRUpA91AQ44KH9KaWUZ/3yi3RbKQtHX0KVUkqpUpimlK0bNapsxZAPH4ZXX5UO8pavvpIeBw0bFiw4cgReeaXoSt98Iz3ar7iiXOO39YNH5mS1auXe/a+/XgpZZ2TI9dWrYcwY2xH29wWzIdeuLbrckd275fz220scR0SE+8OsNPr1A8D4biktS4jy1KxZ0KkMIFpqDoVXy+eNN2TmXnGGASNHSvKYr2zcKJlNjnz7rfyNr7qq9O2UZdKKfc31wYMlONS3r+NsvR49ZJxWXSuQKWyPPlq2fYP8Dd5+W8pA1agB48ZJErtS5eWR4JBpmn8D/gZQkDn0lGmadxmG8SkwFOlYNhL4ryf2p5RSHpecLN9utm+XX1FdFRIiOdhKKaWUm775Bh57TH6fmDnT/fvPnQtTpsgBon3c5Lnn7FaaP9/xSuPGlXncHnP77VLw5fff5Sj7xx9h1qyi6wwaJEGj4ssdueoquOQS74y1MmjTRlKkNmxwPQWooCB1/RiTvXvhvfcuXuX0aSlf9eGHHhxrKV56SeoiOQuq/u1vvhlH794yu3LECMe3JyXBM88Ufd5OnZJxP/982fb59tsS761eXaYHXnWVa9PolCqNp7uVFfcMMM8wjInAz8C7Xt6fUkqVTXIy9OwpX5yUUkopH7Cms1hTeNyVlia/T2RmlpBUk5YmR6J//BF4VXG7dSv7g1fuMwxJe3Gn3VZBcKhJw3z+cDAdD+Daa33fwSstDW64Ab780rf7LS4yUmq6O/PAAxfH4Zo1K9/zlZEh09tWr5Zz7Z6mPCXI0xs0TXOFaZoDCy6nmabZwzTNBNM0bzNN08UZnUop5UO7d8OWLTBkiL9HopRSqgqxOiTZ135xR2qqFAguMeaTlubCSqrKiIuTF46rXGhl7+4my8s0ba/9iqi8z1dmJtSpY2sY6MvnXlVuHg8OKaVUQHr8cenR6uiUmCjrDB7s3zEqpZSqUsobHEpzpXtURT6KVp4XHw/79hWtQVUSKyhUSnDoyBGZ4uQLGRkyNauivqzj4sqfOVSnjjQJbNlSM4eU53h7WplSSvnf55/DtGkwcKDzjixt2kjvVqWUUspHyhMcysuTrkm33lrCSqYpR4433lim8alKKC5OAj2//+5aZW4XMofi4+V8zx7o0MEDYyyFFQypqMGh+HhpjnvunExLc1dmpq0odny8BoeU52hwSClVcVy4AI88AidOuHe/lBTo2BEWLJD8W6WUUiqAlCU4dPCgJH+UeIDcpw9kZVXco2jledZrIS3NveCQ1bvdASs41LWrZLNYDEMKR48ZU3T9G26AFStcH7KzIVXUl7X1fNWpU3S252OPSe340ljTykCeg1Wr5M+jM0dVeWlwSClVcXz9NbzzjvQKdadnZ0ICvPmmBoaUUkoFFCsoVJbgUKnZE3l50v2rdm0YNqxM41OVkBWZcDXdxIXMoa5dYfJkme5k74MPYNmyosGhnBz47ju48ko5lVVMjG+ylLxhwAAYPx7On7ctW7gQli517f6ZmVIVAeTPefq0/G7qTrNdpRzR4JBSquJITpafSrZuhdBQf49GKaWUKpfyBIesIrTWsf5F9u+XA/spU6BevTKNT1VCsbEQHu56FWMXMoeCguDppy9e/ttvsHNn0WX79skmR42SU1VUo8bFbezPnYPZs0vPAMrOlnXtM4dAYn0aHFLlpQWplVIVQ3Y2fPGFFFfQwJBSSqlKICdHzsuaORQcLK2sna4AFXfujfIOd6sYu5A55IxVeNn+rlZMSl+WRcXFSZHt0ionZGbKuRUcsoLD2rFMeYJmDimlAscff0g1Q0c2boSTJ7XdvFJKqUqjPMGh1FRo3ryE30usg3+nqUWqyoqPdz9zqAzBofh4KXl1+LC0XQeNWTrjagaQNXXPCg5ZfVa0KLXyBA0OKaUCxw03wNq1zm+vWRP69fPdeJRSSikvKuu0sr17Yd486NvXbuG5c1K4xGpR/sUX0g6tSROPjFVVInFxsHgxbNkCnTqVvK4VHCpDBNMKeMyYYbu8eLGUgGzc2O3NVWrW8/PBB/Drr5IVeOutUKuWbZ1ff5X/e7AFhyIjoWFDqePUpo38hhqkc4NUGWlwSCkVGNLTJTD0wAMSJHIkIUHmySullFKVQFkzh6z6Ll262C386CP5DLXXtasEiJSy17WrnA8fLhGHklgZQ2UIDnXoIC+/iROLLu/RQwMYxcXFyW+g06fblr34Ivztb7brI0dKIr1hFM286toVvvxSOsCtXAlXX+2zYatKRj8tlFKBYeFCOf/rXzUFXimlVJVQ1syh336TwNCkScUWhofD9u22irb163tknKqSGTECliyBBQsk+FNSpMbKHLIimW5o3BiOHJFuWvb0ZXmxyEjJCDx5Uq5ffjns2mW73TTlXzwpCV5+uejUs+Rk2LABevaU+2hwSJWVBoeUUoEhORk6d9bAkFJKqSqjLJlDpinlYkaMKHZMn5Ym6QQtWnhyiKoyMgy46iqYMwcOHSp5jlc5ppWBtFy32q6rktWqZZtGlpBQtI7QiRMSZOvc+eKaRNWqSTZWSIjWHlLlo8EhpZT3TZkCP/7o/HbTlNvHj/fZkJRSSil/K0twKCNDuhpdVNA3NVWr/CrX2VdAdiU4VIbMIVV2cXHw/fe266UV8g4JkQL12rVMlYcGh5RS3pWRAePGQWxsye0XLr9cfgZVSimlqoiyTCtz2ITMNOWGa67x2NhUJWffA71XL+fraXDIL+Lj4cMPpdtbeLhrzQfj4jRzSJWPBoeUUt71xRfyrTc5Gbp39/dolFJKVRRHjkjB3DNnHN/eqpUUYbbq61RAZckccphBcP/98jxp5pByVbNmcj5qlBSpcfbaKee0MlU2cXES8738cpk2duSILLda1zu7z3vvyRQzR+66Cx5/3PNjVZWH1olXSnlXcjI0bQrduvl7JEopVekYhhFuGMZ6wzD+ZxjGr4Zh/LNg+XWGYWwyDGOzYRgphmEk+Husblu1CpYvh4gIyTy1P2VlSb0U64ipgipL5pA1baTwIDE/H2bPlssDB3psbKqSq1YN/vxnubxsmfP1NHPIL/r2hT/9CRo1kre89u2lS2FkpPP73HMP9O9/8dtldDTs2wfvvuu78auKSTOHlFKet2uXVM3LyYFvv4UHH6zQv+wqpVQAuwBca5rmGcMwQoEUwzC+BmYAt5qmud0wjIeBZ4EkP47TfVaKzBdfSI9ne199BQMGyDqxsb4fm4eUNXMoNhaqVy9YcOiQbOjNNzVzSLnnrbcksFjSXCSrlX1+fumdzZTHxMbCZ5+5d5+ePWHxYse3Pf64BIdMU7+SK+c0OKSU8qyNGy/OEho61D9jUUqpSs40TROw5l2FFpzMgpMVUakFHPT96MopNVV+8i4eGIKi9VKuvNK34/IgKyh04YLr97mo7rSVSqSBIeWu4GDpbldScMjKHAJ5wVar5vVhKc+Lj4ezZ+HYMahf39+jUYFKg0NKKc/65BNpmTBvHoSGypf6q67y96iUUqrSMgwjGNgIJADTTdNcZxjGvcBXhmGcB04Blzu57/3A/QDNrBokgcJqze5Iixby83cFr75qZQ7l5cnl0NDS75OWJiViiiwADQ6psomPL7nFlX1wKCdHg0MVlH1zOg0OKWc0OKSU8hzTlBpD110nE6WVUkp5nWmaeUCiYRi1gYWGYXQAngBuKggU/RV4BbjXwX1nAjMBunXrZnp9sDk5sHWrHJA6ygiyl5oq1VgdCQuDJk0kW3XjRlkWFAQdOrgWYQkQ9mVcsrKcDz0/X562rCwI3ZvKVREnJRwIsGaNPPbmzb0+XlUJxcVJCYCMDKhb9+Lbi2cOqQrJSrZcvtz2PlO9OrRt678xqcCjk0aVUp6zdSvs3g1Dhvh7JEopVeWYpnkSWA7cCHQ2TXNdwU3zgcCYe/XCC9C1a+k/IJw9C3v3lty3uW1bqUfUrZucunaFCRM8O14vsz/WPnvW+XrvvQedO8Pdl/3Gb7Tigbe72R73zJnyPGlGhyqLdu3k/JprHN9uHxxyZ/6jCigtWshbxN//bnvraNcOUlL8PTIVSDRzSClVuo8/lmlipdm7V9L8b73V+2NSSimFYRgxQI5pmicNw4gA+gGTgVqGYbQ2TfO3gmXb/TnOQl9+KecrV0rr9agox+utWiUHpSVNS541CzZtsl0fP14KVf/rXx4brrfZZw4dO+a8tvaXX0rjz+QbviHobZPc9z4gJLq2bQXrAF8pd913HyxZIpWMDx6U9lj27INDWrCmwoqIgHXrpGsZyJ/1ttvkLVOrPyiLBoeUUiXbsgWSkmy9NEsSEiLtEBo08MnQlFJK0RCYXVB3KAj4xDTNxYZh3AcsMAwjH8gERvtzkACcOCFTwHr1kuDPDz/ATTddvN7x4/B//yc/c/fq5Xx7jRvLyfK//8Fzz8Ebbzien9W8OdxwQ/kfhwfZZw4dOQIdO9rdeO4crF3L5u1h1F+0lRevgg6bP4KEBEJG3ePzsapKKjxcAqqLF8N338GIEUVvtw8OHT0qPdVVhZSYKCfLFVdINYjiM1KrV4c77pCv9cePw6JFRV8G9q68Ug4RFi4sus4VV0CnTp5/DMq7NDikVFV36hS8/z5kZzu+/aOPoE4d+XW2Xj2fDk0ppVTJTNPcAnRxsHwhsND3IyrBunVSm27cOGlDv3q14+DQa6/B9u1w880QGen69m++GZ5/Hh591PHthgGZmVCrVtnG7wX2mUNHjhS7cdYseOQR2hDOW2SBNf3jqad8NTxVVXTqJN/1Vq++ODhkta/Pz3fwIlUV2aBB8OST8OCDF9/WoAH06wfTppWcjNmjh7yNjx9fdHm3bvDTTx4drvIBDQ4pVdVNny4TkJ2pVk2mlWlgSCmlVHlYHZESE+Wnamcdkn77TT5zkpPd237nzpKddP78xbd9/TX8+c/SqqfLRbE0v8nJkRrAGRkOjrt37gQggiy+u2o8fT+5XwJcmp2rPC0oCFq1cvw/mZcnqSH792twqJJ54gm46y6J+1mOHpW36F27JDj022/QsiX8+OPF9//73+G//5V1mjaV+D/As8/CggXyW4Bh+OaxKM/Q4JBSVV1yMnTvDt9/7/j2kBBJOVZKKaXKIy1N5ivUry8dkpy1oU9Lk5+dQ8rwNbV2bTkV17WrnKemBlRwKDdXZmyfOePguNvu+Ynq2RkaNvTt4FTVEhcH69dfvDwvT16khw9rcKiScRRrjo2Vr/3W209amsQNHb39dOggkw82biy6TocOUkQ/M9NxAzwVuLRbmVJV2d69sGEDDB0qRUEdnTQwpJRSyhNSU+UA1DCku5az4JC1nie1bCnnzvbpJzk5Uh6pQQMHx912WRx1u3n4+VCquLg4+P33onMdQYJDISES1NXgUKVnGPJSsN5+Sno7tpbv3Fl0Heuys+RQFbg0OKRUVbawoBzF4MH+HYdSSqnKLy3NdtQQFyeVTk+dKrpOZiacPFlyC/uyqFVLpqoFWHAoN9fJcXd+PuzZU3i14ZUtfT84VbXEx0sgyGpnZcnLg+BgJxFMVRlZiZ0nT8qU19KCQ84uB9jbrXKBTitTqiravh2uvlq+hHfsKLmgSimllLeYphwpXH+9XLeCP/37w5o1tsIU1tGEpzOHrH3OnQtr15a83uDBUtjaB6zMofr1paX0p59C7JfvEjv/NVpduADAMaM+MY1q+GQ8qgqz/ueuvx6GDYOJE+W6BoeqnPh4KdN2xRW2647Yv03br2Mt/7//g5decr6fIUOkwaQKHBocUqoq+vhjCQyNGSOtCpRSSilvOnxYCkVbRw3XXCP1h9atkyLS0dGy3JqH4OnMIZAjlblzS15n82YpluHj4NCDD0pwaP58GLfsY+pcOMyqxsP4X0xf2rUxuc4no1FV2mWXwciR8MMP8O67FweHmjaVzrWq0rvnHkkgy8uTcm19+jher0YNKUqdng7XXmtbXr26NKXcutX5Pqy3Wg0OBRYNDilVFSUnQ+/eMHWqv0eilFKqKrCCPlZwqG5dmDMHbr1VsoWs4JCVOdTSC9Oohg2TU0mef176NmdnS7dOL7Omld18s7SDTkuDmNNp7Gx+Pb32fEQvr49AqQIREVJd+IUXpN3UuXMQGWkLDsXFSSur06clKqAqrUsvlW5jrnjhBcfLrdiiM//4B7z4os/eapWLtOaQUlXN9u1yGjLE3yNRSilVVVhBH0dzD+wLU6SmQkyM/w4+4+NlCtzvv/tkd1bmEMjTsXtbNg3z9pHTzAuZU0q5wvoftWpe5edLq3vr/9WuFpZSZRUfLy+tvXv9PRJlTzOHlKos1qyBVatKX8+qtaDTyZRSSvlKWprUFWre3LbMyg6yb2mTluadKWWusm+z44N6fLm5MgXD2nWDC78TTD4hrbU7mfIT+/+B9u1tmUPW/2VqKnTq5L/xqUrB/mWWkODfsSgbDQ4pVRmYpqTKF+8w4cz110Pjxt4dk1JKKWVJTZWaJWFhtmXVq0NsLKxfD7/8Ah06yHo9e/pvnNYB8PLl0uHMXYYBiYkS9cnJkVpLJ086Xb1tJkQHA2ugWw7cyE8A1OyimUPKT6z/ASujz35aGcDKldLMRI/oVTnYv9XWrOl8PestNTzcN+Oq6jQ4pFRlsGGDBIbefhvuvLP09fUdVimllC/t3Vs0a8jSrh18/rmcVq2SzzJ/Zg7FxkLt2jBlipzK4umn4dAhOerZv7/EVd8H+A24EnohpzyCaHi1dhFVflK3rhytWxl9VnCoTh3pWPb66zB9ugQ+69Xz71hVhdWwocTfJ0+WU0n+/nfntY2UZ2lwSKnKIDlZKloOGSLFA5VSSqlAcu6c9Gsv7uOPYdkyaY/z1VdShMIbbexdZRgy/To9vWz3f+wxadGzb58tMPTmm04f0333Sczs2Wfl+s6dYMbUp22HBmXbv1LlZRjyei2eOQSQkiIt9Z59Fn77zdbrXCk3BQVJs8rS3mr/8peSu54pz9LgkFIVnWlKS4FrrpFfe5RSSqlAk5XlOGu1YUOZFp2UBEuXyjJ/BocA2rSRU1l06AC//lo0Y+iOOyTrwoEVYdC9KXB9wa6vL9tulfKo+HjbEbl9cCghAQYPluBQWpoGh1S5uPJW27590Z4Fyrs81q3MMIxwwzDWG4bxP8MwfjUM458Fy68zDGOTYRibDcNIMQxDJ6gqVRa5udC5s7QaLX7atUu7jymllApczoJDIO26mjeXKdLg32ll5RUfL+k/Z8/K9Tp1nAaGwNbKXqmAEhcnXcny84sGh8BWSF6P2JUPxMfLS800/T2SqsGTH0cXgGtN0zxjGEYokGIYxtfADOBW0zS3G4bxMPAskOTB/SpVNaxaBVu2wPDhUtTTXkQE3H23f8allFJKlebChaLFqIuzprGEhUk2UUVVPOuplCwo+1b2SgWM+HjIzoaDB22t7C0REdCoUdEug0p5SVyczEo+ckRKwinv8lhwyDRNEzhTcDW04GQWnKwa5LWAg57ap1JVSnKyfCC//bat761SSilVEZSUOQRyMPrdd3IkEOSxxHavW7YMMjLgttsKFhTPeip2fckS+Ogj2/UTJzRzSAUg+z7jxTOHQF7Xs2fDAw/o1DLlVdZb6L33Fq2eYRhSj6hHD/+Mq7Ly6MeRYRjBwEYgAZhumuY6wzDuBb4yDOM8cAq43MH97gfuB2jWrJknh6RU5ZCfDwsXwg03aGBIKaVUxVNacGjgQAkO3X6778bkARMnSkHVwuDQpZdCly4QFSXTbwYMKLL+yy9LTV8rOapRI+jd26dDVqp09u3sHQWHhg6VjPY33tDgkPKq7t2llf22bUWX79snvyNocMizPBocMk0zD0g0DKM2sNAwjA7AE8BNBYGivwKvAPcWu99MYCZAt27ddEahUiD1CmbNgtOn5afFAwe0rpBSSqmKqbRpZQMHyqmCSU2VmTeF08Pq1oVNm0pcf9AgmDvXd2NUym1Nm0pAyFnm0GOPwaJFWndIeV10NPz888XLe/XSl583eCWR1TTNk4ZhLAduBDqbprmu4Kb5wDfe2KdSlc6TT8Jbb9mux8RUyC/OSimlqrj8fKlfUlLmUAV04YI0JTNN2Lu39DraOTmy3vDhvhmfUmUWGgrNmjnPHAJ5wX/+ue/HphQy8/H77/09isrHk93KYgoyhjAMIwLoB2wHahmG0bpgNWuZUsrerl2wZo3t9PHHEhh64glJxc/KgkOHoHZtf49UKaVUACmhW+yqgk6xmw3DOGgYxiK/DTI7W84rWXAoPd3WQceV2rx798pxdkVuxqaqkPh455lD1u1Hj0qGu1I+Fh8vkyqysvw9ksrFk5lDDYHZBXWHgoBPTNNcbBjGfcACwzDygUxgtAf3qVTFd+AAXHKJ9LO117Il/OtfJafhK6WUquocdos1TbOXtYJhGAuA//pthNa3dy9/nuXmSjYPSLHSyEgJ3pw/L5dLkpfn/kHG9u1FL/fsWfL6Vs2MUhqYKRUY4uJgwQIp7OIoOGS9kLdtgw4d5HK1atp+T/lEXJy8v2/fDq1b25ZHRFSongYBx5PdyrYAXRwsXwgs9NR+lKp0kpPlG+1HH8nEWkv37lp8WimlVIlK6BYLgGEYNYFrgVG+H10BK+rixcyh3Fxo1UqyeSyvvSa1KmbPhldfhTFjHN83Px/atoXdu8u275AQ2baz7ReXkFC2/SjlU61aSc1LkKCPo9sBLrfrNVSnjvwj2beVUsoLrJdf165Fl/fvD99+6/vxVBbaPFMpf0tOhvbt4a67/D0SpZRSFZCjbrF2Nw8ClpmmecrJfb3fMdZK5/Fi5tBPP0lg6E+i1soAACAASURBVM9/hjZtYOZMmDPHlt3z6afOgzc//yzHs6NGQbt27u23aVOoWRN+/dW19Rs3hiZN3NuHUn4xapREPnNzpTtZcYmJ8M47kJEh148cgf/3/2DZMrv2fUp5R/fu8j5/8qRtWUoKfPGFvCQ1Plk2GhxSyp+OHYMffoBx4/w9EqWUUhWUo26xpmluLbh5OPBOCff1fsdYH2QOffedTCWbPBnq1YPMTHjpJbmtTRtYtw6WLnWcAPHZZ3L+0kvQoEHZ9n/TTWW7n1IBq169ktPhDEOisZbcXDlanzcP6teXRiqXXOL9caoqKSgI7ruv6LKePaVG+syZcMUVtuX160vgPy9Pmgg0b+7bsbri118li7VjR/+OQ4NDSnlSXh7cf79UnXTFiRPyTqAt6pVSSpWTXbfYG4CthmFEAz2AwX4dmA+CQz/8AJ07y/EswI032oJDU6fCzTfLdANnunYte2BIKYVkGfXvL3WKkpPl6P3QITkyV8oHuneXjKG//a3o8pAQSWz773/hwQfh4EHbZ0UgOHLEVrYrPd2/wSsNDinlSatXw3vvSdi3Ro3S14+IgKQk+UarlFJKuckwjBggpyAwZHWLnVxw81BgsWma/u3n4oNpZbt2FS0I3asXrF8vH8Vt28q0s5KaKrVt67WhKVV1vPMO/OUvsGkTPPUU/PabBoeUz4SGynv977/blq1fD2PHymfEli3SPHPXrsAKDh06ZLu8Y4cGh5SqPJKTJWf9xx9dCw4ppZRS5eOwW2zBbXcAk/w2MouXM4eys2HfvotbxHfvbrvcrZtXdq2Usle7NlxzjRTWeuopSE2Fq67y96hUFRIXV7QjZIMGEhxKTZUTyLl9HXV/y8y0XU5L8984QINDSnmOaUpwqH9/DQwppZTyCWfdYgtu6+Pb0Tjh5eDQ3r0yQ1tbxCsVIJo3l2ll/j7SVVVey5ZynpZmezkG2svSPjhkBbD8RYNDSrlq2zZbRwZH0tPlG+r48b4akVJKKRX4vDytzPoyXTxzSCnlJ9WqSSs/fx/pqiovIgIaNZKOlFZQKNBellZwqHp1/weuNDiklCt27JBKYWYpjVyqVZOql0oppZQSXs4csr5Ma+aQUgEkLs7/R7rKbXv3vkxYWCNOnPgKaYTpXfXqDSQ29m6v7iMuDhYvhvPn5fq338IddzheNywMXnwRGjf26pCKsIJDl14qzRVGjIB//tOW9eRLGhxSyhWffSaBoYULISrK+XqNGkF0tO/GpZRSSgU6LweHUlPlC33Dhl7ZvFKqLOLj4Ysv/D0K5YacnEzS0p4uuBZMRESC1/dZo0ZXr+9j+HCYNk1KYfXuLcGhzZsvXi8vTzKMrrwSHnjA68MqlJkJwcEwerR02fzwQ+lV9OSTvhuDRYNDSrkiOVneKQYN8vdIlFJKqYrFy9PK0tLkl+GgIK9sXilVFnFx0qP7zJmSf1hVASMra0/h5erVL6F79y1+HI3nPPywnEqTny/T0Hw97SwjA+rUgZEj5VS3rv+mvunHqFKl2bMHfv4Zhgzx90iUUkqpiscH08p0SplSAcb6p9yzp+T1VMA4f942DTA8vOq9qQYFyVQuX8+GzMyU4JDFnzMyNXNIqeLGjYO1a23Xjx6V8wANDuXmnmb37seJi5tCtWo6pU0ppVSAsTKHvBAcMk35hbV3b49vWilVHlaF+LQ06NjRv2NRLsnKsqWrRERUzQr/8fH+Dw7Fx0tegj9ocEgpe4cOSRWyVq2gQQNZVrs2PPaYf6qCueDUqTUcPjyLOnX60aDBcH8PRymllCrKyhzywrSy48dl1opmDikVYKx/yq1b4eqroUYNCPH/oWdeXhb5+ef9PYyAdO7cjsLLERFV8001Lg5SUoq2ly9J7dpgGOXbZ2amTCWzH8PChVIDKTi4fNt2l///Q5UKJIsW2c4vucS/Y3FRdvYRoOg8YaWUUipgZGVJvr4LB4Y7dkD37rB+PbRrV/qmrboMGhxSKsDUrSunZ5+VU+/esGKFX4eUk3OStWubk5d3yq/jCGSGEYZpXiAiopW/h+IXrVrBqVNFgzUlefZZ+Ne/5PLSpVL8eteuoplApcnIgAS72t/x8ZCTA/v3Q/Pmrm/HEzQ4pJS95GRo08a1b6QBIjv7MKDBIaWUUgHqppsgJsaln1fXr5dMoJ9+cu2j2Er/j6+aMyCUCmyffQZbtkgf8ZQUmQda3jSLcjh3bgd5eado1OghIiPb+G0cgaxWravJykqnTp2+/h6KX4wcKb9j5OSUvu7rr8tnluXHH+HECdi2DXr2dG1/eXmwbx8MHWpbdvXV0l3NH3XcNTikKr/8fPmWadU8cCYrC5Yvh6ef9usHl7uszKHz5zU4pJRSKgBdcYWcXGBlArnaqcVar0UL94ellPKya66RU2gofPedlG9o1Mhvw7Fq6jRu/CjVq1ecH4J9rUaNLv4egt/UquVaZzOANWtgwwbbdfvPL1eDQwcOQHZ20ezX1q3l5A8aHFKV35NPwmuvub6+fei2AtDMIaWUUpWFlQnkakHQtDRo2BAiI703JqVUOVlHvmlpfg0OWd24wsNb+G0MqvKIi4MFCyA3V7KN3P38sl83ULJfNTikKrd16yTnb8QIyRMsTe3a0LWr98flQTk5kjl04cJeTDMPw/Bx5TKllFLKQ8oSHAqUL9VKKSfsO5dddZXfhpGVlUa1ao0JDo7w2xhU5REfL4Gh/fsle7U8waFAqZunwSFV8eTnQ2Ii/PKLa+s3aQL//jfUrOndcfmJlTlkmrlcuHCA8PBmfh6RUkop5b5162D1arm8bZs0D7VXvTp06wYrV9qWbd0KAwf6boxKqTKwquo++ywMGuT0O7lpmhw69A45Oce8Mow//kipsl24lOdZAZ2XXoKmTeGwHJLx448Xf345s3y5ZB01beqdMbpLg0Oq4lmzRgJDd9/tWph12LBKGxgCqTkUHt6SrKw9ZGXt0eCQUkqpCunRR+X82mvh++9h3LiL1wkOlgKe9vyYiKCUckW1atChg0RzP/0U/vxnh6udPfsrv/12v1eHEhNzu1e3r6qOjh2lRtHMmXI9OBh69ZKmfI4+v5zp1culZp4+ESDDUMoNycnyITN9eqUO+rgiPz+XnJzj1KnTj6ysPZw/v4fatXv7e1hKKaWU21JT5ZjxnXekU4xp2m47dkwSgfPyYPRomDFDlhuG1LpVSgW4TZvk+3sJ1eatgtFduvxIjRrdvDKMoKBqXtmuqnqio6U7mfWDRVCQBHmys93bTiB9hmlwSFUspinBoX79qnxgCChIuzWpUaM7R4/O1aLUSimlKqSTJyEjA9oUdJcu/mW5USNp63vmDLRqJceYSqkKJDRUirSUEByyCkZHRrbRII6qEIKD5WSvIn8+aXCogjtx4ksiIloTGdnK30PxrLw8eOMNyMwsuvyPPyA9Hf7xD78MK9BYbezDw5sRFta4QgWHTNPkyJEPqFfvFkJD6/h7OMpLjhyZR50611KtWn1/D0UpFcBK69hiGDKTfMuWwCncqZRyU3x8idV6s7LSCA6uSUhIXR8OSill0eBQBZaTk8nWrYOoW/cGOnb8wt/D8azvvoMxYxzfFhMDt97q2/EEKKsYdbVqDYiMvITTp3/y84hcd+rUGnbsSKJZs3HExU3093CUF1y4cJDt24fTuPGjtGo1zd/DUUoFMFc6tljBIe1OplQFFRcHPzn/rnr+fCoREfEYhuHDQSmlLBocqsBOnFiMaeaSkbGU3NzThITU8PeQPCc5WfLHjx2D8HB/jyZgWW3sq1WLJTr6FnbteoSzZ7dTvXo7P4+sdMeOJQNw/PhCDQ5VUmfPbgPkb5yQ8Lp+2VPKCwzDCAd+AMKQ73Wfmab5vCH/cBOB24A8YIZpmgEbpd29W85btnS+jhUU0swhVRXs2fM8hw+/5+9heNbg03DNH7AgBAygXr0ic3Cys49Qr94t/hufUlWcBocqsOPHkzGMapjmBTIyvqZ+/UpSfT8vDxYtggEDNDBUCitzKDS0AdHRg9i16xGOH18Y8MEh0zQLX7/nzm3j7NkdVK/e1t/DUh527tx2AC5c2M/p0xuoWbO7n0ekVKV0AbjWNM0zhmGEAimGYXwNtAOaAm1N08w3DCOg53ampEBCgnR+ceb++6FFC6ijM5FVJWea+Rw8+CahoQ2oWfMyfw/Hc6qdgf1bID9Pag+1jYHLij6+hg1H+2lwSikNDvmQad92o5zy8s6SkfENDRvey7Fjn3Hs2AJiYm7z2PYtXvulv6TnIiUFjh6FIUO8s+9KwjRNsrOPEBQUSUhIFCEhUdSseTnHji2gWbO/+Xt4JTpzZjNZWXto0WI86enjOX48mcjIwB5zcZoFU7pz57YTFFSd/Pwsjh1bUKTziGEYRd4TrefT3ffJst5POaav64rHlBf/mYKroQUnE3gIuNM0zfyC9Y76Z4SlO3UKvvoKHnyw5PVat5aTUv6Qm/sH+flZPtnX2bPbyck5Tnz8K8TG3uOTffrMpQXnffvCF4dh2YuSPaRRX6X8ToNDPrJ//zR2737c49uNiRmKaeZy6NBMVq78xOPbb9bsb8TFvejx7TJoEHz+ufPbw8Lgxhs9v99KIifnBOvXtyMn5xjh4bbiC9HRfyIt7a+sXBnkx9G5KohGjR7mxImv2bNnHHv2jPP3gNxg0Lr1DBo1egCA1NSnOXduBx07lvCariT273+do0c/oUuXlFIDCefObScqqiPBwVHs2zeZffsmAxATM4z27eexZUt/MjO/IzS0Pj16bOfo0Xns2vUXN0Zj0Lr1f4iOHsJPP7UjJ+d4OR6Zkr/DDi0QXwEZhhEMbAQSgOmmaa4zDCMeGGYYxmDgGPCYaZq7/DlOR3JzoUED+c2of39/j0Ypx06f3szGjV2RuKvv1KnT16f786n+/eGZZyA2Vq5//TXccIN/x6RUFafBIR85dOgdIiPbUr/+HR7bZkhIPWrX7k1ERAJhYU35/+3deXhb1ZnH8e/xKtnZ7NiJs++QhSWACwlQmLBT9kCB0oVSKIW2A6WdlrYwZUqnCwWGAm1paaHQAqUhKKxD2SYQKBBIIAvECSQOCQmJlzhOYlvypjN/HCm2E9uRHFlXtn6f59Ej6erq3tfHV9LVq3Pe40oKJM62bc+yZcv9TJjwM9x5Z4Js2QJPP+2GjX2mi2EmM2fCwH5UQynBqqufpLm5itGjr6OwsC2JNnLk17G2BWsbPYwuNnl508nJKeaAA+5h27a+lVSpqHiYLVvuY+TIbxAON7Nly59padlOKLQRn2+s1+H1qpqaf7Jz5xvU1b3LwIFHdLtufX0ZQ4d+jjFjvkdV1XwAdux4g6qq+ezatZTt219iyJB/o7b2FbZte5otW/6M338Aw4dfElMsFRUPsWXLfUAGzc3VjB79XbKyBu3vn5iWmpu3s3nznWzb9jQlJV/xOhyJk7W2FZhpjBkCLDDGHISrQRSy1pYaY+YC9wOf3fO5xpgrgSsBxo5N/vvXpk0QCsGJJ8JZZyV99yIxqat7F7BMmPALsrKGJGWfPt84cnNHJGVfnrj6aigshMZG+Pa3YelSJYdEPKbkUBI0NHxEff1KJk/+DaNHJ773kM83hvHjb0z4dvPyprJq1cXs2PEmQ4Ycm7gNP/mk+4nwlltgxozEbTeNVFUF8PnGM2nS7R16b2RlDWbcuB96GFn8Bg48jIEDD/M6jLhkZPgoL/8hodBGGho+pKVlO+AKL/fGazyV1Ne7OkJVVYFuk0PNzdtpbq4gL28a+fkzyM93r/WdO5ewffsLrFnzdQAOPPDPLFs2h02b7qSu7j0mTbqNMWO+F1MsxuSyfv2PsLYJn28ikybdpmFRPeTqgD1OVVVAyaE+zFpba4xZCJwGbAICkYcWAH/p4jn3AvcClJaWJn185rp17vqGGyAzgb9DiSRSMFgOZDJmzPfJyNDXp4QYOBCuuMLd/vnPu53iXkSSoy+MPenzqqsXAFBUdJ7HkcSnsPBzGJNDdXVg3yvHIxBwRQOmT0/sdtNES8tOtm9/kaKiufoi7JHoa7m6+gmqqwNkZOTh9x+4ewa2/qq1tZ7Gxg0A+3xfiBajzsvrWBx94MAjyM0dQ13dewwYMBO/fxJFRXOpq3sPiO99srjY1SWrq1tGcbFeD/vDGENR0Vy2b3+elpa6fT9BUoYxpjjSYwhjjB84GVgNPAHMiax2PPChNxF2L/p9UNPTSyoLhdbh841TYqi3TJzYlikWEc/oHS7BWlrq2Lz5rg4F6yor/87AgaV9brhJVtZACgtPobLy72RmDoh/A8uXw/bteyy0MPZfcNFs+PimDo/4fBMYMeKyngfcj9TWLmL79pc6fSwUWo+1Tbu/GEvy5eUdQH7+QXz66T00N29j6NDPkZc3gw0bbqa8/Ia4hmFmZuYzatS1ZGbGPzNfc/M2qqufoqTkq0lJjDQ0rAFgyJA51NYuZO3a71FYeBqFhSd3sq5LDu05c140CbF5850UFZ0PQHHx+WzefGckWRT7HNV5eQeQlzeDhoYPKCrS62F/FRfPZfPmuyKzXyZ+ggPpNSOAByN1hzKAedbaZ4wxrwMPG2OuwxWsvsLLILuybh1kZ8OoUV5HItK1YLAcv18ZzF4zaRIsXOh1FCJpT8mhBNu69S+Rwrrtv6hlcMABv/MqpP1SUvI1amqeZ8OG/47/yYMsdFb+YxyQ8SZseLPdQteTffDgz5KXN7knofYb1lrKyr4S6aHR+Rf+/PyDGTRodnIDkw5GjPg6a9dehzFZlJRcht8/mU2b/oeNG38Zx1bccZ+TU0JJyaVxx/DJJ7exceOvyMubyuDBvX88RBM+48bdSH39SjZt+h8qKv7G0Udv2SshtmPHG2RmDsLnG7/XdkpKLqW6OsDw4V8AYPDgoxk4sJSSksvjjmnkyKuoqHiof03165HBg48lO7uY6uqAkkN9iLV2BbDX2FxrbS1wRvIjik95uZueXkPKnObmWrZvf55hwy7yOpSU1NoaorLyEUpKvsrWrQ8QCm0kP38Gra11hEIbem2/DQ1lDBsWWz086YGJE+Fvf4ObbgJjYNgwV5NIPYJFkkrJoQRzU3JP58gjP/A6lIQoLj6P449viv+Jt94KP/gBrF/vzvr2IRTawFtvjae6egFjx34//v31I3V179HYuIEDD7yPESO+5nU40oXRo69h9OhrOiz77Gd3xrUNa8O89dY4qqoWxJ0cstZSVfU44IauJiM55OoNZTJ48LEcc0wVlZXzWLXqInbs+BdDhhy3e71wuIXq6icZOvSsTntRDRx4GLNnb9x935hMjjjinR7FNHr0txk9+ts9eq50ZEwmRUXnUFn5KK2toR71ZhOJ16efwpgxXkeROtasuZzq6gADBswkL+9Ar8NJOdu2PcmaNZeTlTWENWuiPyhkAOFe3nNGh885SbBjjoGsLLj55rZlJ54IB+o1IJJMSg4lUFNTFbW1ixg37sdeh+K9QAAOPzymxBC4GRkGDDiC6upA2ieHXN2aDIYOPdvrUKSXGZNBUdF5bNnyJ1pa6sjKin34ZkPDKoLBjzAmh6qqABMn3tLrQ8saGsrw+yeRkZEDQGHh6RiTS1VVoMNJ844di2hp2aahj31QUdFctmz5M7W1LzN0aMp3OpF+oK4Oioq8jiJ1NDZuAqClpdbjSFJTMLgWgJqa5wFXpy5a2/Pgg5/R+1ZfddJJ0BT5MfrNN+Hoo92YUyWHRJJKBakTIBxuifxS/gQQ7t+1L1pa9n3ZuBHeegvmxtcOxcVz2bnzLUKhDbvbNB0v1dUBhgw5npwcnS2ng6KiuYTDIWpqno3rOHFTwxvGjfsxodA66uqW9fqx2dBQ1qHAdLQuWXV1gHC4uV1sj5OR4aew8FTvGlZ6pKDgBDIzB1FVNZ9wuAVre/vXeEl3dXWQn+91FKkjIyMXgNbWoMeRpKZg0BUtjtZlLChoq3nn86kmUL8wMVJ7UAWqRZIuYT2HjDE+YBGQG9nufGvtTcb9lP3fwOeBVuAea+1didqv19au/S6bNt2x+77PN54BA2Z6GFEv+sUv4MYb3TT0sTgvvtnZiormsn79Dbz11vj4Y+tnRo78ptchSJK4Oi9FrFp1cdzPHTToaEaOvIqPP/4pS5ce3gvR7a2o6Nw97s9l27anWbQoZ4/l55GZqW98fU1GRi5Dh57J1q0PsHXrA0yZ8jtGjdL7kfSe+noY0IM5L/qraHJIPYc656aUh1CoHDAUFJwQecR0WuNO+qBhw1zGWFPbiyRdIoeVNQInWGvrjDHZwOvGmOeAacAYYKq1NmyMGZbAfXqqpuYlNm26g6KiuQwY4GpBFhTM6Z/TKb/3HvzkJ3DyyfDZz+57/TFj4p6qPj9/KlOnPkgotHHfK/djGRm5mrUtjWRkZDF9+j/YseONuJ9bVHQWOTnDmT59Hg0Nq3shuo6MydyrNtLw4V+gpaWG1taGdusZiosv7PV4pHdMmPDf5OfPwNowAwce6XU40s+p51BHGRmu1ldLy56zvQpEk0JObu5o/P7JGJNFTk6J6qT1F8a43kNKDokkXcKSQ9Zai5sqFSA7crHA1cAlNtI33Vpbmah9euHDD6+mrm4lAA0Nq/H7pzBt2kNkZvo9jqwHmpvhi1901SD3pbzcFQV49FEoKOi1kEpKvtJr2xZJVQUFJ7T79TN2q1a52u9/+MMFDPMo7Z6RkcuYMd+Nef0//hFCIbj22rZl1sJVV8EHcdTxHzUKHnoIXn4Znn8e7rhj389Jtocfhnvuabt/4YVwzTVw/fXwr395F9e+TQBc7bzvfjfuEcIiMbNWPYf2lGrJoZ073+GDD84nHG70OhQAmpvbvkb4fBMxJhOfbzw5OSM9jEoSbuJEePZZGD7c3T/pJPehGvXuu/Dtb7sTgIEDvYlRpB9KaEFq46alWQpMBn5nrV1sjJkEXGSMOQ+oAq6x1n60x/OuBK4EGDt2bCJDSqj6+jI+/fQP5OcfQnZ2MYMGHcWECTf3zcQQwMKF8NhjcOSR+35jPfhg+NGPejUxJCLxefppeOAB9+X9rLO8jmbfWlrghhugsRG+8Q3wRX7kXbUK7r0XDj00tsK0u3bBvHlwxRVutOtrr7ntTZ3au/HHw1r46U9dr4jp02HtWnd/7lyX0Js6FUb2ge8yWZq2QnpRMOheK+o51J6b4TFVkkO1ta/Q2PgJI0Z8vdPZJ5PNmCwKCk6ipuYFiovPB2Dy5N+QmakEQb9y/fUwYoS7/fbbbqKbcBgyIuVyX3jBFa5eudIVrxaRhEjoaZ+1thWYaYwZAiwwxhyEq0EUstaWGmPmAvcDn93jefcC9wKUlpbGWNAm+dpmQ3gWn2+0x9EkQCDgzsheeQX8fTTBJZLGNm9214FA30gOvfYabNvmbr/0Epx5prsdCLhe5M8913Yu2J1g0CWR7rkHXn/dLVuwwOWvU8WqVfDRR/D738PVV7v45s6F665zX4bnzYODDvI6ShFv1de7a/UcahMOhwBobq7xOBInFConK6uQAw+81+tQOigqOmf3bc1Q1g/Nnu0u4D5Iv/Ut2Lq17VeV6JCz8nIlh0QSqFdmK7PW1gILgdOATUAg8tAC4JDe2GcyVFUFGDjwqP6RGGpthSeegDPOUGJIpI/a5GY85qmn3CjRVBcIuN5Cgwa52+2XH310bIkhcG9Zn/ucS7hYCyUlHbeXCqIJr3Mj9btPPRXy8mD+fJgyBWbM8DY+kVRQFylGoJ5DbcJhV78tVXoOBYPl+P0TvQ5D0llns5dFb6sukUhCJXK2smKg2Vpba4zxAycDtwBPAHOA9cDxwIeJ2mcyhUIbqKtbysSJt3gdSuwqK2H58s4fW7sWKipUTEKkD9u82SVbampg0SI48cTk7XvpUrffadNgdCf58mAQ3njD9QKPWrAATjvNfRF88knXK7y2FpYtg9tvj2//c+e6RMukSW542Y9+5O4PHrx/f1ei/OMf7kfPaMIrLw9OPx0ef9zF3h/nLRCJl3oO7S1a3D9VkkOhUDkDBiRnNkyRTkWTQ+XlbZPiRJNCmu5eJKESOaxsBPBgpO5QBjDPWvuMMeZ14GFjzHW4gtVXJHCfSVNV5YaUFRX1oWTKhRfCq692/Xh+vvv5XUT6pM2b4ZxzXM+hQCB5yaGVK6G01N2eNs0Vkt4z2XHzzfCrX+393FtvdW89Dz/setMAZGbGn6c+4wz3hfLii+GCC1wto89/Pv6/pTf95jcd7198sUsOXaiJ3EQA9RzqTLTnUHPz/iWHwuFGampexNqm/diKJRT6mOLiFHtzlfQyfrw7yfjnP12NVGthY2Rm40cegdtug+JiT0MU6S8SOVvZCuCwTpbXAn1+MHB1dYD8/EPIy5vsdSix2brVdSW4+mo3I1lnRo5UhX+RPqqlBbZscUOUTj/d9cq5++62Wo296bHH3H6uucYlQMrKXNHlKGvdOscd5wpGR+XmwuGHu3O8pUtd7yJw9YPGj48vhkGDYPVqdz6Yk+MSVNF6RqkgKwuOOKLjsvPPh/Xr4/9bRfor9RzaWzjs3hibm6v2azsVFY+wZs3XEhES+fkHJ2Q7Ij2Sk+N+iXr0UXeJGjcONmyA73/fzc4hIvtN85DEoKmpgh07Xmf8+Ju8DiV2Tz7pvqF985uqeirSD1VUuCFbo0e7ma8CAVi8uK1+Y28KBFzP7h/8AO68091vnxxaudL19L7+ejjmmM63cXgCRimMGtV2O5VmKuuKMUoMibQXTQ6p51Cb6LCyxsaNhMMtZGT07FS9oWE1xuRwxBFvAz0fx2pMDnl5B/b4+SIJ8frr8MknbfdzcmDCBJc0Wr3au7hE+hklh2JQXf0kYPvWkLJAQFVPRfqx6Exlo0a5RE12tnvZ93ZyaM0a10vnzjtdPZ3Zs91+b7yxbZ1oMeZzzul6OyIiGla2t3C4AWNysbaRxsZN+P3je7SdUKgcn28CaH93yAAAIABJREFUAwYcmtgARbxQUOAuezr5ZNd1WkQSQsmhboRCm1i9+ivU13+A3z+Z/HwPe+D86U9w//2xr//OO/Af/6Gqp2lg+3a46ir3Zb2kxOtoJFmiM5WNGuWKMJ90Evzxj21Tu/eW6mp3fd557nruXPdWM2tW29vN6tUuYTVsWO/GIiJ9m4aV7a21tYH8/IOoq1tKKLSux8mhYHCdZhmT/m/iRKiqgl27VCpDJAGUHOpGRcWD1NYupKDgFEpKLsN4lWgJh+GnP3VFPqZNi+05p58OX/9678YlKeGVV2DePFdb/NJLvY5GkiXacyg6U9j117uRpO1nB+sNgwa5AtBjxrj7X/6yS0g1NLStc9RRcN11vRuHiPR96jnUkbWWcLiBAQMOpq5uKcFgOQUF8c80YK0lGFzH4MFdjOsV6S8mTXLX5eVwqHrJiewvJYe6UVUVYNCg2Rx66PPeBrJkifsm+Ne/um9iIu2sWtXxWtLD5s1uyH1Rkbt//PHukmzDhqlHt4jEr7ISvvtdd1vJISccbgTA75+CMdls3Pgrqqrmxb0da8O0tu7E51PPIennotPcX3YZDB3atnz2bDdtqojEJQnz2vRNweDH1NW9mxp1hh5/3E19c9ZZXkciKaisrOO1pIdNm9yEgxo5KiJ90WOPueu5c13NNGmbxj4zcwCjRl1DTk4Jra0NcV/C4RBDhsyhsPB0j/8ikV42Ywacey74/a4Lc0OD60X085/Djh1eRyfS56jnUCd27lxMRcUjABQXn9e7O9u0Cd5/v/t15s+HE0+EIUN6Nxbpk/pacmjtWpfUyMvzOpL4rFsHH33kdRRtPvig42xdIiJ9yYsvutn75s/3OpLUEZ2pLCMjj8mTb/M4GpE+IDd37+7Lixa5rtQLF7rEkYjETMmhPeza9S7vvjsLgAEDjsDvn9S7OzzzTFi+fN/r3XBD78YhfVI47Ir/ZmS4H0pCIfD5vI6qa7W1cMgh8K1vwa23eh1N7FpbXYHlLVu8jqSjyy7zOgIR8ZoxxgcsAnJx53XzrbU3GWMeAI4Hoj+ff9Vau8ybKDsKh933tosuSn7vx8bGLaxYcRozZjxOXt7k3cuXLZtDXZ23zWNtKwCZmRpnJ9Jjs2a5sapf+ELbSfGPfwzf/763cYn0AUoO7aGqaj6QyaGHvkh+/sG9u7MPP3SJoeuv7z6znZMDM2f2bizSJ33yietBe/zx8OqrrmfLwb182O6PZ56BYNAV0P71r/vOkKh//cslhm69FY491uto2qTy/1pEkqYROMFaW2eMyQZeN8Y8F3ns+9balOub8+mnsHMnHHFE8ve9c+db1NevYMeORbuTQy0tO6itfYXBg4/3fOr3jAwfhYWnehqDSJ+WkwN//jO8+aa7HwjAc88pOSQSAyWH2rHWUlX1OAUFcygomNP7O4x2g/zWt9qm/hGJQ3Qo2dy5LjlUVpbaCYNAwF1v3AjvvuvNF4OeCARcz+WrrtKUyyKSWqy1FojM+0V25GK9i2jfysvd9UQP6iWHQm7nwWD57mXR26NH/zvFxecnPygRSayLL3YXgG3b3LSqIrJPKkjdTkPDKoLBD5NXhDoQgM98Rokh6bFocujcc10vnFSuO1RfD//8p+vlm5nZlihKdda6WE89VYkhEUlNxphMY8wyoBJ40Vq7OPLQz40xK4wxdxhjcj0MsYN169y1F8mhYNDtPBRat9cyn6+XSwmISPJNmuS62jc1eR2JSMpTz6F2Kiv/ARiKihJUvOzNN92QscbGvR+zFt55B37xi8TsS9LS8uVu5s6xY11hz1jKV/3hD1BRATfdlPh4/vY3WLkSbrkFbr+9bTYacMmhYBCuuMLt/7e/hZdeSnwMidbc7M4pNCOqiKQq64rVzDTGDAEWGGMOAn4EbAVygHuB64G93smMMVcCVwKMHTs2KfGWl7sfCZK0uw6ivYTa9xyK9iby+zX1u0i/M3GiK3S2YQNMmeJ1NCIpTcmhiGBwPZ98cjtDh55Nbu6I/d9gfT188Yvu23BX9YLOOw+++tX935ekpZUr4eGH4ctfdvfPPhvuugsWL4ajjur8Oe++60YxhsNuSNeZZyYuntWrXeKnqckd9r/9rTv0S0rc44WFLq7jjnP3b7vN5Uj7gosuckP3RERSmbW21hizEDjNWhud7qrRGPMX4D+6eM69uOQRpaWlSXlXLi93iaGeTGEfDK5n587F+16xC/X170e28yEVFY8CsH37/5GdXURW1qAeb1dEUtSkSI/Av/7VnfxqBjORLhmbYt/OSktL7ZIlS5K+3+XLT2Pnzn/xmc+swueLc5hXc7OrrrtjR9uylSvdGJpXX237Niyyh/JyeOMN+NKXul5n/nx4+203/fu117rhY+EwHH2065pfVgZFRa6454wZ4Pd3/bn39NNuxrCCArf+JZck7m954QX3o8yECfDee+565Uo3YYSIyJ6MMUuttaVex9HXGWOKgeZIYsgPvADcAiy11m4xxhjgDiBkrf1hd9tK1jnYrFlumG5Peo8uW3YCtbUL92v/Pt+kDsPKAIYMOYGZM1/er+2KSAqqroZRo9yvlyNGuIr4Immsu/Mv9RwCGhu3sn3784wf/7P4E0PgpmC68UZXsTajXRmnG25QYki69Z//CY884g6TzrrX19fDV77iRiaGw26mrNJSl1BavBj+9CeXGAIYNAjuu891WPvtbzvfX14ePPggFBe7jmtdrdcTubnwxz/C9OmurtDddysxJCKSBCOAB40xmbhakvOstc8YY/4vkjgywDLgKi+DbK+83H0G9URDwxqKis5nwoT/7tHzjcnA759EMFi+e+p4AJ/PgzFuItL7iorcTCjbt0OWvvqKdEevEFwhaoBBg2b1bAOBgBszU1GhNx2JWWOjyyuCm7ju2mv3Xuf5590Qrfnz3dCmQMAlhwIBN1PnRRd1XP+UU6CqKrb9b968f/F3Z+XK3tu2iIi0sdauAA7rZPkJHoSzT7t2uc+pnhSjbm0N0tT0KQMGzCQ/f+p+xZGXp9ojImlj+HB3EZFuabYyoKHBTfGUnz8t/ic3NbmxOueco8SQxOXll93QLp+v65m7AgFXcPqcc2DOHHj88bbZs045BQYOTG7MIiIi+yM6jf2kHkwMFgqtB1Q4WkREpDcoOQTU15eRmTmQnJyR8T954UJXa0jVatNaa6sbAhbP5bHH3FCw73wHXnvN1etp/3htbce849y58OGHbljYhg065EREpO+JJod60nMoOsOYz6fkkIiISKKpqwuu51Be3jRczcY4BQKuquJJJyU+MElpZ54Jo0fD738PBx/sCkPH6wtfcJdf/cpNRd+ZaF2Gc891M41ddpmbAviss3ocuoiIiCfWRepA7ys5tHnzPVRWPtphWVPTVgD8/h50OxIREZFuKTmESw4VFJwS/xNbW+GJJ+CMM9zYIEkbmzbBs8+6gssXX+wSQ1/7GkyNowSCMfD5z8O4cfDQQ51PnjBoEJx+urs9YoQbVrZ2rdtPtBC1iIhIX1Fe7so0DhnS/Xqffvp7mpoqyc+fvntZbu5Ihgz5N7Kz9QEoIiKSaGmfHGpp2UFT05ae1Rt64w2orNT4njT0xBPuur4e/v3fITsbbr993ye7XfniF2Nbr6ezu4iIiKSCdev23WvIWkswWM7IkVcyefIdyQlMREQkzaV9zaH6ejcWKC+vB8mhQMDN3x3t2iFpIxCAAw90yaD334cTT+x5YkhERCRdlJfvOznU1FRBONyAz6fhYyIiIsmS9j2HojOVxZwc+stf4IMP3O2//z1lpoyyFu67D049FcaM8Tqa+MybB9Onw4wZcPfdsHGj1xF1z1p49VX48Y9dYei//U2dx0RERPaltRU+/tgNqe5OKOQKT2tWMhERkeRJ++TQjh2vk5k5CJ9vwr5X3rIFLr/cjSHKznZTSF1xRe8HGYP334evfx2+8Q34wx+8jiZ21dVwySUux3bLLXDtta58U2am15F1b+hQNxSsqgqWLtVwLxERkX355BNoadm751BTU3Wkp9BYAGprFwKalUxERCSZ0jo5FA63UF39JEOHnklGRgxN8eSTrtvI0qVw0EG9H2AcAgF3/cQT8LvfpX5yJerpp90viS+95DplGQPr10NJideRxWbq1LaOZCIiItK1zqaxD4ebefPNEVjbwhFHLAUs69ffCIDPNz7pMYqIiKSrtK45tGPHa7S0bKO4OMYxQYEATJnixj+lmEDA9bipqIA33/Q6mthF425uhrvugmOP7TuJIREREYlddBr7Se1KCTU2bsTaFgDq6lZQV7ccgGnTHiEzUzPBioiIJEta9xyqrg6QkeGnsPC07ldsbXVDyhYuhO99z3VvSZBw2M14tT8+/hhWrID/+i/4xS9cDZ9DD01EdL2rvh5eeAGuusrFvHWraveIiIj0V+XlbkT+6NFty4LB8t23Q6FyrA0DmRQXX5D8AEVERNJYmieHnqaw8FQyM/O7X/GEE2DRInc7juzFggUu8VFWBoWFrj7NtGnwpz+11ag591w3tCoRLr0U3nnHFXW+++7EbDMZLrjA9Ry65x7V7hEREemvysth/PiOQ9+jxaeNyYokisL4fGPJyMj2JEYREZF0lbbJodbWII2NGxgxYh8FpcvLXWLooovgjDPgM5+JeR/PPguVlS75c+mlrh7Qtm2uts5557nHnn0Wzj4bjjtu//6e8ePd5c47Yc6c/dtWMg0d6oaSTZ0KZ50F48Z5HZGIiIj0hnXrOg4pAwgG12FMLoMHzyYUWoe1Yfx+TWEvIiKSbGmbHAqFPgbA79/HLGULFrjrX/4SJsQwo1k7ixe768cfd8mhaNHoF16AXbvgqafcsLKbb07cMLBJk9zIt76muBhOP93rKERERKS3lJfDkUe62y0tdWzY8DOqq5/C75+A3z+Fysq/AzBs2CUeRikiIpKe0rYgdSi0HmDfU9gHAnDYYXEnhnbtcrNY+f0uGbRpE7z8MsyeDY2N8NxzbtMTJ8Ihh/T0rxARERFJfXV1sH17Ww/hqqr5fPLJr2lurmDo0LMoKDiFjAxfpBbkKd4GKyIikobSuOdQNDk0vuuVPv0U3ngDfvazuLe/ZImb9f4733Gdjs4/39XVufVWV7bol790yaNrr01ofWsRERGRlFNR4a6HD3fX27e/SHb2MI4+eismciI0bJiKUIuIiHgljXsOfYwxueTkdDNv+n/+p6uaeNFFcW8/OqTsuutcmaKyMpg1y/Uc+uY33bj7ggL48pd7+AeIiIiI9BHR5FBJCVhr2b79JQoKTtqdGBIRERFvpW3PoWBwPT7feIzpIj/28stw//1w/fUwZUqHh3btgldecfWCuvK//wuTJ7taOm+/3fGxm25yFxEREZF00L7nUHNzFc3NlQwaNMvboERERGS3tE0OhULruy9G/etfw9ixnWZxfvIT+M1v9r2Pyy/fjwBFRERE+on2yaFgcB2AZiUTERFJIWmdHBo06KiuV3j/fTjpJFdRuh1rYf58OPlkuOWW7vcxbVoCAhURERHpgjHGBywCcnHndfOttTe1e/wu4GvW2gEehQi0JYeKi6GmphwAn2+ihxGJiIhIewlLDvWVkxOAlpYdtLRs73qmsh07XDHqTrI7S5a4mcd+/nM3iZmIiIiIhxqBE6y1dcaYbOB1Y8xz1tq3jDGlQIHH8QEuOTR0KGRnR3sOme4nBREREZGkSmRB6ujJyaHATOA0Y8wsgFQ6OQFXjBq6mals9Wp33UlyKBCArCw488zeiU1EREQkVtapi9zNjlysMSYTuBX4gWfBtVNR0TZTWTBYTm7uKDIzfd4GJSIiIrslrOeQtdYC3Z2cXAKcl6j97Y9gMDqNfRc9h1atAuDsH05n4x4lh9atgzlzoLCwNyMUERERiU3kXGspMBn4nbV2sTHmWuApa+2WVJgRrKrKDSkDCIXKNaRMREQkxSS05lBPT06MMVcCVwKMHTs2kSF1KhhcA4DfP7nzFcrKaDI5LNsxgcOP7PjQhAluenoRERGRVGCtbQVmGmOGAAuMMccBnwf+bV/PTdY5WDAIw4ZFb6+jsPDUXtuXiIiIxC+hyaGenpxYa+8F7gUoLS21iYypM/X1ZeTkjCA7e0jn8awq40MO4MJLsrjttt6ORkRERGT/WWtrjTELgTm4H+rWRn6YyzPGrLXW7vWrWLLOwUIh8PmgtTVIU9On6jkkIiKSYhJZc2g3a20tsOfJycdETk56Y5/xaGgoIy+v66nEWlaWscpO02xjIiIiktKMMcWRH+UwxviBk4Gl1toSa+14a+14oKGzxFAyNTZCbq6bLRbA71dySEREJJUkLDnUV05OrLXdJ4dCIbI2racMJYdEREQk5Y0AFhpjVgDvAC9aa5/xOKa9RHsOBYNuGnu/f5LHEYmIiEh7iRxWNgJ4MFJ3KAOYl4onJ01Nn9Lauov8/C4yP++9hwmHeZ+DuEbJIREREUlh1toVwGH7WGdAksLpUjQ5FAqtA9CwMhERkRSTyNnK+sTJSX19GUDXPYeeeIIWk8XyopMpKEhiYCIiIiL9VHRYWTBYTmbmALKzi7wOSURERNrplZpDqayhoZvkkLUQCLBk0ImMPqjzYtUiIiIiEp+2nkPl+HyT6GoGWxEREfFGWiaHMjMHk5NTsveD778Pa9fySNP5qjckIiIikgDhMDQ1RWsOrVMxahERkRSUhsmh1eTnT+vwi1VLC5x5JvzupABhDI8Gz1FySERERCQBmprcdW5umFBovYpRi4iIpKC0Sw41Nm4mN3dsh2WLFsGzz8Jp9QHKij7LSV8YxjnneBSgiIiISD8SCrnrgQO3EA6HVIxaREQkBSVytrI+oaWlhuzsoR2WBQIwI3ctk+pXwM9/wyPXehSciIiISD8TTQ4NGBCdxl7JIRERkVSTVj2HrA3T3FxDVlbh7mXhMCxYAD+YssAtOO88j6ITERER6X8aG911Xp5LDvl8GlYmIiKSatKq51Br6y4gTHa2Sw6tXg0vPBWi9NPnOSvrISgthbFju9+IiIiIiMQs2nPI51sHZODz6VxLREQk1aRVcqi5eRsAWVmFfPghzJwJDzV+iSd5HDYC/36rtwGKiIiI9DPRnkM5OeXk5o4hIyPH24BERERkL2mWHKoBXHLoyivhgswFXMDj1F3zYwZc9SU44ACPIxQRERHpX6I9h7Ky1mmmMhERkRSVVsmhlhaXHHr77aEsejXMjsLvwJRDGXDbf0F2trfBiYiIiPRD0eRQRkY5fv/Z3gYjIiIinUqr5FC059DKlYXMNosZWLMR7v6lEkMiIiIivaSxEQoKtmJMJX7/gV6HIyIiIp1Iq9nKoj2HVq0q5LLBAZcUOuMMj6MSERER6b9CITjiiJcBKCiY43E0IiIi0pm0Sg5FC1IvXzaEM5seh5NOgsGDPY5KREREpP9yyaEXMWYoAwYc5nU4IiIi0om0Sg61tNSQmTmQnLIyShrWw9y5XockIiIi0q81NsLYsavJyTkcY9Lq1FNERKTPSKtPaFdzqJBZLa+5Baef7mk8IiIiIv1dKAR+fx1ZWYO8DkVERES6kFbJoZaWGpqaCpnOKloGDoGRI70OSURERKRfC4XA56snOzvf61BERESkC2mVHGpu3kZdXSHTKINp08AYr0MSERER6dcaG13PoezsAV6HIiIiIl1Is+RQDTU1Q5mRUUbWQdO8DkdERESk34v2HMrJUc8hERGRVJVWyaGWlhpqPs2jOFzpeg6JiIiI9GHGGJ8x5m1jzHJjzAfGmJ9Glt8XWbbCGDPfGONZt53GxlZ8viBZWeo5JCIikqrSJjlkraW5uYbQJ61ugZJDIiIi0vc1AidYaw8FZgKnGWNmAddZaw+11h4CbAS+7VWAJ57YAEBWlnoOiYiIpKq0SQ61tu4EWvHVhNwCJYdERESkj7NOXeRuduRirbU7AYwxBvAD1qMQmTXLhZeRoeSQiIhIqkqb5JCbxh6G7txJa44Pxo3zOCIRERGR/WeMyTTGLAMqgRettYsjy/8CbAWmAnd7FV9raz0AmZkaViYiIpKq0iY51NLikkMjdlURnnwAZGZ6HJGIiIjI/rPWtlprZwKjgSONMQdFll8GjATKgIs6e64x5kpjzBJjzJKqqqpeia+11fUcysxUzyEREZFUlTbJoXA4RDA4nMk7N5J12MFehyMiIiKSUNbaWmAhcFq7Za3Ao8D5XTznXmttqbW2tLi4uFfiCofVc0hERCTVpU1yaPDgY7j/5iVMer8ac+SRXocjIiIist+MMcXGmCGR237gZGCNMWZyZJkBzgZWexWjeg6JiIikviyvA0imgasWuxtHHeVtICIiIiKJMQJ40BiTifvRbx7wLPCaMWYQYIDlwNVeBaiaQyIiIqkvbZJDNTVw4M63ac3MJnPmTK/DEREREdlv1toVwGGdPHRMsmPpinoOiYiIpL60GVZWVgZHsZhdk2ZCbq7X4YiIiIikBfUcEhERSX1pkxyaMLaVY3KXkHOshpSJiIiIJEu051BGhnoOiYiIpKq0GVY2snYVNNaTPUfJIREREZFkaes5lOdxJCIiItKVtOk5BMDnPw+zZ3sdhYiIiEjayM+fzvDhX8KY9DrtFBER6UvSpucQBx8M8+Z5HYWIiIhIWhk27EKGDbvQ6zBERESkG/oJR0REREREREQkjSk5JCIiIiIiIiKSxpQcEhERERERERFJY0oOiYiIiIiIiIiksYQlh4wxPmPM28aY5caYD4wxP40sf9gYs8YY874x5n5jTHai9ikiIiIiIiIiIvsnkT2HGoETrLWHAjOB04wxs4CHganAwYAfuCKB+xQRERERERERkf2QsKnsrbUWqIvczY5crLX2f6PrGGPeBkYnap8iIiIiIiIiIrJ/ElpzyBiTaYxZBlQCL1prF7d7LBv4MvDPTp53pTFmiTFmSVVVVSJDEhERERERERGRbiQ0OWStbbXWzsT1DjrSGHNQu4d/Dyyy1r7WyfPutdaWWmtLi4uLExmSiIiIiIiIiIh0w7jRYL2wYWN+AjRYa28zxtwEHAbMtdaG9/G8KmBDrwTlFAHVvbj9vk7t0z21T/fUPt1T+3RP7dO9/tY+46y1+kUohfTyOVh/O357k9oqdmqr+Ki9Yqe2ip3aKnap0FZdnn8lLDlkjCkGmq21tcYYP/ACcAtQAnwNONFaG0zIzvaDMWaJtbbU6zhSldqne2qf7ql9uqf26Z7ap3tqH+nLdPzGTm0VO7VVfNResVNbxU5tFbtUb6uEFaQGRgAPGmMyccPV5llrnzHGtOB+hXrTGAMQsNbenMD9ioiIiIiIiIhIDyVytrIVuKFjey5PZAJKREREREREREQSKKEFqfuIe70OIMWpfbqn9ume2qd7ap/uqX26p/aRvkzHb+zUVrFTW8VH7RU7tVXs1FaxS+m26rWC1CIiIiIiIiIikvrSseeQiIiIiIiIiIhEKDkkIiIiIiIiIpLG0iY5ZIw5zRizxhiz1hjzQ6/jSQXGmI+NMSuNMcuMMUsiywqNMS8aYz6KXBd4HWeyGGPuN8ZUGmPeb7es0/Ywzl2R42mFMeZw7yJPji7a57+MMZsjx9AyY8zn2j32o0j7rDHGnOpN1MljjBljjFlojFlljPnAGHNtZLmOIbptHx1DgDHGZ4x52xizPNI+P40sn2CMWRxph38YY3Iiy3Mj99dGHh/vZfwi3dE5WEc634idPltjp8+R+BljMo0x7xljnoncV1t1wsTxnTGdX4MAxpghxpj5xpjVxpgyY8zsvtRWaZEcMsZkAr8DTgemA18wxkz3NqqUMcdaO9NaWxq5/0PgZWvtFODlyP108QBw2h7LumqP04EpkcuVwD1JitFLD7B3+wDcETmGZlpr/xcg8vq6GJgRec7vI6/D/qwF+J61djowC/hWpB10DDldtQ/oGAJoBE6w1h4KzAROM8bMAm7Btc9kYDtweWT9y4HtkeV3RNYTSTk6B+vUA+h8I1b6bI2dPkfidy1Q1u6+2qprsX5nTOfXIMCdwD+ttVOBQ3HHV59pq7RIDgFHAmutteXW2ibgUeAcj2NKVecAD0ZuPwic62EsSWWtXQTU7LG4q/Y4B/irdd4ChhhjRiQnUm900T5dOQd41FrbaK1dD6zFvQ77LWvtFmvtu5Hbu3AfBqPQMQR02z5dSatjKHIc1EXuZkcuFjgBmB9ZvufxEz2u5gMnGmNMksIViYfOwfag843Y6bM1dvociY8xZjRwBvDnyH2D2ioeeg3uwRgzGDgOuA/AWttkra2lD7VVuiSHRgGftLu/ie6/lKQLC7xgjFlqjLkysmy4tXZL5PZWYLg3oaWMrtpDx1Sbb0e6Qt5v2oYhpnX7RLobHwYsRsfQXvZoH9AxBOzu3r4MqAReBNYBtdbalsgq7dtgd/tEHt8BDE1uxCIxSbvXcg/ps2If9Nm6b/ocictvgB8A4cj9oaituhLPd8Z0fg1OAKqAv0SGK/7ZGJNPH2qrdEkOSeeOtdYejuvS9i1jzHHtH7TWWtybgaD26MI9wCRc9+UtwO3ehuM9Y8wA4HHgO9bane0f0zHUafvoGIqw1rZaa2cCo3G9LaZ6HJKIeECfFXvTZ2ts9DkSG2PMmUCltXap17H0EfrOGJss4HDgHmvtYUA9e5RoSfW2Spfk0GZgTLv7oyPL0pq1dnPkuhJYgPsQqYh2Z4tcV3oXYUroqj10TAHW2orIiUgY+BNtw37Ssn2MMdm4k9eHrbWByGIdQxGdtY+Oob1FuiAvBGbjuhhnRR5q3wa72yfy+GBgW5JDFYlF2r6W46TPii7oszV++hzZp2OAs40xH+OGup6AqxWjtupEnN8Z0/k1uAnYZK2N9oyfj0sW9Zm2Spfk0DvAlEgF+hxckdOnPI7JU8aYfGPMwOht4BTgfVy7XBpZ7VLgSW8iTBldtcdTwFciVeZnATvadRdMG3txaV4KAAABtklEQVSMiz0PdwyBa5+LjZvdYQKu0NrbyY4vmSJjz+8Dyqy1/9PuIR1DdN0+OoYcY0yxMWZI5LYfOBlXW2MhcEFktT2Pn+hxdQHwf5Ffo0RSjc7BYqPPik7oszV2+hyJnbX2R9ba0dba8bj3pP+z1n4RtdVeevCdMW1fg9barcAnxpgDI4tOBFbRl9rKWpsWF+BzwIe4sbc3eB2P1xdgIrA8cvkg2ia48bMvAx8BLwGFXseaxDb5O25YSzMu83t5V+0BGNzsK+uAlUCp1/F71D5/i/z9K3BvcCParX9DpH3WAKd7HX8S2udYXDfRFcCyyOVzOob22T46htzfegjwXqQd3gd+Elk+EZcUWws8BuRGlvsi99dGHp/o9d+giy5dXXQOtld76Hwj9rbSZ2vsbaXPkZ61278Bz6itumyfuL4zpvNrMPL3zwSWRF6HTwAFfamtTCQwERERERERERFJQ+kyrExERERERERERDqh5JCIiIiIiIiISBpTckhEREREREREJI0pOSQiIiIiIiIiksaUHBIRERERERERSWNKDomIiIiIiIiIpDElh0RERERERERE0tj/A9CjcaXZ6BDXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x1080 with 6 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El_x1gwpSrd0",
        "outputId": "8cc31e52-d06b-4f03-b223-6a84ce63d5dc"
      },
      "source": [
        "print(f'Any Nan values in S2? {(S2.isnull().values.any())}')\n",
        "print(f'Any Nan values in S3? {(S3.isnull().values.any())}')\n",
        "print(f'Any Nan values in S4? {(S4.isnull().values.any())}')\n",
        "print(f'Any Nan values in S5? {(S5.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6? {(S6.isnull().values.any())}')\n",
        "\n",
        "S6FT, S6FM, S6FB, S6MT, S6MM, S6MB, S6RT, S6RM, S6RB\n",
        "\n",
        "print(f'Any Nan values in S6FT? {(S6FT.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6FM? {(S6FM.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6FB? {(S6FB.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6MT? {(S6MT.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6MM? {(S6MM.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6MB? {(S6MB.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6RT? {(S6RT.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6RM? {(S6RM.isnull().values.any())}')\n",
        "print(f'Any Nan values in S6RB? {(S6RB.isnull().values.any())}')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any Nan values in S2? False\n",
            "Any Nan values in S3? False\n",
            "Any Nan values in S4? False\n",
            "Any Nan values in S5? False\n",
            "Any Nan values in S6? True\n",
            "Any Nan values in S6FT? False\n",
            "Any Nan values in S6FM? False\n",
            "Any Nan values in S6FB? True\n",
            "Any Nan values in S6MT? False\n",
            "Any Nan values in S6MM? False\n",
            "Any Nan values in S6MB? False\n",
            "Any Nan values in S6RT? False\n",
            "Any Nan values in S6RM? False\n",
            "Any Nan values in S6RB? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrUyIUjCATVY"
      },
      "source": [
        "**DEFINE THE SEQUENCE CREATING FUNCTION, CONVERT DATASET DATA TO ARRAY AND CREATE SEQUENCE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nJ1eiQHpXnIC",
        "outputId": "51e6d320-d7f0-4b61-f183-f367b674078d"
      },
      "source": [
        "def create_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix-4] # so that we can get the 3rd data of 0-7 sequence \n",
        "\t\t# seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] \n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)\n",
        "\n",
        "S2FTA = pd.DataFrame.to_numpy(S2FT)\n",
        "S2MTA = pd.DataFrame.to_numpy(S2MT)\n",
        "S2RTA = pd.DataFrame.to_numpy(S2RT)\n",
        "S2FMA = pd.DataFrame.to_numpy(S2FM)\n",
        "S2MMA = pd.DataFrame.to_numpy(S2MM)\n",
        "S2RMA = pd.DataFrame.to_numpy(S2RM)\n",
        "S2FBA = pd.DataFrame.to_numpy(S2FB)\n",
        "S2MBA = pd.DataFrame.to_numpy(S2MB)\n",
        "S2RBA = pd.DataFrame.to_numpy(S2RB)\n",
        "\n",
        "S3FTA = pd.DataFrame.to_numpy(S3FT)\n",
        "S3MTA = pd.DataFrame.to_numpy(S3MT)\n",
        "S3RTA = pd.DataFrame.to_numpy(S3RT)\n",
        "S3FMA = pd.DataFrame.to_numpy(S3FM)\n",
        "S3MMA = pd.DataFrame.to_numpy(S3MM)\n",
        "S3RMA = pd.DataFrame.to_numpy(S3RM)\n",
        "S3FBA = pd.DataFrame.to_numpy(S3FB)\n",
        "S3MBA = pd.DataFrame.to_numpy(S3MB)\n",
        "S3RBA = pd.DataFrame.to_numpy(S3RB)\n",
        "\n",
        "S4FTA = pd.DataFrame.to_numpy(S4FT)\n",
        "S4MTA = pd.DataFrame.to_numpy(S4MT)\n",
        "S4RTA = pd.DataFrame.to_numpy(S4RT)\n",
        "S4FMA = pd.DataFrame.to_numpy(S4FM)\n",
        "S4MMA = pd.DataFrame.to_numpy(S4MM)\n",
        "S4RMA = pd.DataFrame.to_numpy(S4RM)\n",
        "S4FBA = pd.DataFrame.to_numpy(S4FB)\n",
        "S4MBA = pd.DataFrame.to_numpy(S4MB)\n",
        "S4RBA = pd.DataFrame.to_numpy(S4RB)\n",
        "\n",
        "S5FTA = pd.DataFrame.to_numpy(S5FT)\n",
        "S5MTA = pd.DataFrame.to_numpy(S5MT)\n",
        "S5RTA = pd.DataFrame.to_numpy(S5RT)\n",
        "S5FMA = pd.DataFrame.to_numpy(S5FM)\n",
        "S5MMA = pd.DataFrame.to_numpy(S5MM)\n",
        "S5RMA = pd.DataFrame.to_numpy(S5RM)\n",
        "S5FBA = pd.DataFrame.to_numpy(S5FB)\n",
        "S5MBA = pd.DataFrame.to_numpy(S5MB)\n",
        "S5RBA = pd.DataFrame.to_numpy(S5RB)\n",
        "\n",
        "S6FTA = pd.DataFrame.to_numpy(S6FT)\n",
        "S6MTA = pd.DataFrame.to_numpy(S6MT)\n",
        "S6RTA = pd.DataFrame.to_numpy(S6RT)\n",
        "S6FMA = pd.DataFrame.to_numpy(S6FM)\n",
        "S6MMA = pd.DataFrame.to_numpy(S6MM)\n",
        "S6RMA = pd.DataFrame.to_numpy(S6RM)\n",
        "S6FBA = pd.DataFrame.to_numpy(S6FB)\n",
        "S6MBA = pd.DataFrame.to_numpy(S6MB)\n",
        "S6RBA = pd.DataFrame.to_numpy(S6RB)\n",
        "\n",
        "n_steps = 7\n",
        "\n",
        "S2FTX, S2FTY = create_sequence(S2FTA, n_steps)\n",
        "S2MTX, S2MTY = create_sequence(S2MTA, n_steps)\n",
        "S2RTX, S2RTY = create_sequence(S2RTA, n_steps)\n",
        "S2FMX, S2FMY = create_sequence(S2FMA, n_steps)\n",
        "S2MMX, S2MMY = create_sequence(S2MMA, n_steps)\n",
        "S2RMX, S2RMY = create_sequence(S2RMA, n_steps)\n",
        "S2FBX, S2FBY = create_sequence(S2FBA, n_steps)\n",
        "S2MBX, S2MBY = create_sequence(S2MBA, n_steps)\n",
        "S2RBX, S2RBY = create_sequence(S2RBA, n_steps)\n",
        "\n",
        "S3FTX, S3FTY = create_sequence(S3FTA, n_steps)\n",
        "S3MTX, S3MTY = create_sequence(S3MTA, n_steps)\n",
        "S3RTX, S3RTY = create_sequence(S3RTA, n_steps)\n",
        "S3FMX, S3FMY = create_sequence(S3FMA, n_steps)\n",
        "S3MMX, S3MMY = create_sequence(S3MMA, n_steps)\n",
        "S3RMX, S3RMY = create_sequence(S3RMA, n_steps)\n",
        "S3FBX, S3FBY = create_sequence(S3FBA, n_steps)\n",
        "S3MBX, S3MBY = create_sequence(S3MBA, n_steps)\n",
        "S3RBX, S3RBY = create_sequence(S3RBA, n_steps)\n",
        "\n",
        "S4FTX, S4FTY = create_sequence(S4FTA, n_steps)\n",
        "S4MTX, S4MTY = create_sequence(S4MTA, n_steps)\n",
        "S4RTX, S4RTY = create_sequence(S4RTA, n_steps)\n",
        "S4FMX, S4FMY = create_sequence(S4FMA, n_steps)\n",
        "S4MMX, S4MMY = create_sequence(S4MMA, n_steps)\n",
        "S4RMX, S4RMY = create_sequence(S4RMA, n_steps)\n",
        "S4FBX, S4FBY = create_sequence(S4FBA, n_steps)\n",
        "S4MBX, S4MBY = create_sequence(S4MBA, n_steps)\n",
        "S4RBX, S4RBY = create_sequence(S4RBA, n_steps)\n",
        "\n",
        "S5FTX, S5FTY = create_sequence(S5FTA, n_steps)\n",
        "S5MTX, S5MTY = create_sequence(S5MTA, n_steps)\n",
        "S5RTX, S5RTY = create_sequence(S5RTA, n_steps)\n",
        "S5FMX, S5FMY = create_sequence(S5FMA, n_steps)\n",
        "S5MMX, S5MMY = create_sequence(S5MMA, n_steps)\n",
        "S5RMX, S5RMY = create_sequence(S5RMA, n_steps)\n",
        "S5FBX, S5FBY = create_sequence(S5FBA, n_steps)\n",
        "S5MBX, S5MBY = create_sequence(S5MBA, n_steps)\n",
        "S5RBX, S5RBY = create_sequence(S5RBA, n_steps)\n",
        "\n",
        "S6FTX, S6FTY = create_sequence(S6FTA, n_steps)\n",
        "S6MTX, S6MTY = create_sequence(S6MTA, n_steps)\n",
        "S6RTX, S6RTY = create_sequence(S6RTA, n_steps)\n",
        "S6FMX, S6FMY = create_sequence(S6FMA, n_steps)\n",
        "S6MMX, S6MMY = create_sequence(S6MMA, n_steps)\n",
        "S6RMX, S6RMY = create_sequence(S6RMA, n_steps)\n",
        "S6FBX, S6FBY = create_sequence(S6FBA, n_steps)\n",
        "S6MBX, S6MBY = create_sequence(S6MBA, n_steps)\n",
        "S6RBX, S6RBY = create_sequence(S6RBA, n_steps)\n",
        "\n",
        "len(S2FTX)\n",
        "len(S3FTX)\n",
        "\n",
        "X_FT = np.concatenate((S2FTX, S3FTX, S4FTX, S5FTX, S6FTX), axis=0)\n",
        "y_FT = np.concatenate((S2FTY, S3FTY, S4FTY, S5FTY, S6FTY), axis=0)\n",
        "X_FM = np.concatenate((S2FMX, S3FMX, S4FMX, S5FMX, S6FMX), axis=0)\n",
        "y_FM = np.concatenate((S2FMY, S3FMY, S4FMY, S5FMY, S6FMY), axis=0)\n",
        "X_FB = np.concatenate((S2FBX, S3FBX, S4FBX, S5FBX, S6FBX), axis=0)\n",
        "y_FB = np.concatenate((S2FBY, S3FBY, S4FBY, S5FBY, S6FBY), axis=0)\n",
        "\n",
        "X_MT = np.concatenate((S2MTX, S3MTX, S4MTX, S5MTX, S6MTX), axis=0)\n",
        "y_MT = np.concatenate((S2MTY, S3MTY, S4MTY, S5MTY, S6MTY), axis=0)\n",
        "X_MM = np.concatenate((S2MMX, S3MMX, S4MMX, S5MMX, S6MMX), axis=0)\n",
        "y_MM = np.concatenate((S2MMY, S3MMY, S4MMY, S5MMY, S6MMY), axis=0)\n",
        "X_MB = np.concatenate((S2MBX, S3MBX, S4MBX, S5MBX, S6MBX), axis=0)\n",
        "y_MB = np.concatenate((S2MBY, S3MBY, S4MBY, S5MBY, S6MBY), axis=0)\n",
        "\n",
        "X_RT = np.concatenate((S2RTX, S3RTX, S4RTX, S5RTX, S6RTX), axis=0)\n",
        "y_RT = np.concatenate((S2RTY, S3RTY, S4RTY, S5RTY, S6RTY), axis=0)\n",
        "X_RM = np.concatenate((S2RMX, S3RMX, S4RMX, S5RMX, S6RMX), axis=0)\n",
        "y_RM = np.concatenate((S2RMY, S3RMY, S4RMY, S5RMY, S6RMY), axis=0)\n",
        "X_RB = np.concatenate((S2RBX, S3RBX, S4RBX, S5RBX, S6RBX), axis=0)\n",
        "y_RB = np.concatenate((S2RBY, S3RBY, S4RBY, S5RBY, S6RBY), axis=0)\n",
        "\n",
        "len(X_FT)\n",
        "\n",
        "#plt.figure(figsize=(12, 10))\n",
        "#plt.plot(y_FT, \"b\", label=\"y_FT\")\n",
        "#plt.plot(y_MT, \"r\", label=\"y_MT\")\n",
        "#plt.plot(y_RT, \"y\", label=\"y_RT\")\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=y_FT, name=\"y_FT\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_MT, name=\"y_MT\", line_shape='linear'))\n",
        "fig.add_trace(go.Scatter(y=y_RT, name=\"y_RT\", line_shape='linear'))\n",
        "fig.show()\n",
        "\n",
        "fig1 = go.Figure()\n",
        "fig1.add_trace(go.Scatter(y=y_FM, name=\"y_FM\", line_shape='linear'))\n",
        "fig1.add_trace(go.Scatter(y=y_MM, name=\"y_MM\", line_shape='linear'))\n",
        "fig1.add_trace(go.Scatter(y=y_RM, name=\"y_RM\", line_shape='linear'))\n",
        "fig1.show()\n",
        "\n",
        "fig2 = go.Figure()\n",
        "fig2.add_trace(go.Scatter(y=y_FB, name=\"y_FB\", line_shape='linear'))\n",
        "fig2.add_trace(go.Scatter(y=y_MB, name=\"y_MB\", line_shape='linear'))\n",
        "fig2.add_trace(go.Scatter(y=y_RB, name=\"y_RB\", line_shape='linear'))\n",
        "fig2.show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"8d655141-5ff3-4406-b194-619c6c36cdbc\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"8d655141-5ff3-4406-b194-619c6c36cdbc\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '8d655141-5ff3-4406-b194-619c6c36cdbc',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_FT\", \"type\": \"scatter\", \"y\": [34.2, 34.5, 34.5, 34.5, 34.5, 34.9, 34.9, 34.5, 34.2, 34.0, 33.8, 33.8, 33.8, 33.8, 34.0, 34.5, 34.3, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.3, 34.2, 34.2, 34.2, 34.0, 33.8, 33.3, 33.1, 34.0, 33.8, 33.4, 33.4, 34.0, 34.2, 35.4, 35.2, 35.2, 35.1, 35.2, 35.2, 35.4, 35.4, 35.6, 35.4, 35.6, 35.4, 35.4, 35.6, 35.8, 36.0, 36.0, 36.3, 36.1, 36.5, 34.5, 34.7, 34.9, 35.1, 35.2, 35.2, 35.8, 36.0, 36.0, 36.1, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.7, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 38.7, 37.9, 37.6, 37.6, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 38.1, 38.3, 38.3, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.6, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.9, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.4, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.9, 39.7, 39.7, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.3, 40.1, 40.3, 40.3, 40.3, 40.1, 40.5, 40.3, 40.1, 40.1, 40.1, 40.1, 39.9, 39.9, 39.9, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.4, 39.4, 39.4, 39.6, 39.4, 39.4, 39.2, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.7, 38.7, 38.7, 38.7, 38.5, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.9, 36.9, 37.8, 37.9, 37.9, 37.8, 37.6, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.4, 37.6, 37.8, 37.8, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.2, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.8, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.9, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 39.9, 39.9, 39.9, 39.9, 39.7, 39.6, 39.6, 39.4, 39.4, 39.2, 39.0, 38.8, 38.7, 38.5, 38.3, 38.3, 38.1, 38.1, 37.9, 37.9, 37.8, 37.6, 37.6, 37.6, 37.4, 37.2, 37.0, 37.0, 37.0, 39.4, 37.9, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.1, 33.6, 33.8, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.8, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 33.4, 33.4, 33.6, 33.6, 33.6, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 35.1, 35.2, 35.6, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 36.0, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_MT\", \"type\": \"scatter\", \"y\": [34.5, 34.7, 34.5, 34.5, 34.9, 34.9, 34.3, 34.0, 33.8, 33.6, 33.8, 33.6, 33.6, 33.8, 34.2, 34.3, 34.3, 34.5, 34.3, 34.5, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.0, 33.8, 33.6, 34.5, 34.2, 34.0, 34.2, 34.3, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 32.4, 32.5, 32.7, 32.9, 33.3, 33.6, 34.0, 34.3, 34.3, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.5, 36.7, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 39.0, 37.4, 36.9, 36.9, 36.9, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.5, 36.3, 36.1, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.6, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 32.7, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.6, 33.8, 34.0, 34.3, 34.7, 34.7, 34.5, 34.5, 34.3, 34.3, 34.2, 34.3, 34.2, 34.2, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.1, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.1, 33.4, 33.3, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.6, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 34.2, 33.8, 33.6, 33.4, 33.8, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 34.0, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.3, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.0, 34.2, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.3, 34.3, 34.7, 34.3, 34.2, 34.2, 34.5, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 32.9, 33.1, 33.4, 33.6, 34.0, 34.2, 34.2, 34.3, 34.5, 34.9, 34.9, 34.5, 34.5, 34.7, 34.9, 34.9, 34.9, 34.9, 35.1, 35.2, 35.4, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.6, 35.4, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.2, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.3, 34.3, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 33.3, 33.4, 33.4, 33.6, 33.8, 34.2, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.9, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 39.9, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.1, 40.3, 40.3, 40.3, 40.3, 40.3, 40.3, 40.3, 40.3, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.5, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.6, 40.8, 40.8, 40.8, 40.8, 40.8, 40.8, 40.8, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.2, 41.2, 41.2, 41.2, 41.2, 41.2, 41.2, 41.2, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.4, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.7, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 41.9, 42.1, 41.9, 41.9, 41.9, 41.9, 41.9, 41.5, 41.2, 41.4, 41.4, 41.4, 41.4, 40.8]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RT\", \"type\": \"scatter\", \"y\": [34.3, 34.7, 34.7, 34.5, 34.5, 34.7, 34.9, 35.1, 34.5, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 34.0, 34.2, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.7, 34.5, 34.3, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 34.5, 34.7, 34.7, 34.9, 35.1, 35.1, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 35.8, 35.8, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 39.7, 39.2, 39.2, 39.0, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.8, 38.7, 38.8, 39.2, 40.5, 39.6, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 39.0, 38.8, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.6, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 39.0, 39.0, 39.0, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.5, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 37.9, 37.6, 37.4, 37.6, 37.6, 37.4, 37.4, 37.6, 37.6, 37.4, 36.7, 33.1, 33.3, 33.4, 33.4, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.4, 33.8, 34.2, 34.3, 34.5, 34.7, 34.7, 34.5, 34.5, 34.7, 34.5, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.9, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.7, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.7, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.1, 36.1, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 36.3, 36.7, 36.9, 37.2, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.0, 36.9, 36.9, 37.0, 37.0, 37.0, 37.2, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.6, 39.7, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.7, 39.6, 39.7, 39.6, 39.6, 39.6, 39.7, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 34.5, 34.7, 35.1, 35.2, 35.4, 35.4, 35.6, 35.6, 35.8, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 37.2, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.3, 36.7, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8d655141-5ff3-4406-b194-619c6c36cdbc');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"168b9dd0-2cf2-4d2d-a4c3-a46567ba9d78\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"168b9dd0-2cf2-4d2d-a4c3-a46567ba9d78\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '168b9dd0-2cf2-4d2d-a4c3-a46567ba9d78',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_FM\", \"type\": \"scatter\", \"y\": [34.3, 34.5, 34.7, 34.7, 34.7, 35.2, 35.2, 35.1, 34.7, 34.3, 34.3, 34.2, 34.2, 34.0, 34.3, 34.3, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 34.7, 34.3, 34.2, 34.0, 34.0, 34.0, 34.0, 33.4, 32.9, 32.9, 34.3, 33.6, 33.4, 33.4, 33.6, 33.8, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.9, 33.3, 33.4, 33.4, 33.3, 33.3, 33.1, 33.3, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.4, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.6, 35.8, 35.6, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.0, 37.2, 37.2, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.5, 38.5, 38.3, 38.3, 38.3, 38.5, 38.5, 38.3, 38.5, 38.7, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 37.9, 37.6, 37.2, 37.2, 37.2, 37.2, 37.4, 37.8, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 37.8, 37.2, 37.2, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.6, 37.2, 37.4, 37.8, 37.8, 37.2, 37.2, 37.0, 37.2, 37.0, 37.2, 37.4, 37.6, 37.8, 37.8, 37.8, 37.2, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 37.2, 37.6, 37.8, 37.8, 37.8, 37.8, 37.9, 37.8, 37.2, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.4, 36.9, 36.5, 36.3, 36.3, 36.1, 36.1, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 36.0, 36.1, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 36.1, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 36.0, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.3, 36.1, 36.0, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 32.0, 32.0, 32.0, 32.4, 32.0, 32.0, 32.0, 32.2, 32.2, 32.2, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.0, 36.0, 35.8, 35.8, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.7, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.2, 34.2, 34.3, 34.2, 34.2, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 32.2, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.9, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.6, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.4, 33.4, 33.3, 33.3, 33.4, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.8, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.8, 33.6, 33.6, 33.4, 33.6, 33.6, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 34.2, 34.0, 34.0, 33.8, 34.0, 33.8, 33.8, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.1, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 34.7, 34.9, 34.9, 35.1, 35.1, 35.2, 35.4, 35.4, 35.6, 35.6, 35.6, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.3, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.7, 36.9, 36.7, 37.2, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.6, 38.1, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.4, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_MM\", \"type\": \"scatter\", \"y\": [34.5, 34.9, 34.9, 34.7, 34.7, 35.2, 35.2, 34.9, 34.5, 34.2, 34.2, 34.2, 34.2, 34.0, 34.2, 34.3, 34.5, 34.5, 34.5, 34.5, 34.7, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.3, 33.8, 33.6, 34.5, 34.3, 34.0, 34.0, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 30.9, 31.3, 31.3, 31.6, 31.6, 31.8, 32.0, 32.0, 32.2, 32.0, 32.0, 32.0, 32.0, 31.8, 31.8, 31.8, 31.8, 32.0, 32.0, 32.2, 32.0, 32.0, 32.2, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 37.9, 37.0, 36.9, 36.7, 36.7, 36.5, 36.7, 36.7, 36.9, 36.7, 36.5, 36.5, 36.5, 36.5, 37.2, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 36.9, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.0, 37.0, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.2, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.4, 35.2, 35.1, 35.1, 35.2, 35.2, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 31.6, 31.6, 31.8, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 32.9, 32.9, 33.1, 33.1, 32.9, 33.1, 32.9, 32.9, 33.1, 32.9, 33.1, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.3, 33.6, 33.4, 33.3, 33.3, 33.3, 33.1, 33.3, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.6, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.6, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.8, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 34.2, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 32.7, 32.9, 32.7, 32.9, 33.1, 33.8, 34.0, 34.2, 34.3, 34.5, 34.3, 33.8, 34.3, 34.7, 34.7, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.4, 35.2, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 35.1, 35.1, 35.2, 35.1, 35.1, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.5, 34.7, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.9, 34.9, 34.7, 34.7, 34.9, 34.9, 34.9, 34.7, 34.9, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.9, 34.7, 34.9, 34.7, 34.9, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 33.4, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 38.7, 38.3, 38.7, 38.5, 38.7, 38.7, 38.5]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RM\", \"type\": \"scatter\", \"y\": [35.1, 35.4, 35.6, 35.2, 35.6, 36.0, 36.0, 35.6, 35.2, 35.1, 35.2, 35.2, 35.1, 35.1, 35.4, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.6, 35.6, 35.6, 35.4, 35.1, 34.7, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 33.6, 33.6, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.4, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.6, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.7, 39.6, 39.4, 39.4, 39.4, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.2, 39.0, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.8, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.8, 38.8, 38.8, 38.8, 38.8, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.7, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.3, 38.3, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.8, 37.9, 38.3, 38.1, 38.1, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.7, 36.7, 36.7, 36.9, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.5, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.1, 36.1, 35.8, 35.6, 35.6, 35.6, 35.4, 35.2, 35.2, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 32.9, 32.7, 32.7, 32.7, 32.7, 32.7, 33.1, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.4, 33.4, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 35.4, 34.7, 34.3, 34.3, 34.7, 34.5, 34.5, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.2, 34.2, 34.3, 34.3, 34.2, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.8, 35.8, 35.6, 35.4, 35.4, 35.4, 35.2, 35.2, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.1, 35.1, 35.1, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.2, 35.4, 35.2, 35.4, 35.2, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 35.1, 34.9, 34.7, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.2, 34.2, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 35.8, 36.0, 36.1, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.0, 35.8, 35.8, 36.1, 36.3, 36.5, 36.5, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.8, 37.4, 37.4, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 33.8, 33.8, 34.0, 34.0, 34.2, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 39.9, 38.3, 37.9, 37.9, 37.8, 37.6, 37.4, 37.4, 37.2, 36.9, 36.9, 36.9, 36.9]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('168b9dd0-2cf2-4d2d-a4c3-a46567ba9d78');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"5633a16b-91db-4167-bd9c-9c0e7ec127c7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"5633a16b-91db-4167-bd9c-9c0e7ec127c7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '5633a16b-91db-4167-bd9c-9c0e7ec127c7',\n",
              "                        [{\"line\": {\"shape\": \"linear\"}, \"name\": \"y_FB\", \"type\": \"scatter\", \"y\": [34.3, 34.5, 34.5, 34.3, 34.5, 34.7, 34.9, 34.5, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.5, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.7, 34.7, 34.5, 34.5, 34.5, 34.3, 34.2, 33.8, 34.0, 34.5, 34.2, 34.0, 34.0, 34.0, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 33.6, 33.8, 33.8, 34.0, 34.0, 34.2, 34.3, 34.2, 34.3, 34.3, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.6, 35.8, 35.8, 35.8, 36.3, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.9, 37.8, 37.8, 37.6, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.3, 38.3, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.3, 38.1, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.3, 38.3, 38.3, 38.3, 38.1, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.3, 38.1, 38.3, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.5, 38.7, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.5, 38.7, 38.5, 38.5, 38.7, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.5, 38.7, 38.7, 38.7, 38.5, 38.5, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.7, 38.5, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.3, 38.5, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.3, 38.5, 38.5, 38.5, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.3, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.2, 31.6, 31.8, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.0, 35.8, 35.6, 35.4, 35.2, 35.2, 35.2, 35.2, 35.1, 34.9, 34.9, 34.9, 35.1, 35.1, 34.9, 34.9, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.3, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 35.1, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.9, 34.9, 35.2, 35.1, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 36.0, 36.0, 35.6, 35.6, 35.6, 35.8, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.6, 35.6, 35.6, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_MB\", \"type\": \"scatter\", \"y\": [34.3, 34.5, 34.7, 34.5, 34.5, 34.9, 35.1, 34.7, 34.3, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.5, 34.3, 34.0, 34.0, 34.0, 34.0, 33.6, 33.1, 32.9, 34.2, 33.8, 33.4, 33.4, 33.8, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 31.1, 31.1, 31.3, 31.5, 32.0, 32.2, 32.2, 32.2, 32.0, 31.8, 31.8, 32.0, 32.0, 32.0, 31.8, 31.8, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.5, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.6, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.1, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 35.2, 35.2, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.9, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.3, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.2, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.2, 34.2, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.4, 33.4, 33.4, 33.8, 33.8, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 33.8, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.2, 34.3, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 33.8, 33.8, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 32.4, 32.5, 32.7, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.8, 33.8, 33.6, 33.6, 33.8, 33.6, 33.6, 33.8, 33.6, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.2, 34.0, 34.0, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 33.8, 33.8, 34.0, 33.8, 33.8, 34.0, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.0, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.5, 34.3, 34.3, 34.3, 34.2, 34.2, 34.2, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.7, 34.5, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.3, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.2, 34.3, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 33.8, 34.0, 34.0, 34.0, 34.0, 34.3, 34.3, 34.3, 34.3, 34.5, 34.3, 34.2, 34.3, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.2, 35.2, 35.1, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.7, 34.7, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 32.4, 32.4, 32.5, 32.5, 32.5, 32.9, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.8, 33.8, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 36.9, 36.9, 37.0, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0]}, {\"line\": {\"shape\": \"linear\"}, \"name\": \"y_RB\", \"type\": \"scatter\", \"y\": [35.6, 36.0, 36.1, 35.8, 36.0, 36.3, 36.1, 35.6, 35.1, 35.1, 35.1, 35.2, 35.1, 35.2, 35.4, 35.6, 35.6, 35.6, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 36.0, 36.0, 35.8, 35.6, 35.4, 35.4, 35.2, 35.2, 35.1, 34.7, 34.5, 34.9, 34.9, 34.7, 34.9, 34.9, 35.1, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 36.0, 33.4, 33.6, 33.8, 33.8, 34.0, 34.0, 34.0, 34.2, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.3, 36.5, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 37.0, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.9, 36.7, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.8, 37.4, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.8, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.8, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.8, 37.8, 37.8, 37.8, 37.8, 37.6, 37.8, 37.8, 37.8, 37.9, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.8, 37.9, 37.9, 37.8, 37.9, 37.8, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 38.1, 37.9, 37.8, 37.8, 37.6, 37.6, 37.4, 37.2, 37.2, 37.2, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.4, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.6, 37.4, 37.4, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.2, 37.4, 37.4, 37.2, 37.4, 37.2, 37.4, 37.2, 37.2, 37.2, 37.0, 37.0, 37.4, 37.2, 37.2, 37.2, 37.2, 37.2, 37.0, 37.0, 37.0, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 37.0, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.9, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.7, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 31.8, 31.8, 31.8, 31.8, 31.8, 32.0, 31.8, 32.0, 32.4, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.2, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.4, 32.5, 32.4, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.7, 32.5, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.7, 32.9, 32.9, 32.9, 32.9, 33.1, 32.9, 32.9, 32.9, 32.9, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.1, 33.3, 33.1, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.3, 33.4, 33.8, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.4, 33.6, 33.4, 33.4, 33.4, 33.4, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.6, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 33.8, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.2, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.3, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.5, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.7, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 34.9, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 36.0, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.6, 35.8, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 36.3, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.4, 35.4, 35.4, 35.4, 35.6, 35.4, 35.4, 35.4, 35.4, 35.2, 35.2, 35.4, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.4, 35.6, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 35.8, 35.6, 35.6, 35.8, 36.0, 36.1, 36.1, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.5, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.1, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.7, 36.3, 36.3, 36.3, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.3, 36.1, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.3, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.3, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.7, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 34.3, 34.5, 34.5, 34.7, 34.5, 34.9, 34.7, 34.9, 34.9, 34.9, 34.9, 35.1, 34.9, 35.1, 35.2, 35.2, 35.1, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.2, 35.6, 35.6, 35.4, 35.4, 35.2, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.4, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.6, 35.8, 35.8, 35.8, 35.8, 35.8, 35.8, 36.0, 35.8, 36.0, 35.8, 35.8, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.1, 36.0, 36.1, 36.1, 36.1, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.1, 36.0, 36.0, 36.0, 36.0, 36.5, 36.3, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1, 36.1]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5633a16b-91db-4167-bd9c-9c0e7ec127c7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrWvDeTvBuN1",
        "outputId": "ffc00ef5-fb5a-4df6-91fe-0c3cd98283c0"
      },
      "source": [
        "X_FT[10:20]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[34.5, 34.2, 34. , 33.8, 33.8, 33.8, 33.8],\n",
              "       [34.2, 34. , 33.8, 33.8, 33.8, 33.8, 34. ],\n",
              "       [34. , 33.8, 33.8, 33.8, 33.8, 34. , 34.5],\n",
              "       [33.8, 33.8, 33.8, 33.8, 34. , 34.5, 34.3],\n",
              "       [33.8, 33.8, 33.8, 34. , 34.5, 34.3, 34.5],\n",
              "       [33.8, 33.8, 34. , 34.5, 34.3, 34.5, 34.5],\n",
              "       [33.8, 34. , 34.5, 34.3, 34.5, 34.5, 34.5],\n",
              "       [34. , 34.5, 34.3, 34.5, 34.5, 34.5, 34.7],\n",
              "       [34.5, 34.3, 34.5, 34.5, 34.5, 34.7, 34.5],\n",
              "       [34.3, 34.5, 34.5, 34.5, 34.7, 34.5, 34.5]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUar_pv0B69U",
        "outputId": "5f6aa6a2-411c-46eb-dc5c-19f82a456325"
      },
      "source": [
        "y_FT[10:20]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([33.8, 33.8, 33.8, 33.8, 34. , 34.5, 34.3, 34.5, 34.5, 34.5])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XhBtO6ACRj"
      },
      "source": [
        "**Predict Rear Bottom Sensor Values by the values given for the front top sensor**\n",
        "\n",
        "**INPUT IS ASSIGNED TO MODEL INPUT VARIABLE**\n",
        "\n",
        "**ADD THE THIRD DIMENSION FOR NUMBER OF FEATURES TO THE TRAIN INPUT.**\n",
        "\n",
        "**THIS IS NECESSARY FOR CONV1D MODEL**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq51Cs1jQl-h"
      },
      "source": [
        "# define baseline model 1\n",
        "# create model\n",
        "def modelG(inp_shp):\n",
        "  global model\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(inp_shp, 1)))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dense(1))\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "  model.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "  #model1.summary()\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ev6TwMNRjN"
      },
      "source": [
        "def datagenerator(n_sensors, Y_in, X_in1=np.array([]), X_in2=np.array([]), X_in3=np.array([]), X_in4=np.array([]), X_in5=np.array([]), X_in6=np.array([]), X_in7=np.array([])):\n",
        "#def datagenerator(Y_in, X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, X_in7, n_sensors):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  InputsX = [X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, X_in7]\n",
        "  InputsX = InputsX[0:n_sensors]\n",
        "  X_in = np.concatenate([x for x in InputsX if x.size > 0], axis=1)\n",
        "\n",
        "  #X_in = np.concatenate((X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, X_in7), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "  \n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], 1))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], 1))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], 1))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n",
        "#X_train_Input.shape"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(X_in1 = X_FT, X_in2 = X_FM, X_in3, X_in4, X_in5, X_in6, X_in7, Y_in = y_MB)\n",
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(2, y_MB, X_FT, X_FM, np.array([]), np.array([]), np.array([]), np.array([]), np.array([]))"
      ],
      "metadata": {
        "id": "QS3uoY5Mr9d_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_Input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeNY9TMS0-7o",
        "outputId": "a4fe73c8-ffb9-46fd-869f-7d9efb802df0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1128, 14, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nItmhieXwgdb"
      },
      "source": [
        "def evaldata(n_sensors, \n",
        "             traindata1 = None, traindata2 = None, traindata3 = None, traindata4 = None, \n",
        "             traindata5 = None, traindata6 = None, traindata7 = None, testdata = None, \n",
        "             X_in1=np.array([]), X_in2=np.array([]), X_in3=np.array([]), X_in4=np.array([]), \n",
        "             X_in5=np.array([]), X_in6=np.array([]), X_in7=np.array([]), Y_in=np.array([])):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(n_sensors, Y_in, X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, X_in7)\n",
        "  \n",
        "  history = model.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  #print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  #print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  #print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q41wARCq_Iac"
      },
      "source": [
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "inp_shp = 7\n",
        "n_sensors = 1\n",
        "\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "modelG(inp_shp)\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "my_dictMF1 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), n_sensors):\n",
        "  kX1 = combo[0][0]\n",
        "  vX1 = combo[0][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF1 = evaldata(n_sensors, X_in1=vX1, Y_in=vy, traindata1 = kX1, testdata = ky)\n",
        "      TestLossTotal = resultsMF1[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF1[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF1[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF1[\"DATA_X\"].append(kX1)\n",
        "    my_dictMF1[\"DATA_y\"].append(ky)\n",
        "    my_dictMF1[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model.load_weights('model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "kgncoI3oyMKg",
        "outputId": "b8ee88d6-3866-4a9e-abd1-d3cf450a5d77"
      },
      "source": [
        "CombResults1 = pd.DataFrame.from_dict(my_dictMF1)\n",
        "CombResultsSorted1 = CombResults1.sort_values(by=['Test Loss'])\n",
        "CombResultsSorted1.to_csv('CombResultsSorted1.csv')\n",
        "CombResultsSorted1\n",
        "files.download(\"CombResultsSorted1.csv\")\n",
        "fig = px.box(CombResultsSorted1, x=\"DATA_X\", y=\"Test Loss\", hover_data=[\"DATA_y\"])\n",
        "fig.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_7e8bf579-72cd-42ce-9cdf-bb643e485626\", \"CombResultsSorted1.csv\", 1794)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"5892f216-a4e3-4b85-9040-4168178e457f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"5892f216-a4e3-4b85-9040-4168178e457f\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '5892f216-a4e3-4b85-9040-4168178e457f',\n",
              "                        [{\"alignmentgroup\": \"True\", \"customdata\": [[\"y_MB\"], [\"y_RM\"], [\"y_RM\"], [\"y_RT\"], [\"y_RB\"], [\"y_RB\"], [\"y_RB\"], [\"y_MM\"], [\"y_MM\"], [\"y_RB\"], [\"y_FM\"], [\"y_MB\"], [\"y_MM\"], [\"y_RB\"], [\"y_MM\"], [\"y_RT\"], [\"y_MT\"], [\"y_MB\"], [\"y_MB\"], [\"y_MB\"], [\"y_FM\"], [\"y_RM\"], [\"y_RM\"], [\"y_MM\"], [\"y_FM\"], [\"y_FM\"], [\"y_MB\"], [\"y_RB\"], [\"y_RM\"], [\"y_FM\"], [\"y_RB\"], [\"y_MT\"], [\"y_MB\"], [\"y_MM\"], [\"y_MT\"], [\"y_MT\"], [\"y_RT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FM\"], [\"y_FT\"], [\"y_MT\"], [\"y_RM\"], [\"y_FM\"], [\"y_RT\"], [\"y_RT\"], [\"y_MT\"], [\"y_RM\"], [\"y_MM\"], [\"y_FT\"], [\"y_FT\"], [\"y_MT\"], [\"y_FT\"], [\"y_RT\"], [\"y_FT\"], [\"y_FT\"]], \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}<br>DATA_y=%{customdata[0]}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_MM\", \"X_RT\", \"X_RB\", \"X_RM\", \"X_RM\", \"X_MM\", \"X_FM\", \"X_MT\", \"X_MB\", \"X_RT\", \"X_MM\", \"X_RB\", \"X_RB\", \"X_MB\", \"X_RM\", \"X_RB\", \"X_MM\", \"X_MT\", \"X_FM\", \"X_RT\", \"X_RB\", \"X_MM\", \"X_FM\", \"X_FM\", \"X_MT\", \"X_MB\", \"X_RM\", \"X_MT\", \"X_MB\", \"X_RM\", \"X_FT\", \"X_MB\", \"X_FT\", \"X_RT\", \"X_RB\", \"X_FM\", \"X_FM\", \"X_MM\", \"X_RB\", \"X_FT\", \"X_FM\", \"X_RM\", \"X_MT\", \"X_RT\", \"X_MB\", \"X_FT\", \"X_FT\", \"X_FT\", \"X_FT\", \"X_RT\", \"X_MM\", \"X_RT\", \"X_RM\", \"X_MT\", \"X_MB\", \"X_MT\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.7059104472398758, 0.8058847486972809, 0.8517767727375031, 0.8691348850727081, 0.8782648265361785, 0.8929179251194, 0.9147695899009705, 0.9239143252372741, 0.9734583973884583, 1.0175693571567535, 1.0180003881454467, 1.0323110163211822, 1.0438239991664886, 1.0671079277992248, 1.0719623863697052, 1.0888100147247315, 1.093511551618576, 1.1142449975013733, 1.1220218896865846, 1.124831885099411, 1.129785269498825, 1.1878043413162231, 1.2137558937072754, 1.2178724706172943, 1.2339083671569824, 1.2346011519432067, 1.2533528506755829, 1.281011974811554, 1.300116765499115, 1.3826535582542419, 1.3963159561157226, 1.4343516111373902, 1.4419169425964355, 1.4445732831954956, 1.4507009506225585, 1.4970086574554444, 1.5393321633338928, 1.5766339540481566, 1.581656575202942, 1.5898024082183837, 1.6364784955978393, 1.6868603467941283, 1.7436092972755433, 1.7728848934173584, 1.798678493499756, 1.8504418015480042, 1.8538554072380067, 1.8702918887138367, 1.8988542914390565, 1.9316071271896362, 1.9367382287979127, 1.946904754638672, 1.9613794326782226, 2.0458237767219543, 2.052758538722992, 2.188782739639282], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5892f216-a4e3-4b85-9040-4168178e457f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Rny1g3a9_bTz",
        "outputId": "c7d0d3fa-7b7e-4cbd-a95a-84a99c186170"
      },
      "source": [
        "CombResultsSortedgrouped1 = CombResultsSorted1.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedgroupedsortedMF1 = CombResultsSortedgrouped1.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedgroupedsortedMF1.to_csv('CombResultsSortedgroupedsortedMF1.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedgroupedsortedMF1.csv\")\n",
        "CombResultsSortedgroupedsortedMF1"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a54208d8-0700-45f2-8405-2beff8db4bad\", \"CombResultsSortedgroupedsortedMF1.csv\", 205)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_RB</th>\n",
              "      <td>1.168409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MM</th>\n",
              "      <td>1.201645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RM</th>\n",
              "      <td>1.300515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FM</th>\n",
              "      <td>1.305891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MB</th>\n",
              "      <td>1.408725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RT</th>\n",
              "      <td>1.434894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MT</th>\n",
              "      <td>1.504471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FT</th>\n",
              "      <td>1.700211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Test Loss\n",
              "DATA_X           \n",
              "X_RB     1.168409\n",
              "X_MM     1.201645\n",
              "X_RM     1.300515\n",
              "X_FM     1.305891\n",
              "X_MB     1.408725\n",
              "X_RT     1.434894\n",
              "X_MT     1.504471\n",
              "X_FT     1.700211"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHCi-CUyt1xF"
      },
      "source": [
        "# define baseline model 2\n",
        "# create model\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(14, 1)))\n",
        "model2.add(MaxPooling1D(pool_size=2))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(50, activation='relu'))\n",
        "model2.add(Dense(1))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "model2.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "#model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9dgHFmuuAal"
      },
      "source": [
        "\n",
        "def datageneratorMF2(X_in1, X_in2, Y_in):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  X_in = np.concatenate((X_in1, X_in2), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "\n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], 1))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], 1))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], 1))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbyCU9dvuUQe"
      },
      "source": [
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMS(X_MT, X_MM, y_RM)\n",
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(X_MT, y_RM)\n",
        "#X_train_Input[0:5]\n",
        "#X_train_Input.shape\n",
        "#y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRZ9Y_qKuZ2r"
      },
      "source": [
        "def evaldataMF(X_in1, X_in2, Y_in, traindata1, traindata2, testdata):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMF2(X_in1, X_in2, Y_in)\n",
        "  \n",
        "  history = model2.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model2.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model2.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # PLOTS LOSS VS EPOCH\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  # print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  # print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # PLOTS Y ORIGINAL VS PREDICTED\n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata1)+ str(traindata2)  + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18gXL9lLuxfT"
      },
      "source": [
        "TrainDataSet1 = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TrainDataSet2 = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "\n",
        "model2.save_weights('model2.h5')\n",
        "\n",
        "my_dictMF = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for kX1, vX1  in TrainDataSet1.items():\n",
        "  #TrainDataSet2.popitem()\n",
        "  TrainDataSet2.pop(next(iter(TrainDataSet2)))\n",
        "  for kX2, vX2  in TrainDataSet2.items():\n",
        "    if kX1 == kX2:\n",
        "      continue\n",
        "    for ky, vy  in TestDataSet.items():\n",
        "      if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:]:\n",
        "        continue\n",
        "      \n",
        "      TestLossTotal = 0\n",
        "      TrainLossTotal = 0\n",
        "      ValLossTotal = 0\n",
        "      runs = 10\n",
        "\n",
        "      for i in range(runs):\n",
        "        resultsMF = evaldataMF(vX1, vX2, vy, kX1, kX2, ky)\n",
        "        TestLossTotal = resultsMF[2] + TestLossTotal\n",
        "        TrainLossTotal = resultsMF[0] + TrainLossTotal\n",
        "        ValLossTotal = resultsMF[1] + ValLossTotal\n",
        "      \n",
        "      TestLossAvg = TestLossTotal / runs\n",
        "      TrainLossAvg = TrainLossTotal / runs\n",
        "      ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "      print(\"*****************************************************************************************************************************\")\n",
        "      print(f'Evaluate model for Train Data: {kX1}_{kX2} and Test Data: {ky}')\n",
        "      print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "      print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "      my_dictMF[\"DATA_X\"].append(kX1 + kX2)\n",
        "      my_dictMF[\"DATA_y\"].append(ky)\n",
        "      my_dictMF[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "      # for k, v in my_dict.items():\n",
        "      #   print(k, v)\n",
        "      model2.load_weights('model2.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyIy-WjRvFm_"
      },
      "source": [
        "CombResultsMF2 = pd.DataFrame.from_dict(my_dictMF)\n",
        "print(CombResultsMF2.shape)\n",
        "CombResultsSortedMF2 = CombResultsMF2.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMF2.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7JUwzZ1vMDq"
      },
      "source": [
        "CombResultsSortedMFgrouped = CombResultsSortedMF2.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedMFgroupedsortedMF2 = CombResultsSortedMFgrouped.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMFgroupedsortedMF2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBkDAjxWhYY0"
      },
      "source": [
        "CombResultsSortedMFgroupedsortedMF2.to_csv('CombResultsSortedMFgroupedsortedMF2.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedMFgroupedsortedMF2.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "sBeBEnv2X-EQ",
        "outputId": "3775c8bd-e448-475e-a6b8-6cc4821738dc"
      },
      "source": [
        "CombResultsSortedMF2.to_csv('CombResultsSortedMF2.csv')\n",
        "files.download(\"CombResultsSortedMF2.csv\")\n",
        "\n",
        "fig = px.box(CombResultsSortedMF2, x=\"DATA_X\", y=\"Test Loss\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_70acd70d-491b-472d-b12a-8aecaf42df92\", \"CombResultsSortedMF2.csv\", 6097)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4af31b9d-9028-41a5-afa0-cc47d3dfe5e2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4af31b9d-9028-41a5-afa0-cc47d3dfe5e2\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4af31b9d-9028-41a5-afa0-cc47d3dfe5e2',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_MTX_RT\", \"X_MMX_RT\", \"X_MTX_RT\", \"X_MMX_RB\", \"X_FMX_MB\", \"X_FMX_RT\", \"X_MTX_RB\", \"X_MMX_RM\", \"X_MBX_RM\", \"X_FMX_RT\", \"X_MMX_RT\", \"X_FTX_RT\", \"X_MTX_RM\", \"X_FMX_MT\", \"X_MBX_RT\", \"X_MBX_RT\", \"X_FMX_MM\", \"X_RTX_RB\", \"X_MTX_RT\", \"X_FTX_RT\", \"X_MTX_RT\", \"X_MTX_RM\", \"X_MMX_RM\", \"X_FMX_MB\", \"X_FTX_RB\", \"X_FMX_RM\", \"X_MTX_MM\", \"X_MTX_MB\", \"X_MBX_RM\", \"X_RTX_RM\", \"X_MTX_RB\", \"X_MTX_RM\", \"X_MBX_RB\", \"X_FTX_RM\", \"X_MBX_RM\", \"X_MTX_RM\", \"X_FMX_RM\", \"X_FTX_MT\", \"X_MMX_RB\", \"X_MMX_RM\", \"X_MMX_RB\", \"X_FMX_RB\", \"X_FTX_FM\", \"X_MTX_RB\", \"X_FTX_RM\", \"X_FMX_RM\", \"X_MMX_RT\", \"X_RMX_RB\", \"X_MBX_RB\", \"X_FTX_MM\", \"X_MTX_RB\", \"X_FMX_MM\", \"X_FTX_MM\", \"X_MTX_MM\", \"X_MMX_RT\", \"X_RMX_RB\", \"X_FTX_MM\", \"X_FTX_RB\", \"X_FTX_RB\", \"X_FTX_RM\", \"X_MMX_RM\", \"X_FMX_MT\", \"X_RTX_RB\", \"X_MTX_MM\", \"X_RTX_RB\", \"X_RMX_RB\", \"X_FMX_RB\", \"X_RMX_RB\", \"X_MMX_MB\", \"X_FTX_MT\", \"X_MMX_RM\", \"X_FTX_MB\", \"X_MTX_RT\", \"X_FTX_FM\", \"X_MTX_MM\", \"X_FTX_RB\", \"X_FMX_RB\", \"X_MBX_RB\", \"X_FTX_MB\", \"X_MTX_RM\", \"X_MBX_RB\", \"X_MMX_RT\", \"X_FTX_MB\", \"X_FTX_RM\", \"X_MMX_RB\", \"X_FMX_RM\", \"X_FMX_MM\", \"X_FTX_RM\", \"X_FMX_RT\", \"X_MTX_MB\", \"X_MTX_MB\", \"X_FTX_MT\", \"X_FTX_MM\", \"X_MTX_RB\", \"X_MMX_MB\", \"X_FTX_FM\", \"X_RTX_RB\", \"X_FMX_MB\", \"X_FMX_MM\", \"X_FMX_RT\", \"X_MMX_MB\", \"X_MMX_RB\", \"X_FTX_MT\", \"X_RTX_RM\", \"X_MBX_RT\", \"X_FMX_MT\", \"X_MBX_RM\", \"X_RTX_RM\", \"X_FTX_RB\", \"X_FTX_RT\", \"X_FTX_FM\", \"X_FMX_RB\", \"X_FTX_FM\", \"X_MBX_RT\", \"X_FMX_MB\", \"X_FTX_MB\", \"X_FMX_RT\", \"X_FTX_RT\", \"X_MBX_RT\", \"X_MTX_MB\", \"X_FTX_MM\", \"X_FTX_MB\", \"X_FMX_MT\", \"X_FTX_RT\", \"X_FMX_RM\", \"X_FMX_MT\", \"X_FMX_RB\", \"X_RTX_RM\", \"X_MMX_MB\", \"X_MBX_RM\", \"X_MBX_RB\", \"X_FTX_MT\", \"X_FMX_MT\", \"X_FTX_MB\", \"X_RMX_RB\", \"X_FMX_MB\", \"X_RTX_RB\", \"X_MMX_MB\", \"X_MMX_RB\", \"X_FTX_RB\", \"X_FMX_MM\", \"X_RMX_RB\", \"X_FTX_RT\", \"X_MTX_MM\", \"X_FMX_RB\", \"X_FTX_RM\", \"X_MTX_RM\", \"X_FTX_FM\", \"X_RTX_RB\", \"X_MTX_RB\", \"X_FMX_RM\", \"X_MBX_RB\", \"X_FMX_RT\", \"X_FTX_MM\", \"X_FMX_MB\", \"X_RTX_RM\", \"X_MTX_MB\", \"X_FMX_MM\", \"X_MBX_RT\", \"X_MTX_RT\", \"X_FTX_MT\", \"X_MMX_RT\", \"X_MBX_RM\", \"X_RTX_RM\", \"X_MMX_MB\", \"X_MMX_RM\", \"X_MTX_MB\", \"X_MTX_MM\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.5261743068695068, 0.5378136694431305, 0.5776149064302445, 0.6039110660552979, 0.6086981356143951, 0.6581975162029267, 0.6639798164367676, 0.6646197676658631, 0.6933618664741517, 0.6980075359344482, 0.6985749959945678, 0.6986924290657044, 0.701207572221756, 0.7160346567630768, 0.7389368057250977, 0.7398850679397583, 0.7535640805959701, 0.7545866787433624, 0.760698202252388, 0.7629807949066162, 0.7672264695167541, 0.773972886800766, 0.7742363810539246, 0.7886802971363067, 0.7925959706306458, 0.8001804411411285, 0.8004172056913376, 0.8004257142543793, 0.8010396420955658, 0.8012693703174592, 0.8136477887630462, 0.8197830617427826, 0.8200337409973144, 0.8234956443309784, 0.8246380746364593, 0.8300277829170227, 0.8309011310338974, 0.8361704468727111, 0.8451085209846496, 0.8567206025123596, 0.865886652469635, 0.8664347946643829, 0.8854544997215271, 0.887170821428299, 0.8909062206745147, 0.8920376002788544, 0.8971044421195984, 0.8973917067050934, 0.8983653247356415, 0.9001662850379943, 0.904131805896759, 0.9096692442893982, 0.913791024684906, 0.9167017221450806, 0.9182459473609924, 0.9207487881183625, 0.9332331717014313, 0.939322966337204, 0.9432799518108368, 0.9548518240451813, 0.9555743575096131, 0.960929948091507, 0.9616184711456299, 0.969048821926117, 0.9903261005878449, 0.992265808582306, 1.001711755990982, 1.008250743150711, 1.0126550197601318, 1.021770864725113, 1.0284330368041992, 1.037264096736908, 1.0415588974952699, 1.0458348989486694, 1.0496914982795715, 1.0535849690437318, 1.0548807740211488, 1.0622965753078462, 1.062389862537384, 1.0645675480365753, 1.0719576001167297, 1.0779197812080383, 1.0807103216648102, 1.081166833639145, 1.0813504576683044, 1.0833235681056976, 1.083646035194397, 1.0857297897338867, 1.090717500448227, 1.0954893052577972, 1.102862399816513, 1.1040704488754272, 1.1115025222301482, 1.1115440607070923, 1.1166556537151338, 1.1190181851387024, 1.1279848396778107, 1.1319179296493531, 1.1324189841747283, 1.1328436851501464, 1.1349341690540313, 1.1357545971870422, 1.1373327672481537, 1.1400836765766145, 1.1511807918548584, 1.1703509628772735, 1.1725226104259492, 1.1749932765960693, 1.188001698255539, 1.1945786774158478, 1.206766664981842, 1.226647013425827, 1.2299752354621887, 1.2869935929775238, 1.2904144644737243, 1.291906201839447, 1.3118329405784608, 1.3313405394554139, 1.335911786556244, 1.3519313216209412, 1.3536684572696687, 1.361753559112549, 1.3728007912635802, 1.3763815641403199, 1.3948794245719909, 1.4001413345336915, 1.4074299454689025, 1.4085989713668823, 1.4106548428535461, 1.4132540464401244, 1.4324962735176086, 1.4334052562713624, 1.4509004473686218, 1.4520147442817688, 1.4615373849868774, 1.4616323113441467, 1.4953739285469054, 1.4977857232093812, 1.5049411416053773, 1.5275405526161194, 1.5319352626800538, 1.5550794839859008, 1.5658524751663208, 1.5792065382003784, 1.5896948218345641, 1.5928719520568848, 1.613330078125, 1.6232594847679138, 1.6359298825263977, 1.641461229324341, 1.6500413417816162, 1.6574836015701293, 1.6610089540481567, 1.6931620240211487, 1.6996631026268005, 1.7023163795471192, 1.763371968269348, 1.7755598306655884, 1.8140723347663879, 1.8213908672332764, 1.8705768227577209, 1.9010727643966674, 1.9056068658828735, 1.9192139744758605, 1.944037663936615, 1.9615407586097717, 1.978384506702423, 1.9838356494903564], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4af31b9d-9028-41a5-afa0-cc47d3dfe5e2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWIFrrJ5zcyM"
      },
      "source": [
        "# define baseline model 3\n",
        "# create model\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(21, 1)))\n",
        "model3.add(MaxPooling1D(pool_size=2))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(50, activation='relu'))\n",
        "model3.add(Dense(1))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "model3.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "#model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xeRL4LyzcyN"
      },
      "source": [
        "\n",
        "def datageneratorMF3(X_in1, X_in2, X_in3, Y_in):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  X_in = np.concatenate((X_in1, X_in2, X_in3), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "\n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], 1))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], 1))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], 1))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjYuNqU0zcyN"
      },
      "source": [
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMS(X_MT, X_MM, y_RM)\n",
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(X_MT, y_RM)\n",
        "#X_train_Input[0:5]\n",
        "#X_train_Input.shape\n",
        "#y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUNUAuKKzcyN"
      },
      "source": [
        "def evaldataMF3(X_in1, X_in2, X_in3, Y_in, traindata1, traindata2, traindata3, testdata):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMF3(X_in1, X_in2, X_in3, Y_in)\n",
        "  \n",
        "  history = model3.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model3.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model3.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # PLOTS LOSS VS EPOCH\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  # print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  # print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # PLOTS Y ORIGINAL VS PREDICTED\n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata1)+ str(traindata2)  + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnx0EbtLULQv",
        "outputId": "840e2684-b074-4aa3-84cd-3681b9f39de5"
      },
      "source": [
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "\n",
        "model3.save_weights('model3.h5')\n",
        "\n",
        "my_dictMF3 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), 3):\n",
        "  kX1, kX2, kX3 = combo[0][0], combo[1][0], combo[2][0]\n",
        "  vX1, vX2, vX3 = combo[0][1], combo[1][1], combo[2][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF3 = evaldataMF3(vX1, vX2, vX3, vy, kX1, kX2, kX3, ky)\n",
        "      TestLossTotal = resultsMF3[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF3[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF3[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}_{kX2}_{kX3} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF3[\"DATA_X\"].append(kX1 + kX2 + kX3)\n",
        "    my_dictMF3[\"DATA_y\"].append(ky)\n",
        "    my_dictMF3[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model3.load_weights('model3.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8439918458461761, and Avg Validation Loss (mae) is 0.8905733585357666\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8809112727642059\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8905903398990631, and Avg Validation Loss (mae) is 0.9064926862716675\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9094315767288208\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.498454487323761, and Avg Validation Loss (mae) is 1.4499109745025636\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4146369218826294\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2887068152427674, and Avg Validation Loss (mae) is 1.1283807635307312\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1367985427379608\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9844022452831268, and Avg Validation Loss (mae) is 1.0903772830963134\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0539177179336547\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1621453940868378, and Avg Validation Loss (mae) is 1.177806979417801\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2176247000694276\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7254583895206451, and Avg Validation Loss (mae) is 0.8090100735425949\n",
            "After 10 runs; Avg Test Loss (mae) is 0.80575612783432\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4057013511657714, and Avg Validation Loss (mae) is 1.441427218914032\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3975602030754088\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0388219952583313, and Avg Validation Loss (mae) is 1.2393139839172362\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2007325351238252\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8744056940078735, and Avg Validation Loss (mae) is 0.9716428041458129\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9642712056636811\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.130885624885559, and Avg Validation Loss (mae) is 1.1803382396698\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1780955791473389\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5715388268232345, and Avg Validation Loss (mae) is 0.622262641787529\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6252229958772659\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.471097183227539, and Avg Validation Loss (mae) is 1.547205352783203\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5417815685272216\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2202089548110961, and Avg Validation Loss (mae) is 1.246250069141388\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2501118063926697\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8373961210250854, and Avg Validation Loss (mae) is 0.7692434668540955\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7658763647079467\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3891183733940125, and Avg Validation Loss (mae) is 1.499728000164032\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4959411025047302\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9348151981830597, and Avg Validation Loss (mae) is 1.0080981254577637\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9799071907997131\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0452158570289611, and Avg Validation Loss (mae) is 1.2403916120529175\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2251070320606232\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7141889989376068, and Avg Validation Loss (mae) is 0.7502176612615585\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7492977678775787\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.660597974061966, and Avg Validation Loss (mae) is 0.7288507729768753\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7352758914232254\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.420617437362671, and Avg Validation Loss (mae) is 1.4672378778457642\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4543172240257263\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8122600972652435, and Avg Validation Loss (mae) is 0.8554180026054382\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8671304523944855\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0007320761680603, and Avg Validation Loss (mae) is 0.9796010196208954\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9599252939224243\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7937384903430938, and Avg Validation Loss (mae) is 0.8425170958042145\n",
            "After 10 runs; Avg Test Loss (mae) is 0.852572226524353\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6638727366924286, and Avg Validation Loss (mae) is 0.7927383750677108\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8001748353242875\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2188549041748047, and Avg Validation Loss (mae) is 1.2137017011642457\n",
            "After 10 runs; Avg Test Loss (mae) is 1.191258692741394\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8666915416717529, and Avg Validation Loss (mae) is 0.8299670696258545\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8428997457027435\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9810628771781922, and Avg Validation Loss (mae) is 0.9191516101360321\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9385623633861542\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1381899416446686, and Avg Validation Loss (mae) is 1.0664420545101165\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0606616377830504\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8626913666725159, and Avg Validation Loss (mae) is 0.9949422180652618\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9744688153266907\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8351097702980042, and Avg Validation Loss (mae) is 0.6698903501033783\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6773953020572663\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7037821471691131, and Avg Validation Loss (mae) is 0.7513145923614502\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7492991149425506\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3111115932464599, and Avg Validation Loss (mae) is 1.3209250330924989\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3031520605087281\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8656398773193359, and Avg Validation Loss (mae) is 0.9286645650863647\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9115018427371979\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7629969716072083, and Avg Validation Loss (mae) is 0.794647616147995\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8038605093955994\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9341000318527222, and Avg Validation Loss (mae) is 0.9097791731357574\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8943145573139191\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7551648318767548, and Avg Validation Loss (mae) is 0.6929200232028961\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7057172656059265\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4212314248085023, and Avg Validation Loss (mae) is 1.3828104496002198\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3664314508438111\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3524832487106324, and Avg Validation Loss (mae) is 1.1899022698402404\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1808549404144286\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9467927694320679, and Avg Validation Loss (mae) is 0.8595892727375031\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8510888576507568\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9346317112445831, and Avg Validation Loss (mae) is 0.9047887682914734\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8823894083499908\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6963458776473999, and Avg Validation Loss (mae) is 0.8027712583541871\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7962348163127899\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7569251000881195, and Avg Validation Loss (mae) is 0.9943027496337891\n",
            "After 10 runs; Avg Test Loss (mae) is 1.015767228603363\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6793120861053467, and Avg Validation Loss (mae) is 0.6926444083452225\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6805056035518646\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.647341501712799, and Avg Validation Loss (mae) is 0.7361302614212036\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7327842772006988\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.803855562210083, and Avg Validation Loss (mae) is 0.78491712808609\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7833171129226685\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5686429470777512, and Avg Validation Loss (mae) is 0.7088121324777603\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7166061908006668\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7324223458766937, and Avg Validation Loss (mae) is 0.8392781138420105\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8408981144428254\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7726242363452911, and Avg Validation Loss (mae) is 0.6463779985904694\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6524971723556519\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6885478854179382, and Avg Validation Loss (mae) is 0.6078059554100037\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6005210638046264\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7232231557369232, and Avg Validation Loss (mae) is 0.7357305824756623\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7337491273880005\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.633344316482544, and Avg Validation Loss (mae) is 0.6460575044155121\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6472476184368133\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7830226004123688, and Avg Validation Loss (mae) is 0.6778192818164825\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6894276738166809\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.121976101398468, and Avg Validation Loss (mae) is 1.0518315017223359\n",
            "After 10 runs; Avg Test Loss (mae) is 1.042342060804367\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8237923562526703, and Avg Validation Loss (mae) is 0.8520499050617218\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8438287019729614\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7750332295894623, and Avg Validation Loss (mae) is 0.7843674659729004\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7846168398857116\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0966983020305634, and Avg Validation Loss (mae) is 1.0533097624778747\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0657733380794525\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4771594166755677, and Avg Validation Loss (mae) is 1.4829489707946777\n",
            "After 10 runs; Avg Test Loss (mae) is 1.488307273387909\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.149569296836853, and Avg Validation Loss (mae) is 1.1525127649307252\n",
            "After 10 runs; Avg Test Loss (mae) is 1.139777421951294\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8723493814468384, and Avg Validation Loss (mae) is 0.8811283111572266\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8596225321292877\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7695293605327607, and Avg Validation Loss (mae) is 0.8311750948429107\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8547629058361054\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9582550883293152, and Avg Validation Loss (mae) is 0.8544423580169678\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8556057989597321\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6934854209423065, and Avg Validation Loss (mae) is 0.6424416780471802\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6358593374490737\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5218506157398224, and Avg Validation Loss (mae) is 0.5031656682491302\n",
            "After 10 runs; Avg Test Loss (mae) is 0.48987704515457153\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6691482841968537, and Avg Validation Loss (mae) is 0.8287471860647202\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8375940293073654\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7734145581722259, and Avg Validation Loss (mae) is 0.8826194047927857\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9033761858940125\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0694309532642365, and Avg Validation Loss (mae) is 1.0053774654865264\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9868491172790528\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8110624790191651, and Avg Validation Loss (mae) is 1.0586918264627456\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0775883615016937\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7204552888870239, and Avg Validation Loss (mae) is 0.7694059371948242\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7860630363225937\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6318253517150879, and Avg Validation Loss (mae) is 0.5553746521472931\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5580693989992142\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6918777644634246, and Avg Validation Loss (mae) is 0.7848459780216217\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7948333323001862\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.095543622970581, and Avg Validation Loss (mae) is 1.171647810935974\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1764793276786805\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8253020823001862, and Avg Validation Loss (mae) is 0.7027456641197205\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6794335961341857\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.133668178319931, and Avg Validation Loss (mae) is 1.2718890249729156\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2785745501518249\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7930332839488983, and Avg Validation Loss (mae) is 0.8488775968551636\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8531022071838379\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0852767467498778, and Avg Validation Loss (mae) is 1.1606478810310363\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1766372561454772\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3529056191444397, and Avg Validation Loss (mae) is 1.3717301964759827\n",
            "After 10 runs; Avg Test Loss (mae) is 1.353888201713562\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8157290041446685, and Avg Validation Loss (mae) is 0.779804652929306\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7856266736984253\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7284120082855224, and Avg Validation Loss (mae) is 0.7587217867374421\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7692613661289215\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7712080717086792, and Avg Validation Loss (mae) is 0.6960271775722504\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7042771995067596\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0872043430805207, and Avg Validation Loss (mae) is 1.1147328555583953\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1181313931941985\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3240057229995728, and Avg Validation Loss (mae) is 1.3735522985458375\n",
            "After 10 runs; Avg Test Loss (mae) is 1.359982180595398\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7102816998958588, and Avg Validation Loss (mae) is 0.6911439299583435\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7329189360141755\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8649236977100372, and Avg Validation Loss (mae) is 1.0094528138637542\n",
            "After 10 runs; Avg Test Loss (mae) is 1.020096105337143\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6533421814441681, and Avg Validation Loss (mae) is 0.7421121001243591\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7300049632787704\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9159019708633422, and Avg Validation Loss (mae) is 1.1628848135471344\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1681198358535767\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3171586036682128, and Avg Validation Loss (mae) is 1.332252299785614\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3283859729766845\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7513551652431488, and Avg Validation Loss (mae) is 0.8990219950675964\n",
            "After 10 runs; Avg Test Loss (mae) is 0.899289870262146\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.031198924779892, and Avg Validation Loss (mae) is 1.1500881731510162\n",
            "After 10 runs; Avg Test Loss (mae) is 1.186908060312271\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8069640338420868, and Avg Validation Loss (mae) is 0.707477456331253\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7025822341442108\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0016230642795563, and Avg Validation Loss (mae) is 1.0732187390327455\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0626483619213105\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5139927506446837, and Avg Validation Loss (mae) is 1.5030994296073914\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5288603901863098\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9893787741661072, and Avg Validation Loss (mae) is 1.0067009687423707\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0045916616916657\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0665673673152924, and Avg Validation Loss (mae) is 1.0393365144729614\n",
            "After 10 runs; Avg Test Loss (mae) is 1.031051117181778\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6384767174720765, and Avg Validation Loss (mae) is 0.6381845027208328\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6381431221961975\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9111069917678833, and Avg Validation Loss (mae) is 0.9123786449432373\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9371399283409119\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3645161628723144, and Avg Validation Loss (mae) is 1.2850278973579408\n",
            "After 10 runs; Avg Test Loss (mae) is 1.349532914161682\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0188103437423706, and Avg Validation Loss (mae) is 1.1820249676704406\n",
            "After 10 runs; Avg Test Loss (mae) is 1.186560755968094\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0267696022987365, and Avg Validation Loss (mae) is 1.182277101278305\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1636250078678132\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.755557519197464, and Avg Validation Loss (mae) is 0.7507327795028687\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7671345293521881\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9768341243267059, and Avg Validation Loss (mae) is 0.8972122728824615\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8995300233364105\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4830869674682616, and Avg Validation Loss (mae) is 1.590181565284729\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6074357748031616\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.898845762014389, and Avg Validation Loss (mae) is 0.858200854063034\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8702571988105774\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9805006384849548, and Avg Validation Loss (mae) is 1.0189458012580872\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0228426992893218\n",
            "kx1 = X_FT, kx2 = X_RM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8845726251602173, and Avg Validation Loss (mae) is 1.0058735251426696\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0098844766616821\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.533870553970337, and Avg Validation Loss (mae) is 1.652371621131897\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6711787700653076\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7485364854335785, and Avg Validation Loss (mae) is 0.895721435546875\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9131912231445313\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3883081555366517, and Avg Validation Loss (mae) is 1.4709148287773133\n",
            "After 10 runs; Avg Test Loss (mae) is 1.450066590309143\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9432923674583436, and Avg Validation Loss (mae) is 1.0002369940280915\n",
            "After 10 runs; Avg Test Loss (mae) is 1.009316325187683\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9344416856765747, and Avg Validation Loss (mae) is 1.0680492520332336\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0569895088672638\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4876407384872437, and Avg Validation Loss (mae) is 1.5647563099861146\n",
            "After 10 runs; Avg Test Loss (mae) is 1.555060911178589\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.612198007106781, and Avg Validation Loss (mae) is 0.518487548828125\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5219042509794235\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3771995425224304, and Avg Validation Loss (mae) is 1.3280719637870788\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2973100066184997\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1645566523075104, and Avg Validation Loss (mae) is 1.1012474238872527\n",
            "After 10 runs; Avg Test Loss (mae) is 1.126629936695099\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9499052345752717, and Avg Validation Loss (mae) is 1.0725091338157653\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0861854374408721\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5499261140823364, and Avg Validation Loss (mae) is 1.4198575615882874\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4238465666770934\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6881110012531281, and Avg Validation Loss (mae) is 0.6679247558116913\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6744631499052047\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7641267240047455, and Avg Validation Loss (mae) is 0.8391906976699829\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8392415225505829\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7038623124361039, and Avg Validation Loss (mae) is 0.721397590637207\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7296503037214279\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6711476504802704, and Avg Validation Loss (mae) is 0.8378652662038804\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8389055043458938\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5864641070365906, and Avg Validation Loss (mae) is 1.7524879097938537\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7179574489593505\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5475474447011948, and Avg Validation Loss (mae) is 0.6081213653087616\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6107938617467881\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7508116662502289, and Avg Validation Loss (mae) is 0.8949646532535553\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8860688805580139\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7489375352859498, and Avg Validation Loss (mae) is 0.8853414416313171\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8944894850254059\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6337796688079834, and Avg Validation Loss (mae) is 0.659176105260849\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6615889370441437\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5544456958770752, and Avg Validation Loss (mae) is 1.4852051854133606\n",
            "After 10 runs; Avg Test Loss (mae) is 1.458773136138916\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6281074404716491, and Avg Validation Loss (mae) is 0.6090926676988602\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6148225545883179\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7943391084671021, and Avg Validation Loss (mae) is 0.8381827414035797\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8104053676128388\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0501993775367737, and Avg Validation Loss (mae) is 1.1838673055171967\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1861085891723633\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8951397895812988, and Avg Validation Loss (mae) is 1.004668229818344\n",
            "After 10 runs; Avg Test Loss (mae) is 1.009265238046646\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4950844764709472, and Avg Validation Loss (mae) is 1.5069830179214478\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4867220878601075\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.121898078918457, and Avg Validation Loss (mae) is 1.1852408170700073\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1405730545520782\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4634472727775574, and Avg Validation Loss (mae) is 1.5310852766036986\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5546702861785888\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.100217628479004, and Avg Validation Loss (mae) is 1.148867231607437\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1492115139961243\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8069226443767548, and Avg Validation Loss (mae) is 1.015073651075363\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0256270945072175\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6550570726394653, and Avg Validation Loss (mae) is 1.5424792170524597\n",
            "After 10 runs; Avg Test Loss (mae) is 1.522441565990448\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0375893533229827, and Avg Validation Loss (mae) is 0.9387462556362152\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9175775408744812\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7263572156429291, and Avg Validation Loss (mae) is 0.8971731513738632\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9029195725917816\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6376429349184036, and Avg Validation Loss (mae) is 0.527774554491043\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5328955680131913\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6570945501327514, and Avg Validation Loss (mae) is 0.6190581351518631\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6195835143327713\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5648488640785216, and Avg Validation Loss (mae) is 1.8213647961616517\n",
            "After 10 runs; Avg Test Loss (mae) is 1.81847425699234\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0410789370536804, and Avg Validation Loss (mae) is 1.0499886751174927\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0515564858913422\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7161592900753021, and Avg Validation Loss (mae) is 0.7996225118637085\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7899444013833999\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.725091952085495, and Avg Validation Loss (mae) is 0.7460423111915588\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7367230862379074\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7000585079193116, and Avg Validation Loss (mae) is 0.662650391459465\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6708262920379638\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.376550543308258, and Avg Validation Loss (mae) is 1.3779135465621948\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3620210886001587\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1294697165489196, and Avg Validation Loss (mae) is 1.2072762548923492\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2454916477203368\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7213506639003754, and Avg Validation Loss (mae) is 0.7015823751688004\n",
            "After 10 runs; Avg Test Loss (mae) is 0.705900713801384\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1052764356136322, and Avg Validation Loss (mae) is 1.1184994757175446\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1813489496707916\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7753138065338134, and Avg Validation Loss (mae) is 0.8989990651607513\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8907848596572876\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5486846089363098, and Avg Validation Loss (mae) is 1.5352839350700378\n",
            "After 10 runs; Avg Test Loss (mae) is 1.54054137468338\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.137682443857193, and Avg Validation Loss (mae) is 1.2157679617404937\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2351956367492676\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.705510425567627, and Avg Validation Loss (mae) is 0.7197876155376435\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7344087481498718\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6140401601791382, and Avg Validation Loss (mae) is 0.7100753933191299\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7139562517404556\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.641486519575119, and Avg Validation Loss (mae) is 0.6160765290260315\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6193124920129776\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6158717155456543, and Avg Validation Loss (mae) is 1.788749623298645\n",
            "After 10 runs; Avg Test Loss (mae) is 1.858415699005127\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1769030451774598, and Avg Validation Loss (mae) is 1.147387546300888\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1517080962657928\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6438425600528717, and Avg Validation Loss (mae) is 0.7904411792755127\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7715728044509887\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9309117734432221, and Avg Validation Loss (mae) is 0.8423024594783783\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8698420941829681\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.644596540927887, and Avg Validation Loss (mae) is 0.6330551624298095\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6364879667758941\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.610030746459961, and Avg Validation Loss (mae) is 1.6610451817512513\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6404146552085876\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.204525852203369, and Avg Validation Loss (mae) is 1.2849501371383667\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2954607725143432\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6687093317508698, and Avg Validation Loss (mae) is 0.6720050483942032\n",
            "After 10 runs; Avg Test Loss (mae) is 0.658775681257248\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0465737342834474, and Avg Validation Loss (mae) is 1.1424121975898742\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0946678638458252\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8199812531471252, and Avg Validation Loss (mae) is 0.9264241397380829\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9394492506980896\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.610430932044983, and Avg Validation Loss (mae) is 1.4415062069892883\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4215776205062867\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3822718977928161, and Avg Validation Loss (mae) is 1.3698944091796874\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3405157208442688\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7626875400543213, and Avg Validation Loss (mae) is 0.862434321641922\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8355158269405365\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0173066318035127, and Avg Validation Loss (mae) is 0.9550267696380615\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9496770381927491\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5969836056232453, and Avg Validation Loss (mae) is 0.7770998686552048\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7886918157339096\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6263458967208861, and Avg Validation Loss (mae) is 1.9589346289634704\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9589437246322632\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2931688904762269, and Avg Validation Loss (mae) is 1.3051066517829895\n",
            "After 10 runs; Avg Test Loss (mae) is 1.309616756439209\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9412961602210999, and Avg Validation Loss (mae) is 1.0173424184322357\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0062951624393464\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0348955154418946, and Avg Validation Loss (mae) is 1.0185182511806488\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0149496376514435\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6787616372108459, and Avg Validation Loss (mae) is 0.6932447671890258\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6934183895587921\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5612990856170654, and Avg Validation Loss (mae) is 1.5241342663764954\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5322506427764893\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.362715446949005, and Avg Validation Loss (mae) is 1.3449205040931702\n",
            "After 10 runs; Avg Test Loss (mae) is 1.331775426864624\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8095341086387634, and Avg Validation Loss (mae) is 0.8179752707481385\n",
            "After 10 runs; Avg Test Loss (mae) is 0.812739509344101\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9614823222160339, and Avg Validation Loss (mae) is 1.0330314993858338\n",
            "After 10 runs; Avg Test Loss (mae) is 1.010002326965332\n",
            "kx1 = X_FM, kx2 = X_RM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8058092713356018, and Avg Validation Loss (mae) is 0.8333773910999298\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8275168597698211\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9226847410202026, and Avg Validation Loss (mae) is 2.0182974338531494\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9715250372886657\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0283690571784974, and Avg Validation Loss (mae) is 1.028192138671875\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0395734369754792\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.518180787563324, and Avg Validation Loss (mae) is 1.5509369015693664\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5536993503570558\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.980673736333847, and Avg Validation Loss (mae) is 1.2012922704219817\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1770451605319976\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9390897989273072, and Avg Validation Loss (mae) is 0.8590625762939453\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8394899427890777\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9778353810310363, and Avg Validation Loss (mae) is 2.022988224029541\n",
            "After 10 runs; Avg Test Loss (mae) is 2.05085608959198\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9220520615577698, and Avg Validation Loss (mae) is 0.8980519831180572\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8910372853279114\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7361974596977234, and Avg Validation Loss (mae) is 0.7765924155712127\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7952092111110687\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5346685916185379, and Avg Validation Loss (mae) is 0.5411816954612731\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5480785936117172\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7109087705612183, and Avg Validation Loss (mae) is 0.7708406209945678\n",
            "After 10 runs; Avg Test Loss (mae) is 0.778819328546524\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8805717825889587, and Avg Validation Loss (mae) is 1.9344725608825684\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9361400723457336\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9495270550251007, and Avg Validation Loss (mae) is 0.9081822812557221\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8978036880493164\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7730324149131775, and Avg Validation Loss (mae) is 0.7928578078746795\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7748239874839783\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7295881271362304, and Avg Validation Loss (mae) is 0.8438893020153045\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8342605173587799\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7206100702285767, and Avg Validation Loss (mae) is 0.8851980686187744\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8702048122882843\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6784653067588806, and Avg Validation Loss (mae) is 1.5586092948913575\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5534899711608887\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8039319574832916, and Avg Validation Loss (mae) is 0.8283881783485413\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8016478955745697\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6783145427703857, and Avg Validation Loss (mae) is 0.7049685031175613\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7048095881938934\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9350102961063385, and Avg Validation Loss (mae) is 0.9269835591316223\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9346120893955231\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7137417614459991, and Avg Validation Loss (mae) is 0.681733250617981\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6653080195188522\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.807255208492279, and Avg Validation Loss (mae) is 1.7575917482376098\n",
            "After 10 runs; Avg Test Loss (mae) is 1.771574115753174\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9812567234039307, and Avg Validation Loss (mae) is 1.1034003376960755\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0857521831989287\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6801091432571411, and Avg Validation Loss (mae) is 0.6406645894050598\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6528412789106369\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.707633551955223, and Avg Validation Loss (mae) is 0.8484424442052841\n",
            "After 10 runs; Avg Test Loss (mae) is 0.820217627286911\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7897846400737762, and Avg Validation Loss (mae) is 0.7805761098861694\n",
            "After 10 runs; Avg Test Loss (mae) is 0.780651432275772\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9308817982673645, and Avg Validation Loss (mae) is 1.789454710483551\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8465567111968995\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0195300579071045, and Avg Validation Loss (mae) is 1.0703897178173065\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0573965072631837\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5636888086795807, and Avg Validation Loss (mae) is 0.6050113022327424\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5943840026855469\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7343375623226166, and Avg Validation Loss (mae) is 0.8211941659450531\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8216029524803161\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7471967756748199, and Avg Validation Loss (mae) is 0.8587872684001923\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8395727336406708\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8118193864822387, and Avg Validation Loss (mae) is 1.843728280067444\n",
            "After 10 runs; Avg Test Loss (mae) is 1.901343083381653\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8697014331817627, and Avg Validation Loss (mae) is 0.9911422550678253\n",
            "After 10 runs; Avg Test Loss (mae) is 0.993206799030304\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6924924552440643, and Avg Validation Loss (mae) is 0.6849602818489074\n",
            "After 10 runs; Avg Test Loss (mae) is 0.697810709476471\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.982351940870285, and Avg Validation Loss (mae) is 0.8811417996883393\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8739601373672485\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.837238198518753, and Avg Validation Loss (mae) is 0.8101943373680115\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7954454958438874\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5462803602218629, and Avg Validation Loss (mae) is 1.4678233861923218\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4969469547271728\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9506755530834198, and Avg Validation Loss (mae) is 1.181960016489029\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1789022147655488\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5529546141624451, and Avg Validation Loss (mae) is 0.541180682182312\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5431760519742965\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7716791749000549, and Avg Validation Loss (mae) is 0.7880420625209809\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7650764882564545\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6928693115711212, and Avg Validation Loss (mae) is 0.703885966539383\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7102600932121277\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6431108236312866, and Avg Validation Loss (mae) is 1.5277310729026794\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5568724155426026\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.844759863615036, and Avg Validation Loss (mae) is 0.919538700580597\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9141891539096832\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6996100604534149, and Avg Validation Loss (mae) is 0.6935986846685409\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7110741764307023\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7815155327320099, and Avg Validation Loss (mae) is 0.7983615577220917\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8033157527446747\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6566738545894623, and Avg Validation Loss (mae) is 0.7491622030735016\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7388627588748932\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6638553023338318, and Avg Validation Loss (mae) is 1.6643167734146118\n",
            "After 10 runs; Avg Test Loss (mae) is 1.663049054145813\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7932657241821289, and Avg Validation Loss (mae) is 0.8771896719932556\n",
            "After 10 runs; Avg Test Loss (mae) is 0.861609423160553\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6010076314210892, and Avg Validation Loss (mae) is 0.5859808623790741\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5782956898212432\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7389712989330292, and Avg Validation Loss (mae) is 0.7579510152339936\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7497205555438995\n",
            "kx1 = X_MT, kx2 = X_RM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6907341718673706, and Avg Validation Loss (mae) is 0.9488995373249054\n",
            "After 10 runs; Avg Test Loss (mae) is 0.955156335234642\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8856784224510192, and Avg Validation Loss (mae) is 1.8946411252021789\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8992505192756652\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9388852298259736, and Avg Validation Loss (mae) is 0.9483635365962982\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9434275209903717\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9132972836494446, and Avg Validation Loss (mae) is 0.9733736217021942\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9931180357933045\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.522186890244484, and Avg Validation Loss (mae) is 0.5918604791164398\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5960397303104401\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8272524118423462, and Avg Validation Loss (mae) is 0.9340619385242462\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9500215947628021\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9254266381263734, and Avg Validation Loss (mae) is 1.8451525449752808\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8339879631996154\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.02825688123703, and Avg Validation Loss (mae) is 1.0565224468708039\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0697776317596435\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.021996521949768, and Avg Validation Loss (mae) is 0.9955520331859589\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0020512521266938\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7763310611248017, and Avg Validation Loss (mae) is 0.7239509701728821\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7276834487915039\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.79688281416893, and Avg Validation Loss (mae) is 0.8648661673069\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8390851497650147\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6666576385498046, and Avg Validation Loss (mae) is 1.724691104888916\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7371329426765443\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.816386365890503, and Avg Validation Loss (mae) is 0.8537569582462311\n",
            "After 10 runs; Avg Test Loss (mae) is 0.843631488084793\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.237829613685608, and Avg Validation Loss (mae) is 1.2049538552761079\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2296674013137818\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0309573352336883, and Avg Validation Loss (mae) is 1.0610441267490387\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0521787762641908\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8039838612079621, and Avg Validation Loss (mae) is 0.8145678699016571\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8213133752346039\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8320939898490907, and Avg Validation Loss (mae) is 1.8348327279090881\n",
            "After 10 runs; Avg Test Loss (mae) is 1.850548470020294\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9190498769283295, and Avg Validation Loss (mae) is 1.0952519297599792\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1053870737552642\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9417123377323151, and Avg Validation Loss (mae) is 0.9614395380020142\n",
            "After 10 runs; Avg Test Loss (mae) is 0.96457639336586\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7201491177082062, and Avg Validation Loss (mae) is 0.7419635742902756\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7362071454524994\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7442082941532135, and Avg Validation Loss (mae) is 0.6564702212810516\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6695847332477569\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.683936870098114, and Avg Validation Loss (mae) is 1.594653272628784\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6345489859580993\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8432649374008179, and Avg Validation Loss (mae) is 0.689863657951355\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6749199748039245\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.005205935239792, and Avg Validation Loss (mae) is 1.0172027826309205\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0301107227802277\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6933809638023376, and Avg Validation Loss (mae) is 0.6974494367837906\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6924344390630722\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5005680620670319, and Avg Validation Loss (mae) is 0.5408861815929413\n",
            "After 10 runs; Avg Test Loss (mae) is 0.536593559384346\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.618707299232483, and Avg Validation Loss (mae) is 1.5435969829559326\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5412175893783568\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8429771661758423, and Avg Validation Loss (mae) is 1.0942819714546204\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0672823071479798\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.00765221118927, and Avg Validation Loss (mae) is 1.1140174508094787\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1149709165096282\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7780895113945008, and Avg Validation Loss (mae) is 0.721835657954216\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7153297334909439\n",
            "kx1 = X_MM, kx2 = X_RM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7247503399848938, and Avg Validation Loss (mae) is 0.6077221482992172\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6060028195381164\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8789237141609192, and Avg Validation Loss (mae) is 1.8859928727149964\n",
            "After 10 runs; Avg Test Loss (mae) is 1.836331021785736\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1300913631916045, and Avg Validation Loss (mae) is 1.1721971392631532\n",
            "After 10 runs; Avg Test Loss (mae) is 1.198355758190155\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3197015523910522, and Avg Validation Loss (mae) is 1.2614565372467041\n",
            "After 10 runs; Avg Test Loss (mae) is 1.252750027179718\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6096730500459671, and Avg Validation Loss (mae) is 0.6938922524452209\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7106129884719848\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6800793766975403, and Avg Validation Loss (mae) is 0.7908946931362152\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8055537283420563\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6649017453193664, and Avg Validation Loss (mae) is 1.67341810464859\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6454256892204284\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9529553830623627, and Avg Validation Loss (mae) is 0.9630773663520813\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9728973209857941\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2172903776168824, and Avg Validation Loss (mae) is 1.1690211713314056\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1681439459323884\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8008922517299653, and Avg Validation Loss (mae) is 0.7243778467178345\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7153578221797943\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6222914040088654, and Avg Validation Loss (mae) is 0.5760435670614242\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5748584300279618\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6648754239082337, and Avg Validation Loss (mae) is 1.6419647693634034\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6355172514915466\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0014934599399568, and Avg Validation Loss (mae) is 0.943540358543396\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9549917161464692\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3391663193702699, and Avg Validation Loss (mae) is 1.3139050722122192\n",
            "After 10 runs; Avg Test Loss (mae) is 1.297171413898468\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7638890385627747, and Avg Validation Loss (mae) is 0.7337924540042877\n",
            "After 10 runs; Avg Test Loss (mae) is 0.728352153301239\n",
            "kx1 = X_MB, kx2 = X_RM, kx3 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8401313781738281, and Avg Validation Loss (mae) is 0.8733475565910339\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8761255323886872\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6747604727745056, and Avg Validation Loss (mae) is 1.618015956878662\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6210727572441102\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9880089044570923, and Avg Validation Loss (mae) is 1.0769387423992156\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0889596223831177\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4501282811164855, and Avg Validation Loss (mae) is 1.8386289119720458\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8439070463180542\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8901683270931244, and Avg Validation Loss (mae) is 0.7900677025318146\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7431118667125702\n",
            "kx1 = X_RT, kx2 = X_RM, kx3 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9597873568534852, and Avg Validation Loss (mae) is 0.9547590553760529\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9699355900287628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5ltrKV-szcyO",
        "outputId": "04895b8e-595f-4cb6-8b7e-96606d39976e"
      },
      "source": [
        "CombResultsMF3 = pd.DataFrame.from_dict(my_dictMF3)\n",
        "print(CombResultsMF3.shape)\n",
        "CombResultsSortedMF3 = CombResultsMF3.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMF3.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(280, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA_X</th>\n",
              "      <th>DATA_y</th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>X_FTX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.489877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>X_FMX_MTX_MB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.521904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>X_FMX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.532896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>X_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.536594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>X_MTX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.543176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>X_MTX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.548079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>X_FTX_MMX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.558069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>X_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.574858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>X_MTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.578296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>X_MTX_MBX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.594384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>X_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.596040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>X_FTX_MTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.600521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>X_MMX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.606003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>X_FMX_MTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.610794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>X_FMX_MTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.614823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>X_FMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.619312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>X_FMX_MMX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.619584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>X_FTX_FMX_MB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.625223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>X_FTX_MMX_RT</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.635859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>X_FMX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.636488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>X_FTX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.638143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>X_FTX_MTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.647248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>X_FTX_MTX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.652497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>X_MTX_MBX_RT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.652841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>X_FMX_MBX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.658776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>X_FMX_MTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.661589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>X_MTX_MMX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.665308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>X_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.669585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>X_FMX_MMX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.670826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>X_FMX_MTX_RT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.674463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>X_MMX_RTX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.674920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>X_FTX_MTX_MM</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.677395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>X_FTX_MMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.679434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>X_FTX_MTX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.680506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>X_FTX_MTX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.689428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>X_MMX_RTX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.692434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>X_FMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.693418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>X_MTX_MBX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.697811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>X_FTX_MBX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.702582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>X_FTX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.704277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>X_MTX_MMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.704810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>X_FTX_MTX_MB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.705717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>X_FMX_MMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.705901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>X_MTX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.710260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>X_MBX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.710613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>X_MTX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.711074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>X_FMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.713956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>X_MMX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.715330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>X_MBX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.715358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>X_FTX_MTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.716606</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           DATA_X DATA_y  Test Loss\n",
              "63   X_FTX_MMX_RT   y_RM   0.489877\n",
              "111  X_FMX_MTX_MB   y_MM   0.521904\n",
              "138  X_FMX_MMX_RT   y_RM   0.532896\n",
              "254  X_MMX_RTX_RB   y_RM   0.536594\n",
              "217  X_MTX_RTX_RM   y_MM   0.543176\n",
              "188  X_MTX_MMX_RT   y_RM   0.548079\n",
              "69   X_FTX_MMX_RM   y_RB   0.558069\n",
              "269  X_MBX_RTX_RB   y_RM   0.574858\n",
              "227  X_MTX_RMX_RB   y_MM   0.578296\n",
              "207  X_MTX_MBX_RM   y_MM   0.594384\n",
              "233  X_MMX_MBX_RT   y_RM   0.596040\n",
              "49   X_FTX_MTX_RM   y_RB   0.600521\n",
              "259  X_MMX_RMX_RB   y_RT   0.606003\n",
              "121  X_FMX_MTX_RM   y_MM   0.610794\n",
              "126  X_FMX_MTX_RB   y_MM   0.614823\n",
              "154  X_FMX_MBX_RT   y_RB   0.619312\n",
              "139  X_FMX_MMX_RT   y_RB   0.619584\n",
              "11   X_FTX_FMX_MB   y_MM   0.625223\n",
              "62   X_FTX_MMX_RT   y_MB   0.635859\n",
              "159  X_FMX_MBX_RM   y_RB   0.636488\n",
              "94   X_FTX_RTX_RM   y_RB   0.638143\n",
              "51   X_FTX_MTX_RB   y_MM   0.647248\n",
              "48   X_FTX_MTX_RM   y_RT   0.652497\n",
              "202  X_MTX_MBX_RT   y_MM   0.652841\n",
              "162  X_FMX_MBX_RB   y_MM   0.658776\n",
              "124  X_FMX_MTX_RM   y_RB   0.661589\n",
              "199  X_MTX_MMX_RB   y_RM   0.665308\n",
              "249  X_MMX_RTX_RM   y_RB   0.669585\n",
              "144  X_FMX_MMX_RM   y_RB   0.670826\n",
              "116  X_FMX_MTX_RT   y_MM   0.674463\n",
              "251  X_MMX_RTX_RB   y_FM   0.674920\n",
              "30   X_FTX_MTX_MM   y_FM   0.677395\n",
              "72   X_FTX_MMX_RB   y_MB   0.679434\n",
              "43   X_FTX_MTX_RT   y_RM   0.680506\n",
              "52   X_FTX_MTX_RB   y_MB   0.689428\n",
              "253  X_MMX_RTX_RB   y_MB   0.692434\n",
              "174  X_FMX_RTX_RB   y_RM   0.693418\n",
              "212  X_MTX_MBX_RB   y_MM   0.697811\n",
              "89   X_FTX_MBX_RB   y_RM   0.702582\n",
              "79   X_FTX_MBX_RT   y_RB   0.704277\n",
              "197  X_MTX_MMX_RB   y_MB   0.704810\n",
              "36   X_FTX_MTX_MB   y_MM   0.705717\n",
              "147  X_FMX_MMX_RB   y_MB   0.705901\n",
              "219  X_MTX_RTX_RM   y_RB   0.710260\n",
              "263  X_MBX_RTX_RM   y_MM   0.710613\n",
              "222  X_MTX_RTX_RB   y_MM   0.711074\n",
              "153  X_FMX_MBX_RT   y_RM   0.713956\n",
              "258  X_MMX_RMX_RB   y_MB   0.715330\n",
              "268  X_MBX_RTX_RB   y_MM   0.715358\n",
              "46   X_FTX_MTX_RM   y_MM   0.716606"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RK_3NvelzcyO",
        "outputId": "b51aa01f-f55c-4627-bfaa-3225ee527c19"
      },
      "source": [
        "CombResultsSortedMFgrouped3 = CombResultsSortedMF3.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedMFgroupedsortedMF3 = CombResultsSortedMFgrouped3.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMFgroupedsortedMF3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RM</th>\n",
              "      <td>0.718768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RT</th>\n",
              "      <td>0.734740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RB</th>\n",
              "      <td>0.791319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RT</th>\n",
              "      <td>0.821536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RM</th>\n",
              "      <td>0.862389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MM</th>\n",
              "      <td>0.889042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RT</th>\n",
              "      <td>0.899084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RT</th>\n",
              "      <td>0.901221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RTX_RB</th>\n",
              "      <td>0.913722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RB</th>\n",
              "      <td>0.931974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RTX_RM</th>\n",
              "      <td>0.938872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RTX_RB</th>\n",
              "      <td>0.944863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RM</th>\n",
              "      <td>0.954180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RB</th>\n",
              "      <td>0.956485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RT</th>\n",
              "      <td>0.957938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_RMX_RB</th>\n",
              "      <td>0.961566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RT</th>\n",
              "      <td>0.968683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RM</th>\n",
              "      <td>0.986824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RM</th>\n",
              "      <td>0.992227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MB</th>\n",
              "      <td>0.999681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RB</th>\n",
              "      <td>1.001570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RMX_RB</th>\n",
              "      <td>1.008961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RT</th>\n",
              "      <td>1.012800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RM</th>\n",
              "      <td>1.013505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RTX_RB</th>\n",
              "      <td>1.015337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RB</th>\n",
              "      <td>1.015875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RT</th>\n",
              "      <td>1.022207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RM</th>\n",
              "      <td>1.031903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RT</th>\n",
              "      <td>1.037106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RB</th>\n",
              "      <td>1.052353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RTX_RM</th>\n",
              "      <td>1.053059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RB</th>\n",
              "      <td>1.057057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RM</th>\n",
              "      <td>1.057605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RM</th>\n",
              "      <td>1.062647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RTX_RM</th>\n",
              "      <td>1.065261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RTX_RM</th>\n",
              "      <td>1.067196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MB</th>\n",
              "      <td>1.067619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MB</th>\n",
              "      <td>1.072218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RT</th>\n",
              "      <td>1.076371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RB</th>\n",
              "      <td>1.077109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MT</th>\n",
              "      <td>1.079139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RTX_RB</th>\n",
              "      <td>1.080799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_RMX_RB</th>\n",
              "      <td>1.081990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RM</th>\n",
              "      <td>1.094517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RMX_RB</th>\n",
              "      <td>1.098432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RMX_RB</th>\n",
              "      <td>1.102857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MM</th>\n",
              "      <td>1.117189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MB</th>\n",
              "      <td>1.117418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RB</th>\n",
              "      <td>1.125754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RB</th>\n",
              "      <td>1.136785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MBX_RTX_RM</th>\n",
              "      <td>1.160721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RTX_RB</th>\n",
              "      <td>1.196645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MM</th>\n",
              "      <td>1.220148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_RTX_RMX_RB</th>\n",
              "      <td>1.253397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MB</th>\n",
              "      <td>1.271361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MB</th>\n",
              "      <td>1.316267</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Test Loss\n",
              "DATA_X                 \n",
              "X_FTX_MTX_RM   0.718768\n",
              "X_FTX_MMX_RT   0.734740\n",
              "X_FTX_MTX_RB   0.791319\n",
              "X_FTX_MTX_RT   0.821536\n",
              "X_FTX_MMX_RM   0.862389\n",
              "X_FTX_MTX_MM   0.889042\n",
              "X_FMX_MMX_RT   0.899084\n",
              "X_FMX_MTX_RT   0.901221\n",
              "X_MMX_RTX_RB   0.913722\n",
              "X_MTX_MMX_RB   0.931974\n",
              "X_MTX_RTX_RM   0.938872\n",
              "X_MTX_RTX_RB   0.944863\n",
              "X_FMX_MTX_RM   0.954180\n",
              "X_FTX_MMX_RB   0.956485\n",
              "X_FTX_MBX_RT   0.957938\n",
              "X_MTX_RMX_RB   0.961566\n",
              "X_FMX_MBX_RT   0.968683\n",
              "X_FTX_FMX_RM   0.986824\n",
              "X_FTX_MBX_RM   0.992227\n",
              "X_FTX_MTX_MB   0.999681\n",
              "X_FTX_FMX_RB   1.001570\n",
              "X_MMX_RMX_RB   1.008961\n",
              "X_MTX_MMX_RT   1.012800\n",
              "X_FMX_MMX_RM   1.013505\n",
              "X_MBX_RTX_RB   1.015337\n",
              "X_FMX_MTX_RB   1.015875\n",
              "X_MTX_MBX_RT   1.022207\n",
              "X_MTX_MBX_RM   1.031903\n",
              "X_FTX_FMX_RT   1.037106\n",
              "X_MTX_MBX_RB   1.052353\n",
              "X_FTX_RTX_RM   1.053059\n",
              "X_FTX_MBX_RB   1.057057\n",
              "X_FMX_MBX_RM   1.057605\n",
              "X_MTX_MMX_RM   1.062647\n",
              "X_MMX_RTX_RM   1.065261\n",
              "X_FMX_RTX_RM   1.067196\n",
              "X_FTX_MMX_MB   1.067619\n",
              "X_FTX_FMX_MB   1.072218\n",
              "X_MMX_MBX_RT   1.076371\n",
              "X_FMX_MMX_RB   1.077109\n",
              "X_FTX_FMX_MT   1.079139\n",
              "X_FTX_RTX_RB   1.080799\n",
              "X_FTX_RMX_RB   1.081990\n",
              "X_MMX_MBX_RM   1.094517\n",
              "X_MBX_RMX_RB   1.098432\n",
              "X_FMX_RMX_RB   1.102857\n",
              "X_FTX_FMX_MM   1.117189\n",
              "X_FMX_MTX_MB   1.117418\n",
              "X_FMX_MBX_RB   1.125754\n",
              "X_MMX_MBX_RB   1.136785\n",
              "X_MBX_RTX_RM   1.160721\n",
              "X_FMX_RTX_RB   1.196645\n",
              "X_FMX_MTX_MM   1.220148\n",
              "X_RTX_RMX_RB   1.253397\n",
              "X_FMX_MMX_MB   1.271361\n",
              "X_MTX_MMX_MB   1.316267"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "NKeM-y6RRiZt",
        "outputId": "0093e1a6-3e5e-4435-f0fd-48b0d50a31bd"
      },
      "source": [
        "CombResultsSortedMFgroupedsortedMF3.to_csv('CombResultsSortedMFgroupedsortedMF3.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedMFgroupedsortedMF3.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d44d8106-4db0-42f9-82fa-05ae35b21ade\", \"CombResultsSortedMFgroupedsortedMF3.csv\", 1794)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "YOwzdxFUYg50",
        "outputId": "bba9fdff-2d5d-4f11-98f2-85f54fc4f6c1"
      },
      "source": [
        "CombResultsSortedMF3.to_csv('CombResultsSortedMF3.csv')\n",
        "files.download(\"CombResultsSortedMF3.csv\")\n",
        "\n",
        "fig = px.box(CombResultsSortedMF3, x=\"DATA_X\", y=\"Test Loss\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_7e875604-eef6-437a-9b16-95840f51acb3\", \"CombResultsSortedMF3.csv\", 11340)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4fa79aa9-7d00-4e91-bae0-08276c42ea1d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4fa79aa9-7d00-4e91-bae0-08276c42ea1d\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4fa79aa9-7d00-4e91-bae0-08276c42ea1d',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_FTX_MMX_RT\", \"X_FMX_MTX_MB\", \"X_FMX_MMX_RT\", \"X_MMX_RTX_RB\", \"X_MTX_RTX_RM\", \"X_MTX_MMX_RT\", \"X_FTX_MMX_RM\", \"X_MBX_RTX_RB\", \"X_MTX_RMX_RB\", \"X_MTX_MBX_RM\", \"X_MMX_MBX_RT\", \"X_FTX_MTX_RM\", \"X_MMX_RMX_RB\", \"X_FMX_MTX_RM\", \"X_FMX_MTX_RB\", \"X_FMX_MBX_RT\", \"X_FMX_MMX_RT\", \"X_FTX_FMX_MB\", \"X_FTX_MMX_RT\", \"X_FMX_MBX_RM\", \"X_FTX_RTX_RM\", \"X_FTX_MTX_RB\", \"X_FTX_MTX_RM\", \"X_MTX_MBX_RT\", \"X_FMX_MBX_RB\", \"X_FMX_MTX_RM\", \"X_MTX_MMX_RB\", \"X_MMX_RTX_RM\", \"X_FMX_MMX_RM\", \"X_FMX_MTX_RT\", \"X_MMX_RTX_RB\", \"X_FTX_MTX_MM\", \"X_FTX_MMX_RB\", \"X_FTX_MTX_RT\", \"X_FTX_MTX_RB\", \"X_MMX_RTX_RB\", \"X_FMX_RTX_RB\", \"X_MTX_MBX_RB\", \"X_FTX_MBX_RB\", \"X_FTX_MBX_RT\", \"X_MTX_MMX_RB\", \"X_FTX_MTX_MB\", \"X_FMX_MMX_RB\", \"X_MTX_RTX_RM\", \"X_MBX_RTX_RM\", \"X_MTX_RTX_RB\", \"X_FMX_MBX_RT\", \"X_MMX_RMX_RB\", \"X_MBX_RTX_RB\", \"X_FTX_MTX_RM\", \"X_MMX_MBX_RM\", \"X_MBX_RMX_RB\", \"X_FMX_MTX_RT\", \"X_FTX_MBX_RM\", \"X_FTX_MTX_RT\", \"X_FTX_MBX_RM\", \"X_FTX_MTX_RB\", \"X_FMX_MBX_RT\", \"X_FTX_FMX_RT\", \"X_MMX_RTX_RM\", \"X_FMX_MMX_RM\", \"X_MTX_RTX_RB\", \"X_RTX_RMX_RB\", \"X_FTX_FMX_RT\", \"X_FTX_MTX_MM\", \"X_MTX_RMX_RB\", \"X_MTX_RTX_RM\", \"X_FTX_FMX_MB\", \"X_FTX_RTX_RB\", \"X_FTX_MBX_RT\", \"X_FMX_MBX_RM\", \"X_MTX_MMX_RM\", \"X_MTX_MMX_RT\", \"X_MTX_MBX_RT\", \"X_FTX_MTX_RM\", \"X_FTX_MMX_MB\", \"X_FTX_MBX_RT\", \"X_FTX_MMX_RM\", \"X_FMX_RTX_RM\", \"X_FMX_MMX_RM\", \"X_FTX_MMX_RB\", \"X_MTX_MMX_RT\", \"X_MTX_MBX_RB\", \"X_FTX_MTX_RT\", \"X_FTX_FMX_RM\", \"X_MTX_MMX_RB\", \"X_MTX_RTX_RB\", \"X_FTX_MTX_MM\", \"X_MBX_RTX_RM\", \"X_FTX_FMX_MM\", \"X_FMX_MTX_RB\", \"X_FMX_RMX_RB\", \"X_MTX_MBX_RT\", \"X_MMX_MBX_RB\", \"X_MTX_MBX_RM\", \"X_FMX_RMX_RB\", \"X_MTX_MMX_RM\", \"X_FMX_RTX_RM\", \"X_FTX_MMX_RT\", \"X_FMX_MTX_RT\", \"X_MMX_MBX_RM\", \"X_FMX_MTX_RT\", \"X_MTX_MMX_MB\", \"X_MTX_MBX_RM\", \"X_FTX_MTX_RM\", \"X_FTX_FMX_RB\", \"X_MMX_MBX_RB\", \"X_FTX_MTX_RB\", \"X_FTX_MTX_MB\", \"X_FTX_FMX_RM\", \"X_FTX_MMX_RB\", \"X_FTX_MMX_RT\", \"X_FTX_MMX_RT\", \"X_FTX_MMX_MB\", \"X_MTX_RMX_RB\", \"X_FTX_FMX_RM\", \"X_FMX_MBX_RM\", \"X_MTX_MMX_RM\", \"X_FTX_RMX_RB\", \"X_MTX_MBX_RB\", \"X_MBX_RMX_RB\", \"X_FTX_FMX_MT\", \"X_FTX_MTX_RT\", \"X_FMX_MTX_RM\", \"X_FMX_MMX_RB\", \"X_MTX_MMX_RT\", \"X_FTX_MTX_MB\", \"X_FMX_MTX_RM\", \"X_MTX_MMX_RM\", \"X_FTX_MBX_RB\", \"X_FTX_RMX_RB\", \"X_FMX_MMX_RT\", \"X_FTX_MMX_RM\", \"X_FTX_FMX_MT\", \"X_FTX_MTX_MM\", \"X_FMX_MTX_MM\", \"X_MTX_RTX_RB\", \"X_FMX_MMX_RT\", \"X_MTX_MMX_RB\", \"X_FTX_RTX_RB\", \"X_FTX_FMX_RB\", \"X_FMX_MBX_RB\", \"X_MMX_MBX_RT\", \"X_FMX_RTX_RM\", \"X_MMX_MBX_RT\", \"X_MBX_RMX_RB\", \"X_MTX_RMX_RB\", \"X_FTX_FMX_RM\", \"X_FTX_FMX_MM\", \"X_MMX_RTX_RM\", \"X_RTX_RMX_RB\", \"X_MBX_RTX_RB\", \"X_FTX_FMX_RB\", \"X_FTX_FMX_RT\", \"X_FTX_MMX_RM\", \"X_MMX_MBX_RT\", \"X_MTX_MBX_RB\", \"X_MMX_MBX_RM\", \"X_FTX_RTX_RM\", \"X_FMX_RTX_RB\", \"X_FMX_MTX_RB\", \"X_FMX_MTX_MM\", \"X_FTX_RMX_RB\", \"X_FMX_RMX_RB\", \"X_FMX_RTX_RB\", \"X_FTX_MTX_RT\", \"X_FTX_MBX_RM\", \"X_FTX_RMX_RB\", \"X_FMX_MMX_MB\", \"X_MMX_RTX_RB\", \"X_FTX_RTX_RM\", \"X_MTX_MMX_MB\", \"X_FTX_MTX_RB\", \"X_FMX_MMX_RM\", \"X_MMX_MBX_RB\", \"X_FTX_FMX_MT\", \"X_FMX_MTX_MM\", \"X_MTX_MBX_RM\", \"X_FTX_FMX_RB\", \"X_FTX_RTX_RM\", \"X_FTX_MMX_MB\", \"X_MMX_RMX_RB\", \"X_MMX_MBX_RM\", \"X_FTX_MMX_RM\", \"X_MTX_MBX_RT\", \"X_FMX_MTX_MB\", \"X_RTX_RMX_RB\", \"X_FMX_MBX_RB\", \"X_MMX_RTX_RM\", \"X_MMX_RMX_RB\", \"X_FTX_MBX_RM\", \"X_FMX_MTX_MB\", \"X_FTX_FMX_MT\", \"X_FTX_MMX_MB\", \"X_FMX_MMX_MB\", \"X_FMX_MMX_MB\", \"X_FMX_MBX_RM\", \"X_FTX_RTX_RB\", \"X_FTX_MBX_RB\", \"X_MBX_RTX_RB\", \"X_FTX_MMX_RB\", \"X_FTX_MBX_RT\", \"X_MTX_MMX_MB\", \"X_FTX_FMX_MB\", \"X_MTX_RTX_RM\", \"X_FTX_MTX_MB\", \"X_FMX_MMX_RB\", \"X_FMX_MTX_RB\", \"X_FTX_RTX_RB\", \"X_FTX_MBX_RB\", \"X_FTX_FMX_RB\", \"X_MBX_RTX_RM\", \"X_FTX_FMX_MM\", \"X_FTX_FMX_MM\", \"X_FTX_FMX_RT\", \"X_MMX_MBX_RB\", \"X_FMX_MBX_RT\", \"X_FMX_MMX_RB\", \"X_FTX_FMX_MB\", \"X_MBX_RTX_RM\", \"X_FTX_MMX_RB\", \"X_FMX_MBX_RB\", \"X_MBX_RMX_RB\", \"X_FMX_MTX_MB\", \"X_FTX_MTX_MM\", \"X_FMX_RTX_RB\", \"X_FTX_MBX_RB\", \"X_FMX_RMX_RB\", \"X_FMX_RTX_RM\", \"X_FTX_RTX_RB\", \"X_FTX_MBX_RT\", \"X_FTX_MBX_RM\", \"X_FMX_MMX_RB\", \"X_FTX_MTX_MB\", \"X_FTX_FMX_MM\", \"X_FTX_FMX_MT\", \"X_FMX_RTX_RM\", \"X_FMX_MTX_RT\", \"X_FMX_MTX_MM\", \"X_FTX_FMX_RM\", \"X_FMX_MTX_RB\", \"X_FMX_MMX_MB\", \"X_FTX_MMX_MB\", \"X_FTX_FMX_RT\", \"X_MTX_RTX_RM\", \"X_FMX_MMX_RT\", \"X_FTX_RTX_RM\", \"X_FMX_RMX_RB\", \"X_FMX_MBX_RT\", \"X_MMX_RMX_RB\", \"X_FTX_FMX_MB\", \"X_MTX_MMX_RB\", \"X_MTX_MMX_MB\", \"X_FMX_MMX_MB\", \"X_FMX_MTX_MB\", \"X_MTX_RTX_RB\", \"X_FTX_RMX_RB\", \"X_RTX_RMX_RB\", \"X_MMX_RTX_RB\", \"X_MBX_RMX_RB\", \"X_FMX_MBX_RB\", \"X_MBX_RTX_RB\", \"X_MTX_RMX_RB\", \"X_FMX_MTX_MM\", \"X_FMX_MTX_RM\", \"X_MMX_MBX_RB\", \"X_MTX_MBX_RT\", \"X_FMX_MMX_RM\", \"X_MMX_MBX_RM\", \"X_MBX_RTX_RM\", \"X_RTX_RMX_RB\", \"X_MTX_MBX_RM\", \"X_MMX_RTX_RM\", \"X_FMX_MBX_RM\", \"X_MMX_MBX_RT\", \"X_MTX_MBX_RB\", \"X_MTX_MMX_RM\", \"X_FMX_RTX_RB\", \"X_MTX_MMX_MB\", \"X_MTX_MMX_RT\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.48987704515457153, 0.5219042509794235, 0.5328955680131913, 0.536593559384346, 0.5431760519742965, 0.5480785936117172, 0.5580693989992142, 0.5748584300279618, 0.5782956898212432, 0.5943840026855469, 0.5960397303104401, 0.6005210638046264, 0.6060028195381164, 0.6107938617467881, 0.6148225545883179, 0.6193124920129776, 0.6195835143327713, 0.6252229958772659, 0.6358593374490737, 0.6364879667758941, 0.6381431221961975, 0.6472476184368133, 0.6524971723556519, 0.6528412789106369, 0.658775681257248, 0.6615889370441437, 0.6653080195188522, 0.6695847332477569, 0.6708262920379638, 0.6744631499052047, 0.6749199748039245, 0.6773953020572663, 0.6794335961341857, 0.6805056035518646, 0.6894276738166809, 0.6924344390630722, 0.6934183895587921, 0.697810709476471, 0.7025822341442108, 0.7042771995067596, 0.7048095881938934, 0.7057172656059265, 0.705900713801384, 0.7102600932121277, 0.7106129884719848, 0.7110741764307023, 0.7139562517404556, 0.7153297334909439, 0.7153578221797943, 0.7166061908006668, 0.7276834487915039, 0.728352153301239, 0.7296503037214279, 0.7300049632787704, 0.7327842772006988, 0.7329189360141755, 0.7337491273880005, 0.7344087481498718, 0.7352758914232254, 0.7362071454524994, 0.7367230862379074, 0.7388627588748932, 0.7431118667125702, 0.7492977678775787, 0.7492991149425506, 0.7497205555438995, 0.7650764882564545, 0.7658763647079467, 0.7671345293521881, 0.7692613661289215, 0.7715728044509887, 0.7748239874839783, 0.778819328546524, 0.780651432275772, 0.7833171129226685, 0.7846168398857116, 0.7856266736984253, 0.7860630363225937, 0.7886918157339096, 0.7899444013833999, 0.7948333323001862, 0.7952092111110687, 0.7954454958438874, 0.7962348163127899, 0.8001748353242875, 0.8016478955745697, 0.8033157527446747, 0.8038605093955994, 0.8055537283420563, 0.80575612783432, 0.8104053676128388, 0.812739509344101, 0.820217627286911, 0.8213133752346039, 0.8216029524803161, 0.8275168597698211, 0.8342605173587799, 0.8355158269405365, 0.8375940293073654, 0.8389055043458938, 0.8390851497650147, 0.8392415225505829, 0.8394899427890777, 0.8395727336406708, 0.8408981144428254, 0.8428997457027435, 0.843631488084793, 0.8438287019729614, 0.8510888576507568, 0.852572226524353, 0.8531022071838379, 0.8547629058361054, 0.8556057989597321, 0.8596225321292877, 0.861609423160553, 0.8671304523944855, 0.8698420941829681, 0.8702048122882843, 0.8702571988105774, 0.8739601373672485, 0.8761255323886872, 0.8809112727642059, 0.8823894083499908, 0.8860688805580139, 0.8907848596572876, 0.8910372853279114, 0.8943145573139191, 0.8944894850254059, 0.8978036880493164, 0.899289870262146, 0.8995300233364105, 0.9029195725917816, 0.9033761858940125, 0.9094315767288208, 0.9115018427371979, 0.9131912231445313, 0.9141891539096832, 0.9175775408744812, 0.9346120893955231, 0.9371399283409119, 0.9385623633861542, 0.9394492506980896, 0.9434275209903717, 0.9496770381927491, 0.9500215947628021, 0.9549917161464692, 0.955156335234642, 0.9599252939224243, 0.9642712056636811, 0.96457639336586, 0.9699355900287628, 0.9728973209857941, 0.9744688153266907, 0.9799071907997131, 0.9868491172790528, 0.9931180357933045, 0.993206799030304, 1.0020512521266938, 1.0045916616916657, 1.0062951624393464, 1.009265238046646, 1.009316325187683, 1.0098844766616821, 1.010002326965332, 1.0149496376514435, 1.015767228603363, 1.020096105337143, 1.0228426992893218, 1.0256270945072175, 1.0301107227802277, 1.031051117181778, 1.0395734369754792, 1.042342060804367, 1.0515564858913422, 1.0521787762641908, 1.0539177179336547, 1.0569895088672638, 1.0573965072631837, 1.0606616377830504, 1.0626483619213105, 1.0657733380794525, 1.0672823071479798, 1.0697776317596435, 1.0775883615016937, 1.0857521831989287, 1.0861854374408721, 1.0889596223831177, 1.0946678638458252, 1.1053870737552642, 1.1149709165096282, 1.1181313931941985, 1.126629936695099, 1.1367985427379608, 1.139777421951294, 1.1405730545520782, 1.1492115139961243, 1.1517080962657928, 1.1636250078678132, 1.1681198358535767, 1.1681439459323884, 1.1764793276786805, 1.1766372561454772, 1.1770451605319976, 1.1780955791473389, 1.1789022147655488, 1.1808549404144286, 1.1813489496707916, 1.1861085891723633, 1.186560755968094, 1.186908060312271, 1.191258692741394, 1.198355758190155, 1.2007325351238252, 1.2176247000694276, 1.2251070320606232, 1.2296674013137818, 1.2351956367492676, 1.2454916477203368, 1.2501118063926697, 1.252750027179718, 1.2785745501518249, 1.2954607725143432, 1.297171413898468, 1.2973100066184997, 1.3031520605087281, 1.309616756439209, 1.3283859729766845, 1.331775426864624, 1.3405157208442688, 1.349532914161682, 1.353888201713562, 1.359982180595398, 1.3620210886001587, 1.3664314508438111, 1.3975602030754088, 1.4146369218826294, 1.4215776205062867, 1.4238465666770934, 1.450066590309143, 1.4543172240257263, 1.458773136138916, 1.4867220878601075, 1.488307273387909, 1.4959411025047302, 1.4969469547271728, 1.522441565990448, 1.5288603901863098, 1.5322506427764893, 1.54054137468338, 1.5412175893783568, 1.5417815685272216, 1.5534899711608887, 1.5536993503570558, 1.5546702861785888, 1.555060911178589, 1.5568724155426026, 1.6074357748031616, 1.6210727572441102, 1.6345489859580993, 1.6355172514915466, 1.6404146552085876, 1.6454256892204284, 1.663049054145813, 1.6711787700653076, 1.7179574489593505, 1.7371329426765443, 1.771574115753174, 1.81847425699234, 1.8339879631996154, 1.836331021785736, 1.8439070463180542, 1.8465567111968995, 1.850548470020294, 1.858415699005127, 1.8992505192756652, 1.901343083381653, 1.9361400723457336, 1.9589437246322632, 1.9715250372886657, 2.05085608959198], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4fa79aa9-7d00-4e91-bae0-08276c42ea1d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N87IAQSzcAr"
      },
      "source": [
        "# define baseline model 4\n",
        "# create model\n",
        "\n",
        "model4 = Sequential()\n",
        "model4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(28, 1)))\n",
        "model4.add(MaxPooling1D(pool_size=2))\n",
        "model4.add(Flatten())\n",
        "model4.add(Dense(50, activation='relu'))\n",
        "model4.add(Dense(1))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "model4.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "#model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hl8XS_eMJn_"
      },
      "source": [
        "\n",
        "def datageneratorMF4(X_in1, X_in2, X_in3, X_in4, Y_in):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  X_in = np.concatenate((X_in1, X_in2, X_in3, X_in4), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "\n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], 1))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], 1))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], 1))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIB_dyaoMJoH"
      },
      "source": [
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMS(X_MT, X_MM, y_RM)\n",
        "#X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datagenerator(X_MT, y_RM)\n",
        "#X_train_Input[0:5]\n",
        "#X_train_Input.shape\n",
        "#y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go6TpsmiMJoH"
      },
      "source": [
        "def evaldataMF4(X_in1, X_in2, X_in3, X_in4, Y_in, traindata1, traindata2, traindata3, traindata4, testdata):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMF4(X_in1, X_in2, X_in3, X_in4, Y_in)\n",
        "  \n",
        "  history = model4.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model4.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model4.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # PLOTS LOSS VS EPOCH\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  # print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  # print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # PLOTS Y ORIGINAL VS PREDICTED\n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata1)+ str(traindata2)  + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ8bGnlGMJoL",
        "outputId": "2eb7aaeb-8226-4ab0-ab86-7fd5d58cada0"
      },
      "source": [
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "\n",
        "model4.save_weights('model4.h5')\n",
        "\n",
        "my_dictMF4 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), 4):\n",
        "  kX1, kX2, kX3, kX4 = combo[0][0], combo[1][0], combo[2][0], combo[3][0]\n",
        "  vX1, vX2, vX3, vX4 = combo[0][1], combo[1][1], combo[2][1], combo[3][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:] or ky[-2:] == kX4[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, kx4 = {kX4}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF4 = evaldataMF4(vX1, vX2, vX3, vX4, vy, kX1, kX2, kX3, kX4, ky)\n",
        "      TestLossTotal = resultsMF4[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF4[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF4[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}_{kX2}_{kX3}_{kX4} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF4[\"DATA_X\"].append(kX1 + kX2 + kX3 + kX4)\n",
        "    my_dictMF4[\"DATA_y\"].append(ky)\n",
        "    my_dictMF4[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model4.load_weights('model4.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7831853449344635, and Avg Validation Loss (mae) is 0.9164369761943817\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9031723260879516\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.38702290058136, and Avg Validation Loss (mae) is 1.5308344960212708\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5055761694908143\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9876692235469818, and Avg Validation Loss (mae) is 1.0770114064216614\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0846634745597838\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8310053646564484, and Avg Validation Loss (mae) is 0.9681647837162017\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9754471600055694\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6168127000331879, and Avg Validation Loss (mae) is 0.6069714486598968\n",
            "After 10 runs; Avg Test Loss (mae) is 0.595270323753357\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4413798451423645, and Avg Validation Loss (mae) is 1.5174983382225036\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5204498291015625\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.196589469909668, and Avg Validation Loss (mae) is 1.348422759771347\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3508085787296296\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8838198065757752, and Avg Validation Loss (mae) is 0.8098913967609406\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8200599431991578\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6919079840183258, and Avg Validation Loss (mae) is 0.6474847555160522\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6510933667421341\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7673289716243744, and Avg Validation Loss (mae) is 0.7918132066726684\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7946194410324097\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.667582780122757, and Avg Validation Loss (mae) is 0.7266179144382476\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7134125351905822\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.611797672510147, and Avg Validation Loss (mae) is 0.7324481576681137\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7474124372005463\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4998683363199234, and Avg Validation Loss (mae) is 0.5957714438438415\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5855235636234284\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7944178700447082, and Avg Validation Loss (mae) is 0.8410403966903687\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8306457042694092\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8195726275444031, and Avg Validation Loss (mae) is 0.9358147442340851\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9238629639148712\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6499260008335114, and Avg Validation Loss (mae) is 0.7836663275957108\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7937292993068695\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6454103708267211, and Avg Validation Loss (mae) is 0.6683733940124512\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6719425618648529\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.831845486164093, and Avg Validation Loss (mae) is 0.8335695624351501\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8411434173583985\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0759179294109344, and Avg Validation Loss (mae) is 1.182902890443802\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1854198276996613\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8381096065044403, and Avg Validation Loss (mae) is 0.9337519407272339\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9251620173454285\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0680467307567596, and Avg Validation Loss (mae) is 1.0751705586910247\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0789916396141053\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4550275444984435, and Avg Validation Loss (mae) is 1.593346083164215\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5684947848320008\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0932889342308045, and Avg Validation Loss (mae) is 1.1578657925128937\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1353368520736695\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8485277652740478, and Avg Validation Loss (mae) is 0.9230437755584717\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9291528463363647\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9949109315872192, and Avg Validation Loss (mae) is 0.9842381119728089\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9971264243125916\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6932495296001434, and Avg Validation Loss (mae) is 0.6790214598178863\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6811392217874527\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5098837703466416, and Avg Validation Loss (mae) is 0.6466339230537415\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6445459663867951\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6806365251541138, and Avg Validation Loss (mae) is 0.9024427354335784\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8946038782596588\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9777173161506653, and Avg Validation Loss (mae) is 1.1350460767745971\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1177762508392335\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7459325015544891, and Avg Validation Loss (mae) is 0.758693066239357\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7598480582237244\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8095575332641601, and Avg Validation Loss (mae) is 0.8607370138168335\n",
            "After 10 runs; Avg Test Loss (mae) is 0.882144284248352\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6088659226894378, and Avg Validation Loss (mae) is 0.6482570230960846\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6638850212097168\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0913521885871886, and Avg Validation Loss (mae) is 1.0888320565223695\n",
            "After 10 runs; Avg Test Loss (mae) is 1.071741485595703\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6817666888237, and Avg Validation Loss (mae) is 0.6658777981996536\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6494135320186615\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0565584003925323, and Avg Validation Loss (mae) is 1.2766669034957885\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2695529222488404\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7890848338603973, and Avg Validation Loss (mae) is 0.8834492266178131\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8716757833957672\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0260052144527436, and Avg Validation Loss (mae) is 1.0948271751403809\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0889474987983703\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6438338160514832, and Avg Validation Loss (mae) is 0.748814481496811\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7411780506372452\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6296377778053284, and Avg Validation Loss (mae) is 0.6658883422613144\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6657209485769272\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6466042280197144, and Avg Validation Loss (mae) is 0.6807986557483673\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6830884367227554\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2296449482440948, and Avg Validation Loss (mae) is 1.3024146020412446\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2538671314716339\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.58336923122406, and Avg Validation Loss (mae) is 0.745818418264389\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7605031251907348\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8733407914638519, and Avg Validation Loss (mae) is 0.8229759633541107\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8455284893512726\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6280328452587127, and Avg Validation Loss (mae) is 0.5823958933353424\n",
            "After 10 runs; Avg Test Loss (mae) is 0.578582513332367\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.151823627948761, and Avg Validation Loss (mae) is 1.0985207617282868\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0829204857349395\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6315036386251449, and Avg Validation Loss (mae) is 0.5947866827249527\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6000216245651245\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.077216774225235, and Avg Validation Loss (mae) is 1.1105985283851623\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1274255394935608\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8732975840568542, and Avg Validation Loss (mae) is 0.9846624553203582\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9977567493915558\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4228362321853638, and Avg Validation Loss (mae) is 1.4914901971817016\n",
            "After 10 runs; Avg Test Loss (mae) is 1.522480857372284\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7649365901947022, and Avg Validation Loss (mae) is 0.9201690196990967\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9206372797489166\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0200020730495454, and Avg Validation Loss (mae) is 1.0313408732414246\n",
            "After 10 runs; Avg Test Loss (mae) is 1.043584442138672\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6088625192642212, and Avg Validation Loss (mae) is 0.6053051859140396\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5989613682031631\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3252463698387147, and Avg Validation Loss (mae) is 1.2767002701759338\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3188342452049255\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8517445087432861, and Avg Validation Loss (mae) is 0.9787668824195862\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0164918541908263\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 1.010993778705597, and Avg Validation Loss (mae) is 1.0793147206306457\n",
            "After 10 runs; Avg Test Loss (mae) is 1.072948509454727\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6179248452186584, and Avg Validation Loss (mae) is 0.5664612025022506\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5625763773918152\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3933627963066102, and Avg Validation Loss (mae) is 1.461121106147766\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4853735446929932\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7947984218597413, and Avg Validation Loss (mae) is 0.7662319898605346\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7515765190124511\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9798887372016907, and Avg Validation Loss (mae) is 1.0907809734344482\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0794279754161835\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8522128105163574, and Avg Validation Loss (mae) is 0.8437813103199006\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8376691222190857\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8158748686313629, and Avg Validation Loss (mae) is 0.8201408386230469\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8173675894737243\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3246928453445435, and Avg Validation Loss (mae) is 1.242493212223053\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2503778457641601\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9557942807674408, and Avg Validation Loss (mae) is 0.9920692324638367\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0210439562797546\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8750814378261567, and Avg Validation Loss (mae) is 0.8392983317375183\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8421301782131195\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8215044677257538, and Avg Validation Loss (mae) is 0.8147889196872711\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8001988053321838\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7550572752952576, and Avg Validation Loss (mae) is 0.7446354150772094\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7552183926105499\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5190267533063888, and Avg Validation Loss (mae) is 0.5817939549684524\n",
            "After 10 runs; Avg Test Loss (mae) is 0.581775176525116\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6449737429618836, and Avg Validation Loss (mae) is 0.7307767838239669\n",
            "After 10 runs; Avg Test Loss (mae) is 0.710794597864151\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7748767852783203, and Avg Validation Loss (mae) is 0.8437467634677887\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8503449857234955\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7546846568584442, and Avg Validation Loss (mae) is 0.7049326241016388\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7147448539733887\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7211308360099793, and Avg Validation Loss (mae) is 0.8295082807540893\n",
            "After 10 runs; Avg Test Loss (mae) is 0.832785227894783\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6780922710895538, and Avg Validation Loss (mae) is 0.8484433114528656\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8495836228132247\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6926963031291962, and Avg Validation Loss (mae) is 0.7509936392307281\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7258809506893158\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7183027982711792, and Avg Validation Loss (mae) is 0.8810030281543731\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9066274344921113\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.955904883146286, and Avg Validation Loss (mae) is 0.951376450061798\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9491433739662171\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7130731999874115, and Avg Validation Loss (mae) is 0.8211975693702698\n",
            "After 10 runs; Avg Test Loss (mae) is 0.812053307890892\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.868863332271576, and Avg Validation Loss (mae) is 0.9078481793403625\n",
            "After 10 runs; Avg Test Loss (mae) is 0.914964097738266\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7319912672042846, and Avg Validation Loss (mae) is 0.9078787267208099\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8878775149583816\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7297814577817917, and Avg Validation Loss (mae) is 0.6462148517370224\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6399941504001617\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7673397421836853, and Avg Validation Loss (mae) is 0.6370043218135834\n",
            "After 10 runs; Avg Test Loss (mae) is 0.640024608373642\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.845091599225998, and Avg Validation Loss (mae) is 0.7856126666069031\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7904233276844025\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6206210911273956, and Avg Validation Loss (mae) is 0.6558024436235428\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6614878088235855\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8249273240566254, and Avg Validation Loss (mae) is 0.7133081555366516\n",
            "After 10 runs; Avg Test Loss (mae) is 0.714645916223526\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7027436852455139, and Avg Validation Loss (mae) is 0.6544778019189834\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6577032715082168\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7564655423164368, and Avg Validation Loss (mae) is 0.8044197082519531\n",
            "After 10 runs; Avg Test Loss (mae) is 0.815443480014801\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.689711457490921, and Avg Validation Loss (mae) is 0.8005781143903732\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8005081653594971\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9932158410549163, and Avg Validation Loss (mae) is 0.883081179857254\n",
            "After 10 runs; Avg Test Loss (mae) is 0.89574134349823\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7854762732982635, and Avg Validation Loss (mae) is 0.8216458261013031\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8082094073295594\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8805267989635468, and Avg Validation Loss (mae) is 0.930072408914566\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9345882356166839\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5483444303274154, and Avg Validation Loss (mae) is 0.5530385673046112\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5754513621330262\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7940090000629425, and Avg Validation Loss (mae) is 0.7460655927658081\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7553229033946991\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6667469680309296, and Avg Validation Loss (mae) is 0.6812409996986389\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7027208030223846\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8105071127414704, and Avg Validation Loss (mae) is 0.763477212190628\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7801692128181458\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5966813027858734, and Avg Validation Loss (mae) is 0.5815673738718032\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5666598826646805\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7163972198963166, and Avg Validation Loss (mae) is 0.7692009687423706\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7621240615844727\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6266726791858673, and Avg Validation Loss (mae) is 0.586492532491684\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5906214416027069\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8031620681285858, and Avg Validation Loss (mae) is 0.7782826066017151\n",
            "After 10 runs; Avg Test Loss (mae) is 0.799152547121048\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6287244588136673, and Avg Validation Loss (mae) is 0.6766030937433243\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6772949516773223\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7420036613941192, and Avg Validation Loss (mae) is 0.7998933017253875\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7971456289291382\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6846847295761108, and Avg Validation Loss (mae) is 0.8120036154985428\n",
            "After 10 runs; Avg Test Loss (mae) is 0.820230457186699\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8762141227722168, and Avg Validation Loss (mae) is 0.8571432113647461\n",
            "After 10 runs; Avg Test Loss (mae) is 0.883879441022873\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9616630494594574, and Avg Validation Loss (mae) is 1.1580255508422852\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1445922374725341\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6139179527759552, and Avg Validation Loss (mae) is 0.7458913713693619\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7557681381702424\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6665457904338836, and Avg Validation Loss (mae) is 0.7621348410844803\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7579279601573944\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8025192737579345, and Avg Validation Loss (mae) is 0.8243897378444671\n",
            "After 10 runs; Avg Test Loss (mae) is 0.829956340789795\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0558026611804963, and Avg Validation Loss (mae) is 1.0980590283870697\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0887119591236114\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7824012875556946, and Avg Validation Loss (mae) is 0.7632449418306351\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7351144015789032\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6749669909477234, and Avg Validation Loss (mae) is 0.681650772690773\n",
            "After 10 runs; Avg Test Loss (mae) is 0.690717589855194\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.764313155412674, and Avg Validation Loss (mae) is 0.7235406279563904\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7364757478237152\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1717555284500123, and Avg Validation Loss (mae) is 1.1770152807235719\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2092278063297273\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0664637625217437, and Avg Validation Loss (mae) is 1.0247731804847717\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0222411513328553\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7685143291950226, and Avg Validation Loss (mae) is 0.827955138683319\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8362796902656555\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8024288713932037, and Avg Validation Loss (mae) is 0.79194495677948\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8082313239574432\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.921774172782898, and Avg Validation Loss (mae) is 1.0248406052589416\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0380515933036805\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.67832453250885, and Avg Validation Loss (mae) is 0.869416868686676\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8801316261291504\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6483126282691956, and Avg Validation Loss (mae) is 0.6611922889947891\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6611201286315918\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7166061520576477, and Avg Validation Loss (mae) is 0.7672226071357727\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7696822941303253\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9877294719219207, and Avg Validation Loss (mae) is 1.0529038190841675\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0464218080043792\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6561277896165848, and Avg Validation Loss (mae) is 0.698313695192337\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7046156644821167\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5650690227746964, and Avg Validation Loss (mae) is 0.5357277065515518\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5389934092760086\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7546674013137817, and Avg Validation Loss (mae) is 0.8192323684692383\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8365200579166412\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0526887357234955, and Avg Validation Loss (mae) is 1.0653495967388154\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0435486257076263\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7086292386054993, and Avg Validation Loss (mae) is 0.6559810131788254\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6437096059322357\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6936443686485291, and Avg Validation Loss (mae) is 0.7134260028600693\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7120513379573822\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0555965542793273, and Avg Validation Loss (mae) is 0.957332968711853\n",
            "After 10 runs; Avg Test Loss (mae) is 0.96690132021904\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.253781831264496, and Avg Validation Loss (mae) is 1.2812264561653137\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2738427639007568\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7185543894767761, and Avg Validation Loss (mae) is 0.8143799334764481\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8043825387954712\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7315543830394745, and Avg Validation Loss (mae) is 0.5926847994327545\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5894904226064682\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0397425651550294, and Avg Validation Loss (mae) is 1.196840226650238\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2044025003910064\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2795788884162902, and Avg Validation Loss (mae) is 1.5459879398345948\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4793835699558258\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7813231527805329, and Avg Validation Loss (mae) is 0.7097618579864502\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6964779317378997\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5913434535264969, and Avg Validation Loss (mae) is 0.6112200081348419\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6116054654121399\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9940056443214417, and Avg Validation Loss (mae) is 1.0718270540237427\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0407486855983734\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3035211086273193, and Avg Validation Loss (mae) is 1.4452506065368653\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4195578813552856\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8410138309001922, and Avg Validation Loss (mae) is 0.7064063012599945\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7027892708778382\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.853865110874176, and Avg Validation Loss (mae) is 0.8611146628856658\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8675486803054809\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9332319557666778, and Avg Validation Loss (mae) is 0.9483331561088562\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9447169423103332\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4831875205039977, and Avg Validation Loss (mae) is 1.6270000338554382\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6418753385543823\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8586687266826629, and Avg Validation Loss (mae) is 0.9053933441638946\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8966112911701203\n",
            "kx1 = X_FT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.994758415222168, and Avg Validation Loss (mae) is 1.0458967447280885\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0456037819385529\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.59072242975235, and Avg Validation Loss (mae) is 1.5007795929908752\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4882815718650817\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3425756216049194, and Avg Validation Loss (mae) is 1.2943592190742492\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3245244860649108\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9641738414764405, and Avg Validation Loss (mae) is 0.8793780028820037\n",
            "After 10 runs; Avg Test Loss (mae) is 0.871476811170578\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.797791701555252, and Avg Validation Loss (mae) is 0.8102824866771698\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8098717749118804\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7077759861946107, and Avg Validation Loss (mae) is 1.7692665815353394\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7439912557601929\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6883678555488586, and Avg Validation Loss (mae) is 0.6234034359455108\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6144496887922287\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5783291429281234, and Avg Validation Loss (mae) is 0.525555145740509\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5288140565156937\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6104195892810822, and Avg Validation Loss (mae) is 0.7157333254814148\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7222394287586212\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5556333661079407, and Avg Validation Loss (mae) is 1.76757652759552\n",
            "After 10 runs; Avg Test Loss (mae) is 1.735303556919098\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7347637295722962, and Avg Validation Loss (mae) is 0.717976513504982\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7072295695543289\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7426124274730682, and Avg Validation Loss (mae) is 0.7514724910259247\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7261843323707581\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6480469763278961, and Avg Validation Loss (mae) is 0.815838885307312\n",
            "After 10 runs; Avg Test Loss (mae) is 0.838022118806839\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5323654651641845, and Avg Validation Loss (mae) is 1.6918273091316223\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6479520082473755\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7251137733459473, and Avg Validation Loss (mae) is 0.7759650707244873\n",
            "After 10 runs; Avg Test Loss (mae) is 0.767251405119896\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9546256363391876, and Avg Validation Loss (mae) is 0.9701092481613159\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9784148275852204\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7255145847797394, and Avg Validation Loss (mae) is 0.7240095973014832\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7307227551937103\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5633909702301025, and Avg Validation Loss (mae) is 1.5841822743415832\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6252970933914184\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6001340270042419, and Avg Validation Loss (mae) is 0.6078034400939941\n",
            "After 10 runs; Avg Test Loss (mae) is 0.588164547085762\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.682887077331543, and Avg Validation Loss (mae) is 0.741809195280075\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7428317219018936\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6780517280101777, and Avg Validation Loss (mae) is 0.6412467688322068\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6428664028644562\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6193062782287597, and Avg Validation Loss (mae) is 1.7436344742774963\n",
            "After 10 runs; Avg Test Loss (mae) is 1.757822859287262\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5235251903533935, and Avg Validation Loss (mae) is 0.4887329161167145\n",
            "After 10 runs; Avg Test Loss (mae) is 0.4931711435317993\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7232645511627197, and Avg Validation Loss (mae) is 0.6784732580184937\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7001205742359161\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6565116941928864, and Avg Validation Loss (mae) is 0.6110783994197846\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6149511456489563\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.459285521507263, and Avg Validation Loss (mae) is 1.4495582938194276\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4105515718460082\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6077520549297333, and Avg Validation Loss (mae) is 0.6225233942270278\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6206890255212784\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0029843389987945, and Avg Validation Loss (mae) is 1.0641821205615998\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0523074865341187\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8168029904365539, and Avg Validation Loss (mae) is 0.9667006731033325\n",
            "After 10 runs; Avg Test Loss (mae) is 0.96524977684021\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5240335702896117, and Avg Validation Loss (mae) is 1.502420675754547\n",
            "After 10 runs; Avg Test Loss (mae) is 1.514040434360504\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5931850850582123, and Avg Validation Loss (mae) is 0.6154879659414292\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6210460126399994\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8026958465576172, and Avg Validation Loss (mae) is 0.827919089794159\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8024623215198516\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6144223749637604, and Avg Validation Loss (mae) is 0.5521799683570862\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5617314875125885\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5890093803405763, and Avg Validation Loss (mae) is 1.5674471616744996\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5558860778808594\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6565922260284424, and Avg Validation Loss (mae) is 0.6124403953552247\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6186796844005584\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7842456579208374, and Avg Validation Loss (mae) is 0.7321510553359986\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7308360457420349\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6952004730701447, and Avg Validation Loss (mae) is 0.6484913110733033\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6339463621377945\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4042991638183593, and Avg Validation Loss (mae) is 1.433710789680481\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4600336074829101\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5475575447082519, and Avg Validation Loss (mae) is 0.5394760370254517\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5374155402183532\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7454039812088012, and Avg Validation Loss (mae) is 0.8273926019668579\n",
            "After 10 runs; Avg Test Loss (mae) is 0.844562566280365\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6905249297618866, and Avg Validation Loss (mae) is 0.6301366448402405\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6421148121356964\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5194939613342284, and Avg Validation Loss (mae) is 1.7751892447471618\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7134063839912415\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9527417898178101, and Avg Validation Loss (mae) is 1.0031290829181672\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0122862994670867\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5082944303750991, and Avg Validation Loss (mae) is 0.6494101256132125\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6545455396175385\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6233410000801086, and Avg Validation Loss (mae) is 0.5896336883306503\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5938299000263214\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.665052318572998, and Avg Validation Loss (mae) is 1.6240321755409242\n",
            "After 10 runs; Avg Test Loss (mae) is 1.623496985435486\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9236254155635834, and Avg Validation Loss (mae) is 0.9709862470626831\n",
            "After 10 runs; Avg Test Loss (mae) is 0.975106930732727\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7529253363609314, and Avg Validation Loss (mae) is 0.8462150633335114\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8305558353662491\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6938418686389923, and Avg Validation Loss (mae) is 0.702882245182991\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6939826220273971\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4654637575149536, and Avg Validation Loss (mae) is 1.4655978798866272\n",
            "After 10 runs; Avg Test Loss (mae) is 1.452764594554901\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0649342358112335, and Avg Validation Loss (mae) is 1.020360243320465\n",
            "After 10 runs; Avg Test Loss (mae) is 1.035676747560501\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0940744996070861, and Avg Validation Loss (mae) is 1.154119861125946\n",
            "After 10 runs; Avg Test Loss (mae) is 1.164870709180832\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8037134170532226, and Avg Validation Loss (mae) is 0.8983004689216614\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8950881421566009\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5501083612442017, and Avg Validation Loss (mae) is 1.6694005846977233\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5878061652183533\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.03539577126503, and Avg Validation Loss (mae) is 1.0180128812789917\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9983943819999694\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.728756046295166, and Avg Validation Loss (mae) is 0.7180958151817322\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7188370138406753\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6133722990751267, and Avg Validation Loss (mae) is 0.6347854018211365\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6333947151899337\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4677231550216674, and Avg Validation Loss (mae) is 1.397045338153839\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4809793829917908\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0165412425994873, and Avg Validation Loss (mae) is 1.2629952490329743\n",
            "After 10 runs; Avg Test Loss (mae) is 1.250667005777359\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7422483801841736, and Avg Validation Loss (mae) is 0.6916202038526535\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7025327265262604\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5707437992095947, and Avg Validation Loss (mae) is 0.6027360051870346\n",
            "After 10 runs; Avg Test Loss (mae) is 0.581719845533371\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5154070496559142, and Avg Validation Loss (mae) is 1.4364900231361388\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4618986248970032\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0240761399269105, and Avg Validation Loss (mae) is 0.9343877077102661\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9500336349010468\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8218952000141144, and Avg Validation Loss (mae) is 0.7137730479240417\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7027186632156373\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7614981770515442, and Avg Validation Loss (mae) is 0.7300534546375275\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7119886904954911\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6542140007019044, and Avg Validation Loss (mae) is 1.8518649101257325\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8696189045906066\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0621655464172364, and Avg Validation Loss (mae) is 1.2000810742378234\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1924015522003173\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5875485926866532, and Avg Validation Loss (mae) is 0.6416162729263306\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6365107297897339\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6445148050785064, and Avg Validation Loss (mae) is 0.7190988719463348\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7097242444753646\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5797497034072876, and Avg Validation Loss (mae) is 1.6865219354629517\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6542407751083374\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0676522076129913, and Avg Validation Loss (mae) is 1.158380913734436\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1891742587089538\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7471375465393066, and Avg Validation Loss (mae) is 0.7435168206691742\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7318953394889831\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5821165472269059, and Avg Validation Loss (mae) is 0.6811780065298081\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6946804314851761\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5069570541381836, and Avg Validation Loss (mae) is 1.478106713294983\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4936340212821961\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.2233996748924256, and Avg Validation Loss (mae) is 1.2582538366317748\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2593324899673461\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6409268736839294, and Avg Validation Loss (mae) is 0.6191828280687333\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6310675352811813\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8304576992988586, and Avg Validation Loss (mae) is 0.8221632897853851\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8336580574512482\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.482122004032135, and Avg Validation Loss (mae) is 1.587965977191925\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6027301907539369\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3508980989456176, and Avg Validation Loss (mae) is 1.3463256716728211\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3288918256759643\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7672945022583008, and Avg Validation Loss (mae) is 0.8032488524913788\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7967227101325989\n",
            "kx1 = X_FM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9515496492385864, and Avg Validation Loss (mae) is 1.0367812991142273\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0344728648662567\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8585385441780091, and Avg Validation Loss (mae) is 1.8618102550506592\n",
            "After 10 runs; Avg Test Loss (mae) is 1.897109067440033\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8932501375675201, and Avg Validation Loss (mae) is 0.8599138021469116\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8870902776718139\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6286992371082306, and Avg Validation Loss (mae) is 0.475234842300415\n",
            "After 10 runs; Avg Test Loss (mae) is 0.46100509762763975\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7340501189231873, and Avg Validation Loss (mae) is 0.7156269252300262\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7127123057842255\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9060242772102356, and Avg Validation Loss (mae) is 1.8210224986076355\n",
            "After 10 runs; Avg Test Loss (mae) is 1.824454164505005\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9514218747615815, and Avg Validation Loss (mae) is 0.9836432576179505\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9839734375476837\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.744803249835968, and Avg Validation Loss (mae) is 0.6989697098731995\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7080335289239883\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7931574165821076, and Avg Validation Loss (mae) is 1.0529906034469605\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0449564933776856\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6422432661056519, and Avg Validation Loss (mae) is 1.5691762566566467\n",
            "After 10 runs; Avg Test Loss (mae) is 1.586927056312561\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8258174419403076, and Avg Validation Loss (mae) is 0.7711594879627228\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7787872672080993\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9998818397521972, and Avg Validation Loss (mae) is 0.9385516703128814\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9402568280696869\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7178121924400329, and Avg Validation Loss (mae) is 0.8862548410892487\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8655375301837921\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8790091037750245, and Avg Validation Loss (mae) is 1.8247770071029663\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8223390579223633\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8649518609046936, and Avg Validation Loss (mae) is 0.8218801081180572\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8257194340229035\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7647731721401214, and Avg Validation Loss (mae) is 0.7024567067623139\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7112566590309143\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7347317039966583, and Avg Validation Loss (mae) is 0.7776492238044739\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7897050142288208\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6769453167915345, and Avg Validation Loss (mae) is 1.6986249327659606\n",
            "After 10 runs; Avg Test Loss (mae) is 1.71911940574646\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8061999499797821, and Avg Validation Loss (mae) is 0.784274673461914\n",
            "After 10 runs; Avg Test Loss (mae) is 0.80241579413414\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7093429148197175, and Avg Validation Loss (mae) is 0.7412850022315979\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7371816992759704\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5460473090410233, and Avg Validation Loss (mae) is 0.41763947904109955\n",
            "After 10 runs; Avg Test Loss (mae) is 0.4127637356519699\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6270236492156982, and Avg Validation Loss (mae) is 1.8335329413414\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8339412927627563\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8510791838169098, and Avg Validation Loss (mae) is 0.7885550141334534\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7927405297756195\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8086193084716797, and Avg Validation Loss (mae) is 0.8714204847812652\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8774644136428833\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6872547805309296, and Avg Validation Loss (mae) is 0.6361536860466004\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6368489593267441\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9112322807312012, and Avg Validation Loss (mae) is 1.9432548046112061\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9040242433547974\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9999184966087341, and Avg Validation Loss (mae) is 0.9552918791770935\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9970828056335449\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5260254830121994, and Avg Validation Loss (mae) is 0.7299372136592865\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7269042819738388\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7726890325546265, and Avg Validation Loss (mae) is 0.7990979015827179\n",
            "After 10 runs; Avg Test Loss (mae) is 0.78412424325943\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6861114978790284, and Avg Validation Loss (mae) is 1.618547749519348\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6424522042274474\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8316865801811218, and Avg Validation Loss (mae) is 0.9753438413143158\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9808901846408844\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5906507521867752, and Avg Validation Loss (mae) is 0.6883516162633896\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6858241528272628\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.619227385520935, and Avg Validation Loss (mae) is 0.7195544689893723\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7219550520181656\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7793835639953612, and Avg Validation Loss (mae) is 1.7404940605163575\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7928370475769042\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8852171063423157, and Avg Validation Loss (mae) is 0.7909503281116486\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8112910151481628\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6552053660154342, and Avg Validation Loss (mae) is 0.5294095695018768\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5259494870901108\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6996421456336975, and Avg Validation Loss (mae) is 0.7209807395935058\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7424564898014069\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6946339011192322, and Avg Validation Loss (mae) is 1.9154647588729858\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8458290815353393\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.901241272687912, and Avg Validation Loss (mae) is 0.9177862465381622\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9015669226646423\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5996519058942795, and Avg Validation Loss (mae) is 0.7979498028755188\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7896629810333252\n",
            "kx1 = X_MT, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7870292901992798, and Avg Validation Loss (mae) is 0.8001717686653137\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8108003556728363\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.9085269212722777, and Avg Validation Loss (mae) is 2.182427501678467\n",
            "After 10 runs; Avg Test Loss (mae) is 2.182398736476898\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9766041755676269, and Avg Validation Loss (mae) is 1.0453195571899414\n",
            "After 10 runs; Avg Test Loss (mae) is 1.063833999633789\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9200676500797271, and Avg Validation Loss (mae) is 1.032676202058792\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0306084513664246\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6801563739776612, and Avg Validation Loss (mae) is 0.7413901507854461\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7403877377510071\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6780722618103028, and Avg Validation Loss (mae) is 1.6980600595474242\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7281222939491272\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8445441782474518, and Avg Validation Loss (mae) is 0.8088159203529358\n",
            "After 10 runs; Avg Test Loss (mae) is 0.814286595582962\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0112828850746154, and Avg Validation Loss (mae) is 1.0633175671100616\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0571388185024262\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4736372768878937, and Avg Validation Loss (mae) is 0.5349415302276611\n",
            "After 10 runs; Avg Test Loss (mae) is 0.532750454545021\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6625808000564575, and Avg Validation Loss (mae) is 1.7446388840675353\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7214571714401246\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8382030189037323, and Avg Validation Loss (mae) is 1.1086418807506562\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1065994679927826\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0431769132614135, and Avg Validation Loss (mae) is 1.1842330873012543\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1839335024356843\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RM, kx4 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7152458012104035, and Avg Validation Loss (mae) is 0.7741463750600814\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7937247574329376\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6731439471244811, and Avg Validation Loss (mae) is 1.7240403652191163\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7554096698760986\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7985629558563232, and Avg Validation Loss (mae) is 0.8490874290466308\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8799227833747864\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0382147431373596, and Avg Validation Loss (mae) is 1.2704027593135834\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2705976903438567\n",
            "kx1 = X_MM, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7519878268241882, and Avg Validation Loss (mae) is 0.8056088089942932\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7980920732021332\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6719141244888305, and Avg Validation Loss (mae) is 1.6353500604629516\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6669735550880431\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8972643315792084, and Avg Validation Loss (mae) is 0.9949556291103363\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9933142900466919\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.201099932193756, and Avg Validation Loss (mae) is 1.123299640417099\n",
            "After 10 runs; Avg Test Loss (mae) is 1.142539644241333\n",
            "kx1 = X_MB, kx2 = X_RT, kx3 = X_RM, kx4 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6617547929286957, and Avg Validation Loss (mae) is 0.6836602538824081\n",
            "After 10 runs; Avg Test Loss (mae) is 0.690594545006752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XihPNGJcMJoM",
        "outputId": "be2747a9-7781-4c9d-d1fc-bd31a05a40c8"
      },
      "source": [
        "CombResultsMF4 = pd.DataFrame.from_dict(my_dictMF4)\n",
        "print(CombResultsMF4.shape)\n",
        "CombResultsSortedMF4 = CombResultsMF4.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMF4.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(280, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA_X</th>\n",
              "      <th>DATA_y</th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>X_MTX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.412764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>X_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.461005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>X_FMX_MTX_MBX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.493171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>X_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.525949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>X_FMX_MTX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.528814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>X_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.532750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>X_FMX_MTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.537416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>X_FTX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.538993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>X_FMX_MTX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.561731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>X_FTX_FMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.562576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>X_FTX_MTX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.566660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>X_FTX_MTX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.575451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>X_FTX_FMX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.578583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>X_FMX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.581720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>X_FTX_MTX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.581775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>X_FTX_FMX_MTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.585524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>X_FMX_MTX_MBX_RT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.588165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>X_FTX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.589490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>X_FTX_MTX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.590621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>X_FMX_MMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.593830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>X_FTX_FMX_MTX_MB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.595270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>X_FTX_FMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.598961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>X_FTX_FMX_MBX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.600022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>X_FTX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.611605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>X_FMX_MTX_MMX_RT</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.614450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>X_FMX_MTX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.614951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>X_FMX_MTX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.618680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>X_FMX_MTX_MBX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.620689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>X_FMX_MTX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.621046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>X_FMX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.631068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>X_FMX_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.633395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>X_FMX_MTX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.633946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>X_FMX_MBX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.636511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>X_MTX_MMX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.636849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>X_FTX_MTX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.639994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>X_FTX_MTX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.640025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>X_FMX_MTX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.642115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>X_FMX_MTX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.642866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>X_FTX_MMX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.643710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>X_FTX_FMX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.644546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>X_FTX_FMX_MMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.649414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>X_FTX_FMX_MTX_RT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.651093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>X_FMX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.654546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>X_FTX_MTX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.657703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>X_FTX_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.661120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>X_FTX_MTX_MBX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.661488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>X_FTX_FMX_MMX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.663885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>X_FTX_FMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.665721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>X_FTX_FMX_MTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.671943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>X_FTX_MTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.677295</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               DATA_X DATA_y  Test Loss\n",
              "239  X_MTX_MMX_RTX_RB   y_RM   0.412764\n",
              "222  X_MTX_MMX_MBX_RT   y_RM   0.461005\n",
              "161  X_FMX_MTX_MBX_RM   y_MM   0.493171\n",
              "254  X_MTX_MBX_RMX_RB   y_MM   0.525949\n",
              "146  X_FMX_MTX_MMX_RT   y_RM   0.528814\n",
              "267  X_MMX_MBX_RTX_RB   y_RM   0.532750\n",
              "177  X_FMX_MTX_RMX_RB   y_MM   0.537416\n",
              "119  X_FTX_MMX_RTX_RB   y_RM   0.538993\n",
              "171  X_FMX_MTX_RTX_RM   y_RB   0.561731\n",
              "55   X_FTX_FMX_RTX_RB   y_RM   0.562576\n",
              "93   X_FTX_MTX_RTX_RB   y_MM   0.566660\n",
              "89   X_FTX_MTX_RTX_RM   y_MM   0.575451\n",
              "43   X_FTX_FMX_MBX_RM   y_RB   0.578583\n",
              "199  X_FMX_MMX_RTX_RB   y_RM   0.581720\n",
              "66   X_FTX_MTX_MMX_RT   y_RM   0.581775\n",
              "12   X_FTX_FMX_MTX_RM   y_MM   0.585524\n",
              "157  X_FMX_MTX_MBX_RT   y_MM   0.588165\n",
              "127  X_FTX_MBX_RTX_RM   y_RB   0.589490\n",
              "95   X_FTX_MTX_RTX_RB   y_RM   0.590621\n",
              "183  X_FMX_MMX_MBX_RT   y_RB   0.593830\n",
              "4    X_FTX_FMX_MTX_MB   y_MM   0.595270\n",
              "51   X_FTX_FMX_RTX_RM   y_RB   0.598961\n",
              "45   X_FTX_FMX_MBX_RB   y_MM   0.600022\n",
              "131  X_FTX_MBX_RTX_RB   y_RM   0.611605\n",
              "145  X_FMX_MTX_MMX_RT   y_MB   0.614450\n",
              "163  X_FMX_MTX_MBX_RM   y_RB   0.614951\n",
              "173  X_FMX_MTX_RTX_RB   y_MM   0.618680\n",
              "165  X_FMX_MTX_MBX_RB   y_MM   0.620689\n",
              "169  X_FMX_MTX_RTX_RM   y_MM   0.621046\n",
              "214  X_FMX_MBX_RMX_RB   y_MM   0.631068\n",
              "195  X_FMX_MMX_RTX_RM   y_RB   0.633395\n",
              "175  X_FMX_MTX_RTX_RB   y_RM   0.633946\n",
              "206  X_FMX_MBX_RTX_RM   y_MM   0.636511\n",
              "243  X_MTX_MMX_RMX_RB   y_RT   0.636849\n",
              "78   X_FTX_MTX_MBX_RT   y_RM   0.639994\n",
              "79   X_FTX_MTX_MBX_RT   y_RB   0.640025\n",
              "179  X_FMX_MTX_RMX_RB   y_RT   0.642115\n",
              "159  X_FMX_MTX_MBX_RT   y_RB   0.642866\n",
              "122  X_FTX_MMX_RMX_RB   y_MB   0.643710\n",
              "26   X_FTX_FMX_MMX_RT   y_RM   0.644546\n",
              "33   X_FTX_FMX_MMX_RB   y_MB   0.649414\n",
              "8    X_FTX_FMX_MTX_RT   y_MM   0.651093\n",
              "182  X_FMX_MMX_MBX_RT   y_RM   0.654546\n",
              "83   X_FTX_MTX_MBX_RM   y_RB   0.657703\n",
              "115  X_FTX_MMX_RTX_RM   y_RB   0.661120\n",
              "81   X_FTX_MTX_MBX_RM   y_MM   0.661488\n",
              "31   X_FTX_FMX_MMX_RM   y_RB   0.663885\n",
              "38   X_FTX_FMX_MBX_RT   y_RM   0.665721\n",
              "16   X_FTX_FMX_MTX_RB   y_MM   0.671943\n",
              "97   X_FTX_MTX_RMX_RB   y_MM   0.677295"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "mAeOw0XyMJoM",
        "outputId": "88c727ec-1a73-4752-d5e0-3a64eee5b5ca"
      },
      "source": [
        "CombResultsSortedMFgrouped4 = CombResultsSortedMF4.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedMFgroupedsortedMF4 = CombResultsSortedMFgrouped4.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMFgroupedsortedMF4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RTX_RB</th>\n",
              "      <td>0.674894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RM</th>\n",
              "      <td>0.706065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RT</th>\n",
              "      <td>0.711997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RT</th>\n",
              "      <td>0.726634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RTX_RM</th>\n",
              "      <td>0.742021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_RTX_RMX_RB</th>\n",
              "      <td>1.176006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MB</th>\n",
              "      <td>1.177994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_RTX_RMX_RB</th>\n",
              "      <td>1.190704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RMX_RB</th>\n",
              "      <td>1.201429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RTX_RM</th>\n",
              "      <td>1.254307</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows  1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Test Loss\n",
              "DATA_X                     \n",
              "X_FTX_MTX_RTX_RB   0.674894\n",
              "X_FTX_MTX_MBX_RM   0.706065\n",
              "X_FTX_MTX_MMX_RT   0.711997\n",
              "X_FTX_FMX_MTX_RT   0.726634\n",
              "X_FTX_MTX_RTX_RM   0.742021\n",
              "...                     ...\n",
              "X_MMX_RTX_RMX_RB   1.176006\n",
              "X_FTX_FMX_MMX_MB   1.177994\n",
              "X_FMX_RTX_RMX_RB   1.190704\n",
              "X_MMX_MBX_RMX_RB   1.201429\n",
              "X_MMX_MBX_RTX_RM   1.254307\n",
              "\n",
              "[70 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "r8JL-9aVmWd6",
        "outputId": "f9687da5-38da-432b-e398-8c63b569f6fa"
      },
      "source": [
        "CombResultsSortedMFgroupedsortedMF4.to_csv('CombResultsSortedMFgroupedsortedMF4.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedMFgroupedsortedMF4.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0d1c61d9-9541-4169-bfe9-1f17e7c41d6d\", \"CombResultsSortedMFgroupedsortedMF4.csv\", 2519)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "X6UIC9iInIVT",
        "outputId": "e4d47426-3328-4ecd-e967-14c733468563"
      },
      "source": [
        "CombResultsSortedMF4.to_csv('CombResultsSortedMF4.csv')\n",
        "files.download(\"CombResultsSortedMF4.csv\")\n",
        "\n",
        "fig = px.box(CombResultsSortedMF4, x=\"DATA_X\", y=\"Test Loss\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_01b3c53f-d871-4419-9713-0c1e115c3799\", \"CombResultsSortedMF4.csv\", 12460)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"16332097-63c2-449c-9cb1-55fcaee5d6f2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"16332097-63c2-449c-9cb1-55fcaee5d6f2\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '16332097-63c2-449c-9cb1-55fcaee5d6f2',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_MTX_MMX_RTX_RB\", \"X_MTX_MMX_MBX_RT\", \"X_FMX_MTX_MBX_RM\", \"X_MTX_MBX_RMX_RB\", \"X_FMX_MTX_MMX_RT\", \"X_MMX_MBX_RTX_RB\", \"X_FMX_MTX_RMX_RB\", \"X_FTX_MMX_RTX_RB\", \"X_FMX_MTX_RTX_RM\", \"X_FTX_FMX_RTX_RB\", \"X_FTX_MTX_RTX_RB\", \"X_FTX_MTX_RTX_RM\", \"X_FTX_FMX_MBX_RM\", \"X_FMX_MMX_RTX_RB\", \"X_FTX_MTX_MMX_RT\", \"X_FTX_FMX_MTX_RM\", \"X_FMX_MTX_MBX_RT\", \"X_FTX_MBX_RTX_RM\", \"X_FTX_MTX_RTX_RB\", \"X_FMX_MMX_MBX_RT\", \"X_FTX_FMX_MTX_MB\", \"X_FTX_FMX_RTX_RM\", \"X_FTX_FMX_MBX_RB\", \"X_FTX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RT\", \"X_FMX_MTX_MBX_RM\", \"X_FMX_MTX_RTX_RB\", \"X_FMX_MTX_MBX_RB\", \"X_FMX_MTX_RTX_RM\", \"X_FMX_MBX_RMX_RB\", \"X_FMX_MMX_RTX_RM\", \"X_FMX_MTX_RTX_RB\", \"X_FMX_MBX_RTX_RM\", \"X_MTX_MMX_RMX_RB\", \"X_FTX_MTX_MBX_RT\", \"X_FTX_MTX_MBX_RT\", \"X_FMX_MTX_RMX_RB\", \"X_FMX_MTX_MBX_RT\", \"X_FTX_MMX_RMX_RB\", \"X_FTX_FMX_MMX_RT\", \"X_FTX_FMX_MMX_RB\", \"X_FTX_FMX_MTX_RT\", \"X_FMX_MMX_MBX_RT\", \"X_FTX_MTX_MBX_RM\", \"X_FTX_MMX_RTX_RM\", \"X_FTX_MTX_MBX_RM\", \"X_FTX_FMX_MMX_RM\", \"X_FTX_FMX_MBX_RT\", \"X_FTX_FMX_MTX_RB\", \"X_FTX_MTX_RMX_RB\", \"X_FTX_FMX_MMX_RT\", \"X_FTX_FMX_MBX_RT\", \"X_MTX_MBX_RTX_RB\", \"X_MBX_RTX_RMX_RB\", \"X_FTX_MMX_MBX_RM\", \"X_FMX_MMX_MBX_RM\", \"X_FMX_MBX_RTX_RB\", \"X_FTX_MBX_RTX_RB\", \"X_FMX_MTX_MBX_RM\", \"X_FMX_MMX_RTX_RB\", \"X_FMX_MMX_RMX_RB\", \"X_FTX_MTX_RTX_RM\", \"X_FTX_MBX_RMX_RB\", \"X_FTX_MMX_RTX_RB\", \"X_FMX_MTX_MMX_RM\", \"X_MTX_MMX_MBX_RM\", \"X_FMX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_RT\", \"X_MTX_MMX_RTX_RM\", \"X_FMX_MMX_RMX_RB\", \"X_FTX_MMX_RMX_RB\", \"X_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MTX_RT\", \"X_FTX_MTX_MBX_RM\", \"X_FTX_MTX_MMX_RM\", \"X_FMX_MMX_RTX_RM\", \"X_MTX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RT\", \"X_FTX_MTX_MMX_RB\", \"X_FMX_MTX_MMX_RM\", \"X_MTX_MBX_RTX_RM\", \"X_FMX_MTX_MMX_RB\", \"X_FMX_MTX_RTX_RB\", \"X_FMX_MBX_RTX_RB\", \"X_FTX_MMX_MBX_RM\", \"X_FTX_MMX_MBX_RB\", \"X_MTX_MMX_RTX_RB\", \"X_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MBX_RT\", \"X_MTX_MBX_RMX_RB\", \"X_FMX_MTX_MBX_RT\", \"X_FTX_FMX_MTX_RT\", \"X_FTX_FMX_RMX_RB\", \"X_FTX_MTX_MMX_RT\", \"X_FTX_MTX_RTX_RM\", \"X_FTX_MMX_MBX_RT\", \"X_FTX_MMX_MBX_RT\", \"X_FTX_FMX_MMX_RM\", \"X_FTX_FMX_MBX_RM\", \"X_FTX_MTX_RTX_RB\", \"X_FMX_MTX_MMX_RB\", \"X_FTX_MMX_RTX_RB\", \"X_MTX_MMX_MBX_RB\", \"X_FTX_MTX_RTX_RB\", \"X_MTX_MBX_RTX_RM\", \"X_MTX_RTX_RMX_RB\", \"X_MTX_MMX_RTX_RM\", \"X_FTX_MTX_MBX_RM\", \"X_MTX_MMX_RMX_RB\", \"X_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_RM\", \"X_FTX_FMX_MTX_RT\", \"X_FMX_RTX_RMX_RB\", \"X_FTX_MTX_RMX_RB\", \"X_MMX_RTX_RMX_RB\", \"X_FTX_MTX_RMX_RB\", \"X_FTX_MTX_MMX_RT\", \"X_FTX_MTX_MBX_RB\", \"X_MTX_MMX_RTX_RB\", \"X_FMX_MTX_RTX_RM\", \"X_FTX_MBX_RTX_RM\", \"X_FTX_MTX_MBX_RB\", \"X_FTX_MMX_RTX_RM\", \"X_FMX_MTX_MMX_MB\", \"X_MTX_RTX_RMX_RB\", \"X_MTX_MBX_RMX_RB\", \"X_FTX_MTX_MMX_RB\", \"X_MMX_MBX_RTX_RB\", \"X_FTX_MTX_MBX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_FTX_FMX_MTX_MB\", \"X_FTX_MTX_RMX_RB\", \"X_MTX_MMX_RTX_RM\", \"X_FTX_MMX_MBX_RM\", \"X_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_RM\", \"X_FTX_MTX_MMX_RM\", \"X_FMX_MBX_RMX_RB\", \"X_FTX_MMX_MBX_RB\", \"X_FTX_MMX_RMX_RB\", \"X_FTX_FMX_RMX_RB\", \"X_FMX_MTX_MMX_RM\", \"X_FTX_FMX_MTX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_FMX_MTX_RMX_RB\", \"X_FTX_FMX_MBX_RM\", \"X_FTX_MTX_MMX_RM\", \"X_FTX_MTX_MMX_RM\", \"X_MTX_MMX_MBX_RB\", \"X_FTX_MBX_RMX_RB\", \"X_FMX_MTX_MMX_MB\", \"X_FTX_FMX_MMX_RB\", \"X_MTX_MMX_RMX_RB\", \"X_MMX_RTX_RMX_RB\", \"X_FTX_MMX_RTX_RM\", \"X_FTX_FMX_MMX_RM\", \"X_FTX_MMX_MBX_RT\", \"X_MTX_MMX_MBX_RT\", \"X_FTX_MTX_MBX_RT\", \"X_FTX_FMX_MMX_RT\", \"X_FMX_MMX_MBX_RB\", \"X_FTX_MTX_MBX_RB\", \"X_FTX_RTX_RMX_RB\", \"X_MTX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MM\", \"X_FTX_MTX_MMX_RB\", \"X_FTX_MTX_MBX_RT\", \"X_FTX_FMX_RTX_RM\", \"X_FTX_FMX_MTX_RM\", \"X_FTX_FMX_MTX_RB\", \"X_FTX_FMX_MMX_MB\", \"X_FTX_MTX_RTX_RM\", \"X_MTX_MMX_MBX_RB\", \"X_FTX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RB\", \"X_FMX_MMX_RMX_RB\", \"X_FMX_MTX_MBX_RB\", \"X_FTX_MBX_RTX_RM\", \"X_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_MM\", \"X_FMX_MTX_MMX_RB\", \"X_MTX_MBX_RTX_RB\", \"X_MTX_MMX_MBX_RM\", \"X_MBX_RTX_RMX_RB\", \"X_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MMX_RT\", \"X_FTX_FMX_MBX_RB\", \"X_FMX_MMX_RTX_RM\", \"X_FMX_MMX_MBX_RT\", \"X_FTX_FMX_RTX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_FTX_MMX_MBX_RB\", \"X_MMX_MBX_RTX_RM\", \"X_FMX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RB\", \"X_FTX_MMX_RTX_RM\", \"X_FTX_MBX_RMX_RB\", \"X_FTX_MMX_RMX_RB\", \"X_FTX_FMX_RTX_RM\", \"X_MTX_MMX_MBX_RM\", \"X_FTX_RTX_RMX_RB\", \"X_FTX_MMX_RTX_RB\", \"X_FMX_MTX_MBX_RB\", \"X_MMX_MBX_RTX_RB\", \"X_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MMX_RB\", \"X_FTX_FMX_RTX_RB\", \"X_FTX_FMX_MMX_MB\", \"X_FTX_FMX_RMX_RB\", \"X_FTX_FMX_MBX_RB\", \"X_FTX_FMX_MTX_MM\", \"X_FTX_MMX_MBX_RM\", \"X_FTX_FMX_MBX_RT\", \"X_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RM\", \"X_FTX_FMX_MBX_RB\", \"X_FTX_FMX_MMX_MB\", \"X_MBX_RTX_RMX_RB\", \"X_FTX_MMX_MBX_RT\", \"X_FMX_MMX_MBX_RB\", \"X_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_RB\", \"X_FMX_MBX_RTX_RB\", \"X_FMX_MBX_RTX_RM\", \"X_FTX_MBX_RTX_RB\", \"X_FTX_MMX_MBX_RB\", \"X_FTX_MTX_MMX_MB\", \"X_FMX_MMX_RTX_RB\", \"X_FTX_FMX_MBX_RM\", \"X_FMX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RB\", \"X_MMX_RTX_RMX_RB\", \"X_FTX_MBX_RTX_RM\", \"X_FTX_FMX_RTX_RB\", \"X_FMX_MTX_MMX_MB\", \"X_FMX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MB\", \"X_FMX_MTX_MBX_RB\", \"X_FTX_MBX_RMX_RB\", \"X_FMX_MMX_MBX_RB\", \"X_FMX_MTX_RMX_RB\", \"X_FMX_MMX_RMX_RB\", \"X_FTX_MBX_RTX_RB\", \"X_FMX_MMX_RTX_RB\", \"X_FTX_FMX_RMX_RB\", \"X_FMX_MTX_MMX_MB\", \"X_FMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MM\", \"X_FMX_MTX_RTX_RM\", \"X_FTX_FMX_MTX_MB\", \"X_FTX_FMX_RTX_RM\", \"X_FMX_MTX_RTX_RB\", \"X_FTX_FMX_MMX_MB\", \"X_MTX_MMX_MBX_RB\", \"X_FMX_MMX_RTX_RM\", \"X_FMX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RM\", \"X_FMX_MTX_MBX_RT\", \"X_FTX_RTX_RMX_RB\", \"X_MTX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RB\", \"X_FMX_MBX_RTX_RB\", \"X_MBX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RT\", \"X_MTX_MMX_RTX_RB\", \"X_MMX_MBX_RMX_RB\", \"X_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RM\", \"X_FMX_MTX_MMX_RT\", \"X_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MBX_RM\", \"X_MTX_MBX_RMX_RB\", \"X_MTX_MMX_RTX_RM\", \"X_MTX_MMX_MBX_RM\", \"X_MTX_MMX_RMX_RB\", \"X_MTX_RTX_RMX_RB\", \"X_FMX_MBX_RTX_RM\", \"X_MTX_MMX_MBX_RT\", \"X_MTX_MBX_RTX_RM\", \"X_MMX_MBX_RTX_RM\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.4127637356519699, 0.46100509762763975, 0.4931711435317993, 0.5259494870901108, 0.5288140565156937, 0.532750454545021, 0.5374155402183532, 0.5389934092760086, 0.5617314875125885, 0.5625763773918152, 0.5666598826646805, 0.5754513621330262, 0.578582513332367, 0.581719845533371, 0.581775176525116, 0.5855235636234284, 0.588164547085762, 0.5894904226064682, 0.5906214416027069, 0.5938299000263214, 0.595270323753357, 0.5989613682031631, 0.6000216245651245, 0.6116054654121399, 0.6144496887922287, 0.6149511456489563, 0.6186796844005584, 0.6206890255212784, 0.6210460126399994, 0.6310675352811813, 0.6333947151899337, 0.6339463621377945, 0.6365107297897339, 0.6368489593267441, 0.6399941504001617, 0.640024608373642, 0.6421148121356964, 0.6428664028644562, 0.6437096059322357, 0.6445459663867951, 0.6494135320186615, 0.6510933667421341, 0.6545455396175385, 0.6577032715082168, 0.6611201286315918, 0.6614878088235855, 0.6638850212097168, 0.6657209485769272, 0.6719425618648529, 0.6772949516773223, 0.6811392217874527, 0.6830884367227554, 0.6858241528272628, 0.690594545006752, 0.690717589855194, 0.6939826220273971, 0.6946804314851761, 0.6964779317378997, 0.7001205742359161, 0.7025327265262604, 0.7027186632156373, 0.7027208030223846, 0.7027892708778382, 0.7046156644821167, 0.7072295695543289, 0.7080335289239883, 0.7097242444753646, 0.710794597864151, 0.7112566590309143, 0.7119886904954911, 0.7120513379573822, 0.7127123057842255, 0.7134125351905822, 0.714645916223526, 0.7147448539733887, 0.7188370138406753, 0.7219550520181656, 0.7222394287586212, 0.7258809506893158, 0.7261843323707581, 0.7269042819738388, 0.7307227551937103, 0.7308360457420349, 0.7318953394889831, 0.7351144015789032, 0.7364757478237152, 0.7371816992759704, 0.7403877377510071, 0.7411780506372452, 0.7424564898014069, 0.7428317219018936, 0.7474124372005463, 0.7515765190124511, 0.7552183926105499, 0.7553229033946991, 0.7557681381702424, 0.7579279601573944, 0.7598480582237244, 0.7605031251907348, 0.7621240615844727, 0.767251405119896, 0.7696822941303253, 0.7787872672080993, 0.7801692128181458, 0.78412424325943, 0.7896629810333252, 0.7897050142288208, 0.7904233276844025, 0.7927405297756195, 0.7937247574329376, 0.7937292993068695, 0.7946194410324097, 0.7967227101325989, 0.7971456289291382, 0.7980920732021332, 0.799152547121048, 0.8001988053321838, 0.8005081653594971, 0.80241579413414, 0.8024623215198516, 0.8043825387954712, 0.8082094073295594, 0.8082313239574432, 0.8098717749118804, 0.8108003556728363, 0.8112910151481628, 0.812053307890892, 0.814286595582962, 0.815443480014801, 0.8173675894737243, 0.8200599431991578, 0.820230457186699, 0.8257194340229035, 0.829956340789795, 0.8305558353662491, 0.8306457042694092, 0.832785227894783, 0.8336580574512482, 0.8362796902656555, 0.8365200579166412, 0.8376691222190857, 0.838022118806839, 0.8411434173583985, 0.8421301782131195, 0.844562566280365, 0.8455284893512726, 0.8495836228132247, 0.8503449857234955, 0.8655375301837921, 0.8675486803054809, 0.871476811170578, 0.8716757833957672, 0.8774644136428833, 0.8799227833747864, 0.8801316261291504, 0.882144284248352, 0.883879441022873, 0.8870902776718139, 0.8878775149583816, 0.8946038782596588, 0.8950881421566009, 0.89574134349823, 0.8966112911701203, 0.9015669226646423, 0.9031723260879516, 0.9066274344921113, 0.914964097738266, 0.9206372797489166, 0.9238629639148712, 0.9251620173454285, 0.9291528463363647, 0.9345882356166839, 0.9402568280696869, 0.9447169423103332, 0.9491433739662171, 0.9500336349010468, 0.96524977684021, 0.96690132021904, 0.975106930732727, 0.9754471600055694, 0.9784148275852204, 0.9808901846408844, 0.9839734375476837, 0.9933142900466919, 0.9970828056335449, 0.9971264243125916, 0.9977567493915558, 0.9983943819999694, 1.0122862994670867, 1.0164918541908263, 1.0210439562797546, 1.0222411513328553, 1.0306084513664246, 1.0344728648662567, 1.035676747560501, 1.0380515933036805, 1.0407486855983734, 1.0435486257076263, 1.043584442138672, 1.0449564933776856, 1.0456037819385529, 1.0464218080043792, 1.0523074865341187, 1.0571388185024262, 1.063833999633789, 1.071741485595703, 1.072948509454727, 1.0789916396141053, 1.0794279754161835, 1.0829204857349395, 1.0846634745597838, 1.0887119591236114, 1.0889474987983703, 1.1065994679927826, 1.1177762508392335, 1.1274255394935608, 1.1353368520736695, 1.142539644241333, 1.1445922374725341, 1.164870709180832, 1.1839335024356843, 1.1854198276996613, 1.1891742587089538, 1.1924015522003173, 1.2044025003910064, 1.2092278063297273, 1.2503778457641601, 1.250667005777359, 1.2538671314716339, 1.2593324899673461, 1.2695529222488404, 1.2705976903438567, 1.2738427639007568, 1.3188342452049255, 1.3245244860649108, 1.3288918256759643, 1.3508085787296296, 1.4105515718460082, 1.4195578813552856, 1.452764594554901, 1.4600336074829101, 1.4618986248970032, 1.4793835699558258, 1.4809793829917908, 1.4853735446929932, 1.4882815718650817, 1.4936340212821961, 1.5055761694908143, 1.514040434360504, 1.5204498291015625, 1.522480857372284, 1.5558860778808594, 1.5684947848320008, 1.586927056312561, 1.5878061652183533, 1.6027301907539369, 1.623496985435486, 1.6252970933914184, 1.6418753385543823, 1.6424522042274474, 1.6479520082473755, 1.6542407751083374, 1.6669735550880431, 1.7134063839912415, 1.71911940574646, 1.7214571714401246, 1.7281222939491272, 1.735303556919098, 1.7439912557601929, 1.7554096698760986, 1.757822859287262, 1.7928370475769042, 1.8223390579223633, 1.824454164505005, 1.8339412927627563, 1.8458290815353393, 1.8696189045906066, 1.897109067440033, 1.9040242433547974, 2.182398736476898], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('16332097-63c2-449c-9cb1-55fcaee5d6f2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9bABRGTnKVp"
      },
      "source": [
        "# define baseline model 5\n",
        "# create model\n",
        "\n",
        "model5 = Sequential()\n",
        "model5.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(35, 1)))\n",
        "model5.add(MaxPooling1D(pool_size=2))\n",
        "model5.add(Flatten())\n",
        "model5.add(Dense(50, activation='relu'))\n",
        "model5.add(Dense(1))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "model5.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "#model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h68hbp59nKVq"
      },
      "source": [
        "\n",
        "def datageneratorMF5(X_in1, X_in2, X_in3, X_in4, X_in5, Y_in):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  X_in = np.concatenate((X_in1, X_in2, X_in3, X_in4, X_in5), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "\n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], 1))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], 1))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], 1))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcrwvrknnKVq"
      },
      "source": [
        "def evaldataMF5(X_in1, X_in2, X_in3, X_in4, X_in5, Y_in, traindata1, traindata2, traindata3, traindata4, traindata5, testdata):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMF5(X_in1, X_in2, X_in3, X_in4, X_in5, Y_in)\n",
        "  \n",
        "  history = model5.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model5.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model5.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # PLOTS LOSS VS EPOCH\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  # print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  # print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # PLOTS Y ORIGINAL VS PREDICTED\n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata1)+ str(traindata2)  + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMj4oSI3nKVq",
        "outputId": "71180348-05cb-4fa7-eba7-e252d587cf98"
      },
      "source": [
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "\n",
        "model5.save_weights('model5.h5')\n",
        "\n",
        "my_dictMF5 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), 5):\n",
        "  kX1, kX2, kX3, kX4, kX5 = combo[0][0], combo[1][0], combo[2][0], combo[3][0], combo[4][0]\n",
        "  vX1, vX2, vX3, vX4, vX5 = combo[0][1], combo[1][1], combo[2][1], combo[3][1], combo[4][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:] or ky[-2:] == kX4[-2:] or ky[-2:] == kX5[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, kx4 = {kX4}, kx5 = {kX5}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF5 = evaldataMF5(vX1, vX2, vX3, vX4, vX5, vy, kX1, kX2, kX3, kX4, kX5, ky)\n",
        "      TestLossTotal = resultsMF5[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF5[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF5[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}_{kX2}_{kX3}_{kX4}_{kX5} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF5[\"DATA_X\"].append(kX1 + kX2 + kX3 + kX4 + kX5)\n",
        "    my_dictMF5[\"DATA_y\"].append(ky)\n",
        "    my_dictMF5[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model5.load_weights('model5.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3200432121753694, and Avg Validation Loss (mae) is 1.2342382729053498\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2037676274776459\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9219467461109161, and Avg Validation Loss (mae) is 0.9372364699840545\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9571249604225158\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7435882449150085, and Avg Validation Loss (mae) is 0.8135351479053498\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8201122939586639\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6291895627975463, and Avg Validation Loss (mae) is 0.6603876411914825\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6695978134870529\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5893002390861511, and Avg Validation Loss (mae) is 0.5382329553365708\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5447245329618454\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5967609465122223, and Avg Validation Loss (mae) is 0.7318939983844757\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7403128743171692\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7044037222862244, and Avg Validation Loss (mae) is 0.8264419674873352\n",
            "After 10 runs; Avg Test Loss (mae) is 0.821193951368332\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7297820925712586, and Avg Validation Loss (mae) is 0.6905987650156021\n",
            "After 10 runs; Avg Test Loss (mae) is 0.683251827955246\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6806114912033081, and Avg Validation Loss (mae) is 0.7197974562644959\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7360159456729889\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7271349608898163, and Avg Validation Loss (mae) is 0.766979557275772\n",
            "After 10 runs; Avg Test Loss (mae) is 0.75755635201931\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0511201441287994, and Avg Validation Loss (mae) is 1.2263967156410218\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2143423020839692\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7415220379829407, and Avg Validation Loss (mae) is 0.887241679430008\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8707434564828873\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5892566323280335, and Avg Validation Loss (mae) is 0.5714721411466599\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5860203176736831\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6289970993995666, and Avg Validation Loss (mae) is 0.6880136609077454\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6868196874856949\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6428837180137634, and Avg Validation Loss (mae) is 0.519656366109848\n",
            "After 10 runs; Avg Test Loss (mae) is 0.534953448176384\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5127233475446701, and Avg Validation Loss (mae) is 0.5878366947174072\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5663093835115433\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7636947393417358, and Avg Validation Loss (mae) is 0.6982540309429168\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6690142393112183\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6527710586786271, and Avg Validation Loss (mae) is 0.6780120491981506\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6876676887273788\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6815367221832276, and Avg Validation Loss (mae) is 0.9799261152744293\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9579869002103806\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9970089673995972, and Avg Validation Loss (mae) is 1.1051006674766541\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1161822855472565\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8446315288543701, and Avg Validation Loss (mae) is 1.1291063725948334\n",
            "After 10 runs; Avg Test Loss (mae) is 1.120361989736557\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6739500850439072, and Avg Validation Loss (mae) is 0.5776392370462418\n",
            "After 10 runs; Avg Test Loss (mae) is 0.568505248427391\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7702372670173645, and Avg Validation Loss (mae) is 0.818953275680542\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8254526078701019\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6383229047060013, and Avg Validation Loss (mae) is 0.6737939119338989\n",
            "After 10 runs; Avg Test Loss (mae) is 0.676747339963913\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6617928147315979, and Avg Validation Loss (mae) is 0.6934041470289231\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6876679807901382\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.834026426076889, and Avg Validation Loss (mae) is 0.7968123376369476\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7937827169895172\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6220429569482804, and Avg Validation Loss (mae) is 0.5167291820049286\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5205554097890854\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5408269673585892, and Avg Validation Loss (mae) is 0.5055735021829605\n",
            "After 10 runs; Avg Test Loss (mae) is 0.523774042725563\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7811449289321899, and Avg Validation Loss (mae) is 0.7304211556911469\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7264602363109589\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7049900650978088, and Avg Validation Loss (mae) is 0.897276645898819\n",
            "After 10 runs; Avg Test Loss (mae) is 0.925607168674469\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9785193562507629, and Avg Validation Loss (mae) is 1.0131892621517182\n",
            "After 10 runs; Avg Test Loss (mae) is 1.014307087659836\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5806235909461975, and Avg Validation Loss (mae) is 0.6956115126609802\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6824948072433472\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6447633981704712, and Avg Validation Loss (mae) is 0.6454755425453186\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6572823137044906\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.972747141122818, and Avg Validation Loss (mae) is 1.002729058265686\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0120429813861846\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8628961324691773, and Avg Validation Loss (mae) is 0.9557465612888336\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9572633981704712\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7123929738998414, and Avg Validation Loss (mae) is 0.8504927337169648\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8642674863338471\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0837490379810333, and Avg Validation Loss (mae) is 1.0249719977378846\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0009860038757323\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1633759558200836, and Avg Validation Loss (mae) is 1.1947186827659606\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1708573520183563\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8563571035861969, and Avg Validation Loss (mae) is 0.8029439210891723\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7809270441532135\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0074008703231812, and Avg Validation Loss (mae) is 1.2161241471767426\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2020801961421967\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.739872133731842, and Avg Validation Loss (mae) is 0.7002626717090606\n",
            "After 10 runs; Avg Test Loss (mae) is 0.702626422047615\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5852676212787629, and Avg Validation Loss (mae) is 0.6728052645921707\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6671377897262574\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0450275003910066, and Avg Validation Loss (mae) is 0.9613332927227021\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9748745799064636\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.69008469581604, and Avg Validation Loss (mae) is 0.9093592941761017\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8984594762325286\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.4977502375841141, and Avg Validation Loss (mae) is 0.6395107269287109\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6271134555339813\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0484813153743744, and Avg Validation Loss (mae) is 1.078258740901947\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0464823603630067\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6951841413974762, and Avg Validation Loss (mae) is 0.7248370617628097\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7521167784929276\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7823135256767273, and Avg Validation Loss (mae) is 1.0168296098709106\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0150964319705964\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9908586084842682, and Avg Validation Loss (mae) is 1.2813290119171143\n",
            "After 10 runs; Avg Test Loss (mae) is 1.270822960138321\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5637276858091355, and Avg Validation Loss (mae) is 0.6471650630235672\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6529379516839982\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5734357535839081, and Avg Validation Loss (mae) is 0.6599893450737\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6609559535980225\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0174355626106262, and Avg Validation Loss (mae) is 1.048004412651062\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0450845062732697\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6487378001213073, and Avg Validation Loss (mae) is 0.7486970663070679\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7617838025093079\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5943260908126831, and Avg Validation Loss (mae) is 0.51938516497612\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5293367594480515\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1598608493804932, and Avg Validation Loss (mae) is 1.3827145159244538\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3624408543109894\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6566777884960174, and Avg Validation Loss (mae) is 0.6870039016008377\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6996959805488586\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8091870725154877, and Avg Validation Loss (mae) is 0.9663768410682678\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9662828326225281\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.3002270221710206, and Avg Validation Loss (mae) is 1.3765116214752198\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3759739756584168\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8024003684520722, and Avg Validation Loss (mae) is 0.9082742631435394\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9113840162754059\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9971939980983734, and Avg Validation Loss (mae) is 0.9184741139411926\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9088384449481964\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7761647284030915, and Avg Validation Loss (mae) is 0.7617583215236664\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7727800667285919\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6666549682617188, and Avg Validation Loss (mae) is 0.6607094675302505\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6652423948049545\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5506611645221711, and Avg Validation Loss (mae) is 0.6347503513097763\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6535855591297149\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7337062537670136, and Avg Validation Loss (mae) is 0.707870364189148\n",
            "After 10 runs; Avg Test Loss (mae) is 0.706611156463623\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7236825793981552, and Avg Validation Loss (mae) is 0.7828900903463364\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7904301673173905\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6701736330986023, and Avg Validation Loss (mae) is 0.7802827090024949\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7731788277626037\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7229247748851776, and Avg Validation Loss (mae) is 0.6242432594299316\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6178241193294525\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9988035202026367, and Avg Validation Loss (mae) is 1.159078848361969\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1804408490657807\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8616952121257782, and Avg Validation Loss (mae) is 0.8854235827922821\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8814655900001526\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7781689703464508, and Avg Validation Loss (mae) is 0.8198851406574249\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8215929210186005\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7524265885353089, and Avg Validation Loss (mae) is 0.646038830280304\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6477336645126343\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5980286419391632, and Avg Validation Loss (mae) is 0.7536452293395997\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7279122561216355\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7829183995723724, and Avg Validation Loss (mae) is 0.8011413156986237\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8085051834583282\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7718131899833679, and Avg Validation Loss (mae) is 0.7173572599887847\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7466978669166565\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5728424489498138, and Avg Validation Loss (mae) is 0.5779311269521713\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5749097615480423\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8092540442943573, and Avg Validation Loss (mae) is 0.782386589050293\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7686871767044068\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7372551739215851, and Avg Validation Loss (mae) is 0.8927127122879028\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8818337202072144\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8152804493904113, and Avg Validation Loss (mae) is 0.9461792171001434\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9446096301078797\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8709723591804505, and Avg Validation Loss (mae) is 0.8192005753517151\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7932888209819794\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6169308841228485, and Avg Validation Loss (mae) is 0.7234485477209092\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7268843621015548\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6550973862409591, and Avg Validation Loss (mae) is 0.8089237868785858\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8176390022039414\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8099449276924133, and Avg Validation Loss (mae) is 1.0013423323631288\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0064898550510406\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6361641347408294, and Avg Validation Loss (mae) is 0.7639622002840042\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7721850126981735\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6243634283542633, and Avg Validation Loss (mae) is 0.569554191827774\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5722467541694641\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8049218773841857, and Avg Validation Loss (mae) is 0.8387478470802308\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8562291860580444\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6871269166469574, and Avg Validation Loss (mae) is 0.5194786489009857\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5194498151540756\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7119284212589264, and Avg Validation Loss (mae) is 0.8656939476728439\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8885425060987473\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7750816464424133, and Avg Validation Loss (mae) is 0.8373150944709777\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8483352661132812\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6483899086713791, and Avg Validation Loss (mae) is 0.6159198641777038\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6184046119451523\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.754954320192337, and Avg Validation Loss (mae) is 0.871948653459549\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8569511890411377\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8364180028438568, and Avg Validation Loss (mae) is 0.770922714471817\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7923787593841553\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9488189935684204, and Avg Validation Loss (mae) is 0.940294373035431\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9262053310871124\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7074283003807068, and Avg Validation Loss (mae) is 0.6572691857814789\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6493793308734894\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7827467679977417, and Avg Validation Loss (mae) is 0.7239248096942902\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7077098965644837\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0228658258914947, and Avg Validation Loss (mae) is 0.9593535602092743\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9206148207187652\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5197433084249496, and Avg Validation Loss (mae) is 0.8216166913509368\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8107947200536728\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7599138975143432, and Avg Validation Loss (mae) is 0.6754622161388397\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6850207865238189\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1854864835739136, and Avg Validation Loss (mae) is 1.0213437139987946\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0387152969837188\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7914988994598389, and Avg Validation Loss (mae) is 0.8807191550731659\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8835650324821472\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7958835542201996, and Avg Validation Loss (mae) is 0.8848042726516724\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9041616141796112\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0366641581058502, and Avg Validation Loss (mae) is 1.1084493160247804\n",
            "After 10 runs; Avg Test Loss (mae) is 1.08059783577919\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7426836311817169, and Avg Validation Loss (mae) is 0.7869915038347244\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8019292294979096\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.897613775730133, and Avg Validation Loss (mae) is 1.0220877408981324\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0283564507961274\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.236196720600128, and Avg Validation Loss (mae) is 1.3365966439247132\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3179348945617675\n",
            "kx1 = X_FT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8508919060230256, and Avg Validation Loss (mae) is 0.878921702504158\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8716294556856156\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5335955500602723, and Avg Validation Loss (mae) is 1.759936797618866\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7186411261558532\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5402106463909149, and Avg Validation Loss (mae) is 0.5993800342082978\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5920722365379334\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6287408649921418, and Avg Validation Loss (mae) is 0.6270804882049561\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6154503554105759\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5556188941001892, and Avg Validation Loss (mae) is 1.6235225558280946\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6201168656349183\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8530597627162934, and Avg Validation Loss (mae) is 0.907095217704773\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9067461341619492\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6794235646724701, and Avg Validation Loss (mae) is 0.6423286378383637\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6490008056163787\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.500529158115387, and Avg Validation Loss (mae) is 1.4534944295883179\n",
            "After 10 runs; Avg Test Loss (mae) is 1.399537777900696\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0989846289157867, and Avg Validation Loss (mae) is 1.0467086732387543\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0289480268955231\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7459596455097198, and Avg Validation Loss (mae) is 0.7690590888261795\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7758831471204758\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5731669187545776, and Avg Validation Loss (mae) is 1.7164541721343993\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7355593681335448\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7996826171875, and Avg Validation Loss (mae) is 0.7375070750713348\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7499327600002289\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6182416200637817, and Avg Validation Loss (mae) is 0.6144275784492492\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6175020217895508\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4826111793518066, and Avg Validation Loss (mae) is 1.5577282428741455\n",
            "After 10 runs; Avg Test Loss (mae) is 1.58767329454422\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.756538587808609, and Avg Validation Loss (mae) is 1.0155574440956117\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0351037502288818\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5945754617452621, and Avg Validation Loss (mae) is 0.7251329630613327\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7218288630247116\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5297208428382874, and Avg Validation Loss (mae) is 1.502449381351471\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5494090914726257\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7348233759403229, and Avg Validation Loss (mae) is 0.6961371928453446\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7006945908069611\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6158488094806671, and Avg Validation Loss (mae) is 0.6209985733032226\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6342916995286941\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.619351291656494, and Avg Validation Loss (mae) is 1.7424392819404602\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7157914996147157\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5035817563533783, and Avg Validation Loss (mae) is 0.6043847918510437\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6036746889352799\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5785094708204269, and Avg Validation Loss (mae) is 0.5940567523241043\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6077757924795151\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.591105008125305, and Avg Validation Loss (mae) is 1.8386289715766906\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8535438537597657\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6011777967214584, and Avg Validation Loss (mae) is 0.6985482960939408\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6805864214897156\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6809480875730515, and Avg Validation Loss (mae) is 0.6911576867103577\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7083380967378616\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5699546098709107, and Avg Validation Loss (mae) is 1.7966354727745055\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7757896661758423\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.559338527917862, and Avg Validation Loss (mae) is 0.6555840581655502\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6607230365276336\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7079955577850342, and Avg Validation Loss (mae) is 0.6392769277095794\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6182780176401138\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4109760165214538, and Avg Validation Loss (mae) is 1.4246920943260193\n",
            "After 10 runs; Avg Test Loss (mae) is 1.4335633397102356\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7125426918268204, and Avg Validation Loss (mae) is 0.5828006446361542\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5705945581197739\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7649656713008881, and Avg Validation Loss (mae) is 0.8143133819103241\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8212356626987457\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.463207471370697, and Avg Validation Loss (mae) is 1.650541365146637\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6422390937805176\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0196340262889863, and Avg Validation Loss (mae) is 1.0279547333717347\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0263186037540435\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5899729251861572, and Avg Validation Loss (mae) is 0.6237943798303605\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6263241797685624\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.492358958721161, and Avg Validation Loss (mae) is 1.537666881084442\n",
            "After 10 runs; Avg Test Loss (mae) is 1.521420168876648\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0222213447093964, and Avg Validation Loss (mae) is 0.8604043185710907\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8444447696208954\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5283860862255096, and Avg Validation Loss (mae) is 0.6099665820598602\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6083805859088898\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4946648240089417, and Avg Validation Loss (mae) is 1.506151306629181\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5142107844352721\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0050821602344513, and Avg Validation Loss (mae) is 1.1856746792793273\n",
            "After 10 runs; Avg Test Loss (mae) is 1.195470243692398\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7749391198158264, and Avg Validation Loss (mae) is 0.87497219145298\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8761414736509323\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.4404402375221252, and Avg Validation Loss (mae) is 1.44171804189682\n",
            "After 10 runs; Avg Test Loss (mae) is 1.3955662369728088\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0423363864421844, and Avg Validation Loss (mae) is 1.0353708803653716\n",
            "After 10 runs; Avg Test Loss (mae) is 1.010916829109192\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.748887550830841, and Avg Validation Loss (mae) is 0.738764888048172\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7412774384021759\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5782134294509889, and Avg Validation Loss (mae) is 1.650182342529297\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6154344439506532\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.17463059425354, and Avg Validation Loss (mae) is 1.1448150992393493\n",
            "After 10 runs; Avg Test Loss (mae) is 1.135063773393631\n",
            "kx1 = X_FM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6037672311067581, and Avg Validation Loss (mae) is 0.6775905966758728\n",
            "After 10 runs; Avg Test Loss (mae) is 0.673886439204216\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.8513050198554992, and Avg Validation Loss (mae) is 1.8867359161376953\n",
            "After 10 runs; Avg Test Loss (mae) is 1.873452115058899\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9214436292648316, and Avg Validation Loss (mae) is 0.9255023717880249\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9040434122085571\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7293478727340699, and Avg Validation Loss (mae) is 0.8340410351753235\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8287821233272552\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7151884078979491, and Avg Validation Loss (mae) is 1.8381817102432252\n",
            "After 10 runs; Avg Test Loss (mae) is 1.8716179132461548\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.819833368062973, and Avg Validation Loss (mae) is 0.7569329380989075\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7647003471851349\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.49709696173667905, and Avg Validation Loss (mae) is 0.5132486939430236\n",
            "After 10 runs; Avg Test Loss (mae) is 0.506665951013565\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.7943065643310547, and Avg Validation Loss (mae) is 1.6807139039039611\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6932968735694884\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8070100843906403, and Avg Validation Loss (mae) is 0.9128987550735473\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9362431168556213\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RM, kx5 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8443661689758301, and Avg Validation Loss (mae) is 0.8755082815885544\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8739004045724869\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6679284572601318, and Avg Validation Loss (mae) is 1.6506307005882264\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6803820610046387\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8063002765178681, and Avg Validation Loss (mae) is 0.7678840160369873\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7744557738304139\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.741264808177948, and Avg Validation Loss (mae) is 0.8384483218193054\n",
            "After 10 runs; Avg Test Loss (mae) is 0.821572208404541\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6548907279968261, and Avg Validation Loss (mae) is 1.6128825545310974\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5987466335296632\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8814609050750732, and Avg Validation Loss (mae) is 1.071738713979721\n",
            "After 10 runs; Avg Test Loss (mae) is 1.059808224439621\n",
            "kx1 = X_MT, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5669212758541107, and Avg Validation Loss (mae) is 0.5826317697763443\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5817748755216599\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6506418943405152, and Avg Validation Loss (mae) is 1.7321259140968324\n",
            "After 10 runs; Avg Test Loss (mae) is 1.768338131904602\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8099815607070923, and Avg Validation Loss (mae) is 0.8023740768432617\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8077674567699432\n",
            "kx1 = X_MM, kx2 = X_MB, kx3 = X_RT, kx4 = X_RM, kx5 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.005869245529175, and Avg Validation Loss (mae) is 0.9793323338031769\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9966343402862549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ULPDHbotnKVr",
        "outputId": "76fdc5ef-a0fc-4e43-87c1-e33d8fe33ad8"
      },
      "source": [
        "CombResultsMF5 = pd.DataFrame.from_dict(my_dictMF5)\n",
        "print(CombResultsMF5.shape)\n",
        "CombResultsSortedMF5 = CombResultsMF5.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMF5.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA_X</th>\n",
              "      <th>DATA_y</th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>X_MTX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.506666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>X_FTX_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.519450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.520555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>X_FTX_FMX_MTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.523774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>X_FTX_FMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.529337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.534953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.544725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.566309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.568505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>X_FMX_MTX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.570595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>X_FTX_MTX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.572247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>X_FTX_MTX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.574910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>X_MTX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.581775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RT</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.586020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.592072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>X_FMX_MTX_MBX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.603675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>X_FMX_MTX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.607776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>X_FMX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.608381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.615450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>X_FMX_MTX_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.617502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.617824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>X_FMX_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.618278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>X_FTX_MTX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.618405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>X_FMX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.626324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>X_FTX_FMX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.627113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>X_FMX_MTX_MMX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.634292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>X_FTX_MTX_MMX_RTX_RM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.647734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.649001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>X_FTX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.649379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>X_FTX_FMX_MBX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.652938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.653586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.657282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>X_FMX_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.660723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>X_FTX_FMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.660956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.665242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>X_FTX_FMX_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.667138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.669014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RT</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.669598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>X_FMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.673886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.676747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>X_FMX_MTX_MBX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.680586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.682495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.683252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>X_FTX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.685021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.686820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.687668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.687668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>X_FTX_FMX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.699696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>X_FMX_MTX_MMX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.700695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>X_FTX_FMX_MMX_RTX_RM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.702626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   DATA_X DATA_y  Test Loss\n",
              "155  X_MTX_MMX_MBX_RTX_RB   y_RM   0.506666\n",
              "85   X_FTX_MTX_MBX_RMX_RB   y_MM   0.519450\n",
              "26   X_FTX_FMX_MTX_RTX_RB   y_RM   0.520555\n",
              "27   X_FTX_FMX_MTX_RMX_RB   y_MM   0.523774\n",
              "53   X_FTX_FMX_MBX_RTX_RB   y_RM   0.529337\n",
              "14   X_FTX_FMX_MTX_MBX_RT   y_RB   0.534953\n",
              "4    X_FTX_FMX_MTX_MMX_RT   y_RM   0.544725\n",
              "15   X_FTX_FMX_MTX_MBX_RM   y_MM   0.566309\n",
              "21   X_FTX_FMX_MTX_RTX_RM   y_MM   0.568505\n",
              "133  X_FMX_MTX_RTX_RMX_RB   y_MM   0.570595\n",
              "83   X_FTX_MTX_MBX_RTX_RB   y_RM   0.572247\n",
              "74   X_FTX_MTX_MMX_RTX_RB   y_RM   0.574910\n",
              "164  X_MTX_MBX_RTX_RMX_RB   y_MM   0.581775\n",
              "12   X_FTX_FMX_MTX_MBX_RT   y_MM   0.586020\n",
              "106  X_FMX_MTX_MMX_MBX_RT   y_RM   0.592072\n",
              "124  X_FMX_MTX_MBX_RTX_RM   y_MM   0.603675\n",
              "125  X_FMX_MTX_MBX_RTX_RM   y_RB   0.607776\n",
              "140  X_FMX_MMX_MBX_RTX_RB   y_RM   0.608381\n",
              "107  X_FMX_MTX_MMX_MBX_RT   y_RB   0.615450\n",
              "116  X_FMX_MTX_MMX_RTX_RM   y_RB   0.617502\n",
              "66   X_FTX_MTX_MMX_MBX_RB   y_FM   0.617824\n",
              "131  X_FMX_MTX_MBX_RMX_RB   y_RT   0.618278\n",
              "88   X_FTX_MTX_RTX_RMX_RB   y_MM   0.618405\n",
              "137  X_FMX_MMX_MBX_RTX_RM   y_RB   0.626324\n",
              "44   X_FTX_FMX_MMX_RTX_RB   y_RM   0.627113\n",
              "122  X_FMX_MTX_MMX_RMX_RB   y_RT   0.634292\n",
              "70   X_FTX_MTX_MMX_RTX_RM   y_MB   0.647734\n",
              "110  X_FMX_MTX_MMX_MBX_RM   y_RB   0.649001\n",
              "92   X_FTX_MMX_MBX_RTX_RM   y_RB   0.649379\n",
              "49   X_FTX_FMX_MBX_RTX_RM   y_MM   0.652938\n",
              "62   X_FTX_MTX_MMX_MBX_RT   y_RB   0.653586\n",
              "32   X_FTX_FMX_MMX_MBX_RT   y_RB   0.657282\n",
              "130  X_FMX_MTX_MBX_RMX_RB   y_MM   0.660723\n",
              "50   X_FTX_FMX_MBX_RTX_RM   y_RB   0.660956\n",
              "61   X_FTX_MTX_MMX_MBX_RT   y_RM   0.665242\n",
              "41   X_FTX_FMX_MMX_RTX_RM   y_RB   0.667138\n",
              "16   X_FTX_FMX_MTX_MBX_RM   y_RT   0.669014\n",
              "3    X_FTX_FMX_MTX_MMX_RT   y_MB   0.669598\n",
              "149  X_FMX_MBX_RTX_RMX_RB   y_MM   0.673886\n",
              "23   X_FTX_FMX_MTX_RTX_RM   y_RB   0.676747\n",
              "127  X_FMX_MTX_MBX_RTX_RB   y_MM   0.680586\n",
              "31   X_FTX_FMX_MMX_MBX_RT   y_RM   0.682495\n",
              "7    X_FTX_FMX_MTX_MMX_RM   y_RT   0.683252\n",
              "96   X_FTX_MMX_MBX_RMX_RB   y_FM   0.685021\n",
              "13   X_FTX_FMX_MTX_MBX_RT   y_RM   0.686820\n",
              "17   X_FTX_FMX_MTX_MBX_RM   y_RB   0.687668\n",
              "24   X_FTX_FMX_MTX_RTX_RB   y_MM   0.687668\n",
              "55   X_FTX_FMX_MBX_RMX_RB   y_MM   0.699696\n",
              "121  X_FMX_MTX_MMX_RMX_RB   y_MB   0.700695\n",
              "40   X_FTX_FMX_MMX_RTX_RM   y_MB   0.702626"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LxD7gkY-nKVr",
        "outputId": "3816011d-5057-49de-d33e-42e2dde37061"
      },
      "source": [
        "CombResultsSortedMFgrouped5 = CombResultsSortedMF5.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedMFgroupedsortedMF5 = CombResultsSortedMFgrouped5.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMFgroupedsortedMF5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RT</th>\n",
              "      <td>0.602598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RM</th>\n",
              "      <td>0.640997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RT</th>\n",
              "      <td>0.651545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RTX_RB</th>\n",
              "      <td>0.667335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RTX_RM</th>\n",
              "      <td>0.690235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RT</th>\n",
              "      <td>0.697203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RTX_RB</th>\n",
              "      <td>0.710038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RMX_RB</th>\n",
              "      <td>0.725280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RTX_RM</th>\n",
              "      <td>0.732413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RM</th>\n",
              "      <td>0.746821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RMX_RB</th>\n",
              "      <td>0.754741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RM</th>\n",
              "      <td>0.756740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_RTX_RMX_RB</th>\n",
              "      <td>0.774564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RTX_RB</th>\n",
              "      <td>0.778735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RTX_RM</th>\n",
              "      <td>0.779271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RTX_RB</th>\n",
              "      <td>0.783641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RT</th>\n",
              "      <td>0.784695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>0.789321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.813040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RTX_RB</th>\n",
              "      <td>0.833483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RTX_RM</th>\n",
              "      <td>0.857281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RTX_RM</th>\n",
              "      <td>0.861572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RMX_RB</th>\n",
              "      <td>0.865044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>0.869100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RB</th>\n",
              "      <td>0.893244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>0.928896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RMX_RB</th>\n",
              "      <td>0.937899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_RTX_RMX_RB</th>\n",
              "      <td>0.941798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RM</th>\n",
              "      <td>0.944525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RB</th>\n",
              "      <td>0.947547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RMX_RB</th>\n",
              "      <td>0.961465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RT</th>\n",
              "      <td>0.975388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RTX_RM</th>\n",
              "      <td>0.975747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RB</th>\n",
              "      <td>0.984257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.991415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MB</th>\n",
              "      <td>0.993668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RMX_RB</th>\n",
              "      <td>1.009473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RMX_RB</th>\n",
              "      <td>1.018264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RTX_RM</th>\n",
              "      <td>1.034331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>1.047661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_RTX_RMX_RB</th>\n",
              "      <td>1.049254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RM</th>\n",
              "      <td>1.058621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RB</th>\n",
              "      <td>1.064844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_RTX_RMX_RB</th>\n",
              "      <td>1.065399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RB</th>\n",
              "      <td>1.068123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.072640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.080110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RTX_RB</th>\n",
              "      <td>1.080823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>1.092137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RTX_RM</th>\n",
              "      <td>1.098294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RTX_RB</th>\n",
              "      <td>1.114869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.141462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>1.167813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.190913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RMX_RB</th>\n",
              "      <td>1.195274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>1.202093</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Test Loss\n",
              "DATA_X                         \n",
              "X_FTX_FMX_MTX_MBX_RT   0.602598\n",
              "X_FTX_FMX_MTX_MBX_RM   0.640997\n",
              "X_FTX_FMX_MTX_MMX_RT   0.651545\n",
              "X_FTX_FMX_MTX_RTX_RB   0.667335\n",
              "X_FTX_FMX_MTX_RTX_RM   0.690235\n",
              "X_FTX_MTX_MMX_MBX_RT   0.697203\n",
              "X_FTX_MTX_MMX_RTX_RB   0.710038\n",
              "X_FTX_FMX_MTX_RMX_RB   0.725280\n",
              "X_FTX_MTX_MMX_RTX_RM   0.732413\n",
              "X_FTX_FMX_MTX_MMX_RM   0.746821\n",
              "X_FTX_MTX_MBX_RMX_RB   0.754741\n",
              "X_FTX_MTX_MMX_MBX_RM   0.756740\n",
              "X_FTX_MTX_RTX_RMX_RB   0.774564\n",
              "X_FTX_FMX_MBX_RTX_RB   0.778735\n",
              "X_FTX_MTX_MBX_RTX_RM   0.779271\n",
              "X_FTX_MTX_MBX_RTX_RB   0.783641\n",
              "X_FTX_FMX_MMX_MBX_RT   0.784695\n",
              "X_FTX_MMX_MBX_RTX_RM   0.789321\n",
              "X_FTX_MMX_MBX_RTX_RB   0.813040\n",
              "X_FTX_FMX_MMX_RTX_RB   0.833483\n",
              "X_FTX_FMX_MMX_RTX_RM   0.857281\n",
              "X_FTX_FMX_MBX_RTX_RM   0.861572\n",
              "X_FTX_MTX_MMX_RMX_RB   0.865044\n",
              "X_FTX_MMX_MBX_RMX_RB   0.869100\n",
              "X_FTX_MTX_MMX_MBX_RB   0.893244\n",
              "X_FTX_MMX_RTX_RMX_RB   0.928896\n",
              "X_FTX_FMX_MMX_RMX_RB   0.937899\n",
              "X_FMX_MTX_RTX_RMX_RB   0.941798\n",
              "X_FTX_FMX_MMX_MBX_RM   0.944525\n",
              "X_FTX_FMX_MTX_MMX_RB   0.947547\n",
              "X_FMX_MTX_MMX_RMX_RB   0.961465\n",
              "X_FMX_MTX_MMX_MBX_RT   0.975388\n",
              "X_FMX_MTX_MBX_RTX_RM   0.975747\n",
              "X_FTX_FMX_MMX_MBX_RB   0.984257\n",
              "X_FMX_MMX_MBX_RTX_RB   0.991415\n",
              "X_FTX_FMX_MTX_MMX_MB   0.993668\n",
              "X_FTX_FMX_MBX_RMX_RB   1.009473\n",
              "X_FMX_MTX_MBX_RMX_RB   1.018264\n",
              "X_FMX_MTX_MMX_RTX_RM   1.034331\n",
              "X_MTX_MMX_MBX_RTX_RB   1.047661\n",
              "X_FMX_MMX_RTX_RMX_RB   1.049254\n",
              "X_FMX_MTX_MMX_MBX_RM   1.058621\n",
              "X_FTX_FMX_MTX_MBX_RB   1.064844\n",
              "X_FTX_FMX_RTX_RMX_RB   1.065399\n",
              "X_FMX_MTX_MMX_MBX_RB   1.068123\n",
              "X_FTX_MBX_RTX_RMX_RB   1.072640\n",
              "X_MTX_MBX_RTX_RMX_RB   1.080110\n",
              "X_FMX_MTX_MBX_RTX_RB   1.080823\n",
              "X_MTX_MMX_RTX_RMX_RB   1.092137\n",
              "X_FMX_MMX_MBX_RTX_RM   1.098294\n",
              "X_FMX_MTX_MMX_RTX_RB   1.114869\n",
              "X_FMX_MBX_RTX_RMX_RB   1.141462\n",
              "X_MTX_MMX_MBX_RMX_RB   1.167813\n",
              "X_MMX_MBX_RTX_RMX_RB   1.190913\n",
              "X_FMX_MMX_MBX_RMX_RB   1.195274\n",
              "X_MTX_MMX_MBX_RTX_RM   1.202093"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "oQooIBxSnKVr",
        "outputId": "f908990f-a70f-41d9-ee9e-205e3a9a84d0"
      },
      "source": [
        "CombResultsSortedMFgroupedsortedMF5.to_csv('CombResultsSortedMFgroupedsortedMF5.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedMFgroupedsortedMF5.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_451b5671-912a-4602-a2bd-cadd82be08e0\", \"CombResultsSortedMFgroupedsortedMF5.csv\", 2246)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "wo3dCd9L9zyI",
        "outputId": "479080a9-6bff-4659-b35b-82a266fa3a54"
      },
      "source": [
        "CombResultsSortedMF5.to_csv('CombResultsSortedMF5.csv')\n",
        "files.download(\"CombResultsSortedMF5.csv\")\n",
        "\n",
        "fig = px.box(CombResultsSortedMF5, x=\"DATA_X\", y=\"Test Loss\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_732c9904-c954-42b8-975b-45366b7bc981\", \"CombResultsSortedMF5.csv\", 8118)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9691068b-d695-4f4b-95ed-3c881b9f53e5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9691068b-d695-4f4b-95ed-3c881b9f53e5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9691068b-d695-4f4b-95ed-3c881b9f53e5',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_MTX_MMX_MBX_RTX_RB\", \"X_FTX_MTX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_RTX_RB\", \"X_FTX_FMX_MTX_RMX_RB\", \"X_FTX_FMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MBX_RT\", \"X_FTX_FMX_MTX_MMX_RT\", \"X_FTX_FMX_MTX_MBX_RM\", \"X_FTX_FMX_MTX_RTX_RM\", \"X_FMX_MTX_RTX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_RTX_RB\", \"X_MTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RT\", \"X_FMX_MTX_MMX_MBX_RT\", \"X_FMX_MTX_MBX_RTX_RM\", \"X_FMX_MTX_MBX_RTX_RM\", \"X_FMX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_MBX_RT\", \"X_FMX_MTX_MMX_RTX_RM\", \"X_FTX_MTX_MMX_MBX_RB\", \"X_FMX_MTX_MBX_RMX_RB\", \"X_FTX_MTX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MMX_RTX_RB\", \"X_FMX_MTX_MMX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RM\", \"X_FMX_MTX_MMX_MBX_RM\", \"X_FTX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MMX_MBX_RT\", \"X_FMX_MTX_MBX_RMX_RB\", \"X_FTX_FMX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RM\", \"X_FTX_FMX_MTX_MMX_RT\", \"X_FMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_RTX_RM\", \"X_FMX_MTX_MBX_RTX_RB\", \"X_FTX_FMX_MMX_MBX_RT\", \"X_FTX_FMX_MTX_MMX_RM\", \"X_FTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RT\", \"X_FTX_FMX_MTX_MBX_RM\", \"X_FTX_FMX_MTX_RTX_RB\", \"X_FTX_FMX_MBX_RMX_RB\", \"X_FMX_MTX_MMX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RM\", \"X_FTX_MTX_MMX_MBX_RM\", \"X_FTX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RTX_RB\", \"X_FTX_FMX_MTX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RM\", \"X_FTX_FMX_MTX_MMX_RT\", \"X_FMX_MMX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RB\", \"X_FMX_MTX_MMX_RTX_RM\", \"X_FTX_FMX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_RB\", \"X_FTX_FMX_MBX_RTX_RB\", \"X_MTX_MMX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_MBX_RT\", \"X_FTX_MTX_MMX_MBX_RM\", \"X_MTX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RB\", \"X_FTX_FMX_MMX_MBX_RB\", \"X_FTX_MTX_MMX_MBX_RM\", \"X_FTX_MMX_MBX_RTX_RM\", \"X_FTX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_RTX_RB\", \"X_FTX_MMX_RTX_RMX_RB\", \"X_MMX_MBX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RB\", \"X_FTX_MMX_MBX_RTX_RB\", \"X_FTX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_MB\", \"X_FTX_FMX_MTX_MMX_RM\", \"X_FMX_MTX_RTX_RMX_RB\", \"X_MTX_MMX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_RTX_RM\", \"X_MTX_MMX_MBX_RTX_RM\", \"X_FMX_MMX_MBX_RTX_RB\", \"X_FTX_MTX_RTX_RMX_RB\", \"X_FTX_MTX_MBX_RMX_RB\", \"X_FTX_MTX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_MMX_RB\", \"X_FTX_MBX_RTX_RMX_RB\", \"X_MTX_MMX_MBX_RMX_RB\", \"X_FMX_MMX_MBX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RB\", \"X_FTX_MTX_MMX_RMX_RB\", \"X_FTX_MMX_MBX_RMX_RB\", \"X_FTX_MTX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RB\", \"X_MTX_MMX_MBX_RTX_RM\", \"X_FTX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RM\", \"X_FTX_FMX_RTX_RMX_RB\", \"X_FTX_FMX_RTX_RMX_RB\", \"X_FTX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_RMX_RB\", \"X_FTX_MMX_MBX_RTX_RM\", \"X_MTX_MMX_MBX_RMX_RB\", \"X_FTX_MTX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_MB\", \"X_FTX_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_MBX_RB\", \"X_FTX_FMX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RB\", \"X_MMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RB\", \"X_FTX_MTX_MBX_RTX_RB\", \"X_FMX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RM\", \"X_FTX_FMX_MMX_MBX_RT\", \"X_FTX_FMX_MMX_RMX_RB\", \"X_FMX_MMX_MBX_RTX_RM\", \"X_FTX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RB\", \"X_FMX_MTX_MMX_RTX_RB\", \"X_FTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MBX_RTX_RB\", \"X_FTX_FMX_MMX_RMX_RB\", \"X_MTX_MBX_RTX_RMX_RB\", \"X_FTX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RB\", \"X_FTX_FMX_MTX_MBX_RB\", \"X_FMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RB\", \"X_FTX_MTX_MMX_MBX_RB\", \"X_FMX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_MB\", \"X_FTX_FMX_MTX_MMX_RB\", \"X_FTX_FMX_MBX_RTX_RM\", \"X_FTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MBX_RMX_RB\", \"X_FTX_FMX_RTX_RMX_RB\", \"X_FMX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RB\", \"X_FMX_MTX_RTX_RMX_RB\", \"X_FMX_MMX_MBX_RMX_RB\", \"X_FMX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_RMX_RB\", \"X_FMX_MTX_MMX_RTX_RB\", \"X_MTX_MBX_RTX_RMX_RB\", \"X_FMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RM\", \"X_FMX_MMX_MBX_RTX_RM\", \"X_MTX_MMX_RTX_RMX_RB\", \"X_MTX_MMX_MBX_RMX_RB\", \"X_FMX_MTX_MBX_RTX_RM\", \"X_FMX_MTX_MMX_MBX_RT\", \"X_FMX_MTX_MMX_RTX_RM\", \"X_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MBX_RMX_RB\", \"X_FMX_MTX_MBX_RTX_RB\", \"X_MTX_MMX_MBX_RTX_RB\", \"X_MTX_MMX_MBX_RTX_RM\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.506665951013565, 0.5194498151540756, 0.5205554097890854, 0.523774042725563, 0.5293367594480515, 0.534953448176384, 0.5447245329618454, 0.5663093835115433, 0.568505248427391, 0.5705945581197739, 0.5722467541694641, 0.5749097615480423, 0.5817748755216599, 0.5860203176736831, 0.5920722365379334, 0.6036746889352799, 0.6077757924795151, 0.6083805859088898, 0.6154503554105759, 0.6175020217895508, 0.6178241193294525, 0.6182780176401138, 0.6184046119451523, 0.6263241797685624, 0.6271134555339813, 0.6342916995286941, 0.6477336645126343, 0.6490008056163787, 0.6493793308734894, 0.6529379516839982, 0.6535855591297149, 0.6572823137044906, 0.6607230365276336, 0.6609559535980225, 0.6652423948049545, 0.6671377897262574, 0.6690142393112183, 0.6695978134870529, 0.673886439204216, 0.676747339963913, 0.6805864214897156, 0.6824948072433472, 0.683251827955246, 0.6850207865238189, 0.6868196874856949, 0.6876676887273788, 0.6876679807901382, 0.6996959805488586, 0.7006945908069611, 0.702626422047615, 0.706611156463623, 0.7077098965644837, 0.7083380967378616, 0.7218288630247116, 0.7264602363109589, 0.7268843621015548, 0.7279122561216355, 0.7360159456729889, 0.7403128743171692, 0.7412774384021759, 0.7466978669166565, 0.7499327600002289, 0.7521167784929276, 0.75755635201931, 0.7617838025093079, 0.7647003471851349, 0.7686871767044068, 0.7721850126981735, 0.7727800667285919, 0.7731788277626037, 0.7744557738304139, 0.7758831471204758, 0.7809270441532135, 0.7904301673173905, 0.7923787593841553, 0.7932888209819794, 0.7937827169895172, 0.8019292294979096, 0.8077674567699432, 0.8085051834583282, 0.8107947200536728, 0.8176390022039414, 0.8201122939586639, 0.821193951368332, 0.8212356626987457, 0.821572208404541, 0.8215929210186005, 0.8254526078701019, 0.8287821233272552, 0.8444447696208954, 0.8483352661132812, 0.8562291860580444, 0.8569511890411377, 0.8642674863338471, 0.8707434564828873, 0.8716294556856156, 0.8739004045724869, 0.8761414736509323, 0.8814655900001526, 0.8818337202072144, 0.8835650324821472, 0.8885425060987473, 0.8984594762325286, 0.9040434122085571, 0.9041616141796112, 0.9067461341619492, 0.9088384449481964, 0.9113840162754059, 0.9206148207187652, 0.925607168674469, 0.9262053310871124, 0.9362431168556213, 0.9446096301078797, 0.9571249604225158, 0.9572633981704712, 0.9579869002103806, 0.9662828326225281, 0.9748745799064636, 0.9966343402862549, 1.0009860038757323, 1.0064898550510406, 1.010916829109192, 1.0120429813861846, 1.014307087659836, 1.0150964319705964, 1.0263186037540435, 1.0283564507961274, 1.0289480268955231, 1.0351037502288818, 1.0387152969837188, 1.0450845062732697, 1.0464823603630067, 1.059808224439621, 1.08059783577919, 1.1161822855472565, 1.120361989736557, 1.135063773393631, 1.1708573520183563, 1.1804408490657807, 1.195470243692398, 1.2020801961421967, 1.2037676274776459, 1.2143423020839692, 1.270822960138321, 1.3179348945617675, 1.3624408543109894, 1.3759739756584168, 1.3955662369728088, 1.399537777900696, 1.4335633397102356, 1.5142107844352721, 1.521420168876648, 1.5494090914726257, 1.58767329454422, 1.5987466335296632, 1.6154344439506532, 1.6201168656349183, 1.6422390937805176, 1.6803820610046387, 1.6932968735694884, 1.7157914996147157, 1.7186411261558532, 1.7355593681335448, 1.768338131904602, 1.7757896661758423, 1.8535438537597657, 1.8716179132461548, 1.873452115058899], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9691068b-d695-4f4b-95ed-3c881b9f53e5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71S6Zan390mE"
      },
      "source": [
        "# define baseline model 6\n",
        "# create model\n",
        "\n",
        "model6 = Sequential()\n",
        "model6.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(42, 1)))\n",
        "model6.add(MaxPooling1D(pool_size=2))\n",
        "model6.add(Flatten())\n",
        "model6.add(Dense(50, activation='relu'))\n",
        "model6.add(Dense(1))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01) #0.001 LR is the default\n",
        "model6.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "#model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdjGNlaj90mE"
      },
      "source": [
        "\n",
        "def datageneratorMF6(X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, Y_in):\n",
        "  Y_in = Y_in.reshape((Y_in.shape[0],1))\n",
        "  X_in = np.concatenate((X_in1, X_in2, X_in3, X_in4, X_in5, X_in6), axis=1)\n",
        "  X_in_Y_in = np.concatenate((X_in, Y_in), axis=1)\n",
        "  X_in_Y_in = shuffle(X_in_Y_in)\n",
        "\n",
        "  train_Input, val_Input, test_input = np.split(X_in_Y_in, [int(.6 * len(X_in_Y_in)), int(.8 * len(X_in_Y_in))])\n",
        "\n",
        "  X_train_Input = train_Input[:,:-1]\n",
        "  y_train= train_Input[:,-1]\n",
        "  X_val_Input = val_Input[:,:-1]\n",
        "  y_val= val_Input[:,-1]\n",
        "  X_test_Input = test_input[:,:-1]\n",
        "  y_test= test_input[:,-1]\n",
        "\n",
        "  #Xs_MB, ys_MB = shuffle(X_MB, y_MB)\n",
        "\n",
        "  X_train_Input = X_train_Input.reshape((X_train_Input.shape[0], X_train_Input.shape[1], 1))\n",
        "  X_val_Input = X_val_Input.reshape((X_val_Input.shape[0], X_val_Input.shape[1], 1))\n",
        "  X_test_Input = X_test_Input.reshape((X_test_Input.shape[0], X_test_Input.shape[1], 1))\n",
        "  X_train_Input.shape\n",
        "  return(X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bulC2l-T90mE"
      },
      "source": [
        "def evaldataMF6(X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, Y_in, traindata1, traindata2, traindata3, traindata4, traindata5, traindata6, testdata):\n",
        "  \n",
        "  X_train_Input, y_train, X_val_Input, y_val, X_test_Input, y_test = datageneratorMF6(X_in1, X_in2, X_in3, X_in4, X_in5, X_in6, Y_in)\n",
        "  \n",
        "  history = model6.fit(X_train_Input, y_train, epochs=10, verbose=0, validation_data=(X_val_Input , y_val))\n",
        "    \n",
        "  lossarray = history.history[\"loss\"]\n",
        "  val_lossarray = history.history[\"val_loss\"]\n",
        "  epochs = range(1,len(lossarray),1)\n",
        "  print(f'')\n",
        "\n",
        "  train_loss = lossarray[len(epochs)]\n",
        "  val_loss = val_lossarray[len(epochs)]  \n",
        "  test_loss = model6.evaluate(X_test_Input, y_test, verbose=0)\n",
        "\n",
        "  y_test_results = model6.predict(X_test_Input, verbose=0)\n",
        "  #print(X_test_Input)\n",
        "  y_test_results = np.ravel(y_test_results) ## Convert to raveled array\n",
        "  #print(y_test_results)\n",
        "  #print(y_test)\n",
        "\n",
        "  # PLOTS LOSS VS EPOCH\n",
        "  # fig1 = go.Figure()\n",
        "  # fig1.add_trace(go.Scatter(y=lossarray, name=\"Training loss\", line_shape='linear'))\n",
        "  # fig1.add_trace(go.Scatter(y=val_lossarray, name=\"Validation loss\", line_shape='linear'))\n",
        "  # fig1.update_layout( title=(\"Trained with  \" + str(traindata) + \" - Tested on  \" + str(testdata)) )\n",
        "  # #fig1.add_trace(go.Scatter(y=y_test, name=\"y_test\", line_shape='linear'))\n",
        "  # #fig1.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig1.show()\n",
        "\n",
        "  # print(f'Training Loss (mae) is {lossarray[len(epochs)]}, and Validation Loss (mae) is {val_lossarray[len(epochs)]}')\n",
        "  # print(f'Test Loss (mae) is {test_loss[0]}')\n",
        "  \n",
        "  # PLOTS Y ORIGINAL VS PREDICTED\n",
        "  # fig2 = go.Figure()\n",
        "  # fig2.add_trace(go.Scatter(y=y_test_results, name= (str(testdata) + \"_predicted\"), line_shape='linear'))\n",
        "  # fig2.add_trace(go.Scatter(y=y_test, name= (str(testdata) + \"_original\"), line_shape='linear'))\n",
        "  # fig2.update_layout( title=(\"Trained with  \" + str(traindata1)+ str(traindata2)  + \" - Tested on  \" + str(testdata)), width=800, height=400 )\n",
        "  # #fig.add_trace(go.Scatter(y=test_Output, name=\"y_test\", line_shape='linear'))\n",
        "  # fig2.show()\n",
        "\n",
        "  return [train_loss, val_loss, test_loss[0], y_test_results, lossarray, val_lossarray, epochs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywYKZ43D90mF",
        "outputId": "8124168a-0d0b-4e8e-8dc5-51b9fb6d0ab0"
      },
      "source": [
        "TrainDataSet = { 'X_FT': X_FT, 'X_FM': X_FM, 'X_MT':X_MT, 'X_MM':X_MM, 'X_MB':X_MB, 'X_RT':X_RT, 'X_RM':X_RM, 'X_RB':X_RB }\n",
        "TestDataSet = { 'y_FT': y_FT, 'y_FM': y_FM, 'y_MT':y_MT, 'y_MM':y_MM, 'y_MB':y_MB, 'y_RT':y_RT, 'y_RM':y_RM, 'y_RB':y_RB }\n",
        "#took out the X_FB and y_FB because of missing values\n",
        "\n",
        "model6.save_weights('model6.h5')\n",
        "\n",
        "my_dictMF6 = {\"DATA_X\":[],\"DATA_y\":[],\"Test Loss\":[]};\n",
        "\n",
        "for combo in combinations(TrainDataSet.items(), 6):\n",
        "  kX1, kX2, kX3, kX4, kX5, kX6 = combo[0][0], combo[1][0], combo[2][0], combo[3][0], combo[4][0], combo[5][0]\n",
        "  vX1, vX2, vX3, vX4, vX5, vX6 = combo[0][1], combo[1][1], combo[2][1], combo[3][1], combo[4][1], combo[5][1]\n",
        "  for ky, vy  in TestDataSet.items():\n",
        "    if ky[-2:] == kX1[-2:] or ky[-2:] == kX2[-2:] or ky[-2:] == kX3[-2:] or ky[-2:] == kX4[-2:] or ky[-2:] == kX5[-2:] or ky[-2:] == kX6[-2:]:\n",
        "      continue\n",
        "    print(f'kx1 = {kX1}, kx2 = {kX2}, kx3 = {kX3}, kx4 = {kX4}, kx5 = {kX5}, kx6 = {kX6}, ky = {ky},')\n",
        "    TestLossTotal = 0\n",
        "    TrainLossTotal = 0\n",
        "    ValLossTotal = 0\n",
        "    runs = 10\n",
        "\n",
        "    for i in range(runs):\n",
        "      resultsMF6 = evaldataMF6(vX1, vX2, vX3, vX4, vX5, vX6, vy, kX1, kX2, kX3, kX4, kX5, kX6, ky)\n",
        "      TestLossTotal = resultsMF6[2] + TestLossTotal\n",
        "      TrainLossTotal = resultsMF6[0] + TrainLossTotal\n",
        "      ValLossTotal = resultsMF6[1] + ValLossTotal\n",
        "      \n",
        "    TestLossAvg = TestLossTotal / runs\n",
        "    TrainLossAvg = TrainLossTotal / runs\n",
        "    ValLossAvg = ValLossTotal / runs\n",
        "      \n",
        "    print(\"*****************************************************************************************************************************\")\n",
        "    print(f'Evaluate model for Train Data: {kX1}_{kX2}_{kX3}_{kX4}_{kX5}_{kX6} and Test Data: {ky}')\n",
        "    print(f'After {runs} runs; Avg Training Loss (mae) is {TrainLossAvg}, and Avg Validation Loss (mae) is {ValLossAvg}')\n",
        "    print(f'After {runs} runs; Avg Test Loss (mae) is {TestLossAvg}')\n",
        "\n",
        "    my_dictMF6[\"DATA_X\"].append(kX1 + kX2 + kX3 + kX4 + kX5 + kX6)\n",
        "    my_dictMF6[\"DATA_y\"].append(ky)\n",
        "    my_dictMF6[\"Test Loss\"].append(TestLossAvg)\n",
        "\n",
        "    # for k, v in my_dict.items():\n",
        "    #   print(k, v)\n",
        "    model6.load_weights('model6.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RT, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RT and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.77279734313488, and Avg Validation Loss (mae) is 0.6670779943466186\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6719517081975936\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RT, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RT and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6411697924137115, and Avg Validation Loss (mae) is 0.561491721868515\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5607773184776306\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RM, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RM and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8216216802597046, and Avg Validation Loss (mae) is 0.843624472618103\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8798842549324035\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7133135974407196, and Avg Validation Loss (mae) is 0.6015431493520736\n",
            "After 10 runs; Avg Test Loss (mae) is 0.584307712316513\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0985134601593018, and Avg Validation Loss (mae) is 1.2214823961257935\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2426615357398987\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_MB, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_MB_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8407521665096283, and Avg Validation Loss (mae) is 0.6914427727460861\n",
            "After 10 runs; Avg Test Loss (mae) is 0.688959801197052\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RM, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT_X_RM and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7042907297611236, and Avg Validation Loss (mae) is 0.8229497313499451\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8333911240100861\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5365205526351928, and Avg Validation Loss (mae) is 0.6187895864248276\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6048680394887924\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.719013112783432, and Avg Validation Loss (mae) is 0.6615360707044602\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6645403385162354\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RT, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5056878238916397, and Avg Validation Loss (mae) is 0.6217894077301025\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6046994656324387\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7160328686237335, and Avg Validation Loss (mae) is 0.7146202981472015\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7189769119024276\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MM, kx5 = X_RM, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MM_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6833497524261475, and Avg Validation Loss (mae) is 0.6246087193489075\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6233897715806961\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT_X_RM and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5855263382196426, and Avg Validation Loss (mae) is 0.5008409351110459\n",
            "After 10 runs; Avg Test Loss (mae) is 0.4961110830307007\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7384768307209015, and Avg Validation Loss (mae) is 0.5418456763029098\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5217514365911484\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5863152951002121, and Avg Validation Loss (mae) is 0.7230722963809967\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7230144262313842\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5955146819353103, and Avg Validation Loss (mae) is 0.5344132006168365\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5366623282432557\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5065715491771698, and Avg Validation Loss (mae) is 0.5130334854125976\n",
            "After 10 runs; Avg Test Loss (mae) is 0.508341309428215\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7511391997337341, and Avg Validation Loss (mae) is 0.7553286910057068\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7271753907203674\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5256078034639359, and Avg Validation Loss (mae) is 0.5873008042573928\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5888545632362365\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MT, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MT_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7717780590057373, and Avg Validation Loss (mae) is 0.8188381612300872\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7969058990478516\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT_X_RM and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9525642156600952, and Avg Validation Loss (mae) is 1.0530806124210357\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0559014976024628\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6017502099275589, and Avg Validation Loss (mae) is 0.5938538312911987\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6106295675039292\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9123253166675568, and Avg Validation Loss (mae) is 1.0922289669513703\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0689674973487855\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5550614953041076, and Avg Validation Loss (mae) is 0.6634684264659881\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6783225536346436\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.0063424944877624, and Avg Validation Loss (mae) is 0.9267925679683685\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9551758229732513\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7274925291538239, and Avg Validation Loss (mae) is 0.8907397478818894\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8737967818975448\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.1366213023662568, and Avg Validation Loss (mae) is 1.158558452129364\n",
            "After 10 runs; Avg Test Loss (mae) is 1.1730066061019897\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7663483262062073, and Avg Validation Loss (mae) is 0.7578514456748963\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7679475486278534\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.128751963376999, and Avg Validation Loss (mae) is 1.290661132335663\n",
            "After 10 runs; Avg Test Loss (mae) is 1.2699739336967468\n",
            "kx1 = X_FT, kx2 = X_FM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_FM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5463902235031128, and Avg Validation Loss (mae) is 0.759946808218956\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7667256176471711\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7540203869342804, and Avg Validation Loss (mae) is 0.8108002245426178\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8033052742481231\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.614471685886383, and Avg Validation Loss (mae) is 0.6044879347085953\n",
            "After 10 runs; Avg Test Loss (mae) is 0.5951890796422958\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7498919427394867, and Avg Validation Loss (mae) is 0.7147040009498596\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7088811039924622\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5134344547986984, and Avg Validation Loss (mae) is 0.6376852810382843\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6518845081329345\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7112331449985504, and Avg Validation Loss (mae) is 0.7363190412521362\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7232519924640656\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.676491504907608, and Avg Validation Loss (mae) is 0.8015824556350708\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7888930261135101\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8101574718952179, and Avg Validation Loss (mae) is 0.7384260833263397\n",
            "After 10 runs; Avg Test Loss (mae) is 0.74239262342453\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.693831992149353, and Avg Validation Loss (mae) is 0.695836728811264\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6733625054359436\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.8586259007453918, and Avg Validation Loss (mae) is 0.7265279412269592\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7382008194923401\n",
            "kx1 = X_FT, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5846782177686691, and Avg Validation Loss (mae) is 0.6495327234268189\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6705285876989364\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7335352778434754, and Avg Validation Loss (mae) is 0.8803340673446656\n",
            "After 10 runs; Avg Test Loss (mae) is 0.8688406109809875\n",
            "kx1 = X_FT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FT_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.025480979681015, and Avg Validation Loss (mae) is 1.027120006084442\n",
            "After 10 runs; Avg Test Loss (mae) is 1.0253298044204713\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5642086267471313, and Avg Validation Loss (mae) is 1.6120816707611083\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6385905981063842\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RM, ky = y_RB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT_X_RM and Test Data: y_RB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.6515841007232666, and Avg Validation Loss (mae) is 0.7028522312641143\n",
            "After 10 runs; Avg Test Loss (mae) is 0.704246562719345\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5857311248779298, and Avg Validation Loss (mae) is 1.585286283493042\n",
            "After 10 runs; Avg Test Loss (mae) is 1.584087324142456\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RT, kx6 = X_RB, ky = y_RM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RT_X_RB and Test Data: y_RM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.5615503162145614, and Avg Validation Loss (mae) is 0.68747578561306\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6955746531486511\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5153998613357544, and Avg Validation Loss (mae) is 1.5016406655311585\n",
            "After 10 runs; Avg Test Loss (mae) is 1.5005632996559144\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_MB, kx5 = X_RM, kx6 = X_RB, ky = y_RT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_MB_X_RM_X_RB and Test Data: y_RT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.686877292394638, and Avg Validation Loss (mae) is 0.7817332744598389\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7881002724170685\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5980255842208861, and Avg Validation Loss (mae) is 1.8968071937561035\n",
            "After 10 runs; Avg Test Loss (mae) is 1.9136064052581787\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MM, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MB,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MM_X_RT_X_RM_X_RB and Test Data: y_MB\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7408090651035308, and Avg Validation Loss (mae) is 0.7377983212471009\n",
            "After 10 runs; Avg Test Loss (mae) is 0.7433415502309799\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5713826656341552, and Avg Validation Loss (mae) is 1.7183814644813538\n",
            "After 10 runs; Avg Test Loss (mae) is 1.739976668357849\n",
            "kx1 = X_FM, kx2 = X_MT, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MT_X_MB_X_RT_X_RM_X_RB and Test Data: y_MM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.48324345946311953, and Avg Validation Loss (mae) is 0.6096194833517075\n",
            "After 10 runs; Avg Test Loss (mae) is 0.6093438506126404\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.5225081324577332, and Avg Validation Loss (mae) is 1.6481477260589599\n",
            "After 10 runs; Avg Test Loss (mae) is 1.6506933569908142\n",
            "kx1 = X_FM, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_MT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_FM_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_MT\n",
            "After 10 runs; Avg Training Loss (mae) is 0.9731314599514007, and Avg Validation Loss (mae) is 0.9162154912948608\n",
            "After 10 runs; Avg Test Loss (mae) is 0.9180058240890503\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FT,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FT\n",
            "After 10 runs; Avg Training Loss (mae) is 1.6703500747680664, and Avg Validation Loss (mae) is 1.8288312792778014\n",
            "After 10 runs; Avg Test Loss (mae) is 1.7668201327323914\n",
            "kx1 = X_MT, kx2 = X_MM, kx3 = X_MB, kx4 = X_RT, kx5 = X_RM, kx6 = X_RB, ky = y_FM,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************************************************************************\n",
            "Evaluate model for Train Data: X_MT_X_MM_X_MB_X_RT_X_RM_X_RB and Test Data: y_FM\n",
            "After 10 runs; Avg Training Loss (mae) is 0.7848660409450531, and Avg Validation Loss (mae) is 0.8709991097450256\n",
            "After 10 runs; Avg Test Loss (mae) is 0.872760671377182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "316H8Rh_90mF",
        "outputId": "d4d27f82-e2d2-4bcd-9c29-87cbce8a6a95"
      },
      "source": [
        "CombResultsMF6 = pd.DataFrame.from_dict(my_dictMF6)\n",
        "print(CombResultsMF6.shape)\n",
        "CombResultsSortedMF6 = CombResultsMF6.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMF6.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(56, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA_X</th>\n",
              "      <th>DATA_y</th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RTX_RM</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.496111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.508341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.521751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.536662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.560777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.584308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.588855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.595189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.604699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.604868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>X_FMX_MTX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.609344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.610630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.623390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.651885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RTX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.664540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>X_FTX_MTX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.670529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RT</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.671952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>X_FTX_MTX_MMX_RTX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.673363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.678323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.688960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_RM</td>\n",
              "      <td>0.695575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_RB</td>\n",
              "      <td>0.704247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.708881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.718977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RTX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.723014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.723252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>X_FTX_FMX_MTX_MBX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.727175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>X_FTX_MTX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.738201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>X_FTX_MTX_MMX_RTX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.742393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>X_FMX_MTX_MMX_RTX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.743342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>X_FTX_FMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MM</td>\n",
              "      <td>0.766726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>X_FTX_FMX_MMX_RTX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.767948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.788100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.788893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>X_FTX_FMX_MTX_RTX_RMX_RB</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.796906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>X_FTX_MTX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.803305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_RTX_RM</td>\n",
              "      <td>y_MB</td>\n",
              "      <td>0.833391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>X_FTX_MMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.868841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>X_MTX_MMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_FM</td>\n",
              "      <td>0.872761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.873797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RM</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>0.879884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>X_FMX_MMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>0.918006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>0.955176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>X_FTX_MMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>1.025330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RTX_RM</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>1.055901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>X_FTX_FMX_MMX_MBX_RTX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>1.068967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>X_FTX_FMX_MMX_RTX_RMX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>1.173007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>X_FTX_FMX_MTX_MMX_MBX_RB</td>\n",
              "      <td>y_RT</td>\n",
              "      <td>1.242662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>X_FTX_FMX_MBX_RTX_RMX_RB</td>\n",
              "      <td>y_MT</td>\n",
              "      <td>1.269974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>X_FMX_MTX_MMX_MBX_RMX_RB</td>\n",
              "      <td>y_FT</td>\n",
              "      <td>1.500563</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      DATA_X DATA_y  Test Loss\n",
              "12  X_FTX_FMX_MTX_MBX_RTX_RM   y_MM   0.496111\n",
              "16  X_FTX_FMX_MTX_MBX_RMX_RB   y_MM   0.508341\n",
              "13  X_FTX_FMX_MTX_MBX_RTX_RM   y_RB   0.521751\n",
              "15  X_FTX_FMX_MTX_MBX_RTX_RB   y_RM   0.536662\n",
              "1   X_FTX_FMX_MTX_MMX_MBX_RT   y_RB   0.560777\n",
              "3   X_FTX_FMX_MTX_MMX_MBX_RM   y_RB   0.584308\n",
              "18  X_FTX_FMX_MTX_RTX_RMX_RB   y_MM   0.588855\n",
              "31  X_FTX_MTX_MMX_MBX_RTX_RM   y_RB   0.595189\n",
              "9   X_FTX_FMX_MTX_MMX_RTX_RB   y_RM   0.604699\n",
              "7   X_FTX_FMX_MTX_MMX_RTX_RM   y_RB   0.604868\n",
              "51  X_FMX_MTX_MBX_RTX_RMX_RB   y_MM   0.609344\n",
              "21  X_FTX_FMX_MMX_MBX_RTX_RM   y_RB   0.610630\n",
              "11  X_FTX_FMX_MTX_MMX_RMX_RB   y_RT   0.623390\n",
              "33  X_FTX_MTX_MMX_MBX_RTX_RB   y_RM   0.651885\n",
              "8   X_FTX_FMX_MTX_MMX_RTX_RB   y_MB   0.664540\n",
              "39  X_FTX_MTX_MBX_RTX_RMX_RB   y_MM   0.670529\n",
              "0   X_FTX_FMX_MTX_MMX_MBX_RT   y_RM   0.671952\n",
              "37  X_FTX_MTX_MMX_RTX_RMX_RB   y_MB   0.673363\n",
              "23  X_FTX_FMX_MMX_MBX_RTX_RB   y_RM   0.678323\n",
              "5   X_FTX_FMX_MTX_MMX_MBX_RB   y_RM   0.688960\n",
              "45  X_FMX_MTX_MMX_MBX_RTX_RB   y_RM   0.695575\n",
              "43  X_FMX_MTX_MMX_MBX_RTX_RM   y_RB   0.704247\n",
              "32  X_FTX_MTX_MMX_MBX_RTX_RB   y_FM   0.708881\n",
              "10  X_FTX_FMX_MTX_MMX_RMX_RB   y_MB   0.718977\n",
              "14  X_FTX_FMX_MTX_MBX_RTX_RB   y_MM   0.723014\n",
              "34  X_FTX_MTX_MMX_MBX_RMX_RB   y_FM   0.723252\n",
              "17  X_FTX_FMX_MTX_MBX_RMX_RB   y_RT   0.727175\n",
              "38  X_FTX_MTX_MBX_RTX_RMX_RB   y_FM   0.738201\n",
              "36  X_FTX_MTX_MMX_RTX_RMX_RB   y_FM   0.742393\n",
              "49  X_FMX_MTX_MMX_RTX_RMX_RB   y_MB   0.743342\n",
              "29  X_FTX_FMX_MBX_RTX_RMX_RB   y_MM   0.766726\n",
              "27  X_FTX_FMX_MMX_RTX_RMX_RB   y_MB   0.767948\n",
              "47  X_FMX_MTX_MMX_MBX_RMX_RB   y_RT   0.788100\n",
              "35  X_FTX_MTX_MMX_MBX_RMX_RB   y_RT   0.788893\n",
              "19  X_FTX_FMX_MTX_RTX_RMX_RB   y_MB   0.796906\n",
              "30  X_FTX_MTX_MMX_MBX_RTX_RM   y_FM   0.803305\n",
              "6   X_FTX_FMX_MTX_MMX_RTX_RM   y_MB   0.833391\n",
              "40  X_FTX_MMX_MBX_RTX_RMX_RB   y_FM   0.868841\n",
              "55  X_MTX_MMX_MBX_RTX_RMX_RB   y_FM   0.872761\n",
              "25  X_FTX_FMX_MMX_MBX_RMX_RB   y_RT   0.873797\n",
              "2   X_FTX_FMX_MTX_MMX_MBX_RM   y_RT   0.879884\n",
              "53  X_FMX_MMX_MBX_RTX_RMX_RB   y_MT   0.918006\n",
              "24  X_FTX_FMX_MMX_MBX_RMX_RB   y_MT   0.955176\n",
              "41  X_FTX_MMX_MBX_RTX_RMX_RB   y_MT   1.025330\n",
              "20  X_FTX_FMX_MMX_MBX_RTX_RM   y_MT   1.055901\n",
              "22  X_FTX_FMX_MMX_MBX_RTX_RB   y_MT   1.068967\n",
              "26  X_FTX_FMX_MMX_RTX_RMX_RB   y_MT   1.173007\n",
              "4   X_FTX_FMX_MTX_MMX_MBX_RB   y_RT   1.242662\n",
              "28  X_FTX_FMX_MBX_RTX_RMX_RB   y_MT   1.269974\n",
              "46  X_FMX_MTX_MMX_MBX_RMX_RB   y_FT   1.500563"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "Kogpu-7P90mF",
        "outputId": "ceb4ac99-317d-47de-df20-d9b05001f5ad"
      },
      "source": [
        "CombResultsSortedMFgrouped6 = CombResultsSortedMF6.groupby(['DATA_X']).mean()\n",
        "CombResultsSortedMFgroupedsortedMF6 = CombResultsSortedMFgrouped6.sort_values(by=['Test Loss'])\n",
        "CombResultsSortedMFgroupedsortedMF6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Test Loss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATA_X</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RTX_RM</th>\n",
              "      <td>0.508931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RT</th>\n",
              "      <td>0.616365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RMX_RB</th>\n",
              "      <td>0.617758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MBX_RTX_RB</th>\n",
              "      <td>0.629838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RTX_RB</th>\n",
              "      <td>0.634620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RMX_RB</th>\n",
              "      <td>0.671183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.680383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_RTX_RMX_RB</th>\n",
              "      <td>0.692880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>0.699247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>0.704365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>0.707878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_RTX_RM</th>\n",
              "      <td>0.719130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RM</th>\n",
              "      <td>0.732096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>0.756073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RTX_RM</th>\n",
              "      <td>0.833266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RTX_RB</th>\n",
              "      <td>0.873645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_MBX_RMX_RB</th>\n",
              "      <td>0.914486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>0.947085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MTX_MMX_MBX_RB</th>\n",
              "      <td>0.965811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MMX_RTX_RMX_RB</th>\n",
              "      <td>0.970477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FTX_FMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.018350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RTX_RB</th>\n",
              "      <td>1.139831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RMX_RB</th>\n",
              "      <td>1.144332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_MBX_RTX_RM</th>\n",
              "      <td>1.171419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.174660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.284350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_MTX_MMX_MBX_RTX_RMX_RB</th>\n",
              "      <td>1.319790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_FMX_MTX_MMX_RTX_RMX_RB</th>\n",
              "      <td>1.328474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          Test Loss\n",
              "DATA_X                             \n",
              "X_FTX_FMX_MTX_MBX_RTX_RM   0.508931\n",
              "X_FTX_FMX_MTX_MMX_MBX_RT   0.616365\n",
              "X_FTX_FMX_MTX_MBX_RMX_RB   0.617758\n",
              "X_FTX_FMX_MTX_MBX_RTX_RB   0.629838\n",
              "X_FTX_FMX_MTX_MMX_RTX_RB   0.634620\n",
              "X_FTX_FMX_MTX_MMX_RMX_RB   0.671183\n",
              "X_FTX_MTX_MMX_MBX_RTX_RB   0.680383\n",
              "X_FTX_FMX_MTX_RTX_RMX_RB   0.692880\n",
              "X_FTX_MTX_MMX_MBX_RTX_RM   0.699247\n",
              "X_FTX_MTX_MBX_RTX_RMX_RB   0.704365\n",
              "X_FTX_MTX_MMX_RTX_RMX_RB   0.707878\n",
              "X_FTX_FMX_MTX_MMX_RTX_RM   0.719130\n",
              "X_FTX_FMX_MTX_MMX_MBX_RM   0.732096\n",
              "X_FTX_MTX_MMX_MBX_RMX_RB   0.756073\n",
              "X_FTX_FMX_MMX_MBX_RTX_RM   0.833266\n",
              "X_FTX_FMX_MMX_MBX_RTX_RB   0.873645\n",
              "X_FTX_FMX_MMX_MBX_RMX_RB   0.914486\n",
              "X_FTX_MMX_MBX_RTX_RMX_RB   0.947085\n",
              "X_FTX_FMX_MTX_MMX_MBX_RB   0.965811\n",
              "X_FTX_FMX_MMX_RTX_RMX_RB   0.970477\n",
              "X_FTX_FMX_MBX_RTX_RMX_RB   1.018350\n",
              "X_FMX_MTX_MMX_MBX_RTX_RB   1.139831\n",
              "X_FMX_MTX_MMX_MBX_RMX_RB   1.144332\n",
              "X_FMX_MTX_MMX_MBX_RTX_RM   1.171419\n",
              "X_FMX_MTX_MBX_RTX_RMX_RB   1.174660\n",
              "X_FMX_MMX_MBX_RTX_RMX_RB   1.284350\n",
              "X_MTX_MMX_MBX_RTX_RMX_RB   1.319790\n",
              "X_FMX_MTX_MMX_RTX_RMX_RB   1.328474"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "f6oILgq190mG",
        "outputId": "72a4e50a-4917-4417-baad-8f27f3ac2aac"
      },
      "source": [
        "CombResultsSortedMFgroupedsortedMF6.to_csv('CombResultsSortedMFgroupedsortedMF6.csv')\n",
        "from google.colab import files\n",
        "files.download(\"CombResultsSortedMFgroupedsortedMF6.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e335eef7-d02a-450d-b1a4-9bd1759c82b7\", \"CombResultsSortedMFgroupedsortedMF6.csv\", 1246)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "z0kTGYAvxngl",
        "outputId": "23849fcc-57da-4c2d-f9b1-a277a71806b7"
      },
      "source": [
        "CombResultsSortedMF6.to_csv('CombResultsSortedMF6.csv')\n",
        "files.download(\"CombResultsSortedMF6.csv\")\n",
        "\n",
        "fig = px.box(CombResultsSortedMF6, x=\"DATA_X\", y=\"Test Loss\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_58f4a01a-2a56-4bab-811b-e57bd588394d\", \"CombResultsSortedMF6.csv\", 2918)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e9ce287f-4a12-4d07-accd-9fe8c67a4c8b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e9ce287f-4a12-4d07-accd-9fe8c67a4c8b\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e9ce287f-4a12-4d07-accd-9fe8c67a4c8b',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"DATA_X=%{x}<br>Test Loss=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"notched\": false, \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"box\", \"x\": [\"X_FTX_FMX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RT\", \"X_FTX_FMX_MTX_MMX_MBX_RM\", \"X_FTX_FMX_MTX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_RTX_RM\", \"X_FMX_MTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_RTX_RB\", \"X_FTX_MTX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RT\", \"X_FTX_MTX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RM\", \"X_FTX_MTX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MTX_MMX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RTX_RB\", \"X_FTX_MTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MBX_RMX_RB\", \"X_FTX_MTX_MBX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_RTX_RMX_RB\", \"X_FTX_MTX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MTX_MMX_RTX_RM\", \"X_FTX_MMX_MBX_RTX_RMX_RB\", \"X_MTX_MMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RM\", \"X_FMX_MMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RMX_RB\", \"X_FTX_MMX_MBX_RTX_RMX_RB\", \"X_FTX_FMX_MMX_MBX_RTX_RM\", \"X_FTX_FMX_MMX_MBX_RTX_RB\", \"X_FTX_FMX_MMX_RTX_RMX_RB\", \"X_FTX_FMX_MTX_MMX_MBX_RB\", \"X_FTX_FMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RMX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RB\", \"X_FMX_MTX_MMX_MBX_RTX_RM\", \"X_FMX_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MBX_RTX_RMX_RB\", \"X_MTX_MMX_MBX_RTX_RMX_RB\", \"X_FMX_MTX_MMX_RTX_RMX_RB\"], \"x0\": \" \", \"xaxis\": \"x\", \"y\": [0.4961110830307007, 0.508341309428215, 0.5217514365911484, 0.5366623282432557, 0.5607773184776306, 0.584307712316513, 0.5888545632362365, 0.5951890796422958, 0.6046994656324387, 0.6048680394887924, 0.6093438506126404, 0.6106295675039292, 0.6233897715806961, 0.6518845081329345, 0.6645403385162354, 0.6705285876989364, 0.6719517081975936, 0.6733625054359436, 0.6783225536346436, 0.688959801197052, 0.6955746531486511, 0.704246562719345, 0.7088811039924622, 0.7189769119024276, 0.7230144262313842, 0.7232519924640656, 0.7271753907203674, 0.7382008194923401, 0.74239262342453, 0.7433415502309799, 0.7667256176471711, 0.7679475486278534, 0.7881002724170685, 0.7888930261135101, 0.7969058990478516, 0.8033052742481231, 0.8333911240100861, 0.8688406109809875, 0.872760671377182, 0.8737967818975448, 0.8798842549324035, 0.9180058240890503, 0.9551758229732513, 1.0253298044204713, 1.0559014976024628, 1.0689674973487855, 1.1730066061019897, 1.2426615357398987, 1.2699739336967468, 1.5005632996559144, 1.584087324142456, 1.6385905981063842, 1.6506933569908142, 1.739976668357849, 1.7668201327323914, 1.9136064052581787], \"y0\": \" \", \"yaxis\": \"y\"}],\n",
              "                        {\"boxmode\": \"group\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"DATA_X\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Test Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e9ce287f-4a12-4d07-accd-9fe8c67a4c8b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}